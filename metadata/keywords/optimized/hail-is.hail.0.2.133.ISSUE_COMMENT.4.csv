quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:195737,Optimiz,Optimize,195737,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,nd.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199259,Optimiz,Optimize,199259,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"nd.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11924,Optimiz,Optimize,11924,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:219109,Optimiz,OptimizePass,219109,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:203136,Optimiz,OptimizePass,203136,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:211088,Optimiz,OptimizePass,211088,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ndle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.Compile,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198484,Optimiz,OptimizePass,198484,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"nerated/C2.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method1 instruction count: 112; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method2 instruction count: 82; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C2.method3 instruction count: 252; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.<init> instruction count: 3; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 28; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.apply instruction count: 12; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method1 instruction count: 100; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method2 instruction count: 106; 2019-01-22 13:11:48 root: INFO: is/hail/codegen/generated/C3.method3 instruction count: 252; 2019-01-22 13:11:48 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:50 root: INFO: Index reader cache queries: 1168; 2019-01-22 13:11:50 root: INFO: Index reader cache hit rate: 0.747431506849315; 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String],rsid:String,varid:String},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:27331,cache,cache,27331,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,4,['cache'],['cache']
Performance,nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached Pillow-10.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB); Collecting plotly==5.16.1; Using cached plotly-5.16.1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-no,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38048,cache,cached,38048,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ng Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:2588,Load,LoadVCF,2588,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,2,['Load'],['LoadVCF']
Performance,"ng is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11331,Optimiz,Optimize,11331,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,ng.AnyIR total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.809ms self 0.073ms children 0.736ms %children 91.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.280ms self 0.280ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:169876,Optimiz,OptimizePass,169876,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ng.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.303ms self 0.051ms children 1.252ms %children 96.10%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.383ms self 0.383ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:26559,Optimiz,OptimizePass,26559,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:152384,Optimiz,Optimize,152384,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:136597,Optimiz,Optimize,136597,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120879,Optimiz,Optimize,120879,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:157582,Optimiz,Optimize,157582,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141795,Optimiz,Optimize,141795,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvalu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126077,Optimiz,Optimize,126077,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:180906,Optimiz,Optimize,180906,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:187931,Optimiz,Optimize,187931,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ng.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14641,Optimiz,Optimize,14641,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ngDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7312,Optimiz,Optimize,7312,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ngPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.525ms self 0.004ms children 0.521ms %children 99.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.503ms self 0.061ms children 0.443ms %children 87.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:214488,Optimiz,OptimizePass,214488,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206467,Optimiz,OptimizePass,206467,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:158324,Optimiz,OptimizePass,158324,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:142537,Optimiz,OptimizePass,142537,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ngPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126819,Optimiz,OptimizePass,126819,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"nitialized for thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 : INFO: TaskReport: stage=0, partition=38854, attempt=0, peakBytes=65536, peakBytesReadable=64.00 KiB, chunks requested=0, cache hits=0; 2023-09-13 16:37:38.903 : INFO: RegionPool: FREE: 64.0K allocated (64.0K blocks / 0 chunks), regions.size = 1, 0 current java objects, thread 9: pool-2-thread-1; 2023-09-13 16:37:38.903 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c04",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:2930,concurren,concurrent,2930,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['concurren'],['concurrent']
Performance,nnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:70); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2.apply(AnnotateVariantsExpr.scala:64); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.variant.VariantSampleMatrix$$anonfun$25.apply(VariantSampleMatrix.scala:423); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at org.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); I0824 16:44:07.061986 9121 sched.cpp:1771] Asked to stop the driver; I0824 16:44:07.062144 8743 sched.cpp:1040] Stopping framework '0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:11564,concurren,concurrent,11564,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,2,['concurren'],['concurrent']
Performance,"no, its just a question of where we put the checking logic. When we add a reference genome in Emit, we'll probably just have to set some boolean flag that tells us if something has already been set, and only call the `loader` code if it hasn't been set.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3357#issuecomment-380970181:218,load,loader,218,https://hail.is,https://github.com/hail-is/hail/pull/3357#issuecomment-380970181,1,['load'],['loader']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:50766,concurren,concurrent,50766,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:52093,concurren,concurrent,52093,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 11.0 in stage 0.0 (TID 11, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:54758,concurren,concurrent,54758,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:48112,concurren,concurrent,48112,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:49439,concurren,concurrent,49439,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:106407,concurren,concurrent,106407,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:100720,concurren,concurrent,100720,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 9 from BlockManagerMaster.; 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 9 requested; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:125396,concurren,concurrent,125396,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 17.0 in stage 0.0 (TID 17, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:118263,concurren,concurrent,118263,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:139174,concurren,concurrent,139174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:147953,concurren,concurrent,147953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:161083,concurren,concurrent,161083,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:171632,concurren,concurrent,171632,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:183131,concurren,concurrent,183131,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:192487,concurren,concurrent,192487,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33220,cache,cached,33220,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/dataproc/0.2.129/*"" --temporary-hold; ```. Note the following:; - mill is not invoked; - deploy.yaml is re-made with the correct uris; - the wheel is built",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1837,cache,cache,1837,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['cache'],['cache']
Performance,nonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3559,concurren,concurrent,3559,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['concurren'],['concurrent']
Performance,nonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218376,concurren,concurrent,218376,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ntextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.Batc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217284,concurren,concurrent,217284,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ntime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4378,concurren,concurrent,4378,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,o==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33518,cache,cached,33518,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,oader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	... 30 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:2526,load,loadClass,2526,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,3,['load'],['loadClass']
Performance,ocketAPI2$.$anonfun$main$5$adapted(ServiceBackend.scala:430); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.$anonfun$main$4(ServiceBackend.scala:430); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:77); E 	at is.hail.backend.service.ServiceBackendSocketAPI2$.main(ServiceBackend.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.pu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4148,concurren,concurrent,4148,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['concurren'],['concurrent']
Performance,"oh, man, this is super exciting. 3x on the combiner? yes please!. We can probably make incremental performance improvements to the LIR method splitting code to bring the compile and execute back down, and that one I consider a little less critical anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044:99,perform,performance,99,https://hail.is,https://github.com/hail-is/hail/pull/8963#issuecomment-650837044,1,['perform'],['performance']
Performance,"oh, whoa. splitting the code up and performing an action in between (`filter` followed by `collect` followed by `show`) fixes the problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7393#issuecomment-547003344:36,perform,performing,36,https://hail.is,https://github.com/hail-is/hail/issues/7393#issuecomment-547003344,1,['perform'],['performing']
Performance,oldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collecti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7560,Optimiz,Optimize,7560,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,oldConstants.scala:47); E 	at is.hail.expr.ir.RewriteBottomUp$.$anonfun$apply$2(RewriteBottomUp.scala:11); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:60); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7162,Optimiz,Optimize,7162,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,ollection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.maybeSpill(Spillable.scala:93); at org.apache.spark.util.collection.ExternalAppendOnlyMap.maybeSpill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:158); at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69); at org.apache.spark.rdd.RDD.iterator(RDD.scala:268); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); at java.util.concurrent.ThreadPoolExecutor.runWorker(Thread,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:3871,Cache,CacheManager,3871,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['Cache'],['CacheManager']
Performance,oml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-an,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33857,cache,cached,33857,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,ommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:6629,concurren,concurrent,6629,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['concurren'],['concurrent']
Performance,"on.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord_ltNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92860ord_lt(Unknown Source); E 	at __C92844etypeDecode.__m92857arraySorter_merge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3871,load,loadInt,3871,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['load'],['loadInt']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:134439,Optimiz,Optimize,134439,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150226,Optimiz,Optimize,150226,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:118721,Optimiz,Optimize,118721,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94730,Optimiz,OptimizePass,94730,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109168,Optimiz,OptimizePass,109168,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:73551,Optimiz,OptimizePass,73551,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59153,Optimiz,OptimizePass,59153,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:50625,Optimiz,OptimizePass,50625,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,onalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42097,Optimiz,OptimizePass,42097,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"once this is in, am I right that the plan is:; - get transposed array<struct> etype in similarly; - make transposed array<struct> ptype; - optimize these three in tandem to beat current performance / file size",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559:139,optimiz,optimize,139,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,one-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36117,cache,cached,36117,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,oogle_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached pandas-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB); Collecting parsimonious==0.10.0; Using cached parsimonious-0.10.0-py3-none-any.whl (48 kB); Collecting pillow==10.0.0; Using cached,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36745,cache,cached,36745,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"oops, still need to fix the `null` loadFrom stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9646#issuecomment-728896170:35,load,loadFrom,35,https://hail.is,https://github.com/hail-is/hail/pull/9646#issuecomment-728896170,1,['load'],['loadFrom']
Performance,op (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 16; LDC 9999; ILOAD 16; ISUB; ISTORE 17; GOTO L12; L12; FRAME APPEND [T I]; ALOAD 0; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; LLOAD 14; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; ILOAD 17; INVOKEVIRTUAL __C46CompiledWithAggs.__m69take_by_seqop (ZJZI)V; RETURN; L10; FRAME CHOP 2; LDC 0; ISTORE 17; GOTO L12; L7; FRAME CHOP 3; LDC 0; LSTORE 14; GOTO L9; L13; LOCALVARIABLE get_tup_elem_o J L0 L13 6; LOCALVARIABLE bool Z L0 L13 8; LOCALVARIABLE get_tup_elem_o J L0 L13 9; LOCALVARIABLE bool Z L0 L13 11; LOCALVARIABLE invoke I L0 L13 12; LOCALVARIABLE local13 I L0 L13 13; LOCALVAR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:1771,load,loadInt,1771,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408,1,['load'],['loadInt']
Performance,"operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.21; Branch HEAD; Compiled by user liangchi on 2023-02-11T02:24:04Z; Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6; Url https://github.com/apache/spark; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```. <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=10; HOSTNAME=ip-192-168-124-160; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 60539 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:6185,load,load-spark-env,6185,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['load'],['load-spark-env']
Performance,optimizer / compiler bug. Shouldn't be too hard to fix!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5212#issuecomment-457887049:0,optimiz,optimizer,0,https://hail.is,https://github.com/hail-is/hail/issues/5212#issuecomment-457887049,1,['optimiz'],['optimizer']
Performance,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107861,concurren,concurrent,107861,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102174,concurren,concurrent,102174,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103628,concurren,concurrent,103628,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:105080,concurren,concurrent,105080,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144989,concurren,concurrent,144989,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140628,concurren,concurrent,140628,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143535,concurren,concurrent,143535,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142082,concurren,concurrent,142082,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198396,concurren,concurrent,198396,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193941,concurren,concurrent,193941,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196942,concurren,concurrent,196942,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195488,concurren,concurrent,195488,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.5 KB, free: 21.2 GB); 2019-01-22 13:12:04 BlockManagerInfo: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177447,concurren,concurrent,177447,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175993,concurren,concurrent,175993,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173085,concurren,concurrent,173085,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174539,concurren,concurrent,174539,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition 25, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 TaskSetMa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166898,concurren,concurrent,166898,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165444,concurren,concurrent,165444,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162537,concurren,concurrent,162537,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163991,concurren,concurrent,163991,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:00 TaskSetMana",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153768,concurren,concurrent,153768,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149407,concurren,concurrent,149407,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150861,concurren,concurrent,150861,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152315,concurren,concurrent,152315,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Removing block manager BlockMa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188946,concurren,concurrent,188946,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187492,concurren,concurrent,187492,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:186038,concurren,concurrent,186038,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execut",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184584,concurren,concurrent,184584,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1360,load,loaded,1360,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,3,['load'],['loaded']
Performance,ore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:34656,cache,cached,34656,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"ormed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Exec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:2503,Load,LoadVCF,2503,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,"ost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 18 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 18 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 18; 2019-01-22 13:12:00 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.194:35002) with ID 15; 2019-01-22 13:12:00 TaskSetManager: INFO: Starting task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15, partition 5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:153707,concurren,concurrent,153707,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:165383,concurren,concurrent,165383,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:195427,concurren,concurrent,195427,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:143474,concurren,concurrent,143474,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 7.2 in stage 0.0 (TID 53, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:173024,concurren,concurrent,173024,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ost task 9.1 in stage 0.0 (TID 65, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:184523,concurren,concurrent,184523,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,otal 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6207,Optimiz,OptimizePass,6207,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"ow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df17cef; Error summary: IndexOutOfBoundsException: 3; ```; (NB: a custom VEP/LOFTEE, but that shouldn't matter - ran same thing on `devel-cd48e11` and it worked fine)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:6869,concurren,concurrent,6869,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,2,['concurren'],['concurrent']
Performance,owerAndExecute total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply total 1m41.8s self 1m40.6s children 1.127s %children 1.11%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2371,Optimiz,OptimizePass,2371,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,p-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-n,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32461,cache,cached,32461,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"p.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3644,concurren,concurrent,3644,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,"p/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 241. Traceback (most recent call last):; File ""/hail-vep/vep.py"", line 218, in <module>; main(action, consequence, tolerate_parse_error, block_size, input_file, output_file, part_id, vep_cmd); File ""/hail-vep/vep.py"", line 199, in main; results = run_vep(vep_cmd, input_file, block_size, consequence, tolerate_parse_error, part_id, os.environ); File ""/hail-vep/vep.py"", line 127, in run_vep; raise ValueError(f'VEP command {vep_cmd} failed with non-zero exit status {proc.returncode}\n'; ValueError: VEP command ['/vep/vep', '--input_file', '/io/input', '--format', 'vcf', '--json', '--everything', '--allele_number', '--no_stats', '--cache', '--offline', '--minimal', '--assembly', 'GRCh38', '--fasta', '/vep_data//homo_sapiens/95_GRCh38/Homo_sapiens.GRCh38.dna.toplevel.fa.gz', '--plugin', 'LoF,loftee_path:/vep/ensembl-vep/Plugins/,gerp_bigwig:/vep_data//gerp_conservation_scores.homo_sapiens.GRCh38.bw,human_ancestor_fa:/vep_data//human_ancestor.fa.gz,conservation_file:/vep_data//loftee.sql', '--dir_plugins', '/vep/ensembl-vep/Plugins/', '--dir_cache', '/vep_data/', '-o', 'STDOUT'] failed with non-zero exit status -9; VEP error output:; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 175.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/de_novo_donor.pl line 214.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 191.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 194.; Smartmatch is experimental at /vep/ensembl-vep/Plugins/splice_site_scan.pl line 238.; Smartmatch is experimental at /vep/ensembl-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224:1731,cache,cache,1731,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1802287224,1,['cache'],['cache']
Performance,p://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1164,Cache,Cache,1164,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['Cache'],['Cache']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215616,Optimiz,Optimize,215616,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207595,Optimiz,Optimize,207595,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199643,Optimiz,Optimize,199643,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.C,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194991,Optimiz,Optimize,194991,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209096,Optimiz,OptimizePass,209096,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217117,Optimiz,OptimizePass,217117,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196492,Optimiz,OptimizePass,196492,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201144,Optimiz,OptimizePass,201144,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"pache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spark.storage.StorageLevel; import org.apache.{hadoop => hd}; import org.json4s.JValue; import org.json4s.JsonAST._; import org.json4s._; import org.json4s.jackson.JsonMethods; import org.json4s.jackson.JsonMethods._; import org.json4s.jackson.Serialization; import org.json4s.jackson.{JsonMethods, Serialization}; import org.json4s.{DefaultFormats, Formats}; import org.sparkproject.guava.util.concurrent.MoreExecutors; ```. We explicitly depend on; - `htsjdk`; - `breeze`; - `json4s`. That leaves:. ```; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import org.apache.commons.math3.distribution.{ChiSquaredDistribution, NormalDistribution}; import org.apache.commons.math3.random.JDKRandomGenerator; import org.apache.commons.math3.util.CombinatoricsUtils.factorialLog; import org.apache.hadoop; import org.apache.log4j.{ConsoleAppender, PatternLayout}; import org.apache.spark.SparkContext; import org.apache.spark.SparkException; import org.apache.spark.mllib.linalg.Vectors; import org.apache.spark.mllib.linalg.distributed.{DistributedMatrix, IndexedRow, IndexedRowMatrix}; import org.apache.spark.rdd.RDD; import org.apache.spark.sql.Row; import org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:3953,concurren,concurrent,3953,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,2,['concurren'],['concurrent']
Performance,park$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.gua,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215862,concurren,concurrent,215862,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"park.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %children 55.66%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.367ms self 0.367ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.127ms self 0.127ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardRelationalLets, iteration: 0 total 0.030ms self 0.030ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/is.hail.expr.ir.TypeCheck.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/PruneDeadFields, iteration: 0 total 0.572ms self 0.572ms children 0.000ms %ch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:13155,Optimiz,Optimize,13155,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:218761,Optimiz,Optimize,218761,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:210740,Optimiz,Optimize,210740,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:202788,Optimiz,Optimize,202788,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,parkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198136,Optimiz,Optimize,198136,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,part of query optimizer.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/821#issuecomment-301784690:14,optimiz,optimizer,14,https://hail.is,https://github.com/hail-is/hail/issues/821#issuecomment-301784690,1,['optimiz'],['optimizer']
Performance,"passes on local now: test_rectangles_to_numpy. I think the easiest thing to do is to just switch on spark vs non-spark. For spark its important we're not using HDFS for tofile, but for Local and Service, ideally, we just use a normal `hl.TemporaryFilename` and load it via `hl.current_backend().fs.open`. I'm not sure how finicky numpy.fromfile / tofile are about their file objects though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11637#issuecomment-1078079261:261,load,load,261,https://hail.is,https://github.com/hail-is/hail/pull/11637#issuecomment-1078079261,1,['load'],['load']
Performance,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1729,load,loading,1729,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['load'],['loading']
Performance,"pc_rel['kin'] >= 0.0883); print(""finding Max ind set""); related_samples_to_remove = hl.maximal_independent_set(pairs.i, pairs.j,keep=False); print(""writing related samples to remove""); related_samples_to_remove.export(""file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/related_samples_""+pop+"".tsv""); result = mt.filter_cols(; hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False); print(""final unrelated count""); print(result.count); eigenvalues, scores, loadings = hl.hwe_normalized_pca(result.GT, k=10); scores.export('file:////restricted/projectnb/adgc/topmed.r2.analysis/pc_relate/unrelated_pcs_'+pop+'.tsv'). ```. ```; Running on Apache Spark version 2.4.3; SparkUI available at http://scc-hadoop.bu.edu:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.46-6ef64c08b000; LOGGING: writing to logs/adgc_pc_relate.autosome_all.log; 2020-08-20 10:14:00 Hail: INFO: Reading table to impute column types; [Stage 0:===========================================================(1 + 0) / 1]2020-08-20 10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Sta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:4491,Load,Loading,4491,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Load'],['Loading']
Performance,performance tests should catch stuff like this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778:0,perform,performance,0,https://hail.is,https://github.com/hail-is/hail/pull/4178#issuecomment-414487778,1,['perform'],['performance']
Performance,"pleElement 0 (Ref __iruid_490))))))\n (Let __iruid_491\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_491)))))))))\n2022-11-15 20:30:18.191 root: INFO: optimize optimize: compileLowerer, after InlineApplyIR: after: IR size 56:\n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (Let __iruid_509\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_510\n __iruid_511\n (ToStream False\n (Literal Array[Struct{start:Int32,end:Int32}]\n <literal value>))\n (MakeStruct (__iruid_458 (Ref __iruid_508)))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0\n (GetField __iruid_458 (Ref __iruid_511)))))\n (StreamFor __iruid_512\n (StreamMap __iruid_513\n (StreamRange -1 True\n (GetField start (Ref __iruid_510))\n (GetField end (Ref __iruid_510))\n (I32 1))\n (MakeStruct (idx (Ref __iruid_513))))\n (Begin\n (SeqOp 0 (Collect (CollectStateSig +PInt32))\n ((GetField idx (Ref __iruid_512)))))))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32))))\n (NA String))\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (Begin\n (InitFromSerializedValue 0 \n (CollectStateSig +PInt32)\n (GetTupleElement 0 (Ref __iruid_508))))\n (StreamFor __iruid_514\n (ToStream True (Ref __iruid_509))\n (Begin\n (CombOpValue 0 (Collect (CollectStateSig +PInt32))\n (GetTupleElement 0 (Ref __iruid_514))))))\n (Let __iruid_515\n (MakeTuple (0)\n (ResultOp 0 (Collect (CollectStateSig +PInt32))))\n (MakeStruct\n (idx (GetTupleElement 0 (Ref __iruid_515)))))))))\n2022-11-15 20:30:18.195 root: INFO: optimize optimize: compileLowerer, after LowerArrayAggsToRunAggs: before: IR size 56: \n(MakeTuple (0)\n (Let __iruid_508\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (Coll",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:7896,optimiz,optimize,7896,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,"plink removed 160 of 7516. Hail removed 424. Also, Hail spent 2s removing 419, and 9 minutes removing the other 5. ```; 2018-10-06 09:42:36 Hail: INFO: ld_prune: running local pruning stage with max queue size of 2166480 variants; 2018-10-06 09:42:37 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:42:38 Hail: INFO: wrote 7097 items in 100 partitions; 2018-10-06 09:42:40 Hail: INFO: wrote 7097 items in 100 partitions to file:/tmp/hail.jM6D3jREhNqh/Jx7rAbqyTP; 2018-10-06 09:42:40 Hail: INFO: ld_prune: local pruning stage retained 7097 variants; 2018-10-06 09:42:41 Hail: INFO: Wrote all 2 blocks of 7097 x 284 matrix with block size 4096.; 2018-10-06 09:42:59 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:46:52 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:48:58 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:48:58 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:46 Hail: INFO: Ordering unsorted dataset with network shuffle; 2018-10-06 09:51:46 Hail: INFO: wrote 5 items in 3 partitions; 2018-10-06 09:51:46 Hail: INFO: ld_prune: correlation graph of locally-pruned variants has 5 edges,; finding maximal independent set...; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: Coerced sorted dataset; 2018-10-06 09:51:47 Hail: INFO: wrote 7092 items in 100 partitions; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506#issuecomment-427575965:199,queue,queue,199,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427575965,1,['queue'],['queue']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:151618,Optimiz,OptimizePass,151618,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135831,Optimiz,OptimizePass,135831,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._appl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120113,Optimiz,OptimizePass,120113,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109930,Optimiz,Optimize,109930,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:95492,Optimiz,Optimize,95492,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59915,Optimiz,Optimize,59915,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:74313,Optimiz,Optimize,74313,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:51387,Optimiz,Optimize,51387,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42859,Optimiz,Optimize,42859,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ply$27.apply(ContextRDD.scala:359); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:139); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-f80a6f10bc84; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:12087,concurren,concurrent,12087,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,2,['concurren'],['concurrent']
Performance,ply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217684,concurren,concurrent,217684,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply total 20.227ms self 0.484ms children 19.744ms %children 97.61%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:191849,Optimiz,OptimizePass,191849,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass total 0.035ms self 0.018ms children 0.018ms %children 50.25%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.010ms self 0.010ms children 0.000ms %ch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:184824,Optimiz,OptimizePass,184824,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets total 1.058ms self 0.966ms children 0.092ms %children 8.70%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LiftRelationalValuesToRelationalLets/is.hail.expr.ir.lowering.MatrixLoweredToTable total 0.033ms self 0.033ms children 0.000m,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:18559,Optimiz,OptimizePass,18559,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass total 4.041ms self 3.971ms children 0.070ms %children 1.74%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerMatrixToTablePass/is.hail.expr.ir.lowering.AnyIR total 0.027ms self 0.027ms children 0.000ms %children 0.00%; is.hail.backend.BackendH,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:11575,Optimiz,OptimizePass,11575,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPip,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:7283,Optimiz,OptimizePass,7283,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,poch 11); 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 11 on scc-q17.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:137847,concurren,concurrent,137847,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 13); 2019-01-22 13:12:02 YarnScheduler: ERROR: Lost executor 15 on scc-q10.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:159756,concurren,concurrent,159756,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 15); 2019-01-22 13:12:05 YarnScheduler: ERROR: Lost executor 21 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:181804,concurren,concurrent,181804,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,poch 16); 2019-01-22 13:12:06 YarnScheduler: ERROR: Lost executor 12 on scc-q03.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:191160,concurren,concurrent,191160,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"port get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3330,load,loads,3330,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['load'],['loads']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138747,Optimiz,OptimizePass,138747,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:154534,Optimiz,OptimizePass,154534,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.Spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:123029,Optimiz,OptimizePass,123029,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:71462,Optimiz,OptimizePass,71462,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107079,Optimiz,OptimizePass,107079,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57064,Optimiz,OptimizePass,57064,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:92641,Optimiz,OptimizePass,92641,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:48536,Optimiz,OptimizePass,48536,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40006,Optimiz,OptimizePass,40006,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112067,Optimiz,OptimizePass,112067,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:97629,Optimiz,OptimizePass,97629,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:76450,Optimiz,OptimizePass,76450,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62052,Optimiz,OptimizePass,62052,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:53524,Optimiz,OptimizePass,53524,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:44996,Optimiz,OptimizePass,44996,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141058,Optimiz,Optimize,141058,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156845,Optimiz,Optimize,156845,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:125340,Optimiz,Optimize,125340,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:134410,Optimiz,OptimizePass,134410,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150197,Optimiz,OptimizePass,150197,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.039ms self 0.039ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:118692,Optimiz,OptimizePass,118692,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.500ms self 0.649ms children 0.852ms %children 56.76%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.557ms self 0.557ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:94759,Optimiz,Optimize,94759,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.561ms self 0.732ms children 0.829ms %children 53.12%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.525ms self 0.525ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:109197,Optimiz,Optimize,109197,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.862ms self 0.753ms children 1.109ms %children 59.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.743ms self 0.743ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:73580,Optimiz,Optimize,73580,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.010ms self 0.828ms children 1.182ms %children 58.80%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.736ms self 0.736ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.055ms self 0.055ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:59182,Optimiz,Optimize,59182,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.451ms self 0.949ms children 1.502ms %children 61.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.977ms self 0.977ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:50654,Optimiz,Optimize,50654,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.148ms self 0.148ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:42126,Optimiz,Optimize,42126,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,pr.ir.lowering.InlineApplyIR.transform total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR/is.hail.expr.ir.lowering.CompilableIRNoApply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:205819,Optimiz,OptimizePass,205819,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,precated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35745,cache,cached,35745,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.tryFailure(Promise.scala:112); at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153); at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218567,concurren,concurrent,218567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:7063,Load,LoadVCF,7063,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['Load'],['LoadVCF']
Performance,"q12.scc.bu.edu, executor 21, partition 29, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:12:03 YarnScheduler: ERROR: Lost executor 13 on scc-q16.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:170244,concurren,concurrent,170244,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"qq uses key_by, but yeah, it's an optimization. I mean, there's just so few cases where you actually want to use a spark shuffle. I don't think that many people are creating qq plots with 1 billion points.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7144#issuecomment-535750439:34,optimiz,optimization,34,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535750439,1,['optimiz'],['optimization']
Performance,"r both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1259,cache,cache,1259,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['cache'],['cache']
Performance,"r than just being marked ""distributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile),",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1164,perform,performance,1164,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['perform'],['performance']
Performance,"r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0/is.hail.expr.ir.NormalizeNames.apply total 0.358ms self 0.358ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/Simplify, iteration: 0 total 0.077ms self 0.077ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ForwardLets, iteration: 0 total 0.659ms self 0.292ms children 0.367ms %chil",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:11631,Optimiz,Optimize,11631,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.092ms self 0.092ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:175549,Optimiz,Optimize,175549,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.201ms self 0.201ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:32232,Optimiz,Optimize,32232,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35086,cache,cached,35086,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"r.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmult",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2086,queue,queue,2086,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['queue'],['queue']
Performance,"r.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Transform total 3.992ms self 3.992ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/LowerMatrixToTable/Verify total 0.019ms self 0.019ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.132ms self 0.132ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:10516,Optimiz,Optimize,10516,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,r.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.lowering.CompilableIR total 0.007ms self 0.007ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.601ms self 0.006ms children 0.595ms %children 99.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:186938,Optimiz,OptimizePass,186938,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.737ms self 0.071ms children 3.665ms %children 98.09%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.412ms self 0.412ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:135103,Optimiz,OptimizePass,135103,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.531ms self 0.089ms children 4.442ms %children 98.03%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.379ms self 0.379ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:150890,Optimiz,OptimizePass,150890,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 8.695ms self 0.088ms children 8.607ms %children 98.99%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.598ms self 0.598ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:119385,Optimiz,OptimizePass,119385,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:106401,Optimiz,Optimize,106401,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91963,Optimiz,Optimize,91963,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70784,Optimiz,Optimize,70784,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.396ms self 0.396ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:56386,Optimiz,Optimize,56386,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.447ms self 0.447ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:47858,Optimiz,Optimize,47858,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 6.134ms self 6.134ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:39328,Optimiz,Optimize,39328,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LowerAndExecuteShufflesPass#after total 0.035ms self 0.035ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 1.367ms self 0.010ms children 1.356ms %children 99.24%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.029ms self 0.029ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:25049,Optimiz,OptimizePass,25049,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:121608,Optimiz,Optimize,121608,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:137326,Optimiz,Optimize,137326,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipelin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153113,Optimiz,Optimize,153113,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,r_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); java.lang.UnsatisfiedLinkError: /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1535651898654_0001/container_1535651898654_0001_01_000002/tmp/libhail8271834084559267793.so); at java.lang.ClassLoader$NativeLibrary.load(Native Method); at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941); at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824); at java.lang.Runtime.load0(Runtime.java:809); at java.lang.System.load(System.java:1086); at is.hail.nativecode.NativeCode.<clinit>(NativeCode.java:27); at is.hail.nativecode.NativeBase.<init>(NativeBase.scala:22); at is.hail.annotations.Region.<init>(Region.scala:33); at is.hail.annotations.Region$.apply(Region.scala:15); at is.hail.rvd.RVDContext$.default(RVDContext.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:8); at is.hail.rvd.package$RVDContextIsPointed$.point(package.scala:6); at is.hail.utils.package$.point(package.scala:593); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:1724,load,load,1724,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['load'],['load']
Performance,r_struct_of_r_int32ANDr_int64END_TO_r_dict_of_r_int32ANDr_int64(Unknown Source); E 	at __C3100collect_distributed_array.__m3141DECODE_r_struct_of_r_int32ANDr_array_of_r_int32ANDr_float64ANDr_array_of_r_float64ANDr_int64ANDr_array_of_r_struct_of_r_int32ANDr_int64ENDEND_TO_SBaseStructPointer(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor_region24_75(Unknown Source); E 	at __C3100collect_distributed_array.__m3128split_StreamFor(Unknown Source); E 	at __C3100collect_distributed_array.__m3125begin_group_0(Unknown Source); E 	at __C3100collect_distributed_array.__m3110split_Let(Unknown Source); E 	at __C3100collect_distributed_array.apply(Unknown Source); E 	at __C3100collect_distributed_array.apply(Unknown Source); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$2(BackendUtils.scala:31); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$1(BackendUtils.scala:30); E 	at is.hail.backend.service.Worker$.main(Worker.scala:133); E 	at is.hail.backend.service.Main$.main(Main.scala:32); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317:2972,concurren,concurrent,2972,https://hail.is,https://github.com/hail-is/hail/pull/11681#issuecomment-1079785317,6,['concurren'],['concurrent']
Performance,rator.scala:1334); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:655); 	at is.hail.rvd.RVD$$anonfun$count$2.apply(RVD.scala:653); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$28.apply(ContextRDD.scala:405); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$8.apply(ContextRDD.scala:192); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); 	at scala.collection.Iterator$class.foreach(Iterator.scala:891); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334); 	at scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212); 	at scala.collection.AbstractIterator.fold(Iterator.scala:1334); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); 	at org.apache.spark.scheduler.Task.run(Task.scala:121); 	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:7296,concurren,concurrent,7296,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,2,['concurren'],['concurrent']
Performance,recreating because the CI page is getting too slow to load.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7523#issuecomment-557588150:54,load,load,54,https://hail.is,https://github.com/hail-is/hail/pull/7523#issuecomment-557588150,1,['load'],['load']
Performance,relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:326); 		at java.io.FilterOutputStream.close(FilterOutputStrea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:6026,concurren,concurrent,6026,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['concurren'],['concurrent']
Performance,ren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172654,Optimiz,Optimize,172654,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ren 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29337,Optimiz,Optimize,29337,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ren 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183019,Optimiz,OptimizePass,183019,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190044,Optimiz,OptimizePass,190044,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:16754,Optimiz,OptimizePass,16754,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:9770,Optimiz,OptimizePass,9770,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ren 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.526ms self 0.526ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:5479,Optimiz,OptimizePass,5479,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216721,concurren,concurrent,216721,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,rent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.im,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:218143,concurren,concurrent,218143,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,result.7028; 2023-09-27 16:44:22.668 GoogleStorageFS$: INFO: close: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPq,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7123,concurren,concurrent,7123,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,rg.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Prom,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215547,concurren,concurrent,215547,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.759ms self 0.759ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.137ms self 0.137ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:71491,Optimiz,Optimize,71491,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.561ms self 0.561ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.144ms self 0.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:107108,Optimiz,Optimize,107108,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.043ms self 0.043ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.785ms self 0.785ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.143ms self 0.143ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:57093,Optimiz,Optimize,57093,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.614ms self 0.614ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.292ms self 0.292ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:92670,Optimiz,Optimize,92670,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.051ms self 0.051ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.948ms self 0.948ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.222ms self 0.222ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:48565,Optimiz,Optimize,48565,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.135ms self 0.135ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 3.651ms self 3.651ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:40035,Optimiz,Optimize,40035,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112096,Optimiz,Optimize,112096,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:97658,Optimiz,Optimize,97658,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:76479,Optimiz,Optimize,76479,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62081,Optimiz,Optimize,62081,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:53553,Optimiz,Optimize,53553,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.low,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:45025,Optimiz,Optimize,45025,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ring.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 47.766ms self 0.019ms children 47.747ms %children 99.96%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.210ms self 0.210ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 47.463ms self 0.201ms children 47.262ms %children 99.58%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.exp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:37274,Optimiz,OptimizePass,37274,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,riteRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2087,concurren,concurrent,2087,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['concurren'],['concurrent']
Performance,rk_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217142,concurren,concurrent,217142,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"rking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1327,concurren,concurrent,1327,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,rnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215800,concurren,concurrent,215800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ruction count: 16: __C1124collect_distributed_array_table_aggregate_singlestage.__m1168ENCODE_SBinaryPointer_TO_r_binary\n2022-11-15 20:30:18.227 root: INFO: instruction count: 9: __C1124collect_distributed_array_table_aggregate_singlestage.setPartitionIndex\n2022-11-15 20:30:18.227 root: INFO: instruction count: 4: __C1124collect_distributed_array_table_aggregate_singlestage.addPartitionRegion\n2022-11-15 20:30:18.227 root: INFO: instruction count: 4: __C1124collect_distributed_array_table_aggregate_singlestage.setPool\n2022-11-15 20:30:18.227 root: INFO: instruction count: 3: __C1124collect_distributed_array_table_aggregate_singlestage.addHailClassLoader\n2022-11-15 20:30:18.227 root: INFO: instruction count: 3: __C1124collect_distributed_array_table_aggregate_singlestage.addFS\n2022-11-15 20:30:18.228 root: INFO: instruction count: 3: __C1148Tuple3.<init>\n2022-11-15 20:30:18.228 root: INFO: instruction count: 12: __C1148Tuple3.<init>\n2022-11-15 20:30:18.237 root: INFO: encoder cache hit\n2022-11-15 20:30:18.237 root: INFO: instruction count: 3: __C1093HailClassLoaderContainer.<init>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1093HailClassLoaderContainer.<clinit>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1094FSContainer.<init>\n2022-11-15 20:30:18.238 root: INFO: instruction count: 3: __C1094FSContainer.<clinit>\n2022-11-15 20:30:18.245 root: INFO: instruction count: 3: __C1095Compiled.<init>\n2022-11-15 20:30:18.245 root: INFO: instruction count: 222: __C1095Compiled.apply\n2022-11-15 20:30:18.245 root: INFO: instruction count: 73: __C1095Compiled.__m1103begin_group_0\n2022-11-15 20:30:18.245 root: INFO: instruction count: 11: __C1095Compiled.__m1106setup_null\n2022-11-15 20:30:18.245 root: INFO: instruction count: 109: __C1095Compiled.__m1108blockLinkedListSerialize\n2022-11-15 20:30:18.245 root: INFO: instruction count: 4: __C1095Compiled.__m1109ENCODE_SInt32$_TO_r_int32\n2022-11-15 20:30:18.246 root: INFO: instruction co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:17784,cache,cache,17784,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['cache'],['cache']
Performance,s children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.154ms self 0.069ms children 0.085ms %children 55.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:172625,Optimiz,OptimizePass,172625,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.346ms self 0.121ms children 0.225ms %children 65.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowerin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29308,Optimiz,OptimizePass,29308,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32936,cache,cached,32936,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"s.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.Sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2386,load,loadClass,2386,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['load'],['loadClass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.034ms self 0.034ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.200ms self 0.200ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:159778,Optimiz,OptimizePass,159778,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:143991,Optimiz,OptimizePass,143991,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.305ms self 0.305ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lower,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:128273,Optimiz,OptimizePass,128273,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.219ms self 0.219ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.032ms children 0.070ms %children 68.44%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:215587,Optimiz,OptimizePass,215587,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.064ms self 0.064ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.101ms self 0.033ms children 0.068ms %children 67.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:207566,Optimiz,OptimizePass,207566,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.070ms self 0.070ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.010ms self 0.010ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.108ms self 0.034ms children 0.074ms %children 68.32%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:199614,Optimiz,OptimizePass,199614,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.654ms self 0.654ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.102ms self 0.035ms children 0.067ms %children 66.15%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:194962,Optimiz,OptimizePass,194962,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105674,Optimiz,OptimizePass,105674,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91236,Optimiz,OptimizePass,91236,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.Lowe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70057,Optimiz,OptimizePass,70057,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,s.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:8069,concurren,concurrent,8069,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['concurren'],['concurrent']
Performance,s.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.RewriteBottomUp$.apply(RewriteBottomUp.scala:21); E 	at is.hail.expr.ir.FoldConstants$.foldConstants(FoldConstants.scala:13); E 	at is.hail.expr.ir.FoldConstants$.$anonfun$apply$1(FoldConstants.scala:10); E 	at is.hail.backend.ExecuteContext$.$anonfun$scopedNewRegion$1(ExecuteContext.scala:86); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); E 	at is.hail.backend.ExecuteContext$.scopedNewRegion(ExecuteContext.scala:83); E 	at is.hail.expr.ir.FoldConstants$.apply(FoldConstants.scala:9); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$4(Optimize.scala:22); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$1(Optimize.scala:15); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.runOpt$1(Optimize.scala:15); E 	at is.hail.expr.ir.Optimize$.$anonfun$apply$2(Optimize.scala:22); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.Optimize$.apply(Optimize.scala:18); E 	at is.hail.expr.ir.lowering.OptimizePass.transform(LoweringPass.scala:40); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:26); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:24); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:23); E 	at is.hail.expr.ir.lowering.OptimizePass.apply(LoweringPass.scala:36); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$app,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:7331,Optimiz,Optimize,7331,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['Optimiz'],['Optimize']
Performance,s.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:2803,concurren,concurrent,2803,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,"s.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-e6de08e; Error summary: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [3c5f402fed564ccd85257c0919d4bffb] entered state [ERROR] while waiting for [DONE].; Traceback (most recent call last):; File ""pyhail.py"", line 128, in <module>; main(args, pass_through_args); File ""pyhail.py"", line 109, in main; subprocess.check_output(job); File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 573, in check_output; raise CalledProcessError(retcode, cmd, output=output); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', '/Users/gtiao/gnomad_qc/hail/sample_qc/assign_subpops.py', '--cluster', 'gt1', '--files=gs://hail-common/builds/devel/jars/hail-devel-38dbf156b630-Spark-2.2.0.jar', '--py-files=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:7322,concurren,concurrent,7322,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['concurren'],['concurrent']
Performance,"s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to up",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14253,load,loaded,14253,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['load'],['loaded']
Performance,"s.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSessi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:1149,load,load,1149,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['load'],['load']
Performance,"s.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: ; ```; -----------------------------; Step2 : read the file with sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:1133,load,load,1133,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['load'],['load']
Performance,"s/batch/worker/worker.py"", line 2272, in run; await self.jvm.execute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.retryingRead(GoogleStorageFS.scala:220); 	at is.hail.io.fs.GoogleStora",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1086,concurren,concurrent,1086,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:11975,perform,performanceLog,11975,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['perform'],['performanceLog']
Performance,s=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32078,cache,cached,32078,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.660ms self 0.648ms children 0.012ms %children 1.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 2.298ms self 2.298ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.050ms self 0.050ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:6595,Optimiz,OptimizePass,6595,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using cached tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB); Collecting typer==0.9.0; Using cached typer-0.9.0-py3-none-any.whl (45 kB); Collecting typing-extensions==4.7.1; Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB); Col,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39535,cache,cached,39535,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"simpler:; ```; In [16]: import hail as hl; ...: ; ...: t1kg = hl.utils.range_matrix_table(1,1); ...: t1kg = t1kg.key_rows_by(locus=hl.locus(hl.str(t1kg.row_idx+1), t1kg.row_idx+1), alleles=['A','T']); ...: t1kg.write('/tmp/foo.mt', overwrite=True); 2018-10-11 13:42:55 Hail: INFO: Coerced sorted dataset; 2018-10-11 13:42:55 Hail: INFO: wrote 1 items in 1 partitions to /tmp/foo.mt; ^[[A; In [17]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt'); ...: t1kg = hl.split_multi(t1kg); ...: t1kg._force_count_rows(); ```; error:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}}; ```; describe:; ```; In [18]: import hail as hl; ...: ; ...: t1kg = hl.read_matrix_table('/tmp/foo.mt').describe(); ...: ; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 'col_idx': int32 ; ----------------------------------------; Row fields:; 'row_idx': int32 ; 'locus': locus<GRCh37> ; 'alleles': array<str> ; ----------------------------------------; Entry fields:; None; ----------------------------------------; Column key: ['col_idx']; Row key: ['locus', 'alleles']; ----------------------------------------; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397:583,optimiz,optimization,583,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429051397,2,['optimiz'],['optimization']
Performance,"so I can remove the computeEigenvalues option and just always return them, but I'm less clear on the loadings. @tpoterba are you suggesting that I can remove the computeLoadings option because the computation is lazy? Does passing the KeyTable object through to python count as ""using"", though?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540:101,load,loadings,101,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348580540,2,['load'],['loadings']
Performance,"so like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: INFO: Running Hail version 0.2.127-d18228b9bc5b; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:1793,load,loads,1793,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,1,['load'],['loads']
Performance,"sorry my internet was bad and wasn't reloading the bottom of the page for a while. Can respond now:. > As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of itsSeq[IR]'s are different. > I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass). The ptype inference for apply methods is handled by some stuff I wrote recently. IRFunction now has a `returnPType` method that takes arg ptypes. > PVoid. Void isn't a catch-all type like Nothing in Scala - it's a specific I-don't-return-anything type used by IRs like TableWrite. The exception in your above message is coming from the Apply node being inferred as a `PVoid` by your `case _ => PVoid` code. Writing the rule for the apply node should fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812:493,optimiz,optimization,493,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812,1,['optimiz'],['optimization']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:219138,Optimiz,Optimize,219138,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.InlineApplyIR total 0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:203165,Optimiz,Optimize,203165,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerArrayAggsToRunAgg,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:211117,Optimiz,Optimize,211117,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.238ms self 0.238ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:198513,Optimiz,Optimize,198513,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ssLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.MatrixVCFReader.coercer$lzycompute(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.coercer(LoadVCF.scala:974); at is.hail.io.vcf.MatrixVCFReader.apply(LoadVCF.scala:1008); at is.hail.expr.ir.MatrixRead.execute(MatrixIR.scala:426); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:647); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:57); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:32); at is.hail.variant.MatrixTable.write(MatrixTable.scala:1238); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.Ca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:7160,Load,LoadVCF,7160,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"st task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:150800,concurren,concurrent,150800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 15.2 in stage 0.0 (TID 50, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:162476,concurren,concurrent,162476,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:198335,concurren,concurrent,198335,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 7.1 in stage 0.0 (TID 46, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:142021,concurren,concurrent,142021,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 BlockManagerMaster: INFO: Removal of executor 13 requested; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Trying to remove executor 13 from BlockManagerMaster.; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 13; 2019-01-22 13:12:03 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:45213 with 21.2 GB RAM, BlockManagerId(21, scc-q12.scc.bu.edu, 45213, None); 2019-01-22 13:12:03 BlockManagerInfo: INFO: Added broadcast_1_piece0 in memory on scc-q12.scc.bu.edu:45213 (size: 24.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:177386,concurren,concurrent,177386,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 9.0 in stage 0.0 (TID 9, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:103567,concurren,concurrent,103567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 BlockManagerMaster: INFO: Removal of executor 21 requested; 2019-01-22 13:12:05 BlockManagerMasterEndpoint: INFO: Trying to remove executor 21 from BlockManagerMaster.; 2019-01-22 13:12:05 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 21; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Disabling executor 14.; 2019-01-22 13:12:06 DAGScheduler: INFO: Executor lost: 14 (epoch 16); 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 14 from BlockManagerMaster.; 2019-01-22 13:12:06 Blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:188885,concurren,concurrent,188885,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.1 in stage 0.0 (TID 41, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:149346,concurren,concurrent,149346,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.2 in stage 0.0 (TID 51, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 BlockManagerMaster: INFO: Removal of executor 15 requested; 2019-01-22 13:12:02 BlockManagerMasterEndpoint: INFO: Trying to remove executor 15 from BlockManagerMaster.; 2019-01-22 13:12:02 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 15; 2019-01-22 13:12:03 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.187:49620) with ID 12; 2019-01-22 13:12:03 TaskSetManager: INFO: Starting task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12, partition ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:166837,concurren,concurrent,166837,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 25.3 in stage 0.0 (TID 60, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 15.3 in stage 0.0 (TID 63, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:196881,concurren,concurrent,196881,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 27.1 in stage 0.0 (TID 44, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 17.1 in stage 0.0 (TID 47, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:140567,concurren,concurrent,140567,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 27.2 in stage 0.0 (TID 55, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:174478,concurren,concurrent,174478,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 29.0 in stage 0.0 (TID 29, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 19.0 in stage 0.0 (TID 19, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:102113,concurren,concurrent,102113,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 19.1 in stage 0.0 (TID 66, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:187431,concurren,concurrent,187431,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.1 in stage 0.0 (TID 43, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 TaskSetManager: WARN: Lost task 5.1 in stage 0.0 (TID 42, scc-q02.scc.bu.edu, executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000028; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:152254,concurren,concurrent,152254,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.2 in stage 0.0 (TID 49, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:02 TaskSetManager: WARN: Lost task 5.2 in stage 0.0 (TID 48, scc-q10.scc.bu.edu, executor 15): ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000025 on host: scc-q10.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000025; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:163930,concurren,concurrent,163930,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 35.3 in stage 0.0 (TID 62, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hado",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:193880,concurren,concurrent,193880,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 37.1 in stage 0.0 (TID 45, scc-q17.scc.bu.edu, executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000021 on host: scc-q17.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000021; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:59 BlockManagerMaster: INFO: Removal of executor 11 requested; 2019-01-22 13:11:59 BlockManagerMasterEndpoint: INFO: Trying to remove executor 11 from BlockManagerMaster.; 2019-01-22 13:11:59 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 11; 2019-01-22 13:11:59 YarnScheduler: ERROR: Lost executor 18 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000028 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_01",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:144928,concurren,concurrent,144928,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 37.2 in stage 0.0 (TID 52, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:03 TaskSetManager: WARN: Lost task 17.2 in stage 0.0 (TID 54, scc-q16.scc.bu.edu, executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000023 on host: scc-q16.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000023; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:175932,concurren,concurrent,175932,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 39.0 in stage 0.0 (TID 39, scc-q20.scc.bu.edu, executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 10 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 10 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 10; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 8 on scc-q19.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_017",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:107800,concurren,concurrent,107800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"st task 39.1 in stage 0.0 (TID 64, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:05 TaskSetManager: WARN: Lost task 29.1 in stage 0.0 (TID 67, scc-q12.scc.bu.edu, executor 21): ExecutorLostFailure (executor 21 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000036 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000036; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:185977,concurren,concurrent,185977,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:563,cache,cachefrom,563,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,1,['cache'],['cachefrom']
Performance,t is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.fs.FS.open(FS.scala:569); E 	at is.hail.io.fs.FS.open$(FS.scala:569); E 	at is.hail.io.fs.HadoopFS.open(HadoopFS.scala:85); E 	at is.hail.io.index.IndexReader$.readTypes(IndexReader.scala:65); E 	at is.hail.io.bgen.LoadBgen$.$anonfun$getBgenFileMetadata$2(LoadBgen.scala:208); E 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198); E 	at scala.collection.TraversableLike.map(TraversableLike.scala:286); E 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279); E 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198); E 	at is.hail.io.bgen.LoadBgen$.getBgenFileMetadata(LoadBgen.scala:207); E 	at is.hail.io.bgen.MatrixBGENReader$.apply(LoadBgen.scala:387); E 	at is.hail.io.bgen.MatrixBGENReader$.fromJValue(LoadBgen.scala:365); E 	at is.hail.expr.ir.MatrixReader$.fromJson(MatrixIR.scala:116); E 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1815); E 	at is.hail.expr.ir.IRParser$.$anonfun$matrix_ir$1(Parser.scala:1738); E 	at is.hail.utils.StackSafe$More.advance(StackSafe.scala:64); E 	at is.hail.utils.StackSafe$.run(StackSafe.scala:16); E 	at is.hail.utils.StackSafe$StackFrame.run(StackSafe.scala:32); E 	at is.hail.expr.ir.IRParser$.$anonfun$parse_matrix_ir$1(Parser.scala:2164); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:2136); E 	at is.hail.expr.ir.IRParser$.parse_matrix_ir(Parser.scala:2164); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$2(Backend.scala:186); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:3283,Load,LoadBgen,3283,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,1,['Load'],['LoadBgen']
Performance,t java.io.ObjectInputStream.readObject(ObjectInputStream.java:461) ~[?:1.8.0_392]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:136) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.utils.package$.using(package.scala:657) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$3(Worker.scala:135) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$2(Worker.scala:134) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.$anonfun$map$1(Try.scala:255) ~[scala-library-2.12.15.jar:?]; 	at scala.util.Success.map(Try.scala:213) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:6928,concurren,concurrent,6928,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['concurren'],['concurrent']
Performance,t.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217361,concurren,concurrent,217361,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"tFrameDecoder.java:182); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893); at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedExcepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:213356,concurren,concurrent,213356,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ta/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs://jbloom/profile225.vcf.bgz'); .filter_multi(); .nirvana(block_size=10000, config='/nirvana/nirvana-cloud-GRCh37.properties'); .variants_table(); .filter(expr='v.start > 24430000 && v.start < 24580000'); .export(output='gs://jbloom/nirvana_cabin1.tsv')); ```. The top-level categories show reasonable-looking variation, except for ""clinvar"" and ""genes"" which are all `null` valued. Comparing a few variants in [gnomad](http://gnomad.broadinstitute.org/gene/ENSG00000099991), the annotations line up nicely. Here's an example of a common missense variant:; ```; 22:244683",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:1544,cache,cache,1544,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,tal 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.06%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:183748,Optimiz,OptimizePass,183748,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.030ms self 0.027ms children 0.003ms %children 11.19%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.023ms self 0.023ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:190773,Optimiz,OptimizePass,190773,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.038ms self 0.038ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.038ms self 0.029ms children 0.009ms %children 22.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.189ms self 0.189ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:17483,Optimiz,OptimizePass,17483,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,tal 0.008ms self 0.008ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.044ms self 0.035ms children 0.009ms %children 20.05%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.193ms self 0.193ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:10499,Optimiz,OptimizePass,10499,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:788); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:64); at is.hail.rvd.RVDPartitionInfo$$anonfun$apply$1.apply(RVDPartitionInfo.scala:37); at is.hail.utils.package$.using(package.scala:587); at is.hail.rvd.RVDPartitionInfo$.apply(RVDPartitionInfo.scala:37); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1059); at is.hail.rvd.RVD$$anonfun$37.apply(RVD.scala:1057); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithIndex$1$$anonfun$apply$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:9170,Load,LoadVCF,9170,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['Load'],['LoadVCF']
Performance,"te/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB);  53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB);  97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB);  53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29792,cache,cached,29792,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.046ms self 0.046ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.655ms self 0.655ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:152355,Optimiz,OptimizePass,152355,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.609ms self 0.609ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:136568,Optimiz,OptimizePass,136568,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.054ms self 0.054ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.566ms self 0.566ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:120850,Optimiz,OptimizePass,120850,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:157553,Optimiz,OptimizePass,157553,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141766,Optimiz,OptimizePass,141766,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,te/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:126048,Optimiz,OptimizePass,126048,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"tected.; 2017-08-28 21:47:35 Hail: INFO: Ordering unsorted dataset with network shuffle; [Stage 2234:============================================> (4 + 1) / 5]2017-08-28 21:47:37 Hail: WARN: Found 2 samples with missing sex information (not 1 or 2).; Missing sex identifiers: [ 0 ]; 2017-08-28 21:47:37 Hail: WARN: 2 samples discarded from .fam: sex of child is missing.; 2017-08-28 21:47:38 Hail: INFO: Found 250 samples in fam file.; 2017-08-28 21:47:38 Hail: INFO: Found 2000 variants in bim file.; 2017-08-28 21:47:38 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:38 Hail: INFO: Modified the genotype schema with annotateGenotypesExpr.; Original: Struct{GT:Call}; New: Genotype; 2017-08-28 21:47:38 Hail: INFO: Reading table to impute column types; [Stage 2258:============================> (1 + 1) / 2]2017-08-28 21:47:40 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: Reading table to impute column types; 2017-08-28 21:47:41 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: rrm: Computing Realized Relationship Matrix...; [Stage 2263:============================> (1 + 1) / 2]2017-08-28 21:47:44 Hail: INFO: rrm: RRM computed using 1000 variants.; 2017-08-28 21:47:45 Hail: INFO: lmmreg: running lmmreg on 250 samples with 2 sample covariates including intercept...; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:45 Hail: INFO: lmmreg: Evals 1 to 20: 14.94768, 2.08278, 2.02984, 1.99490, 1.97532, 1.96462, 1.95253, 1.92972, 1.91744, 1.90489, 1.87748, 1.86775, 1.84180, 1.82938, 1.81619, 1.79946, 1.78303, 1.77441, 1.75651, 1.7511",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:1676,Load,Loading,1676,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,6,['Load'],['Loading']
Performance,ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 1 on scc-q02.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.Li,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:53370,concurren,concurrent,53370,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 14.0 in stage 0.0 (TID 14, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:61841,concurren,concurrent,61841,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ted yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 26.0 in stage 0.0 (TID 26, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:68985,concurren,concurrent,68985,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"ter to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2866,perform,performance,2866,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,"['perform', 'race condition']","['performance', 'race conditions']"
Performance,"textRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD$$anonfun$apply$2.apply(ContextRDD.scala:17); at is.hail.sparkextras.ContextRDD.is$hail$sparkextras$ContextRDD$$sparkManagedContext(ContextRDD.scala:129); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:138); at is.hail.sparkextras.ContextRDD$$anonfun$run$1.apply(ContextRDD.scala:137); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Daniel King: JAR and ZIP are both deleted from google storage. for the record the bad SHA was 5702f5c6b299; ```. A discuss user [is seeing that the JVM crashes when he tries to use hail](http://discuss.hail.is/t/gcp-py4jnetworkerror-answer-from-java-side-is-empty/630):. ```; hl.init(); mt = hl.read_matrix_table('gs://hail-datasets/hail-data/1000_genomes_phase3_autosomes.GRCh37.mt'); mt.describe() ...; mt.rsid.show(5); ERROR:root:Exception while sending command.; Traceback (most recent call last):; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1035, in send_command; raise Py4JNetworkError(""Answer from Java side is empty""); py4j.protocol.Py4JNetworkError: Answer from Java side is empty. During handling of the above exception, another exception occurred:. Traceback (m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186:3248,concurren,concurrent,3248,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417433186,1,['concurren'],['concurrent']
Performance,"the `loader` code is currently just a chunk of code---it gets run whenever it's invoked. When I put it into emit, I was going to protect it from getting run every time the function was evaluated with some checking, but I can put it directly into this function instead--that might be better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3357#issuecomment-380965877:5,load,loader,5,https://hail.is,https://github.com/hail-is/hail/pull/3357#issuecomment-380965877,1,['load'],['loader']
Performance,"there will definitely be a regime where that's faster, yeah. But that's just an optimization to order_by, yes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7144#issuecomment-535749300:80,optimiz,optimization,80,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535749300,1,['optimiz'],['optimization']
Performance,"tialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row)))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, initial IR: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.146 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: before: IR size 8: \n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.148 root: INFO: optimize optimize: darrayLowerer, after LowerMatrixToTable: after: IR size 8:\n(MakeTuple (0)\n (TableAggregate\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (MakeStruct\n (idx\n (ApplyAggOp Collect\n ()\n ((GetField idx (Ref row))))))))\n2022-11-15 20:30:18.160 root: INFO: Aggregate: useTreeAggregate=false\n2022-11-15 20:30:18.160 root: INFO: Aggregate: commutative=false\n2022-11-15 20:30:18.163 root: INFO: optimize optimize: compileLowerer, initial IR: before: IR size 70: \n(MakeTuple (0)\n (Let __iruid_455\n (MakeStruct)\n (Let __iruid_458\n (Let global\n (Ref __iruid_455)\n (RunAgg ((CollectStateSig +PInt32))\n (Begin\n (InitOp 0 (Collect (CollectStateSig +PInt32)) ()))\n (MakeTuple (0)\n (AggStateValue 0 (CollectStateSig +PInt32)))))\n (Let __iruid_460\n (CollectDistributedArray\n table_aggregate_singlestage __iruid_456\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:2124,optimiz,optimize,2124,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,4,['optimiz'],['optimize']
Performance,tionalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 12.144ms self 12.144ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 9.513ms self 3.957ms children 5.556ms %children 58.40%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 4.091ms self 4.091ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:41400,Optimiz,OptimizePass,41400,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:3720,Optimiz,Optimize,3720,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,tpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.073ms self 0.073ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:173238,Optimiz,Optimize,173238,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,tpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.154ms self 0.154ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.056ms self 0.056ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:29921,Optimiz,Optimize,29921,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"traClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN Utils: Your hostname, <my computer name> resolves to a l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:7756,load,load,7756,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['load'],['load']
Performance,"tring},entry:Struct{GT:Call,GP:+Array[+Float64],dosage:+Float64}} False False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (MatrixRowsTable; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}""))); 2019-01-22 13:11:51 root: INFO: optimize: before:; (TableCount; (TableMapRows; (TableMapGlobals; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\""indexFileMap\"":{},\""blockSizeInMB\"":128}"")); (SelectFields (); (Ref global))); (SelectFields (locus alleles); (Ref row)))); 2019-01-22 13:11:51 root: INFO: optimize: after:; (TableCount; (CastMatrixToTable ""the entries! [877f12a8827e18f61222c6c8c5fb04a8]"" ""__cols""; (MatrixRead Matrix{global:Struct{},col_key:[s],col:Struct{s:String},row_key:[[locus,alleles]],row:Struct{locus:Locus(GRCh37),alleles:Array[String]},entry:Struct{}} True False ""{\""name\"":\""MatrixBGENReader\"",\""files\"":[\""/project/ukbiobank/imp/uk.v3/bgen/ukb_imp_chr21_v3.bgen\""],\""sampleFile\"":\""/project/ukbiobank/imp/uk.v3/bgen/ukb19416_imp_chr21_v3_s487327.sample\"",\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:28530,optimiz,optimize,28530,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['optimiz'],['optimize']
Performance,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4945,Latency,Latency,4945,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.184ms self 0.184ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.228ms self 1.353ms children 0.875ms %children 39.29%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.340ms self 0.340ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:153807,Optimiz,OptimizePass,153807,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.195ms self 0.195ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 1.409ms self 0.648ms children 0.762ms %children 54.04%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.463ms self 0.463ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:138020,Optimiz,OptimizePass,138020,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.445ms self 0.445ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 2.252ms self 0.765ms children 1.487ms %children 66.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.505ms self 0.505ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:122302,Optimiz,OptimizePass,122302,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.257ms self 0.257ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.074ms self 0.035ms children 0.040ms %children 53.33%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:96951,Optimiz,Optimize,96951,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.264ms self 0.264ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.073ms self 0.037ms children 0.036ms %children 49.79%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:111389,Optimiz,Optimize,111389,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.318ms self 0.318ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.089ms self 0.043ms children 0.047ms %children 52.21%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:75772,Optimiz,Optimize,75772,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.391ms self 0.391ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.098ms self 0.049ms children 0.049ms %children 50.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:61374,Optimiz,Optimize,61374,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.476ms self 0.476ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.138ms self 0.089ms children 0.049ms %children 35.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:52846,Optimiz,Optimize,52846,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ts.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 1.317ms self 1.317ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.144ms self 0.075ms children 0.069ms %children 48.07%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:44318,Optimiz,Optimize,44318,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"turing things to get a; high level of re-use, but just to opportunistically exploit re-use which happens to occur -; e.g. if the early stages of an analysis involve reading an existing file and filtering in various; ways, then that may not be changed at all by changes to what happen in the real analysis; after the filtering. And this is also influenced by the medium-term goal of having a Hail service; which (amongst other things) can do simple analyses on small data in under ten seconds - in; that realm compilation time could become a critical factor as a serial bottleneck. [The place where persistent cacheing of compiled files helps most of all is in testing,; where you really are running the exact same queries over and over again on the same; small datasets, and in many cases after making small changes which only affect a few; of the queries]. b) We may actually have some version of the locking problem even if we don't try to reuse the files -; since we have several workers on a node, and possibly a master as well, all needing to put code; into a file (or wait for someone else to populate the file) so that they can load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:1444,load,load,1444,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['load'],['load']
Performance,"type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9616,concurren,concurrent,9616,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['concurren'],['concurrent']
Performance,"u linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3960,Latency,Latency,3960,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['Latency'],['Latency']
Performance,uler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216010,concurren,concurrent,216010,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,uler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:529); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:5010,Load,LoadVCF,5010,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['Load'],['LoadVCF']
Performance,"un$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1828); 	at java.io.ObjectOutputStream.close(ObjectOutputStream.java:742); 	at is.hail.utils.package$.using(pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3799,concurren,concurrent,3799,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,"unch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:4260,concurren,concurrent,4260,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,3,['concurren'],['concurrent']
Performance,"up_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an excepti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1261,Tune,Tuned,1261,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Tune'],['Tuned']
Performance,ur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3644,concurren,concurrent,3644,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['concurren'],['concurrent']
Performance,us 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33769,cache,cached,33769,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['cache'],"['cached', 'cachetools-']"
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.024ms self 0.021ms children 0.003ms %children 13.55%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:196929,Optimiz,Optimize,196929,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:209533,Optimiz,Optimize,209533,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.063ms self 0.063ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.002ms self 0.002ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.019ms self 0.016ms children 0.003ms %children 16.42%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:217554,Optimiz,Optimize,217554,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.067ms self 0.067ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.021ms self 0.018ms children 0.003ms %children 15.30%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.003ms self 0.003ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:201581,Optimiz,Optimize,201581,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.641ms self 0.641ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.241ms self 0.241ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:112837,Optimiz,Optimize,112837,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.635ms self 0.635ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.295ms self 0.295ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:98399,Optimiz,Optimize,98399,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.047ms self 0.047ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.830ms self 0.830ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.348ms self 0.348ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:77220,Optimiz,Optimize,77220,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.756ms self 0.756ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.409ms self 0.409ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:62822,Optimiz,Optimize,62822,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.049ms self 0.049ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.885ms self 0.885ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.450ms self 0.450ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:54294,Optimiz,Optimize,54294,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,ute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.069ms self 0.069ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.703ms self 4.703ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.607ms self 0.607ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:45766,Optimiz,Optimize,45766,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 1.0 in stage 0.0 (TID 1, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:57662,concurren,concurrent,57662,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 21.0 in stage 0.0 (TID 21, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:59112,concurren,concurrent,59112,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 31.0 in stage 0.0 (TID 31, scc-q02.scc.bu.edu, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:56210,concurren,concurrent,56210,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 6 on scc-q18.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:60564,concurren,concurrent,60564,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 6 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 6 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 6; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 7 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 7 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:82291,concurren,concurrent,82291,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 0.0 in stage 0.0 (TID 0, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:80841,concurren,concurrent,80841,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 10.0 in stage 0.0 (TID 10, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:77937,concurren,concurrent,77937,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 30.0 in stage 0.0 (TID 30, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:79389,concurren,concurrent,79389,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 BlockManagerMaster: INFO: Removal of executor 3 requested; 2019-01-22 13:11:55 BlockManagerMasterEndpoint: INFO: Trying to remove executor 3 from BlockManagerMaster.; 2019-01-22 13:11:55 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 3; 2019-01-22 13:11:57 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56628) with ID 18; 2019-01-22 13:11:57 TaskSetManager: INFO: Starting task 15.1 in stage 0.0 (TID 40, scc-q02.scc.bu.edu, executor 18, partition 15, PROCESS_LOCAL, 5147 bytes); 2019-01-22 13:11:57 TaskSetManag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:132835,concurren,concurrent,132835,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 15.0 in stage 0.0 (TID 15, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:131383,concurren,concurrent,131383,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 25.0 in stage 0.0 (TID 25, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:129931,concurren,concurrent,129931,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 5.0 in stage 0.0 (TID 5, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:128481,concurren,concurrent,128481,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 2 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 2 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 2; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 4 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 4 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:90025,concurren,concurrent,90025,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 13.0 in stage 0.0 (TID 13, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:85671,concurren,concurrent,85671,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 3.0 in stage 0.0 (TID 3, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:87123,concurren,concurrent,87123,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 33.0 in stage 0.0 (TID 33, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:88573,concurren,concurrent,88573,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 5 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 5 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 5; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 10 on scc-q20.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000011 on host: scc-q20.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000011; Exit code: 1; Stack trace: ExitCodeException exitC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:97759,concurren,concurrent,97759,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 12.0 in stage 0.0 (TID 12, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:96307,concurren,concurrent,96307,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 2.0 in stage 0.0 (TID 2, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:93405,concurren,concurrent,93405,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 22.0 in stage 0.0 (TID 22, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:94855,concurren,concurrent,94855,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 24.0 in stage 0.0 (TID 24, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:66256,concurren,concurrent,66256,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 34.0 in stage 0.0 (TID 34, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:64804,concurren,concurrent,64804,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 4.0 in stage 0.0 (TID 4, scc-q18.scc.bu.edu, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:63354,concurren,concurrent,63354,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 7 on scc-q21.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContaine,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:67708,concurren,concurrent,67708,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:2750,concurren,concurrent,2750,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['concurren'],['concurrent']
Performance,utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 1 from BlockManagerMaster.; 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 1 requested; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:74852,concurren,concurrent,74852,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 16.0 in stage 0.0 (TID 16, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:70498,concurren,concurrent,70498,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 36.0 in stage 0.0 (TID 36, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:71950,concurren,concurrent,71950,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 6.0 in stage 0.0 (TID 6, scc-q21.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000008 on host: scc-q21.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000008; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:73402,concurren,concurrent,73402,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 BlockManagerMaster: INFO: Removal of executor 8 requested; 2019-01-22 13:11:53 BlockManagerMasterEndpoint: INFO: Trying to remove executor 8 from BlockManagerMaster.; 2019-01-22 13:11:53 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:115303,concurren,concurrent,115303,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 18.0 in stage 0.0 (TID 18, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:113851,concurren,concurrent,113851,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 28.0 in stage 0.0 (TID 28, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:112399,concurren,concurrent,112399,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 38.0 in stage 0.0 (TID 38, scc-q19.scc.bu.edu, executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000009 on host: scc-q19.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000009; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:110947,concurren,concurrent,110947,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 27.0 in stage 0.0 (TID 27, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:122617,concurren,concurrent,122617,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 37.0 in stage 0.0 (TID 37, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:121165,concurren,concurrent,121165,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 7.0 in stage 0.0 (TID 7, scc-q01.scc.bu.edu, executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:119715,concurren,concurrent,119715,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,utor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:124069,concurren,concurrent,124069,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,ux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39106,cache,cached,39106,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,v already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215366,concurren,concurrent,215366,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2108,load,loaded,2108,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['load'],['loaded']
Performance,"va.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:1240,concurren,concurrent,1240,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.326ms self 0.326ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.042ms self 0.042ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:105703,Optimiz,Optimize,105703,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.342ms self 0.342ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:91265,Optimiz,Optimize,91265,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,valRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.355ms self 0.355ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.041ms self 0.041ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:70086,Optimiz,Optimize,70086,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,"veMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.ClassNotFoundException: com.amazonaws.AmazonClientException; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:419); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:352); 	... 27 more. During handling of the above exception, another exception occurred:. Py4JJavaError Traceback (most recent call last); /bmrn/apps/hail/0.2.72-spark-3.1.2/python/hail-0.2.72-py3-none-any.egg/hail/backend/py4j_backend.py in deco(*args, **kwargs); 15 try:; ---> 16 return f(*args, **kwargs); 17 except py4j.protocol.Py4JJavaError as e:. /bmrn/apps/python/miniconda/64/3.7/envs/piranha_0.2.0_20210812/lib/python3.7/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name); 327 ""An error occurred while calling {0}{1}{2}.\n"".; --> 328 format(target_id, ""."", name), value); 329 else:. Py4JJavaError: An error occurred while calling z:is.hail.backend.spark.SparkBackend.apply.; : java.lang.IllegalArgumentException: requirement failed; 	at scala.Predef$.require(Predef.scala:268); 	at is.hail.backend.spark.SparkBackend$.apply(SparkBackend.scala:218); 	at is.hail.backend.spark.SparkBackend.apply(SparkBackend.scala); 	at sun.reflect.Nat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:2442,load,loadClass,2442,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['load'],['loadClass']
Performance,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2175,Load,LoadModule,2175,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,4,['Load'],['LoadModule']
Performance,w on D's values; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.count:: WARNING: py:class reference target not found: integer -- return number of occurrences of value; /Users/dking/projects/hail/hail/python/hail/expr/expressions/typed_expressions.py:docstring of hail.expr.expressions.typed_expressions.TupleExpression.index:: WARNING: py:class reference target not found: integer -- return first index of value.; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_cols:9: WARNING: py:class reference target not found: hail.Table; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.annotate_rows:11: WARNING: py:class reference target not found: TVariant; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.cache:11: WARNING: py:func reference target not found: hail.MatrixTable.persist; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:15: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:17: WARNING: py:data reference target not found: int64; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.compute_entry_filter_stats:19: WARNING: py:data reference target not found: float32; /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_cols:17: WARNING: py:meth reference target not found: index_cols(exprs); /Users/dking/projects/hail/hail/python/hail/matrixtable.py:docstring of hail.matrixtable.MatrixTable.index_rows:17: WARNING: py:meth reference target not found: index_rows(exprs); /Users/dking/projects/hail/hail/pyt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666:14781,cache,cache,14781,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-685996666,2,['cache'],['cache']
Performance,we can get the performance back in the short term with a 2-line change moving decompression from advance() to getValue() -- the `data` field isn't used anywhere in `advance`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862#issuecomment-401760799:15,perform,performance,15,https://hail.is,https://github.com/hail-is/hail/issues/3862#issuecomment-401760799,1,['perform'],['performance']
Performance,"we do for correctness, but not partitioning/performance it seems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11969#issuecomment-1168752606:44,perform,performance,44,https://hail.is,https://github.com/hail-is/hail/pull/11969#issuecomment-1168752606,1,['perform'],['performance']
Performance,"we need to make sure we keep the optimized joins, though. I'm not sure exactly how this'll look (special case for variant or locus keyed keytable)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1158#issuecomment-266392470:33,optimiz,optimized,33,https://hail.is,https://github.com/hail-is/hail/issues/1158#issuecomment-266392470,1,['optimiz'],['optimized']
Performance,"we now have our dev test environment running with hail 0.2.126 and this query took ~92 seconds. So faster, but we do still need some performance enhancements. @ehigham let me know what would be helpful for you to get you started on this effort",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654:133,perform,performance,133,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1795785654,1,['perform'],['performance']
Performance,we're compiling like 20x fewer classes based on that. Every single cache hit is one fewer class compiled. The compilations there are value IRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7528#issuecomment-554563029:67,cache,cache,67,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554563029,1,['cache'],['cache']
Performance,"werMatrixToTable/Verify total 0.019ms self 0.019ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.TypeCheck.apply total 0.132ms self 0.132ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable total 2.591ms self 0.010ms children 2.581ms %children 99.63%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Verify total 0.011ms self 0.011ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform total 2.548ms self 0.302ms children 2.246ms %children 88.15%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/FoldConstants, iteration: 0 total 0.307ms self 0.307ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/ExtractIntervalFilters, iteration: 0 total 0.016ms self 0.016ms children 0.000ms %children 0.00%; timing is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/Optimize: relationalLowerer, after LowerMatrixToTable/Transform/NormalizeNames, iteration: 0 total 0.363ms self 0.004ms children 0.358ms %children 98.82%; timing is.hail.backend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966:10776,Optimiz,Optimize,10776,https://hail.is,https://github.com/hail-is/hail/pull/14679#issuecomment-2341990966,2,['Optimiz'],['Optimize']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.719ms self 0.009ms children 3.709ms %children 99.75%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.030ms self 0.030ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.641ms self 0.052ms children 3.589ms %children 98.56%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:104349,Optimiz,OptimizePass,104349,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 3.925ms self 0.009ms children 3.916ms %children 99.78%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.036ms self 0.036ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 3.847ms self 0.051ms children 3.797ms %children 98.69%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:89911,Optimiz,OptimizePass,89911,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 4.618ms self 0.011ms children 4.607ms %children 99.77%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.072ms self 0.072ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 4.479ms self 0.057ms children 4.422ms %children 98.72%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:68732,Optimiz,OptimizePass,68732,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:141029,Optimiz,OptimizePass,141029,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.040ms self 0.040ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156816,Optimiz,OptimizePass,156816,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,wering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.052ms self 0.052ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execut,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:125311,Optimiz,OptimizePass,125311,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached botocore-1.31.41-py3-none-any.whl (11.2 MB); Collecting cachetools==5.3.1; Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB); Collecting certifi==2023.7.22; Using cached certifi-2023.7.22-py3-none-any.whl (158 kB); Collecting cffi==1.15.1; Using cached cffi-1.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB); Collecting charset-normalizer==3.2.0; Using cached charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB); Requirement already satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:33320,cache,cached,33320,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,why do you need the cache?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5305#issuecomment-462482330:20,cache,cache,20,https://hail.is,https://github.com/hail-is/hail/pull/5305#issuecomment-462482330,1,['cache'],['cache']
Performance,why? people can cache this themselves,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1759#issuecomment-299068258:16,cache,cache,16,https://hail.is,https://github.com/hail-is/hail/pull/1759#issuecomment-299068258,1,['cache'],['cache']
Performance,"with a code change:; ```; FatalError: HailException: optimization changed type!; before: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:3}; after: Matrix{global:Struct{},col_key:[col_idx],col:Struct{col_idx:Int32},row_key:[[locus,alleles]],row:Struct{row_idx:Int32,locus:Locus(GRCh37),alleles:Array[String],a_index:Int32,was_split:Boolean,old_locus:Locus(GRCh37),old_alleles:Array[String]},entry:Struct{}entriesIndex:7}. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4527#issuecomment-429057183:53,optimiz,optimization,53,https://hail.is,https://github.com/hail-is/hail/issues/4527#issuecomment-429057183,1,['optimiz'],['optimization']
Performance,x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply total 1.106s self 33.060ms children 1.072s %children 97.01%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 31.509ms self 1.445ms children 30.064ms %children 95.41%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.344ms self 0.344ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 29.693ms self 9.419ms children 20.274ms %children 68.28%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.loweri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:2680,Optimiz,OptimizePass,2680,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"xecute(; File ""/usr/local/lib/python3.9/dist-packages/batch/worker/worker.py"", line 2872, in execute; raise JVMUserError(exception); JVMUserError: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at java.util.concurrent.FutureTask.report(FutureTask.java:122); 	at java.util.concurrent.FutureTask.get(FutureTask.java:192); 	at is.hail.JVMEntryway.retrieveException(JVMEntryway.java:253); 	at is.hail.JVMEntryway.finishFutures(JVMEntryway.java:215); 	at is.hail.JVMEntryway.main(JVMEntryway.java:185); Caused by: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:122); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:750); Caused by: java.lang.reflect.InvocationTargetException; 	at sun.reflect.GeneratedMethodAccessor62.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119); 	... 7 more; Caused by: java.lang.IllegalArgumentException: bound must be positive; 	at java.util.Random.nextInt(Random.java:388); 	at scala.util.Random.nextInt(Random.scala:70); 	at is.hail.services.package$.delayMsForTry(package.scala:47); 	at is.hail.services.package$.retryTransientErrors(package.scala:186); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.retryingRead(GoogleStorageFS.scala:220); 	at is.hail.io.fs.GoogleStorageFS$$anon$1.readHandlingRequesterPays(GoogleStorageFS.scala:2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:1148,concurren,concurrent,1148,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['concurren'],['concurrent']
Performance,"xecutor 1; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 2 on scc-q12.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 20.0 in stage 0.0 (TID 20, scc-q12.scc.bu.edu, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000003 on host: scc-q12.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000003; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:76485,concurren,concurrent,76485,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 4; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 5 on scc-q08.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 32.0 in stage 0.0 (TID 32, scc-q08.scc.bu.edu, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000006 on host: scc-q08.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000006; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:91953,concurren,concurrent,91953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 7; 2019-01-22 13:11:53 YarnScheduler: ERROR: Lost executor 4 on scc-q07.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 TaskSetManager: WARN: Lost task 23.0 in stage 0.0 (TID 23, scc-q07.scc.bu.edu, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000005 on host: scc-q07.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000005; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:84219,concurren,concurrent,84219,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,xecutor 8; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 9 on scc-q01.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000010 on host: scc-q01.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000010; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:116936,concurren,concurrent,116936,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,"xecutor 9; 2019-01-22 13:11:55 YarnScheduler: ERROR: Lost executor 3 on scc-q09.scc.bu.edu: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:55 TaskSetManager: WARN: Lost task 35.0 in stage 0.0 (TID 35, scc-q09.scc.bu.edu, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000004 on host: scc-q09.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:127029,concurren,concurrent,127029,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,xecutor: 3 (epoch 9); 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000002 on host: scc-q02.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000002; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:11:53 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Container marked as failed: container_e2435_1542127286896_0174_01_000007 on host: scc-q18.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000007; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:46785,concurren,concurrent,46785,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['concurren'],['concurrent']
Performance,xpr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181577,Optimiz,OptimizePass,181577,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188602,Optimiz,OptimizePass,188602,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15312,Optimiz,OptimizePass,15312,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8328,Optimiz,OptimizePass,8328,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xpr.ir.FoldConstants.apply total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4037,Optimiz,OptimizePass,4037,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,xt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32290,cache,cached,32290,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,y dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Coll,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32379,cache,cached,32379,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.621ms self 0.621ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.245ms self 0.245ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.034ms self 0.034ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:159054,Optimiz,OptimizePass,159054,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 0.650ms self 0.650ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.254ms self 0.254ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.024ms self 0.024ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:143267,Optimiz,OptimizePass,143267,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.PruneDeadFields.apply total 4.283ms self 4.283ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.TypeCheck.apply total 0.308ms self 0.308ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.026ms self 0.026ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:127549,Optimiz,OptimizePass,127549,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,"y key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I dont think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i dont do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/2902. In the second branch related to this pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:1750,perform,performance,1750,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['perform'],['performance']
Performance,"y need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2631,perform,performance,2631,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['perform'],['performance']
Performance,y satisfied: click==8.1.7 in /home/hadoop/.local/lib/python3.9/site-packages (8.1.7); Collecting commonmark==0.9.1; Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB); Collecting contourpy==1.1.0; Using cached contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB); Collecting cryptography==41.0.3; Using cached cryptography-41.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB); Collecting decorator==4.4.2; Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB); Collecting deprecated==1.2.14; Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB); Collecting dill==0.3.7; Using cached dill-0.3.7-py3-none-any.whl (115 kB); Collecting frozenlist==1.4.0; Using cached frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB); Collecting google-api-core==2.11.1; Using cached google_api_core-2.11.1-py3-none-any.whl (120 kB); Collecting google-auth==2.22.0; Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB); Collecting google-auth-oauthlib==0.8.0; Using cached google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB); Collecting google-cloud-core==2.3.3; Using cached google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB); Collecting google-cloud-storage==2.10.0; Using cached google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:35181,cache,cached,35181,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,y total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.060ms self 0.060ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181606,Optimiz,Optimize,181606,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.061ms self 0.061ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188631,Optimiz,Optimize,188631,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.153ms self 0.153ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:15341,Optimiz,Optimize,15341,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.356ms self 0.356ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.179ms self 0.179ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.028ms self 0.028ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.328ms self 0.115ms children 0.213ms %children 64.89%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.164ms self 0.164ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:8357,Optimiz,Optimize,8357,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.575ms self 0.049ms children 0.527ms %children 91.53%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.239ms self 0.239ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.006ms self 0.006ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.095ms self 0.095ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.014ms self 0.014ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.114ms self 0.048ms children 0.067ms %children 58.31%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:181260,Optimiz,Optimize,181260,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 0.577ms self 0.041ms children 0.536ms %children 92.90%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.256ms self 0.256ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.005ms self 0.005ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.086ms self 0.086ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.011ms self 0.011ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.123ms self 0.054ms children 0.068ms %children 55.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:188285,Optimiz,Optimize,188285,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 1.096ms self 0.044ms children 1.052ms %children 96.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.259ms self 0.259ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.012ms self 0.012ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.173ms self 0.173ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 0.025ms self 0.025ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 0.315ms self 0.116ms children 0.199ms %children 63.18%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.Opti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:14995,Optimiz,Optimize,14995,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y total 2.942ms self 2.942ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ExtractIntervalFilters.apply total 0.506ms self 0.506ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.NormalizeNames.apply total 0.500ms self 0.500ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.Simplify.apply total 7.523ms self 7.523ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply total 4.611ms self 3.590ms children 1.020ms %children 22.13%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NormalizeNames.apply total 0.442ms self 0.442ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:4066,Optimiz,Optimize,4066,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y$11.apply(ContextRDD.scala:235); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:235); 	at is.hail.sparkextras.ContextRDD$$anonfun$6.apply(ContextRDD.scala:234); 	at scala.Function1$$anonfun$andThen$1.apply(Function1.scala:52); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6427,concurren,concurrent,6427,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['concurren'],['concurrent']
Performance,y/is.hail.expr.ir.lowering.InlineApplyIR/is.hail.expr.ir.lowering.CompilableIRNoApply total 0.015ms self 0.015ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.TypeCheck.apply total 0.004ms self 0.004ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass total 0.518ms self 0.004ms children 0.514ms %children 99.20%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.lowering.AnyIR total 0.009ms self 0.009ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply total 0.496ms self 0.040ms children 0.456ms %children 91.91%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.FoldConstants.apply total 0.228ms self 0.228ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.Lo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:206128,Optimiz,OptimizePass,206128,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['OptimizePass']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.044ms self 0.044ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.938ms self 0.938ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.101ms self 0.049ms children 0.052ms %children 51.59%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:124582,Optimiz,Optimize,124582,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.045ms self 0.045ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.490ms self 0.490ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.083ms self 0.043ms children 0.040ms %children 48.02%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:156087,Optimiz,Optimize,156087,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.NestingDepth.apply total 0.058ms self 0.058ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardLets.apply/is.hail.expr.ir.TypeCheck.apply total 0.240ms self 0.240ms children 0.000ms %children 0.00%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.EvalRelationalLetsPass/is.hail.expr.ir.lowering.EvalRelationalLets.apply/is.hail.expr.ir.lowering.EvalRelationalLets.apply execute/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass/is.hail.expr.ir.CompileAndEvaluate._apply/is.hail.expr.ir.Compile.apply/is.hail.expr.ir.lowering.LoweringPipeline#apply/is.hail.expr.ir.lowering.OptimizePass/is.hail.expr.ir.Optimize.apply/is.hail.expr.ir.ForwardRelationalLets.apply total 0.087ms self 0.042ms children 0.045ms %children 51.66%; is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.CompileAndEvaluate._a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141:140300,Optimiz,Optimize,140300,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417774141,1,['Optimiz'],['Optimize']
Performance,y/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=/result.7028; 2023-09-27 16:44:33.788 JVMEntryway: ERROR: QoB Job threw an exception.; java.lang.reflect.InvocationTargetException: null; 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_382]; 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_382]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/1-day/o?name=parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g%3D/result.7028&uploadType=resumable&upload_id=ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdv7y3A6GjTh6Kv7vrUu2ap2Kv0peLVWsVTAghIs7RCZk9X3fI1BDkeHag1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:7200,concurren,concurrent,7200,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['concurren'],['concurrent']
Performance,y3-none-any.whl (114 kB); Collecting google-crc32c==1.5.0; Using cached google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB); Collecting google-resumable-media==2.5.0; Using cached google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB); Collecting googleapis-common-protos==1.60.0; Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB); Collecting humanize==1.1.0; Using cached humanize-1.1.0-py3-none-any.whl (52 kB); Collecting idna==3.4; Using cached idna-3.4-py3-none-any.whl (61 kB); Collecting isodate==0.6.1; Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB); Collecting janus==1.0.0; Using cached janus-1.0.0-py3-none-any.whl (6.9 kB); Collecting jinja2==3.1.2; Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB); Collecting jmespath==1.0.1; Using cached jmespath-1.0.1-py3-none-any.whl (20 kB); Collecting jproperties==2.1.1; Using cached jproperties-2.1.1-py2.py3-none-any.whl (17 kB); Collecting markupsafe==2.1.3; Using cached MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Collecting msal==1.23.0; Using cached msal-1.23.0-py2.py3-none-any.whl (90 kB); Collecting msal-extensions==1.0.0; Using cached msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB); Collecting msrest==0.7.1; Using cached msrest-0.7.1-py3-none-any.whl (85 kB); Collecting multidict==6.0.4; Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB); Collecting nest-asyncio==1.5.7; Using cached nest_asyncio-1.5.7-py3-none-any.whl (5.3 kB); Collecting numpy==1.25.2; Using cached numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB); Collecting oauthlib==3.2.2; Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB); Collecting orjson==3.9.5; Using cached orjson-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB); Collecting packaging==23.1; Using cached packaging-23.1-py3-none-any.whl (48 kB); Collecting pandas==2.1.0; Using cached,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:36534,cache,cached,36534,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"yTransientErrors(package.scala:77); 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$4(ServiceBackend.scala:127); 	at is.hail.backend.service.ServiceBackend$$Lambda$2191/716305671.apply$mcV$sp(Unknown Source); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.concurrent.Future$$$Lambda$2188/1126720330.apply(Unknown Source); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.Future$$Lambda$2189/609808342.apply(Unknown Source); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.Promise$$Lambda$2190/183883584.apply(Unknown Source); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). ""pool-2-thread-1"" #26 prio=5 os_prio=0 tid=0x00007f502866e000 nid=0x88c waiting on condition [0x00007f50275fd000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hail.services.Requester.requestWithHandler(Requester.scala:69); 	at is.hail.services.Requester.request(Requester.scala:94); 	at is.hail.services.memory_client.MemoryClient.write(MemoryClient.scala:45); 	at is.hail.io.fs.ServiceCacheableFS$$anon$1.close(ServiceCacheableFS.scala:46); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 	at java.io.ObjectOutputStream$BlockDataOutputStream.close(ObjectOutputStream.java:1828); 	at java.io.Objec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:3714,concurren,concurrent,3714,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['concurren'],['concurrent']
Performance,yasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cached six-1.16.0-py2.py3-none-any.whl (11 kB); Collecting sortedcontainers==2.4.0; Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB); Collecting tabulate==0.9.0; Using cached tabulate-0.9.0-py3-none-any.whl (35 kB); Collecting tenacity==8.2.3; Using cached tenacity-8.2.3-py3-none-any.whl (24 kB); Collecting tornado==6.3.3; Using ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:39227,cache,cached,39227,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['cache'],['cached']
Performance,"yeah, I have a very low prior on this changing performance. This doesn't change the staged code generation at all anyway, and most of what we care about is staged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9989#issuecomment-773456445:47,perform,performance,47,https://hail.is,https://github.com/hail-is/hail/pull/9989#issuecomment-773456445,1,['perform'],['performance']
Performance,"yeah, the `keyed=False` path is for performance and I'm OK leaving that with duplicated nodes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5692#issuecomment-479643139:36,perform,performance,36,https://hail.is,https://github.com/hail-is/hail/pull/5692#issuecomment-479643139,1,['perform'],['performance']
Performance,"yeah, we have loads of tests go through this path. If Python tests pass, I'm quite satisfied this is correct!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299:14,load,loads,14,https://hail.is,https://github.com/hail-is/hail/pull/8057#issuecomment-583559299,1,['load'],['loads']
Performance,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:198,perform,performance,198,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969,1,['perform'],['performance']
Performance,"ylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41550,cache,cache,41550,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['cache'],"['cache', 'cachetools']"
Performance,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673:62,load,loadField,62,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673,2,['load'],['loadField']
Performance,"~~Performance note: Was able to do 100k variants by 500k samples with 1000 partitions on 800 cores in 5m4s. We've never successfully done such a multiply on gCloud, but even 50k by 100k examples with the old method took a little more than 20 minutes, so safe to say this is an improvement.~~. Unfortunately, there was a bug that resulted in not all the work getting done and as such the method is not actually as performant as initial tests suggested.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520:2,Perform,Performance,2,https://hail.is,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520,2,"['Perform', 'perform']","['Performance', 'performant']"
Performance,"1yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:66,Load,Loaded,66,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,4,"['Load', 'Tune', 'cache']","['Loaded', 'Loading', 'Tuned', 'cached']"
Safety,	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.uti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5182,Unsafe,UnsafeRow,5182,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety," 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanag",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:1609,abort,aborted,1609,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['abort'],['aborted']
Safety, 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:1311,recover,recover,1311,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Safety," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:1243,avoid,avoid,1243,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['avoid'],['avoid']
Safety," of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:1116,recover,recover,1116,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Safety," to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 11, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 76, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 237, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-nhxn5owt', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.jar,ZIP=gs://hail-ci-0-1/temp/ba8134f0f121a49ea96d7dd30ea3be330802cfef/784ab2796878cd2f825c554e80d29d304f21d0f4/hail.zip', '--properties=spark:spark.driver.memory=41g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-init-docker.sh', '--master-machine-type=n1-highmem-8', '--master-boot-disk-size=100GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-highmem-8', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=40m']' returned non-zero exit status 1.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515:2047,timeout,timeout,2047,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-431996515,2,['timeout'],['timeout']
Safety," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2448,detect,detected,2448,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['detect'],['detected']
Safety,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1085,unsafe,unsafe,1085,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:440,safe,safe,440,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573,1,['safe'],['safe']
Safety,$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3227,abort,abortStage,3227,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['abort'],['abortStage']
Safety,$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201647,abort,abortStage,201647,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['abort'],['abortStage']
Safety,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2756,abort,abortStage,2756,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['abort'],['abortStage']
Safety,"(also safer, it was totally possible to inadvertently segfault with the previous version, this one guards against that)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9981#issuecomment-772624116:6,safe,safer,6,https://hail.is,https://github.com/hail-is/hail/pull/9981#issuecomment-772624116,1,['safe'],['safer']
Safety,); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3970,abort,abortStage,3970,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['abortStage']
Safety,); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5532,abort,abortStage,5532,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['abortStage']
Safety,", output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.Text.toString(Text.java:280); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at org.apache.spark.SparkContext$$anonfun$textFile$1$$anonfun$apply$8.apply(SparkContext.scala:833); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2483,abort,aborted,2483,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['aborted']
Safety,". Then the paper computes a $p \times n$ matrix $W$ called the **SNP weight matrix**:. $$W \coloneqq X^T U.$$. Suppose that there are $n_r$ individuals in the related set and let $Y$ be the $n_r \times p$ standardized genotype matrix for the related individuals. The paper computes the principal components associated with the related samples with. $$ \frac{1}{p} Y W (\Sigma^2)^{-1}.$$. ### Simplifications. The first simplification that I noticed is that we can do away with the $\frac{1}{p}$ terms. Because $\Psi$ is scaled by $p^{-1}$, the inverse of the eigenvalues, $(\Sigma^2)^{-1}$ is scaled by $p$, which cancels out the $1/p$ term in the calculation of the principal components for the related individuals. From here on, let us redefine $\Sigma^2$ as the diagonal matrix containing the eigenvalues of $XX^T$ (not $\frac{1}{p} XX^T$). Next, by examining the relationship between singular value decomposition (SVD) and eigendecomposition ([Wikipedia link](https://en.m.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition)), I realized that it is not necessary to compute $\Psi$. Instead, we can get $U$ and $\Sigma$ from the SVD of $X$:. $$X = U\Sigma V^T,$$. where $V$ is a $p \times p$ basis of the new PCA coordinate space. Then while investigating the meaning of $W$, I realized that $W = X^T U = V \Sigma^T U^T U = V \Sigma^T$. Taking these simplifications into account, I realized that the paper is, in essence, computing $Y V$ to get the predicted scores associated with the related individuals. (Technically, I think that the paper is computing $Y V \Sigma^{-1}$. I am not sure why they scale the columns by the reciprocal of the eigenvalues here.). ### Simplified Approach. As I understand it, $V$ is a change-of-basis matrix from the original coordinates to the PCA scores. Letting $G$ denote the full standardized genotype matrix of all the individuals, I think we can just return $GV$, where $V$ is defined by the SVD of $X$:. $$X = U\Sigma V^T.$$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184:2045,predict,predicted,2045,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1962279184,2,['predict'],['predicted']
Safety,". debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn't cause trouble.; ; In short, it's a can of worms. Avoiding std::string in libhail.so keeps the can closed for now.; And I believe dataproc will move to using debian9 images as the default in November, so at; some point the need to support old-ABI systems (debian8) will diminish and possibly go away completely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:2053,Avoid,Avoiding,2053,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,1,['Avoid'],['Avoiding']
Safety,".; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated code generated for an SQL query, so very much in; > the same ballpark as parts of Hail),; > which contributes to my concern about this, especially on linux where g++; > is the default.; >; > So in the long run I expect we'll ship a compiler, or specify a compiler.; > But that becomes a problem in itself; > if we want the sh",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:1177,risk,risks,1177,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,1,['risk'],['risks']
Safety,.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4886,Timeout,TimeoutTimeoutSubscriber,4886,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['Timeout'],['TimeoutTimeoutSubscriber']
Safety,".nt2) &; (mt.alleles[1] == mt.ss.nt1)) |; ((flip_text(mt.alleles[0]) == mt.ss.nt2) &; (flip_text(mt.alleles[1]) == mt.ss.nt1)),; mt.ss.ldpred_inf_beta); .or_missing()). # filter bgen matrixtable down to only SNPs with betas; mt = mt.filter_rows(hl.is_defined(mt.beta)). # filter bgen matrixtable to only include people in scoring sample; mt = mt.filter_cols(hl.is_defined(sampleids[mt.s])). # score sample; mt = mt.annotate_cols(prs=hl.agg.sum(mt.beta * mt.dosage)). # write out table with sample IDs and PRS scores; mt.cols().export('gs://ukbb_prs/prs/UKB_'+pheno+'_PRS_22.txt'). parser = argparse.ArgumentParser(); parser.add_argument(""--phenotype"", help=""name of the sumstat phenotype""); args = parser.parse_args(). try:; start = time.time(); main(args.phenotype); end = time.time(); message = ""Success! Job was completed in %s"" % time.strftime(""%H:%M:%S"", time.gmtime(end - start)); send_message(message); except Exception as e:; send_message(""Fail.""); ```. ""Failure Reason"":; ```; Job aborted due to stage failure: Task 98 in stage 13.0 failed 20 times, most recent failure: Lost task 98.19 in stage 13.0 (TID 22699, ccarey-sw-xt4j.c.ukbb-robinson.internal, executor 68): java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rv",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:2826,abort,aborted,2826,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['abort'],['aborted']
Safety,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1156,timeout,timeout,1156,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,6,"['abort', 'timeout']","['aborting', 'timeout']"
Safety,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:2562,avoid,avoid,2562,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,2,['avoid'],['avoid']
Safety,"1. I'm not sure why we don't throw an error. My bash isn't good enough to run both commands and then detect if either failed if the exit code is indeed not equal to 0. My only thought is that maybe the exit code isn't 0 if there are no VMs or disks to delete. 2. Yes, I'll see if I can PR the fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046:101,detect,detect,101,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737459046,1,['detect'],['detect']
Safety,"1. It's feasible to build and ship the compiler + libraries for a limited number of known platforms; (at Physics Speed I did this for Ubuntu-16.04 and one particular version of CentOS). It gets nuts if; you have many different OS'es each of which needs its own compiler build (and it then becomes; another build-system/packaging issue to get all those compilers built correctly for each OS).; Possibly a good thing to do in the long run. Probably not something I could do in the limited time; available. 2. If you build your own compiler + library, then you risk becoming incompatible with other ; libraries on the target system which were built against that system's ""standard"" compiler; and library and header files. e.g. BLAS. [Though this only applies to libraries compiled from C++,; not libraries in C, which might conatin the damage]. So it seemed like the least disruptive path in the short term was to excise the few uses of; std::string and std::stringstream, so that we can build a libhail.so which should work across; a wide variety of Linux systems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733:558,risk,risk,558,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424744733,1,['risk'],['risk']
Safety,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5571,timeout,timeout,5571,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,6,['timeout'],['timeout']
Safety,"181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2427,detect,detector,2427,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['detect'],['detector']
Safety,209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5261,Unsafe,UnsafeRow,5261,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:9406,timeout,timeout,9406,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['timeout'],['timeout']
Safety,"4:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5232,abort,abortStage,5232,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['abortStage']
Safety,"4; 1135 for temp_arg in temp_args:. /Users/tpoterba/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.pyc in deco(*a, **kw); 61 def deco(*a, **kw):; 62 try:; ---> 63 return f(*a, **kw); 64 except py4j.protocol.Py4JJavaError as e:; 65 s = e.java_exception.toString(). /Users/tpoterba/hail/python/hail/java.py in deco(*a, **kw); 109 # deepest = env.jutils.deepestMessage(e.java_exception); 110 # msg = env.jutils.getMinimalMessage(e.java_exception); --> 111 raise FatalError('%s\n\nJava stack trace:\n%s\n\nERROR SUMMARY: %s' % (deepest, full, deepest)); 112 except py4j.protocol.Py4JError as e:; 113 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi... Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): is.hail.utils.HailException: malformed.vcf: caught htsjdk.tribble.TribbleException$InternalCodecException: The allele with index 10026357 is not defined in the REF/ALT columns in the record; offending line: 20	10026348	.	A	G	7535.22	PASS	HWP=1.0;AC=2;culprit=Inbreedi...; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:10); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.utils.TextContext.wrapException(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:1798,abort,aborted,1798,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['aborted']
Safety,"541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1222,timeout,timeout,1222,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['timeout'],['timeout']
Safety,"6); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:77); at is.hail.variant.MatrixTable.countRows(MatrixTable.scala:552); at is.hail.variant.MatrixTable.count(MatrixTable.scala:550); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:745). Hail version: 0.2.4-d602a3d7472d; Error summary: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:11447,abort,aborted,11447,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['aborted']
Safety,"627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 12:51:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 12:51:55 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:55 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 12:51:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; Impo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:1298,detect,detected,1298,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['detect'],['detected']
Safety,"627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; /usr/local/lib/python3.5/site-packages/IPython/core/history.py:228: UserWarning: IPython History requires SQLite, your history will not be saved; warn(""IPython History requires SQLite, your history will not be saved""); Python 3.5.2 (default, Jul 12 2017, 14:00:23) ; Type ""copyright"", ""credits"" or ""license"" for more information. IPython 5.1.0 -- An enhanced Interactive Python.; ? -> Introduction and overview of IPython's features.; %quickref -> Quick reference.; help -> Python's own help system.; object? -> Details about 'object', use 'object??' for extra details.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 08:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 08:41:32 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 08:41:32 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: ; ```; -----------------------------; Step2 : read the file with sc.textFile; ```; In [1]: rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); ```; -----------------------------; Step3, import hail an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:1282,detect,detected,1282,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,1,['detect'],['detected']
Safety,"7,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2266,Detect,Detected,2266,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['Detect'],['Detected']
Safety,78); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3307,abort,abortStage,3307,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['abort'],['abortStage']
Safety,"8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:3308,abort,aborted,3308,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['abort'],['aborted']
Safety,": global model fit: beta = Map(intercept -> 0.0370042272400176, sa.cov -> -0.012886009824596447); 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: globa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5635,detect,detected,5635,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,1,['detect'],['detected']
Safety,:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12422,abort,abortStage,12422,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['abort'],['abortStage']
Safety,; 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1795); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1158); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3129,abort,abortStage,3129,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['abort'],['abortStage']
Safety,"; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7656,abort,aborted,7656,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['aborted']
Safety,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2478,timeout,timeoutSeconds,2478,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['timeout'],['timeoutSeconds']
Safety,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:3061,unsafe,unsafe,3061,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403:159,avoid,avoid,159,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403,2,['avoid'],['avoid']
Safety,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:78,avoid,avoid,78,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391,2,['avoid'],['avoid']
Safety,"> A couple notes. I'm not sure what this should ultimately look like. I think I want `hailctl create user`, which probably creates a batch that does all the creation? That avoids setting up tunnels to the sql server, etc. Sure, if that's the way we want this to work, I will modify to do that. Seems more elegant than having tunnels, which predates our current deployment solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206:172,avoid,avoids,172,https://hail.is,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206,1,['avoid'],['avoids']
Safety,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:54,Timeout,TimeoutError,54,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,3,['Timeout'],['TimeoutError']
Safety,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:477,avoid,avoid,477,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,4,"['avoid', 'timeout']","['avoid', 'timeouts']"
Safety,"> And there's a slight question about whether your libstdc++ will work against the other systems libc.so. Is there something special about libstdc++ here? Our code will certainly call libc directly, too. As far as I know there no proposal on the table handles incompatible libc's. I think we'd have to make multiple builds. Luckily, as far as I know, recent distributions have compatible libc so this shouldn't be an issue. > Avoiding std::string in libhail.so keeps the can closed for now. Can you explain why std::string is special here vs the rest of the standard library? What assumption are you working under?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424794222:426,Avoid,Avoiding,426,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424794222,1,['Avoid'],['Avoiding']
Safety,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664:655,Avoid,Avoid,655,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664,1,['Avoid'],['Avoid']
Safety,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:296,timeout,timeouts,296,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['timeout'],['timeouts']
Safety,"> Forks would indeed need to overwrite ours, but since the file wouldn't change much it seems like that's not much of a hassle to maintain, right Leo? And ya this seems like a fine change but we would need to follow up right afterward with our own credentials. Yes, alternatively we could also use the GitHub organization name or something similar when constructing the file path to encrypted credentials, to avoid collisions completely. (Forks like the CPG one would only add files to their deployments.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130:409,avoid,avoid,409,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130,1,['avoid'],['avoid']
Safety,"> I do not know why the retries setting in pip.conf did not catch https://ci.hail.is/batches/167314/jobs/27, but more retries never hurt anyone. Is the whole pip process crashing because of the exception? Perhaps a different kind of crash that pip can't recover from and retry. > Another CI-related PR. This one changes the base image of everything else: hail-ubuntu. It's an ubuntu image with two scripts that make pip and apt more resilient. Take a look at docker/hail-ubuntu. Should we do this for `apt` too?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129:254,recover,recover,254,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767033129,1,['recover'],['recover']
Safety,"> I don't see how a hard coded list in the repo is any different than a compare and swap with the SKU stored in the database. I don't think we can get around parsing the description in some form when we initialize the product. . I think I'm missing context here, especially surrounding what the source of truth is for a ""product"". Maybe we should have a meeting or discussion in a different forum at some point? I don't know how volatile the product descriptions are, but my current understanding is that this is all pretty low risk/priority. For posterity, my main concern is around reproducibility. Assume a description change happens tomorrow and now however we're parsing the description ends up pointing at a different SKU. Now at any point in the future, if we have some data loss and have to rebuild these tables, the SKU will suddenly change, but only because we rebuilt the database not because of some actual change in GCP. Or if we or anyone else stands up a new batch cluster, they will get a SKU that differs from what we have currently in `hail-vdc`, just because they started up their cluster at a different point in time. With my current understanding that is an undesirable scenario, but I don't actually know what the implications of such a scenario are.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736105248:528,risk,risk,528,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736105248,1,['risk'],['risk']
Safety,"> I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our pinned-requirements.txt use pandas 2.0, fix whatever issues arise, but leave requirements.txt flexible for folks?. I think there are compromises either way, but I would be surprised if this just worked. It seems very easy to accidentally adopt new functionality so at that point why even have a lower-bound? I think that, while it's very hard to make sure that all our dependencies are compatible at all possible version combinations, and these things will happen, it just feels like an easily-avoidable lie to say we support 1.x and 2.x but then use functionality exclusive to 2.x. So I'm ok keeping the bounds more relaxed if we can guarantee that *our* usage of pandas is compatible with both. FWIW, I think that our primary dependencies release major versions infrequently enough that it is reasonable to only support the most recent major version, in much the same way that we don't support python versions indefinitely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573:631,avoid,avoidable,631,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573,1,['avoid'],['avoidable']
Safety,"> I kinda prefer a fresh file with freshly written tests? I feel like it's a bit hard to get a total view of what is and is not tests when we're using annotations. I agree a single file feels nice, but I'm a little hesitant to copy paste tests. Unless you think I should move where these tests are? That also feels weird. Does the following pytest invocation make you feel better about markers?. ```; (hail) dgoldste@wmce3-cb7 hail % pytest --collect-only -m qobtest hail/python/test; ============================================================================== test session starts ===============================================================================; platform darwin -- Python 3.9.17, pytest-7.4.0, pluggy-1.3.0; rootdir: /Users/dgoldste/hail/hail/python/test; configfile: pytest.ini; plugins: anyio-4.0.0, xdist-2.5.0, instafail-0.5.0, timeout-2.1.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, forked-1.6.0; asyncio: mode=auto; collected 8218 items / 8133 deselected / 85 selected. <Package hail>; <Module test_context.py>; <Function test_init_hail_context_twice>; <Function test_top_level_functions_are_do_not_error>; <Function test_tmpdir_runs>; <Module test_randomness.py>; <Function test_table_explode>; <Package backend>; <Module test_service_backend.py>; <Function test_tiny_driver_has_tiny_memory>; <Function test_big_driver_has_big_memory>; <Function test_tiny_worker_has_tiny_memory>; <Function test_big_worker_has_big_memory>; <Function test_regions>; <Package expr>; <Module test_expr.py>; <UnitTestCase Tests>; <TestCaseFunction test_aggregators>; <TestCaseFunction test_densify_table>; <TestCaseFunction test_scan>; <Package genetics>; <Module test_reference_genome.py>; <Function test_reference_genome>; <Function test_reference_genome_sequence>; <Function test_reference_genome_liftover>; <Function test_read_custom_reference_genome>; <Package matrixtable>; <Module test_grouped_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851:851,timeout,timeout-,851,https://hail.is,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851,1,['timeout'],['timeout-']
Safety,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494:157,avoid,avoid,157,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494,2,['avoid'],['avoid']
Safety,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731:555,avoid,avoid,555,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731,2,['avoid'],['avoid']
Safety,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:699,safe,safe,699,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039,1,['safe'],['safe']
Safety,"> I'm not sure how this is unsafe. Could you explain?. Partitioner just says what elements are in what partitions. OrderedRDD also guarantees those elements are in a specific order. It is not safe to take an RDD partitioned with an OrderedPartitioner and just ""make"" it an OrderedRDD. Case where it fails: in read, union, which creates a partition-aware RDD that unions corresponding partition from a set of identically partitioned RDDs. > i think the better solution is to check that the ordered key inside the partitioner is the same as the implicit ordered key supplied. this also could have caught the errors. The error was making it into an OrderedRDD without sorting. It is perfectly valid to have an OrderedPartitioner-partitioned RDD that is sorted. (Above read case.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349:27,unsafe,unsafe,27,https://hail.is,https://github.com/hail-is/hail/pull/1864#issuecomment-303237349,2,"['safe', 'unsafe']","['safe', 'unsafe']"
Safety,"> If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?. That's correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889:93,safe,safely,93,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004244889,1,['safe'],['safely']
Safety,"> In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. Unless I'm missing something, this code splitter does rely on the evaluation order, which is why I asked about it. If a statement `x` has large children, it moves them to execute before `x`, evaluated in the `children` array order (more precisely, in the post-order traversal order), storing their results in locals. I think it's possible to avoid relying on order of evaluation, if instead of only splitting out sequences of statements, we can also split out a `ValueX` directly, replacing it by a call to new method which returns the result. I don't have strong feelings about whether the order should be guaranteed, just want to be clear which it is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246:28,safe,safest,28,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645546246,4,"['avoid', 'safe']","['avoid', 'safest']"
Safety,> Mainly that the commit procedure branches on whether the start id is 1. I was trying to avoid an expensive query to update all of the n_pending_parents when we know it's the first update being committed. I think I'd prefer to keep this branching there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755:90,avoid,avoid,90,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755,1,['avoid'],['avoid']
Safety,"> No tests for this right now, although I can write . I think an UnsafeSuite test for this should be easy and effective.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357:65,Unsafe,UnsafeSuite,65,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357,1,['Unsafe'],['UnsafeSuite']
Safety,"> One conceptual comment: you described SUnreachable as a type representing a value that will never be constructed at runtime, i.e. a bottom type. But I think what you've actually built is something different, that may have other uses. Each virtual type has a designated ""default value"", and the corresponding SUnreachable type is a subtype which can only have that default value, with a compact (usually 0 bytes) runtime representation. I do think the usages right now do fit this description - any usage of a SUnreachableCode will never be on a live path at runtime. I think any of the methods on SUnreachableCode/Value could safely be replaced with thrown exceptions. That's not to say there might be other applications this helps streamline! Do you have any specific ideas?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10539#issuecomment-870772732:628,safe,safely,628,https://hail.is,https://github.com/hail-is/hail/pull/10539#issuecomment-870772732,1,['safe'],['safely']
Safety,"> Only external requests go to the gateway. Why doesn't this do that (where baseurl is `http://{service}.namespace`. ```; base_url = internal_base_url(namespace, name, port); async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5.0)) as session:; while True:; try:; async with session.get(f'{base_url}/healthcheck') as resp:; ```. edit: Ah it's just routing through Kubernetes. I thought there would need to be something that specified the dns to use... Ok.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738:208,timeout,timeout,208,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738,1,['timeout'],['timeout']
Safety,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:417,avoid,avoid,417,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,1,['avoid'],['avoid']
Safety,"> So I just put back the dependencies on `native-lib-prebuilt`. Since that just calls make recursively, it would probably be better to let mill invoke make, but I didn't want to deal with that, and this is a pretty uncommon use case (I think). That's sane to me, and avoids needing to deal with the make jobserver at the mill level. Feel free to un-WIP this whenever you're ready to merge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252:267,avoid,avoids,267,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930537252,1,['avoid'],['avoids']
Safety,"> Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?. With the array decoding, I suspect a lot of the speedup wasn't from avoiding branches, but from avoiding a bunch of extra operations handling missing bits one at a time, especially computing the address of the containing byte and loading it from memory every time. We should be able to do something similar for structs, though it will be more complicated. I think that's worth trying independently of trying to reduce branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107:607,avoid,avoiding,607,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107,2,['avoid'],['avoiding']
Safety,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:1202,avoid,avoid,1202,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375,2,['avoid'],['avoid']
Safety,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596:1278,risk,risk,1278,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596,2,['risk'],['risk']
Safety,"> This suggests to me that the dataframe created by hail maintains reference to hail objects and pandas is attempting to recreate these objects when unpickling. I suspect this is not intentional. Hi @anh151, you are correct that `to_pandas` is creating dataframes that contain hail objects. In your example, the hail type in question is the [`Locus`](https://hail.is/docs/0.2/genetics/hail.genetics.Locus.html#locus), but we also have a couple auxiliary classes like `Interval` and `Call` that could end up in the pandas table. I see how this can be unintuitive especially with your point of round-tripping through CSV (which uses the `str` of the object by default and thus will avoid the class lookup on read), but I hesitate to call it unintentional. I'll broach this question with the team as to what would be the least confusing behavior, but I suspect many users are using `to_pandas` results in the same hail session in which case it might be expected to get small hail objects in their result. In the meantime, you can look at the schema of your resultant table and translate within pandas to your desired representation, i.e. convert Locus entries to dicts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14004#issuecomment-1813149330:680,avoid,avoid,680,https://hail.is,https://github.com/hail-is/hail/issues/14004#issuecomment-1813149330,1,['avoid'],['avoid']
Safety,"> ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. Without a concrete example, I'm having some trouble understanding what this problem is. What IR nodes can return a container type that aren't included in the streamify match? Isn't it just ToSet/ToDict/GroupByKey? Calling `boundary` on a ToSet is no problem at all, I think. > ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). Can you write such an IR, and what it should return after streamify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159:107,safe,safe,107,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586582159,2,['safe'],['safe']
Safety,"> Using the automatic Json deserializer kind of makes us write the wrong code here, because we really want one LZ4CodecSpec which takes the appropriate parameters, and then deserializer can construct it with the proper arguments based on the specific case being derserialized (a legacy format, or the current one with parameters). Yeah, I agree. I think it would be a couple days of work to write ser/deser stuff that avoids the json4s case class extraction, but I'm not sure how high-priority that is right now since this stuff is somewhat stable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9292#issuecomment-675510814:418,avoid,avoids,418,https://hail.is,https://github.com/hail-is/hail/pull/9292#issuecomment-675510814,1,['avoid'],['avoids']
Safety,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:1067,avoid,avoiding,1067,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,2,['avoid'],['avoiding']
Safety,"> Would it just be index which is the correct path relative to the metadata file?. Yes. So you should take the index path in the metadata interpreted with respect to the base of the index file (directory) rather than hardcoding it. This might seem like overkill but this flexibility has proved useful for the Matrix/Table format and I'd like to copy the idiom here. > I don't use the firstKeyOffset in the internal nodes anywhere. What were you envisioning it would be used for? Otherwise, I think we should delete it. So having indices like this effectively make a Matrix/Table arbitrarily repartitionable. For example, we can double the partitioning for free by splitting one partition into two by splitting at the roughly the midpoint row which I want to get from the root block. That's what I was thinking firstKeyOffset would be used for. This would be for a rough split. For a split accurate to the row-level, we'd have to read down to the leaf node blocks. Maybe this isn't really worth it. I'm on the fence. Delete it if you want. I thought of one more thing we should support in the file: store arbitrary user annotations of the keys. If the annotation is `+struct {}` it would have no overhead. This will give us some future flexibility and I can imagine the following use case: When we go to a sparse VCF, we'll want to store ""checkpoint"" rows to avoid having to search backward arbitrarily far to find ref blocks that might overlap with our variant of interest. The alternative is to make the first row of each partition a checkpoint block, but in that case, we can no longer seek into the middle of the partition if we want to track overlapping ref blocks. So we want to search for the first checkpoint row before our row of interest. Can we add this to the format but not use it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897:1358,avoid,avoid,1358,https://hail.is,https://github.com/hail-is/hail/pull/4049#issuecomment-412184897,1,['avoid'],['avoid']
Safety,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:337,risk,risk,337,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170,2,['risk'],['risk']
Safety,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:73,risk,risk,73,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370,2,['risk'],"['risk', 'risks']"
Safety,> the intention of this PR is to make sure every pool has 1 core?. Yes. I will increase the timeout to 30 minutes to prevent an inopportune restart.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13150#issuecomment-1581433995:92,timeout,timeout,92,https://hail.is,https://github.com/hail-is/hail/pull/13150#issuecomment-1581433995,1,['timeout'],['timeout']
Safety,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1014,detect,detects,1014,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748,1,['detect'],['detects']
Safety,"@asvetlov thanks for your reply, and for your work on the Sanic project! I was really curious about the Techempower issue. Do you know why Sanic, on past rounds failed to complete test subsets without error? I havent had much of a chance to look into that yet, but https://github.com/huge-success/sanic/issues/53 doesnt divulge much, and my own attempts to give Sanic problems havent yielded anything worrisome (i.e asyncpg works great under 2000 simultaneous connection load, request standard deviation is about as tight as aiohttp, and number of extremes / timeouts is smaller than aiohttp). Techwmpower benchmark was on version 0.7, if not earlier (the linked file in the Techempower issue is 0.7), and that version may have been affected by the issue described here: https://github.com/huge-success/sanic/issues/1176 which seems to have been largely addressed. . Edit: furthermore, other recent tests showed no significant issues with Sanic https://fgimian.github.io/blog/2018/06/05/python-api-framework-benchmarks/. Still the addressing the Techempower issues may help people feel more confident.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481:562,timeout,timeouts,562,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461425481,1,['timeout'],['timeouts']
Safety,@catoverdrive `updateKey` is the method you're looking for in the unsafe case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2424#issuecomment-344368487:66,unsafe,unsafe,66,https://hail.is,https://github.com/hail-is/hail/pull/2424#issuecomment-344368487,1,['unsafe'],['unsafe']
Safety,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966:305,redund,redundant,305,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966,2,['redund'],['redundant']
Safety,"@chrisvittal Just to draw your attention: I removed my wrapper for FsDataInputStream (HailInputStream).... it is used prominently in BGzipInputStream, which relies on Hadoop compression libraries. I found it more complicated to avoid FsDataInputStream, with I think little obvious benefit to doing so: the [FSDataInputStream constructor just takes an InputStream](https://hadoop.apache.org/docs/r3.0.3/api/org/apache/hadoop/fs/FSDataInputStream.html), which is Java stdlib, which I assume will be returned by most future FS implementations. Happy to explore a different solution if you think there's a better abstraction here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054:228,avoid,avoid,228,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492742054,1,['avoid'],['avoid']
Safety,"@chrisvittal: Ed has already used mill on his machine, and helped with the PR. It would be great to both get your eyes on the changes, as well as checking out the branch and testing the setup instructions above (I've been using a seperate git worktree to avoid clobbering my gradle based intellij project).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1912321464:255,avoid,avoid,255,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1912321464,1,['avoid'],['avoid']
Safety,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:616,avoid,avoid,616,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,1,['avoid'],['avoid']
Safety,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252825829:762,abort,aborted,762,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829,2,['abort'],['aborted']
Safety,"@cseed As I briefly mentioned in an email on Saturday, I moved notebook loading to a prefetched model. Data and page loading are decoupled: When one clicks on the Notebook link, routing to the page happens without waiting for data to come in from notebook-api.hail.is. If data is available, it is shown, else a loading indicator. To avoid loading screens, I call notebook-api.hail.is asynchronously immediately after a user hits app.hail.is, whether they're on the Notebook page or not. Therefore, typically a user will never see a Notebook page loading indicator, when they click from one page to Notebook (say they land on the home page: by the time they click on Notebook, the data is fetched, so no loading screen). Furthermore, web sockets keep that Notebook data up to date, again regardless of whether a user is on the Notebook page. So in all instances except when a user lands on Notebook directly, the time it takes to reach the Notebook page is << 16ms (seemingly ~3ms). A similar approach is now used for scorecard, except Scorecard will render data in the SSR phase (because I consider scorecard less important, and because most users of the web app won't need it), and will not prefetch data when on another page. However, clicking on ""Scorecard"" from another page will not cause routing to block while waiting for the https://scorecard.hail.is/json response; instead a loading indicator will be shown. In practice I find this preferable, because the GUI feels more responsive, and users get more feedback. This demonstrates the core benefit of universal rendering approaches, and how they can improve page loading times over even the leanest server-side rendered application.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996:333,avoid,avoid,333,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-462221996,2,['avoid'],['avoid']
Safety,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:519,detect,detect,519,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039,1,['detect'],['detect']
Safety,"@cseed I managed to reproduce the problem you were seeing on a cluster. It's not a problem with the size of the table (as I previously suspected), but rather a race condition caused by dropping a table then recreating it immediately. Adding a sleep avoids the problem for the time being. There are stil some merge conflicts due to the way that the metadata is stored. The best thing would be to factor out the code to do that so it can be shared by the Parquet and Kudu code paths. I'll have a go at doing that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-213006430:249,avoid,avoids,249,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-213006430,1,['avoid'],['avoids']
Safety,"@cseed back to you, I inserted existential types everywhere. I managed this with only two `unsafe` functions: in let-binding and in lambda aggregators. In both cases, the type of the `Code[T]` should be correct if the compilation process is correct. Adding a `checkcast` would create a tremendous amount of unnecessary byte code. I suspect returning the type as a part of the compilation process would enable me to remove the let-binding `unsafe`. I'm not sure about the lambda aggregator case. I'm unconcerned since all that code is disappearing soon anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1695#issuecomment-296220668:91,unsafe,unsafe,91,https://hail.is,https://github.com/hail-is/hail/pull/1695#issuecomment-296220668,2,['unsafe'],['unsafe']
Safety,"@daniel-goldstein , I love this change, can we just make isort run on `batch/batch`, `ci/ci`, etc. that avoids all the sql migrations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11231#issuecomment-1050062744:104,avoid,avoids,104,https://hail.is,https://github.com/hail-is/hail/pull/11231#issuecomment-1050062744,1,['avoid'],['avoids']
Safety,"@danking . The redirect was caused by lack of X-Forwarded-Host + X-Forwarded-Proto; * https://github.com/expressjs/express/blob/b8e50568af9c73ef1ade434e92c60d389868361d/lib/request.js#L429; * remoteAddress is the url from the last hop, not any forwarded address. > I'll revisit why ghost issues redirects with the new changes this afternoon.; Ghost doesn't read this header. They use X-Forwarded* headers, via Express: https://expressjs.com/en/guide/behind-proxies.html ; * Expresses finds ips using proxy-addr package (the getter is req.ips): https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/lib/request.js#L349; * The test: https://github.com/expressjs/express/blob/3ed5090ca91f6a387e66370d57ead94d886275e1/test/req.ip.js; * proxy-addr does not support x-real-ip: https://github.com/jshttp/proxy-addr/issues/15. Ghost apparently uses X-Forwarded-For to rate limit malicious addresses:. * ""6. Include the X-Forwarded-For header, populated with the remote IP of the original request.; Without this, we aren't able to detect spam traffic patterns and your site risks being rate limited or incorrectly restricted.""; * https://ghost.org/faq/can-i-run-ghost-from-a-subdirectory/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066:1051,detect,detect,1051,https://hail.is,https://github.com/hail-is/hail/pull/8107#issuecomment-587687066,4,"['detect', 'risk']","['detect', 'risks']"
Safety,"@danking ; The info is as follows:; ```; [root@mg hail]# su hdfs; bash-4.2$ pyspark --jars $HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar --conf=spark.executor.extraClassPath=./hail-all-spark.jar; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Aug 4 2017, 00:39:18) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; 17/10/19 08:45:43 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Aug 4 2017 00:39:18); SparkSession available as 'spark'.; >>> import hail; >>> hc = hail.HailContext(); log4j:ERROR setFile(null,false) call failed.; java.io.FileNotFoundException: hail.log (Permission denied); 	at java.io.FileOutputStream.open0(Native Method); 	at java.io.FileOutputStream.open(FileOutputStream.java:270); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:213); 	at java.io.FileOutputStream.<init>(FileOutputStream.java:133); 	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294); 	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165); 	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172); 	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104); 	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842); 	at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198:390,detect,detected,390,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337768198,1,['detect'],['detected']
Safety,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:72,timeout,timeout,72,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840,2,['timeout'],['timeout']
Safety,@danking I made a fix that should make it so we don't need to manually delete instances and batch should recover.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975:105,recover,recover,105,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593645975,1,['recover'],['recover']
Safety,"@danking I'm pretty sure using UnsafeRows doesn't work in this case. When I changed from UnsafeRow to SafeRow, the tests were passing on a saved random dataset that wasn't previously working. This depends on #3724 so you can ignore changes in Relational.scala and the python tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3720#issuecomment-395741181:31,Unsafe,UnsafeRows,31,https://hail.is,https://github.com/hail-is/hail/pull/3720#issuecomment-395741181,3,"['Safe', 'Unsafe']","['SafeRow', 'UnsafeRow', 'UnsafeRows']"
Safety,"@danking If Konrad is fine with this for now and Python 3 will fix the issue, I think we should keep this PR as is and avoid adding additional dependencies. I'll add checking the time zone to the 0.2 to-do list and we can revisit once we've switched to Python 3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2243#issuecomment-331923128:119,avoid,avoid,119,https://hail.is,https://github.com/hail-is/hail/pull/2243#issuecomment-331923128,1,['avoid'],['avoid']
Safety,"@danking Sorry to keep making you break things out, but it is really helpful for me and the changes will go in faster. Can you make a separate PR with the following changes that don't relate to passing the indices and the new index code? Specifically, the following items from your list:. ```; added row_fields which prevents reading and allocation of LID and RSID (also improved python-type-checking for row_fields and entry_fields). I changed several asserts to if's with fatals, so as not to allocate strings. We no longer copy the genotype data into a buffer in the block reader. This was forcing the fastKeys to do an unnecessary data copy. I changed the contract on BgenRecord to require that getValue is called to ""consume"" the record before the next record is taken. getValue(null) just skips bytes (no copy, no decompression). I added RegionValueBuilder.unsafeAdvance which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work. I use RegionValueBuilder.unsafeAdvance to make loading a BGEN without entry fields very fast. I fixed Table.index to not trigger a partition key info gathering; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018:863,unsafe,unsafeAdvance,863,https://hail.is,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018,2,['unsafe'],['unsafeAdvance']
Safety,"@danking This is finally passing. I had to change the test_ci to timeout after 30 minutes. Do you want to do the migration? If not, then I'll figure out how to try and make a faster VM later today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-615284738:65,timeout,timeout,65,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-615284738,1,['timeout'],['timeout']
Safety,@danking This is passing now. This is now primarily a PR that avoids async / sync / async.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1794917590:62,avoid,avoids,62,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1794917590,1,['avoid'],['avoids']
Safety,"@danking addressed the unsafe issue, sha224 (largest that results in a <63 character hex digest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476#issuecomment-468858796:23,unsafe,unsafe,23,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468858796,1,['unsafe'],['unsafe']
Safety,@danking it seems to me that this is exactly the situation for which they exposed `unsafeValueAt`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194:83,unsafe,unsafeValueAt,83,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194,1,['unsafe'],['unsafeValueAt']
Safety,"@jigold Can you elaborate on this one? Timeout to me means testing times out, which does happen in our current CI. Should it be closed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5745#issuecomment-555097320:39,Timeout,Timeout,39,https://hail.is,https://github.com/hail-is/hail/issues/5745#issuecomment-555097320,1,['Timeout'],['Timeout']
Safety,@jigold I just rebased on the current main. Hopefully that will help avoid rebuilding the base image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-723167140:69,avoid,avoid,69,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723167140,1,['avoid'],['avoid']
Safety,@jigold sorry I pushed a change literally as you approved it! It was just removing a redundant cast of `this.asInstanceOf[ReferenceGenome]` that was called on a ReferenceGenome object.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201:85,redund,redundant,85,https://hail.is,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201,1,['redund'],['redundant']
Safety,@jigold this is a minimal adaptation of #3466 which avoids exposing RowMatrix by putting an export command on BlockMatrix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838:52,avoid,avoids,52,https://hail.is,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838,1,['avoid'],['avoids']
Safety,"@jjfarrell Thanks for sharing that! This is really interesting information. I'm quite surprised, but the evidence is pointing to there being a PC that largely separates those 8 replicates from the entire remaining dataset (!!!). I'm personally quite surprised that 8 samples out of thousands could pull this off, but that definitely seems to be the issue, given that the PCs from PC-AiR avoid the issue. Thank you so much for hunting this down! It's very valuable information for us. ### Next Steps . So, clearly we need a solution for users that have substantial numbers of related individuals in their source dataset (especially if the pedigrees are unknown). For your _particular_ use case, I can add a blurb to the docs that recommends removing known replicates _before_ PCA and then projecting them using the loadings from PCA. A longer term solution is to simply implement PC-AiR in hail. I skimmed the implementation section of the paper earlier this week and it looks very straightforward. It seems to boil down to using the KING estimator to estimate relatedness, compute PCA on unrelated individuals, project related individuals into unrelated PC space. Finally, we can use pc_relate to improve on our original estimates of relatedness from KING. The timeline for the latter thing is kind of unclear and a bit further out given some other work I need to finish. I'll get the documentation improvement in this week. Is there anything else I can do that would have helped you avoid this issue? Is there anything else you need to resolve the issue now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992:387,avoid,avoid,387,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-391739992,4,['avoid'],['avoid']
Safety,"@mhebrard The only way I can imagine that we would mutate your environment is if we are accidentally installing `pyspark`. `install-on-cluster` takes pains to avoid that:; ```; install-on-cluster: $(WHEEL) check-pip-lockfile; 	sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; 	-$(PIP) uninstall -y hail; 	$(PIP) install $(WHEEL) --no-deps; 	hailctl config set query/backend spark; ```. But that is broken somehow? When you ran `install-on-cluster` did you see a `pip` output indicating that pyspark got installed?. Can you check if pyspark is installed via pip now? `pip show pyspark` (should say its not installed). If it is installed, try uninstalling it `pip uninstall pyspark`. You might also try removing the first line of `install-on-cluster` entirely. That will leave you without Hail's dependencies installed, but if `pyspark-shell` is still the right version of Scala, then I suspect the issue is that line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906:159,avoid,avoid,159,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906,1,['avoid'],['avoid']
Safety,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:86,avoid,avoid,86,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487,1,['avoid'],['avoid']
Safety,@patrick-schultz Maybe I should make UnsafeOrdering a separate PR...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-348644288:37,Unsafe,UnsafeOrdering,37,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-348644288,1,['Unsafe'],['UnsafeOrdering']
Safety,"@patrick-schultz ah, I think this looks a fair bit better. And I'm in favor of avoiding allocations when it's easy. Generally, I think per-row allocations don't hurt us too much, but it's not hard to avoid it with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893#issuecomment-404172614:79,avoid,avoiding,79,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404172614,2,['avoid'],"['avoid', 'avoiding']"
Safety,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:76,avoid,avoid,76,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567,1,['avoid'],['avoid']
Safety,"@tpoterba Back to you. Addressed comments. Nuked MemoryBlock, moved the array to MemoryBuffer. I think I made the safety tests in MemoryBuffer complete. I changed the array to Array[Byte]. It is working. There might still be an alignment issue (x86 supports unaligned loads but with possible performance penalty) but I'm OK leaving it to be addressed separately. I think ComplexType is good but I agree we can remove representation and just fundamentalType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074#issuecomment-321090734:114,safe,safety,114,https://hail.is,https://github.com/hail-is/hail/pull/2074#issuecomment-321090734,1,['safe'],['safety']
Safety,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:434,unsafe,unsafe,434,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839,1,['unsafe'],['unsafe']
Safety,"@tpoterba Fixed the tests and tutorials. Ready for a look. Just a rebase of 0.2 that you already reviewed, should just need a sanity check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2039#issuecomment-318789172:126,sanity check,sanity check,126,https://hail.is,https://github.com/hail-is/hail/pull/2039#issuecomment-318789172,1,['sanity check'],['sanity check']
Safety,"@tpoterba I added `hl.debug_info()` which is maybe helpful?. I guess we're really talking about something that sanity checks the environment but assuming we already have hail pip-installed. What would you want to do? I guess we could check some spark stuff, try to create a context, if that fails tell the user its a spark problem? We could just do that in init though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191:111,sanity check,sanity checks,111,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191,1,['sanity check'],['sanity checks']
Safety,@tpoterba Ready for another look. I now use the coders to serialize UnsafeRow and UnsafeIndexedSeq.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2097#issuecomment-322357572:68,Unsafe,UnsafeRow,68,https://hail.is,https://github.com/hail-is/hail/pull/2097#issuecomment-322357572,2,['Unsafe'],"['UnsafeIndexedSeq', 'UnsafeRow']"
Safety,A fix to this issue would detect which FASTAs and which chain files (for liftovers) are needed by a pipeline (a call to `ServiceBackend.execute`) and only mount the necessary ones.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13416#issuecomment-1679191381:26,detect,detect,26,https://hail.is,https://github.com/hail-is/hail/issues/13416#issuecomment-1679191381,1,['detect'],['detect']
Safety,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17808,unsafe,unsafe,17808,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,A root file was modified so it has to test every piece. One of its batch jobs gets stuck in the queue and triggers a timeout. Hopefully this passes since there's no traffic right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915:117,timeout,timeout,117,https://hail.is,https://github.com/hail-is/hail/pull/4924#issuecomment-447199915,1,['timeout'],['timeout']
Safety,"AFAIK, unsafe key-by would still do an unnecessary scan across the data to compute partition bounds. This IR is exactly 1 pass (instead of 2).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12679#issuecomment-1428168367:7,unsafe,unsafe,7,https://hail.is,https://github.com/hail-is/hail/pull/12679#issuecomment-1428168367,1,['unsafe'],['unsafe']
Safety,"Actually, a slightly longer answer:. The parent relationship is essentially a representation of the JVM stack. Each X corresponds to 1 JVM bytecode (except NewInstanceX which is fused). During emit, children are pushed on the stack left-to-right by necessity. Slightly more generally, there are two distinct questions here: what the implementation does, and what it guarantees. For what it guarantees, there are two options: fixed (like Java, which evaluates things left-to-right), and undefined (like C). In general, I think it is safest not to rely on the order of evaluation. I'm not aware of where we do. ; Our backend is simple enough and the mapping to the JVM concrete enough that I don't see any reason why we'd have reason to deviate from left-to-right. So I guess I fall somewhere in between (don't rely on it, but violate it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486:532,safe,safest,532,https://hail.is,https://github.com/hail-is/hail/pull/8958#issuecomment-645533486,2,['safe'],['safest']
Safety,"Actually, comparing singular vectors is not a robust test, even accounting for sign. Suppose `A` has two equal (or nearly equal) singular values. Then there is a 2-dimensional subspace of vectors, all of which are equally good singular vectors for that singular value. If the singular values are sufficiently separated, then comparing singular vectors should be safe, but I don't think it's necessary; the other checks should force that. I think we only need to check (all approximate comparisons),; * we got the right singular values, by comparing with numpy (unless we constructed a test matrix with known singular values); * the singular vectors are orthonormal (i.e. `Ut U = Id` and `Vt V = Id`); * the factorization `A = U Sigma Vt`. Then it follows that for each right singular vector `V_i`, `A v_i = sigma_i u_i` holds approximately, so `v_i` is a good singular vector, i.e. it really does capture `sigma_i` variance, and we checked that `sigma_i` is close to the true singular value.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936:362,safe,safe,362,https://hail.is,https://github.com/hail-is/hail/pull/9727#issuecomment-730626936,1,['safe'],['safe']
Safety,"Actually, going to re-open this. We determined that these were filtered because the sites each had an allele that exceeded the maximum length of 150 (set at https://github.com/samtools/htsjdk/blob/master/src/java/htsjdk/variant/variantcontext/VariantContext.java). Jon suggests that Hail should filter using isSymbolic() rather than isSymbolicOrSV() to avoid filtering these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/308#issuecomment-211685443:353,avoid,avoid,353,https://hail.is,https://github.com/hail-is/hail/issues/308#issuecomment-211685443,1,['avoid'],['avoid']
Safety,"Additionally, I think this is actually the correct/intended use of setPixelRatio. Note that if we set linewidth = .5 successfully, which we cannot, devices with devicePixelRatio = 1 would still look twice as thick as those with ratio 2. That is not what I want. Ive calibrated this to look a certain way on devicePixelRatio = 2 displays, and I want the appearance to be as close to this as possible in lower and higher density fixed-pixel displays. . Ideally devices with ratio < 2 would have lines that had the appropriate thinness, but maybe due to lineWidth limits do not, so we truncate those. Above 2, we continue to scale as before to avoid seeing lines that are too thick.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-644427035:642,avoid,avoid,642,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-644427035,1,['avoid'],['avoid']
Safety,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:77,timeout,timeouts,77,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796,1,['timeout'],['timeouts']
Safety,"After spending a couple of hours reading about g++ ABI versions, I'm feeling much less; positive about the plan of building multiple libraries. It seems there are 11 different ABI versions; (most of them are minor bugfixes which were never default behavior for any version of g++,; but still ...). I'm mulling an alternative plan of saying ""well, you've got to have g++, c++, or clang++ somewhere; on your $PATH, or else you've got to define CXX, and also make, but I've got the C++ sources in the ; jarfile and I'll build you a fresh libboot.so and libhail.so if I haven't done it already"". That would involve a little bit more jarfile/Resource magic - but nothing any harder than I've already; done with the header files; avoid a big testing headache; and I hope get us past the whole; ""locking-down"" argument. And then at a later date I'll think about how to have the option of packaging; a recent clang so that we can get C++17 (and perhaps more consistent compile speed than g++); across a wide range on Linuxes. Accordingly I'll close this for now and re-open it when I have a working solution for the library issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410111491:724,avoid,avoid,724,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410111491,1,['avoid'],['avoid']
Safety,"Agh, the unsafeRow and UnsafeIndexedSeq optimizations must be wrong, getting out of bounds memory errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173:9,unsafe,unsafeRow,9,https://hail.is,https://github.com/hail-is/hail/pull/9892#issuecomment-770085173,2,"['Unsafe', 'unsafe']","['UnsafeIndexedSeq', 'unsafeRow']"
Safety,"Agreed, this is good for useability. note its also a special case of simple linreg predicting y from x (we return the square r_sq. The sign is that of beta).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239:84,predict,predicting,84,https://hail.is,https://github.com/hail-is/hail/issues/4479#issuecomment-425769239,2,['predict'],['predicting']
Safety,"Agreed, triggers avoids the need for the join. In the meantime I fixed the joins (maybe).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5959#issuecomment-490237506:17,avoid,avoids,17,https://hail.is,https://github.com/hail-is/hail/pull/5959#issuecomment-490237506,1,['avoid'],['avoids']
Safety,"Ah yeah good point I forgot about that. You have to construct a string to avoid truncation a la:; ```; (base) dking@wm28c-761 /tmp % cat foo.py; def test():; assert False, 'b' * 1000; =========================================== test session starts ============================================; (base) dking@wm28c-761 /tmp % pytest foo.py; platform darwin -- Python 3.10.9, pytest-7.4.3, pluggy-1.3.0; rootdir: /private/tmp; configfile: pytest.ini; plugins: xdist-2.5.0, timeout-2.2.0, instafail-0.5.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, anyio-4.2.0, forked-1.6.0, accept-0.1.9, image-diff-0.0.11; asyncio: mode=strict; collected 1 item . foo.py F [100%]. ================================================= FAILURES =================================================; ___________________________________________________ test ___________________________________________________. def test():; > assert False, 'b' * 1000; E AssertionError: bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb; E assert False. foo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019:74,avoid,avoid,74,https://hail.is,https://github.com/hail-is/hail/pull/14151#issuecomment-1889800019,2,"['avoid', 'timeout']","['avoid', 'timeout-']"
Safety,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:47,recover,recoverable,47,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683,2,"['recover', 'safe']","['recoverable', 'safe']"
Safety,"Ah, I was accidentally modifying an installed version of Hail. I recovered the files and brought them in. I deleted that other debug_info. It doesn't include the batch information. Now every batch test in batch/ and in hail/ should be consistently using `Batch.debug_info`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813:65,recover,recovered,65,https://hail.is,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813,1,['recover'],['recovered']
Safety,"Ah, yeah it looks a fair bit better now that I pushed the all-arity AggOp all the way through. To remove `T`, I need to a `Type => Class[_]` function, which I've been avoiding. I did manage to remove the whole `[Nothing]` hack without doing this though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2623#issuecomment-359557958:167,avoid,avoiding,167,https://hail.is,https://github.com/hail-is/hail/pull/2623#issuecomment-359557958,1,['avoid'],['avoiding']
Safety,"All that messy state twiddling is because Scala's `Iterator` is the wrong model for most things we use it for, which is why I made `FlipbookIterator`. Using that, what you have would become; ```scala; private class BgenRecordStateMachine(; ctx: RVDContext,; p: BgenPartition,; settings: BgenSettings; ) extends StateMachine[RegionValue] {; private[this] val bfis = p.makeInputStream; private[this] val rv = RegionValue(ctx.region); private[this] val rvb = ctx.rvb; ; def isValid: Boolean = p.isValid; def value: RegionValue = rv; def advance() { p.advance(); findNextVariant() }; private def findNextVariant() {; // same as existing advance(), but without advancing p; }. findNextVariant() // make sure iterator is initialized in first valid state; }; ```; giving `BgenPartition` a `FlipbookIterator` style interface, with `isValid`, `value`, and `advance()` instead of `hasNext()` and `next()`. Then to create a new iterator `FlipbookIterator(new BgenRecordStateMachine(...))`. But honestly, what you had was clear enough, so if you benchmarked and the allocation isn't an issue, you should do whatever you find most readable. I've been conditioned to avoid `Option` in low-level code, but I don't have a good intuition for when it is or isn't actually a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507:1153,avoid,avoid,1153,https://hail.is,https://github.com/hail-is/hail/pull/3893#issuecomment-404156507,2,['avoid'],['avoid']
Safety,"Allow the Genotype in VSM to be null. This means you can't call `g.gt` anymore, since `g` might be null. Moved the user-visible Genotype functions to the Genotype companion object and made them null-safe. This makes the behavior of filterGenotypes between VariantDataset and GenericDataset consistent. Fixed the tests. @tpoterba ready for another look.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1872#issuecomment-305223718:199,safe,safe,199,https://hail.is,https://github.com/hail-is/hail/pull/1872#issuecomment-305223718,1,['safe'],['safe']
Safety,Also added redundant interval pruning for `filtervariants intervals`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279:11,redund,redundant,11,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949279,1,['redund'],['redundant']
Safety,"Also, the rules are as specific as they are to avoid using `!important` tags. These rules have slightly higher specificity than the rules they're overriding, so no `!important` tag is needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7291#issuecomment-541356252:47,avoid,avoid,47,https://hail.is,https://github.com/hail-is/hail/pull/7291#issuecomment-541356252,1,['avoid'],['avoid']
Safety,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:613,timeout,timeouts,613,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,2,['timeout'],['timeouts']
Safety,An error is being treated as a timeout but then the job is still considered running?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1076794055:31,timeout,timeout,31,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1076794055,1,['timeout'],['timeout']
Safety,"And I'll remove the bucket after this goes in (it should be unused, but to be safe)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9778#issuecomment-738244457:78,safe,safe,78,https://hail.is,https://github.com/hail-is/hail/pull/9778#issuecomment-738244457,1,['safe'],['safe']
Safety,"And to directly respond to this comment:. > If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. This sounds great! This would resolve question 1 and eliminate the risk. We should charge the highest possible price: 0.23 USD/GiB. Answering question 2 can proceed slowly and carefully knowing that we don't have a cost risk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657:280,risk,risk,280,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657,2,['risk'],['risk']
Safety,Are you asking how to detect that one command of many in a bash *pipeline* failed? We need pipe fail enabled for that:. ```bash; # bad; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 0; ```; ```bash; # good; (base) dking@wm28c-761 ~ % set -o pipefail; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 1; ```. Permissions issues indeed fail the `gcloud` command:; ```; (base) dking@wm28c-761 ~ % gcloud compute instances list --project notmyproject; API [compute.googleapis.com] not enabled on project [notmyproject]. Would you ; like to enable and retry (this will take a few minutes)? (y/N)? y. Enabling service [compute.googleapis.com] on project [notmyproject]...; ERROR: (gcloud.compute.instances.list) PERMISSION_DENIED: Permission denied to enable service [compute.googleapis.com]; Help Token: AVzH8v0NCN6UR5g5Xtu_gFde3SeZCmToYDDOlz7hp5HiVvGHKX8aeJ-kn0N0n72nMovbuw4ksm8MB0OifqPrdxlc6lWwJJKi0CsIJon1a7SSlF_H; - '@type': type.googleapis.com/google.rpc.PreconditionFailure; violations:; - subject: ?error_code=110002&service=serviceusage.googleapis.com&permission=serviceusage.services.enable&resource=notmyproject; type: googleapis.com; - '@type': type.googleapis.com/google.rpc.ErrorInfo; domain: serviceusage.googleapis.com; metadata:; permission: serviceusage.services.enable; resource: notmyproject; service: serviceusage.googleapis.com; reason: AUTH_PERMISSION_DENIED; (base) dking@wm28c-761 ~ % echo $?; 1; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268:22,detect,detect,22,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268,1,['detect'],['detect']
Safety,"As an extra safety net that this PR doesn't change any binding semantics, the ""split up matrix/table/value"" commit passed ci with an assertion on every bindings query that the old and new implementations agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14404#issuecomment-2027600400:12,safe,safety,12,https://hail.is,https://github.com/hail-is/hail/pull/14404#issuecomment-2027600400,1,['safe'],['safety']
Safety,"As an extra safety step, I slapped the stacked PR tag on it which should prevent merging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8308#issuecomment-606016409:12,safe,safety,12,https://hail.is,https://github.com/hail-is/hail/pull/8308#issuecomment-606016409,1,['safe'],['safety']
Safety,"As written here, this feature is incompatible with `overwrite=True`. The checkpoint file makes it possible to recover progress from a partially written file at `url`, so I'm not sure what overwriting would mean in this context. Is there a particular use case you have in mind? This was built for a specific application, and I am eager to hear others.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215#issuecomment-822958412:110,recover,recover,110,https://hail.is,https://github.com/hail-is/hail/pull/10215#issuecomment-822958412,1,['recover'],['recover']
Safety,Avoided by removing this dependency in #4659,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181:0,Avoid,Avoided,0,https://hail.is,https://github.com/hail-is/hail/issues/4653#issuecomment-434014181,1,['Avoid'],['Avoided']
Safety,"Azure is currently running this internal-gateway/gateway, and PRs seem to be doing no worse, and sometimes better afaict, than the nginx in GCP, save for the Connection reset retrying which once in a handful of PRs will stall the test until timeout. If we decide to merge this I would want to watch a few subsequent PRs to confirm that they're not stalling and if so I would feel confident rolling out envoy to gcp as well (and can resize the k8s pool immediately after!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12425#issuecomment-1303825690:241,timeout,timeout,241,https://hail.is,https://github.com/hail-is/hail/pull/12425#issuecomment-1303825690,1,['timeout'],['timeout']
Safety,"Because the linreg keytable is computed via mapAnnotations on sample keytable and then both are returned. So if the user then asks for sample keytable, it is recomputed. As we discussed, this will be avoided once variant annotations are keytables too, which will make it natural to pass in the sample_keytable to linreg_burden instead of the vds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1759#issuecomment-299068662:200,avoid,avoided,200,https://hail.is,https://github.com/hail-is/hail/pull/1759#issuecomment-299068662,1,['avoid'],['avoided']
Safety,"Because we're running the bash script that is generated with +e so it will fail on the first error. To try and implement always_run would require implementing something more complicated in the local backend and not just building up a script. Because you'd have to check whether the parents succeeded for each task. It's not out of the question, but I don't think it's worth doing right now. Hence, the NotImplementedError. Same reasoning for timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210:442,timeout,timeout,442,https://hail.is,https://github.com/hail-is/hail/pull/8258#issuecomment-596109210,1,['timeout'],['timeout']
Safety,"By looking at the full state on a dataset consist of 5000 copies of the same row, I've narrowed the search to this line giving the wrong values in the first row of `xdx` (not including upper left element) on a non-deterministic subset of rows despite `px` and `dpa` being correct:. ```; xdx(r0, r1) := px.t * dpa; ```. This is the only place `px` is used, and indeed, copying `px` is sufficient to fix the bug. I'd say it's a breeze concurrency bug, except that not only does it go away whenever no single computer processes more than one partition (i.e. big partition or single-core workers), it also doesn't occur on my laptop or google VM. I'd prefer a solution that avoids copying `px` per thread, since it's samples by covariates, rather than covariates by covariates (xdx) or covariates by 1 (xdy). But while I'm still working on what the heck is going on, I see no reason not to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786:670,avoid,avoids,670,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416789786,1,['avoid'],['avoids']
Safety,CI doesn't seem to be retesting this one after it failed due to a timeout issue. If you bump this PR it would probably go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6689#issuecomment-522022994:66,timeout,timeout,66,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-522022994,1,['timeout'],['timeout']
Safety,CI recovered once the PR was closed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053:3,recover,recovered,3,https://hail.is,https://github.com/hail-is/hail/pull/6377#issuecomment-503584053,1,['recover'],['recovered']
Safety,"CI test failure means not known to be safe to merge into master. Agreed re: minimizing false failures (i.e. failure due to system load but it's actually an OK change). I think in practice much less than 30s is fine, this test has been in for a month or two and this is the first time I saw it fail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865:38,safe,safe,38,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-470264865,1,['safe'],['safe']
Safety,"Can we make a ""backup"" of what the data source settings are that are working now before we redeploy this in default? I want to make sure we can recover if these changes put us in a bad state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-905883214:144,recover,recover,144,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-905883214,1,['recover'],['recover']
Safety,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:1430,avoid,avoid,1430,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,1,['avoid'],['avoid']
Safety,Commit message: ; ```; Fixed bug in TextTableReader caused by unsafe ArrayBuilder use. ; Bug occurred for text tables with a number of columns equal to a power of 2; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448:62,unsafe,unsafe,62,https://hail.is,https://github.com/hail-is/hail/pull/1127#issuecomment-263676448,1,['unsafe'],['unsafe']
Safety,"Conceptually, if we're not just interested in missingness, then unboxedGT is useful only in route to nNonRefAlleles (and equal to it if we've split). E.g., the simplest way to extend regression to multi-allelic is to use nNonRefAlleles...though thinking about this further, it's upsettingly asymmetric in the case where the ref allele is the minor allele, so perhaps we should just force deliberate choice of splitting, esp if we're moving toward implementing that less painfully under the hood. Or do regression per alternate allele while maintaining multi-allelic form. (edited). We currently compute nNonRefAlleles from unboxedGT through GTPair, which allocates per genotype. For example, PCA currently requires splitting, but uses nNonRefAlleles. And IBD currently requires splitting but allocates per genotype via:; ```; def countRefs(gtIdx: Int): Int = {; val gt = Genotype.gtPair(gtIdx); indicator(gt.j == 0) + indicator(gt.k == 0); ```. So it may make sense to add `unboxedNNonRefAlleles` that avoids allocation, but doesn't require splitting for these.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468:1002,avoid,avoids,1002,https://hail.is,https://github.com/hail-is/hail/issues/1314#issuecomment-276082468,2,['avoid'],['avoids']
Safety,"Current master on representative dataset:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: running: count; [Stage 2:======================================================>(661 + 2) / 663]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 5.760s; filtervariants intervals: 386.191ms; count: 32.617s; total: 38.763s; ```. and new:. ```; [Stage 1:=======================================================> (29 + 1) / 30]hail: info: running: filtervariants intervals -i file:///mnt/lustre/tpoterba/chr1.intervals --keep; hail: info: pruned 0 redundant intervals; hail: info: interval filter loaded 67 of 663 partitions; hail: info: running: count; [Stage 2:=======================================================> (65 + 2) / 67]hail: info: count:; nSamples 5,231; nVariants 76,015; hail: info: timing:; read: 9.911s; filtervariants intervals: 445.193ms; count: 3.356s; total: 13.712s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200:715,redund,redundant,715,https://hail.is,https://github.com/hail-is/hail/pull/1041#issuecomment-257949200,1,['redund'],['redundant']
Safety,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:707,avoid,avoiding,707,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812,2,['avoid'],['avoiding']
Safety,Currently the linear SKAT routine is implemented to be optimal for the case of (genetic variants) k < n (genetic samples). Implementation will process sets of variants associated to the same gene in such a way that there is no redundant computation in the algorithm. . cases handled:; hard call genetic data; dosage genetic data; k << n - (Cannot explicitly form a matrix containing all the genotype data) . ran on chromosome 22 1kgDataset with approximately 100 intervals and the program runs in about 3-4 minutes with 2 workers and 12 pre-emptibles with 8 cores each.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482:227,redund,redundant,227,https://hail.is,https://github.com/hail-is/hail/pull/1973#issuecomment-320358482,1,['redund'],['redundant']
Safety,D$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4068,abort,abortStage,4068,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['abortStage']
Safety,Definitely agree w.r.t. avoiding acronyms and codenames.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431038364:24,avoid,avoiding,24,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431038364,1,['avoid'],['avoiding']
Safety,"Disregard the message above. The auto increment would not work. Now the critical check to make sure no duplicates are added is this line:. ```python3; job_id = parameters.get('job_id'); has_record = await db.jobs.has_record(batch_id, job_id); if has_record:; log.info(f""database has record for ({batch_id}, {job_id})""); abort(400, f'invalid request: batch {batch_id} already has a job_id={job_id}'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583:320,abort,abort,320,https://hail.is,https://github.com/hail-is/hail/pull/6238#issuecomment-498043583,1,['abort'],['abort']
Safety,"Do this if smaller number of partitions than VDS, otherwise unsafe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/797#issuecomment-247439570:60,unsafe,unsafe,60,https://hail.is,https://github.com/hail-is/hail/issues/797#issuecomment-247439570,1,['unsafe'],['unsafe']
Safety,Do we need to manually delete the stopped but not deleted instances for batch to recover?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614:81,recover,recover,81,https://hail.is,https://github.com/hail-is/hail/pull/8220#issuecomment-593643614,1,['recover'],['recover']
Safety,"Due to a noisy co-occurring batch from a user, we discovered several of our tests that assumed they could burst into mostly unused cpu. I backed off on their timeouts and also restricted them to their limited number of cores.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13277#issuecomment-1644771781:158,timeout,timeouts,158,https://hail.is,https://github.com/hail-is/hail/pull/13277#issuecomment-1644771781,1,['timeout'],['timeouts']
Safety,"Eh, you can't directly use base64 because of the restrictions, either. You can use the URL-safe variant if you bracket it with alphanumeric characters and use . instead of = for the pad.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459225923:91,safe,safe,91,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459225923,1,['safe'],['safe']
Safety,"Eventually the status calls returned, but something caused them to hang?; ```; INFO	| 2018-10-23 03:42:37,944 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_github_state HTTP/1.1"" 200 -; ERROR	| 2018-10-23 03:48:38,045 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); ERROR	| 2018-10-23 03:55:38,215 	| ci.py 	| polling_event_loop:357 | Could not poll due to exception: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=360); INFO	| 2018-10-23 03:59:30,821 	| ci.py 	| <lambda>:366 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 200 -; INFO	| 2018-10-23 03:59:30,826 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,828 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; INFO	| 2018-10-23 03:59:30,830 	| ci.py 	| <lambda>:366 | 10.56.143.15 ""GET /status HTTP/1.0"" 200 -; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417:365,timeout,timeout,365,https://hail.is,https://github.com/hail-is/hail/issues/4607#issuecomment-432082417,2,['timeout'],['timeout']
Safety,"Everything now works, well enough for a demo. Outstanding issues:; 1) auth_request during redirect. currently. disabled as. it doesn't handle post-redirect requests. handle these requests. safely; 2) watch MODIFIED events. in. a finer grained way: need to inspect the container for readiness; status.phase, does not provide a sufficient aggregate picture.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460099264:189,safe,safely,189,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460099264,1,['safe'],['safely']
Safety,"Fails with:; ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 491 in stage 1.0 failed 20 times, most recent failure: Lost task 491.19 in stage 1.0 (TID 17951, exomes-sw-gf9k.c.broad-mpg-gnomad.internal): java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:74,abort,aborted,74,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,5,"['Unsafe', 'abort']","['UnsafeRow', 'aborted']"
Safety,"File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 671, in _listfiles_recursive; async for page in await self._storage_client.list_objects(bucket, params=params):; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/aiogoogle/client/storage_client.py"", line 50, in __anext__; self._page = await self._client.get(self._path, params=self._request_params, **self._request_kwargs); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/common/base_client.py"", line 23, in get; async with await self._session.get(url, **kwargs) as resp:; File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/common/session.py"", line 18, in get; return await self.request('GET', url, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/common/session.py"", line 87, in request; auth_headers = await self._credentials.auth_headers(); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/aiogoogle/credentials.py"", line 89, in auth_headers; self._access_token = await self._get_access_token(); File ""/usr/local/lib/python3.8/dist-packages/hailtop/aiocloud/aiogoogle/credentials.py"", line 158, in _get_access_token; return GoogleExpiringAccessToken.from_dict(await resp.json()); File ""/usr/local/lib/python3.8/dist-packages/aiohttp/client_reqrep.py"", line 1099, in json; await self.read(); File ""/usr/local/lib/python3.8/dist-packages/aiohttp/client_reqrep.py"", line 1037, in read; self._body = await self.content.read(); File ""/usr/local/lib/python3.8/dist-packages/aiohttp/streams.py"", line 375, in read; block = await self.readany(); File ""/usr/local/lib/python3.8/dist-packages/aiohttp/streams.py"", line 397, in readany; await self._wait(""readany""); File ""/usr/local/lib/python3.8/dist-packages/aiohttp/streams.py"", line 304, in _wait; await waiter; File ""/usr/local/lib/python3.8/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; asyncio.exceptions.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13029#issuecomment-1542329456:5163,Timeout,TimeoutError,5163,https://hail.is,https://github.com/hail-is/hail/pull/13029#issuecomment-1542329456,2,['Timeout'],['TimeoutError']
Safety,For posterity this is what goes wrong if I don't create a fresh OOS for each object:; ```. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:303,abort,abortStage,303,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,3,['abort'],['abortStage']
Safety,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:760,timeout,timeout,760,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,1,['timeout'],['timeout']
Safety,"From googling, this seems like something that was probably relatively out of our control. Like cluster ran out of disk space, communication failed, etc. Considering no one else has complained about this (to my knowledge anyway), this is probably a safe issue to close, assuming you haven't had this problem since. @tpoterba , does this seem reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1185#issuecomment-306901327:248,safe,safe,248,https://hail.is,https://github.com/hail-is/hail/issues/1185#issuecomment-306901327,1,['safe'],['safe']
Safety,"From that VM, I can get into the container, install the jdk, then run jstack on one of the hung JVMs.; ```; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:299,Unsafe,Unsafe,299,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['Unsafe'],['Unsafe']
Safety,"Got it. In spite of my claim ""I was trying to avoid bulk operations"" I see that close_batch scans over all ready jobs (a bulk operation) to update ready_cores. I'm not quite sure what to do here. I'm not actually sure if a long-running query on close_batch is going to cause problems and it is not trivial to make it incremental.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526:46,avoid,avoid,46,https://hail.is,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526,1,['avoid'],['avoid']
Safety,"Heh, so turns out that `test_weird_urls` is missing the `@pytest.mark.asyncio` decorator, and so it was getting skipped with a warning this whole time. The pytest upgrade added auto-detection of async tests and so it ran this broken test for the first time. I'm PR'ing to treat most warnings as errors in #12322.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498:182,detect,detection,182,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498,1,['detect'],['detection']
Safety,"Here is what I get when invoking pyspark. $ pyspark; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/02 13:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/02 13:56:47 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; 17/08/02 13:56:47 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/Users/ih/languages/hail.is/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.13 (default, Jul 18 2017 09:16:53); SparkSession available as 'spark'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996:618,detect,detected,618,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319749996,1,['detect'],['detected']
Safety,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:296,redund,redundant,296,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,2,['redund'],['redundant']
Safety,"Here's what's currently in the database:. ```; mysql> select name, min_instances, standing_worker_max_idle_time_secs from pools;; +-------------+---------------+------------------------------------+; | name | min_instances | standing_worker_max_idle_time_secs |; +-------------+---------------+------------------------------------+; | highcpu | 0 | 300 |; | highcpu-np | 0 | 300 |; | highmem | 0 | 300 |; | highmem-np | 0 | 300 |; | standard | 1 | 300 |; | standard-np | 1 | 300 |; +-------------+---------------+------------------------------------+; 6 rows in set (0.00 sec); ```. I think we also need to increase the timeouts here as well as make all min_instances = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13150#issuecomment-1581431828:620,timeout,timeouts,620,https://hail.is,https://github.com/hail-is/hail/pull/13150#issuecomment-1581431828,1,['timeout'],['timeouts']
Safety,"Hey @anh151 !. I'm sorry you're having trouble with Hail. The message ""Container killed on request. Exit code is 137"" comes from Apache Spark, our underlying distributed compute framework. It indicates that the worker machines have insufficient RAM. In general, using worker machines with higher RAM-to-core ratios will help. If you're on GCP, try the n1-highmem family. Some other suggestions:. 1. Hail is a ""lazy"" system. Your entire pipeline is executed, from the beginning, when you run ""export"" or ""write"". That means that Hail has to do all of that work at once. You can ease the memory pressure by performing less operations at once, by writing an intermediate file (and reading back in and proceeding with it).; 2. We recommend against directly exporting from a complex operation (like group-by-aggregate). Instead, grab the cols table and write it to Hail's fast, binary, parallel format: `.cols().select('field_of_interest').write('my-cols.ht')`. Then read that table and export that: `hl.read_table('my-cols.ht').field_of_interest.export(...)`. Exporting to a text file requires more memory because we have to construct ASCII strings.; 3. Always use a compressed export: `.export('foo.tsv.bgz')` or `hl.export_vcf(..., 'foo.vcf.bgz')`. This won't help your memory problem, but you can avoid parsing strings to create loci by constructing an `hl.Locus` which is the Python-side representation of loci (`hl.locus` is the inside-Hail representation):; ```python3; def create_intervals(data):; return [; hl.Locus(chromosome, start, reference_genome=""GRCh38""); for i, (chromosome, start) in data[[""CHROM"", ""POS""]].iterrows(); ]; ```. Please reply here if you're still having problems after incorporating the above suggestions as it may indicate a more fundamental issue with Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485:1296,avoid,avoid,1296,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485,1,['avoid'],['avoid']
Safety,"Hey @cseed,. I tried running it as I need a test version of the 5.5K WGS data but it fails:; `hail-spark-lf read -i MacArthur_Merck_Finns.vds head --keep 10000 write -o MacArthur_Merck_Finns.head.vds; hail: info: running: read -i MacArthur_Merck_Finns.vds; [Stage 0:======================================================>(134 + 1) / 135]hail: info: running: head --keep 10000; hail: info: running: write -o MacArthur_Merck_Finns.head.vds; hail: write: caught exception: Job aborted.`. Got the same error on both dataflow and Cray. Also, my implementation somehow fails on Cray (different error) but not on dataflow....yay!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/446#issuecomment-234642054:474,abort,aborted,474,https://hail.is,https://github.com/hail-is/hail/pull/446#issuecomment-234642054,1,['abort'],['aborted']
Safety,"Hi @pettyalex, thank you for the detailed and thoughtful issue. Hopefully I can shed some light and address all your concerns. I think the assertion on Java 8 and 11 was an overly defensive precaution put in place some time ago, as hail uses some unsafe JVM APIs that have been deprecated for a while. But as you noted, the world goes on in Java 17 and I don't see a reason Hail shouldn't be compatible. Since most of our closest users use Hail on GCP Dataproc, we generally keep in lock-step with their platform which is unfortunately still on Java 11 so that is what we test against and officially support. Nevertheless, we should remove the restriction and add some light validation in CI against Java 17 and advertise it as unofficially supported until such a time that Dataproc moves to Java 17. Hopefully Spark 3.6 will force their hand. The release process for 0.2.129 is already underway but expect this to be resolved in 0.2.130. Thanks for your suggestions regarding bundling the JRE and the GC options, we'll definitely consider them. Regarding the `module-info.class` nonsense, my apologies. That just seems like a bug we should fix. I will create a separate tracking issue for that but I'm not yet sure where that will get prioritized. If it is more than an annoyance for you, please let us know. Regarding conda-forge, I don't think we currently have the bandwidth or demand (that we know of) to add more distribution systems. Again, this is something where hearing from the community is the best way to figure out how to direct our efforts. Hopefully this addresses your concerns. Please do follow up if I've missed anything or open more issues if you encounter new problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704:247,unsafe,unsafe,247,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704,1,['unsafe'],['unsafe']
Safety,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:446,timeout,timeout,446,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['timeout'],['timeout']
Safety,"Hi, @danking ; I reconfigurated the spark cluster, with the cloudera spark : version 2.2.0.cloudera1; But I can't import hail this time, How can I fix it?. The test:; ```; >>> spark.sparkContext.master; u'yarn'. bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> spark.sparkContext.master; u'yarn'; >>> import hail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/hail/python/hail/__init__.py"", line 1, in <module>; import hail.expr; File ""/opt/Software/hail/python/hail/expr.py"", line 3, in <module>; from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; File ""/opt/Software/hail/python/hail/representation/__init__.py"", line 1, in <module>; from hail.representation.variant import Variant, Locus, AltAllele; File ""/opt/Software/hail/python/hail/representation/variant.py"", line 2, in <module>; from hail.typecheck import *; File ""/opt/Software/hail/python/hail/typecheck/__init__.py"", line 1, in <module>; from check import *; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 1, in <module>; from decorator import decorator, getargspec; ImportError: cannot import name getargspec; >>> ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486:354,detect,detected,354,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336722486,1,['detect'],['detected']
Safety,"Hi, danking, the result is as follows:; ```; [root@tele-1 ~]# pyspark; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/15 08:58:31 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/15 08:58:31 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile(""/hail/test/BRCA1.raw_indel.vcf"").count(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 1008, in count; return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum(); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 999, in sum; return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add); File ""/opt/Software/spark/spark-2.0.2-bin-hadoop2.6/python/pyspark/rdd.py"", line 873, in fold; vals = self.mapPartitions(func).collect(); File ""/opt/Software/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367:615,detect,detected,615,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322349367,1,['detect'],['detected']
Safety,"Hicseed @cseed , I configured the java related to the Spark cluster, as follows. ```; scala> System.getProperty(""java.version""); res0: String = 1.8.0_91. scala> val rdd = sc.parallelize(0 to 1000, 4); rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27. scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.8.0_91, 1.8.0_91, 1.8.0_91, 1.8.0_91) ; ```. but when testing the `split multi` command use the `split_test.vcf` in the test file hail offered:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. there appeared some errors; 1. `java.io.FileNotFoundException: hail.log (Permission denied)`; 2. `Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): ; java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`; 3. `The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN`. I tested several different vcf files, the errors always existed.; The whole error message was attached as follows ; [splitmulti.txt](https://github.com/hail-is/hail/files/502516/splitmulti.txt) . How can I solve it ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250697347:954,abort,aborted,954,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250697347,1,['abort'],['aborted']
Safety,"Hm ok, I'm going to push this PR through to avoid more merge conflict setbacks and think about how to improve this workflow for the future. Sorry for all the hassle!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10347#issuecomment-826181213:44,avoid,avoid,44,https://hail.is,https://github.com/hail-is/hail/pull/10347#issuecomment-826181213,1,['avoid'],['avoid']
Safety,"Hmm, idempotency is a bit hard to talk about here. This change makes it impossible to not ""cleanup"" the chunks iterator if you hit an exception midway through the chunks iterator. In particular, this now works:; ```; try:; with chunks(...) as data:; raise ValueError(); except ValueError:; pass; with chunks(...) as data:; ... use data ...; ```. In the current code, that does not work. The second call to `chunks` raises an error unless chunks is empty. ---. But you're probably asking about the code that uses chunks? In the Google case it is idempotent: lines 206-215 construct a new request before iterating chunks. The PUT request includes the specific range of bytes we want to write to, so even if we partially succeeded with a previous PUT, this subsequent PUT should overwrite (or, more likely, error). In practice, I don't think we can partially succeed. I think either we write fully or we terminate the connection early and google drops the data. Summary: I think Google is fine. As for Azure, we use a randomly generated block_id. If we error while inside `stage_block` that block_id is never added to `self.block_ids`. As a result, we can safely make a second attempt to upload the block with a new id.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868:1153,safe,safely,1153,https://hail.is,https://github.com/hail-is/hail/pull/12492#issuecomment-1332686868,1,['safe'],['safely']
Safety,"Hmm. Am I correctly reading from this that there are 8 of the exact same genome in the dataset?. I am feeling more confident that this is a symptom of the model. Theoretically, if a group of replicates were perfectly separated from the rest of the dataset by a PC or group of PCs, then the estimator for kinship will get zero's because the  perfectly predicts the genotype.; <img width=""825"" alt=""screen shot 2018-05-07 at 10 44 55 am"" src=""https://user-images.githubusercontent.com/106194/39707912-af6d2bac-51e3-11e8-928b-4dc8d08474b2.png"">. It seems a little odd that 8 samples out of 5000 would manage to get at least one PC to differentiate them from the rest of the dataset. However, if that _is_ happening, then it follows that PC-Relate would dramatically decrease the estimated kinship because all the shared alleles are being marked as markers of ancestral relatedness rather than familial relatedness. Basically, it would be interesting to see the _ancestral_ relatedness as well. If you plot the top 10 PCs and color the replicates a different color, are they clearly separated by any of the PCs?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-387091122:352,predict,predicts,352,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-387091122,2,['predict'],['predicts']
Safety,"Hmm. I really am not a fan of heterogeneity of timeout. OK, if you really think its critical that we have 5s timeouts in Batch, then I'll just put the 20 second timeout into the storage_client.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830#issuecomment-1126338329:47,timeout,timeout,47,https://hail.is,https://github.com/hail-is/hail/pull/11830#issuecomment-1126338329,3,['timeout'],"['timeout', 'timeouts']"
Safety,"How do you feel about:; 1. if entry_to_double = None, then assume entries are doubles (which avoids having to go through the expr parser); 2. `inserted here just because GitHub automatically numbers my 3. to a 2.`; 3. instead of VariantDataset.genotype_matrix_pca, have VariantDataset.normalized_hardcalls (or just a .normalize_by_variant, since vds.hardcalls already exists) that does the normalization, such that it would instead be vds.hardcalls.normalize.pca()?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2454#issuecomment-348532963:93,avoid,avoids,93,https://hail.is,https://github.com/hail-is/hail/pull/2454#issuecomment-348532963,1,['avoid'],['avoids']
Safety,"I also compared `variant_and_sample_qc_nested_with_filters_2` (33% worse on batch) between the two branches on my laptop, and could not detect a difference. I do think the make_ndarray range speedup is real -- there are a few benchmarks that indicate improvement that all are heavily dependent on the performance of the `StreamRange` implementation, which I think slightly improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646:136,detect,detect,136,https://hail.is,https://github.com/hail-is/hail/pull/10229#issuecomment-814325646,1,['detect'],['detect']
Safety,"I backed out the more subtle pieces to get a basic sort on write in now. Sorry, I should have done that sooner. When I didn't have an easy answer for why it was safe to remove that requirement, I realized I needed to do some work solidifying the partitioner semantics. I'm getting closer on that front, but I'm constantly amazed how tricky it is!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3694#issuecomment-401136470:161,safe,safe,161,https://hail.is,https://github.com/hail-is/hail/pull/3694#issuecomment-401136470,1,['safe'],['safe']
Safety,I changed some things to try and avoid rewriting Python functions if the job has already been submitted previously.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1372626480:33,avoid,avoid,33,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1372626480,1,['avoid'],['avoid']
Safety,I changed the default timeout to 60s.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4586#issuecomment-434410954:22,timeout,timeout,22,https://hail.is,https://github.com/hail-is/hail/pull/4586#issuecomment-434410954,1,['timeout'],['timeout']
Safety,"I did some digging into their client library. This exception (from the example where last_chunk=False) is thrown in the following cases:. ```; // Case 4: localNextByteOffset < remoteNextByteOffset; // && driftOffset > chunkSize:; // Throw exception as remoteNextByteOffset has drifted beyond the retriable; // chunk maintained in memory. This is not possible unless there's multiple; // clients uploading to the same resumable upload session. // Case 8: remoteNextByteOffset==-1 && last == false && checkingForLastChunk; // Not last chunk and checkingForLastChunk means this is the second time we; // hit this case, meaning the upload was completed by a different client. // Case 9: Only possible if the client local offset continues beyond the remote; // offset which is not possible.; ```. I'm having a hard time following their code trying to trace how the `uploadId` is generated. Is it possible the issue is somehow the same `uploadId` is being used for multiple blob uploads because we're doing something that is not thread-safe??? One last thing I was trying to figure out but had a hard time following -- is it possible the requester pays additions changed the code path for how the `uploadId` is set if the problem has been more frequent after those changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1564745979:1030,safe,safe,1030,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1564745979,1,['safe'],['safe']
Safety,I don't have a good reason for avoiding `Type => Class[_]` it just feels wrong somehow. I think: non-primitive types have Scala reifications that we never want to produce.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2623#issuecomment-359558221:31,avoid,avoiding,31,https://hail.is,https://github.com/hail-is/hail/pull/2623#issuecomment-359558221,1,['avoid'],['avoiding']
Safety,"I don't know how to avoid this, closing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7016#issuecomment-529558144:20,avoid,avoid,20,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529558144,1,['avoid'],['avoid']
Safety,"I don't understand why my code is causing timeouts, and only in that one test slice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14391#issuecomment-2098885167:42,timeout,timeouts,42,https://hail.is,https://github.com/hail-is/hail/pull/14391#issuecomment-2098885167,1,['timeout'],['timeouts']
Safety,I extracted `UnsafeOrdering` into #2519.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-349039141:13,Unsafe,UnsafeOrdering,13,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-349039141,1,['Unsafe'],['UnsafeOrdering']
Safety,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:414,avoid,avoid,414,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['avoid'],['avoid']
Safety,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:8,timeout,timeout,8,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['timeout'],['timeout']
Safety,I guess I mean really at the level of the IR. there are probably python safety belts on annotate_rows,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4242#issuecomment-417758586:72,safe,safety,72,https://hail.is,https://github.com/hail-is/hail/pull/4242#issuecomment-417758586,1,['safe'],['safety']
Safety,"I had to choose rather small upper bounds to avoid a variety of overflow errors in `Genotype`. I imagine these are practically not a problem, but I didn't take a particularly principled approach to choosing upper bounds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/649#issuecomment-241846815:45,avoid,avoid,45,https://hail.is,https://github.com/hail-is/hail/pull/649#issuecomment-241846815,1,['avoid'],['avoid']
Safety,"I have a fairly bad case of PTSD around over-use of std::unique_ptr and std::move at; Oracle/Endeca. I think std::unique_ptr<T> is deeply confusing and evil because, in the; simplest terms, it doesn't have the normal semantics of a ""pointer"", i.e. two or more pointers; can refer to a single object. And that problem becomes massively aggravated in the; almost-universal situation of ""borrowing"" a pointer for the duration of a procedure call. Once you let std::unique_ptr<T> into your code, it can creep out into a whole lot of places; where it adds complexity and confusion without solving any real problem. In this particular case, the complexity of managing the memory chunks isn't that hard,; they all get cleaned up by the Region destructor, and adding another layer of software; with the Chunk class seems to obscure rather than clarify what is happening. C++11 added some wonderful features, and some lousy ones. std::unique_ptr is best avoided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396297556:945,avoid,avoided,945,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396297556,2,['avoid'],['avoided']
Safety,"I haven't read over this, but I don't like the behavior. Assert and friends are for unexpected errors, and fatal is for expected errors. How is abort different from assert?. All errors should give full JVM + Python stack traces. I see this necessary for two reasons: It makes it much easier for users to report bugs to us, which means they get faster turnaround and we spend less time going back and forth about log files (which usually were ephemeral or they've overwritten) and often ""expected"" bugs are actually correct behavior on the user's end and a bug on our side, but no context is given for us to diagnose the real problem. For usability, it is obviously best if the user-visible error appears at the bottom.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990:144,abort,abort,144,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287147990,2,['abort'],['abort']
Safety,"I let ld prune run for a while on a 30GB bgen file. There are known issues here, but I made some changes to avoid some useless work but that included fusing the variant filtering with the ldprune. It's a single stage spark job. It took 1.6 Hours to do a bit more than a third of this file. So that's about 5700 seconds for ~10 GB compared to 160s for 0.7 GB, which is 35:14. I hope to try this after the BGEN fixes to see what the scaling looks like. Anyway, as a part of BlockMatrix IR changes that Daniel will work on, we'll look into why LD Prune isn't within 8x of PLINK.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078#issuecomment-452520165:108,avoid,avoid,108,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452520165,1,['avoid'],['avoid']
Safety,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:419,detect,detected,419,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['detect'],['detected']
Safety,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:11,timeout,timeout,11,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001,1,['timeout'],['timeout']
Safety,"I mostly agree with the last idea, though I would suggest defaulting to (or really only implementing) subsetting rather than minning: minning is highly inconsistent with what's done by GATK. I don't think it's particularly valid to take reads assigned to another haplotype and call them reference (ESPECIALLY for GT - this is likely to lead to some pretty gnarly reference bias that most will want to avoid). And to confirm, what does subset mean in terms of GT? Say for `1/2` where 2 is filtered - will it be `./1`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240847998:401,avoid,avoid,401,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240847998,1,['avoid'],['avoid']
Safety,"I need help with the emit rule. I have what I believe to be an [XY problem](https://en.wikipedia.org/wiki/XY_problem). . The X: Implement maximal independent set in generated code.; The Y: Compile an IR as a function such that it can be passed _in generated code_ to a helper method akin to [this current implementation](https://github.com/chrisvittal/hail/blob/b286ba4a1463a81ec157e6add6d6d56c00de1138/hail/src/main/scala/is/hail/utils/Graph.scala#L50) of maximal independent set. At this point it becomes a simple method call that takes an `UnsafeIndexedSeq`, unpacks it to an array of tuples, and calls one of the other versions of maximal independent set (returning an array) that is then converted so it can be the return type of this `emitI` match arm. Thoughts, advice?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1272085468:543,Unsafe,UnsafeIndexedSeq,543,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1272085468,1,['Unsafe'],['UnsafeIndexedSeq']
Safety,"I need to investigate further, but now I see a segfault and I think it's coming from the LMM tests. I need to fix the `test-gcp.sh` script so that it looks for the coredump file in the case of a seg fault. ```; [Stage 2225:==========================================> (3 + 1) / 4]2017-08-28 21:47:32 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:32 Hail: INFO: Ordering unsorted dataset with network shuffle; 2017-08-28 21:47:33 Hail: WARN: called redundant split on an already split VDS; 2017-08-28 21:47:33 Hail: INFO: using 2 trios for transmission analysis; [Stage 2229:==========================================> (3 + 1) / 4]2017-08-28 21:47:35 Hail: INFO: while writing:; file:/tmp/hail.16cpq9RzwI7a/out.00000.txt; merge time: 65.459ms; 2017-08-28 21:47:35 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:35 Hail: INFO: Ordering unsorted dataset with network shuffle; [Stage 2234:============================================> (4 + 1) / 5]2017-08-28 21:47:37 Hail: WARN: Found 2 samples with missing sex information (not 1 or 2).; Missing sex identifiers: [ 0 ]; 2017-08-28 21:47:37 Hail: WARN: 2 samples discarded from .fam: sex of child is missing.; 2017-08-28 21:47:38 Hail: INFO: Found 250 samples in fam file.; 2017-08-28 21:47:38 Hail: INFO: Found 2000 variants in bim file.; 2017-08-28 21:47:38 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:38 Hail: INFO: Modified the genotype schema with annotateGenotypesExpr.; Original: Struct{GT:Call}; New: Genotype; 2017-08-28 21:47:38 Hail: INFO: Reading table to impute column types; [Stage 2258:============================> (1 + 1) / 2]2017-08-28 21:47:40 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading column `f1' as type String (imputed); Loading column `f2' as type Float64 (imputed); 2017-08-28 21:47:41 Hail: INFO: Reading table to impute column types; 2017-08-28 21:47:41 Hail: INFO: Finished type imputation; Loading column `f0' as type String (imputed); Loading colu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:328,detect,detected,328,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,3,"['detect', 'redund']","['detected', 'redundant']"
Safety,"I noticed that. This isn't ideal. In general, when you're tempted to do this, you should: (1) see if there is a way to restructure the code to use the existing abstractions and avoid the code duplication, or (2) generalize the abstraction to handle your use case and the previous ones without duplication. I need to study the code a bit more to see how'd I proceed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/398#issuecomment-223023873:177,avoid,avoid,177,https://hail.is,https://github.com/hail-is/hail/pull/398#issuecomment-223023873,1,['avoid'],['avoid']
Safety,"I pushed the logic #2861 down so I can remove this copy. I'll add that change to that PR once this is merged (good catch) since I may need to rebase anyway. See here:; https://github.com/hail-is/hail/pull/2861/files#diff-912e03c9c34a874ecdc0e520a13cb572R133. This avoids the copy if the BDM is compact, which blocks always are. If the BDM is not compact, then we could add logic to stream out the bytes without an intermediate compactification but I don't want to add that complexity now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946:264,avoid,avoids,264,https://hail.is,https://github.com/hail-is/hail/pull/2848#issuecomment-363553946,1,['avoid'],['avoids']
Safety,"I read what you wrote as you wanting to wrap RunAggScan in ToStream. I don't have time now, but I will re-run the above with toStream around that node. However, the issues that originally precipitated this observation/fix had nothing to do with RunAggScan. They had to do with the fact that we cannot safely wrap ToStream on something that is TContainer, and then cast ToArray on that. Even if we don't find a bug now, this can't be safe, we need instead to handle each specific type of TContainer (ToArray, ToDict, ToSet)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981:301,safe,safely,301,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586610981,2,['safe'],"['safe', 'safely']"
Safety,"I see, here's the bit that does suggest the prefix:. > Use a naming convention that distributes load evenly across key ranges; > Auto-scaling of an index range can be slowed when using sequential names, such as object keys based on a sequence of numbers or timestamp. This occurs because requests are constantly shifting to a new index range, making redistributing the load harder and less effective. > In order to maintain a high request rate, avoid using sequential names. Using completely random object names gives you the best load distribution. If you want to use sequential numbers or timestamps as part of your object names, introduce randomness to the object names by adding a hash value before the sequence number or timestamp.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680:445,avoid,avoid,445,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680,1,['avoid'],['avoid']
Safety,"I see, it wasn't doing redundant work, just generating redundant IR by regenerating the IR to load covariates per set of phenotypes. This would only affect chained linear regression, since that's the only time there's more than one `one_y_field_name_set in y_field_names`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871:23,redund,redundant,23,https://hail.is,https://github.com/hail-is/hail/pull/9886#issuecomment-760995871,2,['redund'],['redundant']
Safety,I suggest `asyncio.wait_for` for the timeout: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7592#issuecomment-558807050:37,timeout,timeout,37,https://hail.is,https://github.com/hail-is/hail/pull/7592#issuecomment-558807050,1,['timeout'],['timeout']
Safety,I switched the docker retry function back to having a wrapper/curried because I got a clash with the kwargs for the name parameter in one function and figured this was safer then trying to rename the parameter to something that will never clash.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430:168,safe,safer,168,https://hail.is,https://github.com/hail-is/hail/pull/8049#issuecomment-592050430,1,['safe'],['safer']
Safety,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:76,Timeout,TimeoutError,76,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599,1,['Timeout'],['TimeoutError']
Safety,I think I now understand what you're looking for. I set out to build a drop-in replacement for the current AST so that when Jackie's python UI changes were done we could hook this up in place of AST. I will instead build something that will operate on the new Unsafe representations you're introducing. I'll close for now because after removing `DetailedTypeInfo` many tests are broken because the current IR has no way to return `NA` to its caller. I'll reopen when I have something that includes primitives for the Unsafe data.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2224#issuecomment-332360962:260,Unsafe,Unsafe,260,https://hail.is,https://github.com/hail-is/hail/pull/2224#issuecomment-332360962,2,['Unsafe'],['Unsafe']
Safety,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:155,avoid,avoid,155,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,2,['avoid'],['avoid']
Safety,"I think Tim's suggestion and Cotton's #1 are the same, basically? Stash the (possibly) uncompressed bytes in `data` and then decompress only in `getValue` if necessary. This gets us back to previous performance, but we still pay to copy the data even if we never read it. If this is impacting people, we should do that because it seems low-risk and high-value. As I think we all do, I prefer #3 as the long term solution. I found spreading the code across two methods a little confusing. I think ideally there would be just one method that decodes and writes into the RVB. I can pick up a proper re-write this/next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3862#issuecomment-401787058:340,risk,risk,340,https://hail.is,https://github.com/hail-is/hail/issues/3862#issuecomment-401787058,1,['risk'],['risk']
Safety,I think `format` is doing the wrong thing if you were able to affect python output by modifying UnsafeRow.toString. The format method should be using the Type.str method if objects are not strings or other formattable objects (numerics).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8191#issuecomment-593005968:96,Unsafe,UnsafeRow,96,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-593005968,1,['Unsafe'],['UnsafeRow']
Safety,"I think it can still be a little tighter (though it's already a definite improvement over before). How does this sound -- it avoids 'key value' while still being precise:. ```; Tables are joined at rows that have the same value of non-missing key fields.; The inclusion of a row with no matching key in the opposite table depends on the; join strategy:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292:125,avoid,avoids,125,https://hail.is,https://github.com/hail-is/hail/pull/8467#issuecomment-609470292,1,['avoid'],['avoids']
Safety,"I think it's safe to assume there will only be one project for the foreseeable future, and it's the one the cluster is running in. Also, it is in the service account email.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5633#issuecomment-474453397:13,safe,safe,13,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474453397,1,['safe'],['safe']
Safety,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:13,safe,safe,13,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075,1,['safe'],['safe']
Safety,"I think that should cover everything, however I am still failing one test. I think the current docker approach has a convenient quirk of retrying the docker daemon, which allows container deletion to stop a container while it's running, which in turn allows the loop that checks the running container to `raise` and exit. In the subprocess structure that I lifted from `JVMJob`, I'm unconvinced that this is happening correctly. What I thought would happen is that the `delete_container` coroutine would `kill` the container process while the coroutine running the container is still on `await self.process.wait()`. I would hope that means `self.process.wait` would soon raise but I'm now consistently seeing `test_cancel_left_after_tail` time out, indicating that it wasn't able to successfully cancel the sleeping job. EDIT: Jackie and I investigated and think that `kill` causes the `stdout/stderr` streams not to send EOF but rather set an exception, which means that the `run` coroutine can hang on gathering the output of the container. Going to run this test repeatedly to see if the intermittent timeouts stop and that this is actually the cause of the problem. EDIT2: ugh didn't seem to fix the problem. There's something still wrong in cancel/deleting containers where we intermittently wait until the job timeout regardless.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10376#issuecomment-857271742:1104,timeout,timeouts,1104,https://hail.is,https://github.com/hail-is/hail/pull/10376#issuecomment-857271742,2,['timeout'],"['timeout', 'timeouts']"
Safety,"I think the takeaway is: notebook's ability to create pods makes it a security risk, so we gotta treat all this code with care.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018:79,risk,risk,79,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018,1,['risk'],['risk']
Safety,"I think there should be a warning in the Coalesce command if k > nPartitions, and the VSM.coalesce function should require that k < nPartitions. That seems pretty safe. . Other than that, I am happy with this!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/686#issuecomment-242558645:163,safe,safe,163,https://hail.is,https://github.com/hail-is/hail/pull/686#issuecomment-242558645,1,['safe'],['safe']
Safety,I think this PR still has good changes. We should avoid nesting when possible; I fear it leads to confusing situations. I'm gonna take it off the release 0.2.125 checklist though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341:50,avoid,avoid,50,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341,1,['avoid'],['avoid']
Safety,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:590,timeout,timeout,590,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,8,['timeout'],['timeout']
Safety,I think this is subsumed by the generic genotype interface + ongoing unsafe work.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/55#issuecomment-316223325:69,unsafe,unsafe,69,https://hail.is,https://github.com/hail-is/hail/issues/55#issuecomment-316223325,1,['unsafe'],['unsafe']
Safety,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:418,safe,safest,418,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,1,['safe'],['safest']
Safety,"I think we need this fix:; ```; commit 4cb998d1c7cbc9954d66c6e39d7fd48b0e936f51 (HEAD -> add-version-endpoint); Author: Daniel King <dking@broadinstitute.org>; Date: Mon Mar 22 17:47:22 2021 -0400. fix. diff --git a/build.yaml b/build.yaml; index 7a100adec8..256ca99c91 100644; --- a/build.yaml; +++ b/build.yaml; @@ -86,7 +86,7 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; - make -C hail python/hail/hail_version python/hail/hail_pip_version; + make -C hail python-version-info; git rev-parse HEAD > git_version; outputs:; - from: /io/repo/auth/sql; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index 3b1df5214c..b994d2787c 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -27,10 +27,13 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; + make -C hail python-version-info; timeout: 300; outputs:; - from: /io/repo; to: /; + - from: /io/repo/hail/python/hail/hail_version; + to: /hail_version; dependsOn:; - inline_image; - kind: buildImage; @@ -52,6 +55,10 @@ steps:; publishAs: service-base; dependsOn:; - base_image; + - copy_files; + inputs:; + - from: /hail_version; + to: /hail_version; - kind: buildImage; name: hello_image; dockerFile: ci/test/resources/Dockerfile; ```; EDIT: updated with more changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107:862,timeout,timeout,862,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107,1,['timeout'],['timeout']
Safety,"I think we should avoid the term 'gender' here, and replace it with 'sex'. From Wikipedia:. > The distinction between sex and gender differentiates sex (the anatomy of an individual's reproductive system, and secondary sex characteristics) from gender, which can refer to either social roles based on the sex of the person (gender role) or personal identification of one's own gender based on an internal awareness (gender identity).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/386#issuecomment-224895254:18,avoid,avoid,18,https://hail.is,https://github.com/hail-is/hail/pull/386#issuecomment-224895254,1,['avoid'],['avoid']
Safety,I think you need non-default timeouts in job.py `unschedule_job` and in instance.py `check_is_active_healthy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11830#issuecomment-1126299805:29,timeout,timeouts,29,https://hail.is,https://github.com/hail-is/hail/pull/11830#issuecomment-1126299805,1,['timeout'],['timeouts']
Safety,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:147,timeout,timeout,147,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,3,['timeout'],['timeout']
Safety,"I tossed this up in my namespace and this is what seems to be the issue:; ```. Job Step	Image Pulling Time (s)	Running Time (s)	Error Type	State; main	0.135	30.011	timed out	error; Logs; Main; Log; executable file `sleep 5` not found in $PATH: No such file or directory; Error; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/batch/worker/worker.py"", line 680, in _run; raise ContainerTimeoutError(f'timed out after {self.timeout}s'); ContainerTimeoutError: timed out after Nones",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811:455,timeout,timeout,455,https://hail.is,https://github.com/hail-is/hail/pull/11397#issuecomment-1076793811,1,['timeout'],['timeout']
Safety,"I will do a quick sanity check on this before dismissing the review and requesting anew. Right now no tests cover this, but @jbloom22's framework could if we really wanted (annotate a multi-allelic, split it using this, and assert that each allele has at least one annotation)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4218#issuecomment-416754740:18,sanity check,sanity check,18,https://hail.is,https://github.com/hail-is/hail/pull/4218#issuecomment-416754740,1,['sanity check'],['sanity check']
Safety,"I'd also like to mention that linewidth in general seems to have cross-browser issue (https://github.com/mrdoob/three.js/issues/10357). Most of the issues detailed are for line width > 1, which is not an issue for us. The fact that safari requires this (< 1) to normalize its behavior with other browsers is unfortunate, but presumably when fixed, it would just truncate to 1 anyhow (this is what every other browser appears to do). So we have 2 ideas on the table: 1) use the fix proposed, because linewidth is mainly problematic when > 1, and truncating setPixelRatio is uncontroversial (line resolution is too low with 1, and using setPixelRatio allows us to get a viewport-independent correct resolution), 2) use an entirely different geometry that does not rely on linewidth. Option 2 is both more technically sound, and also a bunch of additional work that yield no visual benefit. . If you want option 2, I am happy to close this request, and have someone else PR an entirely new background geometry. I don't have time for that unfortunately, and it seems too much work without a visible problem to address. Alternatively we accept this PR, take the fix, and keep an eye out for visual inconsistencies in the corner case that we are worried about (low resolution displays, and safari, which has something like 2% market share). edit: A third solution is to try to detect safari and implement linewidth < 1 only for it. Unfortunately browser detection can be inconsistent, which is why I didn't use that solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-645598185:1371,detect,detect,1371,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645598185,2,['detect'],"['detect', 'detection']"
Safety,"I'd like to avoid the backwards incompatibility here if possible, but if we're going to do it, we should make sure future changes can be done backwards compatibility.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1260921810:12,avoid,avoid,12,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1260921810,1,['avoid'],['avoid']
Safety,"I'll do a performance test, but there's still foreign key constraints on these rows. They're just redundant. We don't need a check on both `batches` and `attempts`. The rows in `attempts` wouldn't have been inserted without the check in `batches`. All of these proposed changes don't change anything about data integrity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811:98,redund,redundant,98,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811,1,['redund'],['redundant']
Safety,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:867,redund,redundant,867,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,1,['redund'],['redundant']
Safety,"I'm a little worried that the LAPACK calls we do currently for QR, SVD, etc. might be incompatible with slices. Not a hard fix, but all of the stride arguments to those mostly just inspect the shape, which is no longer a safe thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10614#issuecomment-867764533:221,safe,safe,221,https://hail.is,https://github.com/hail-is/hail/pull/10614#issuecomment-867764533,1,['safe'],['safe']
Safety,I'm going to stack this PR because I need to build on `unsafeInsert`. Will reopen from a branch on hail-is/hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2317#issuecomment-337983661:55,unsafe,unsafeInsert,55,https://hail.is,https://github.com/hail-is/hail/pull/2317#issuecomment-337983661,1,['unsafe'],['unsafeInsert']
Safety,"I'm gonna push a change that puts a hard 2 minute limit on all tests, we'll see which ones timeout, then I'll mark the ones that are legitimately slow with a per-test timeout. Hopefully this will isolate us down to both the test and particular portion of code that's getting stuck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144:91,timeout,timeout,91,https://hail.is,https://github.com/hail-is/hail/pull/13122#issuecomment-1568417144,2,['timeout'],['timeout']
Safety,"I'm looking at `worker.py` now and it looks like you worked around this with the addition of `ignore_job_deletion`, which maybe Dan wasn't aware of but is still a workaround since Timings just shouldn't care about deletion in the first place. Without that flag you'd get this:. 1. Running step would start; 2. Job is cancelled, so `Job.deleted` would be set to `True`.; 3. Job would set the Container's `deleted_event`, which would abort the run function inside `run_until_done_or_deleted`; 4. Container would jump to the uploading logs step, which would raise a job deleted error before running `upload_log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946:432,abort,abort,432,https://hail.is,https://github.com/hail-is/hail/pull/11429#issuecomment-1054613946,1,['abort'],['abort']
Safety,I'm not sure how I flubbed the network. I had to do that manually to avoid recreating everything. I should've verified the terraform was still accurate afterwards.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12991#issuecomment-1564515180:69,avoid,avoid,69,https://hail.is,https://github.com/hail-is/hail/pull/12991#issuecomment-1564515180,1,['avoid'],['avoid']
Safety,"I'm not sure how this is unsafe. Could you explain?. I think that if we have code producing an RDD with a mismatched partitioner, we should correct that code. We have to be able to assume that an RDD is accurately reflected by its partitioner",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1864#issuecomment-302978218:25,unsafe,unsafe,25,https://hail.is,https://github.com/hail-is/hail/pull/1864#issuecomment-302978218,1,['unsafe'],['unsafe']
Safety,I'm personally in favor of this change adding a switch to the protocol. It avoids running the compiler.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11598#issuecomment-1069292016:75,avoid,avoids,75,https://hail.is,https://github.com/hail-is/hail/pull/11598#issuecomment-1069292016,1,['avoid'],['avoids']
Safety,I'm waiting for #3997 to pass so I can do an unsafe double merge though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3996#issuecomment-408198375:45,unsafe,unsafe,45,https://hail.is,https://github.com/hail-is/hail/pull/3996#issuecomment-408198375,1,['unsafe'],['unsafe']
Safety,"I've addressed the two comments: now using an `entry_fields` parameter and throwing an error if 'dosage' is requested and any variant is multi-allelic. Docs updated accordingly. I considered setting dosage on multi-allelics to missing rather than throwing an error, but I think error is safest since I could imagine the missingness leading to QC confusion, and if users want dosage in the presence of multi-allelics than they should either use a custom expression or split and then use `hl.gp_dosage`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913:287,safe,safest,287,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913,1,['safe'],['safest']
Safety,"I've been looking into why this is happening. The problem is triggered in VEP.scala, line 350 (even though this line succeeds when `csq=False`). The first two calls are successful (i.e. lines 348349):; ```scala; rvb.addAnnotation(vepRowType.types(0), v.asInstanceOf[Row].get(0)); rvb.addAnnotation(vepRowType.types(1), v.asInstanceOf[Row].get(1)); ```; but the last one fails:; ```scala; rvb.addAnnotation(vepRowType.types(2), vep); ```. In my experiments, `vepRowType` has the value `struct{locus: locus<GRCh37>, alleles: array<str>, vep: array<str>}`. `vep`'s string representation is also `array<str>`, so I'm not sure what's causing the issue. The exception is actually generated in `RegionValueBuilder.scala`, in the `addAnnotation` method. In the pattern match on type, the `TArray` case is correctly matched on, but neither `UnsafeIndexedSeq` or `IndexedSeq[Annotation]` then match, so we get the `MatchError`. I'm afraid I don't know enough about Hail's type system. Is there a case missing from the `TArray` match, or are the current ones correct and the annotations are being created wrong?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790#issuecomment-400290111:833,Unsafe,UnsafeIndexedSeq,833,https://hail.is,https://github.com/hail-is/hail/issues/3790#issuecomment-400290111,1,['Unsafe'],['UnsafeIndexedSeq']
Safety,"I've rebased `jbloom22:lmm_getthisin` onto `jbloom22:py_reg` (which should go in first) and made the parallel modifications (LinearMixedModelCommand is gone, command line relics gone, refactored using RegressionUtils, tests modified to accommodate changing interface, etc). I'll do some command line testing next to be safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058:319,safe,safe,319,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-272765058,1,['safe'],['safe']
Safety,"If there's no requester pays, is it just impossible to have ""public"" data in Azure storage safely? Like anything in there could be downloaded infinity times to drive up a bill?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518:91,safe,safely,91,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1004205518,1,['safe'],['safely']
Safety,"In `httpx` the 5s is a ""total timeout"" which is applied to read and connect. I don't know what connection idle and write exactly refer to but setting to 5s and seeing how the tests react seems like a good start. In general, in a datacenter, I'd expect any ordinary network operation to be faster than 5s regardless of write vs read.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13344#issuecomment-1659278243:30,timeout,timeout,30,https://hail.is,https://github.com/hail-is/hail/pull/13344#issuecomment-1659278243,1,['timeout'],['timeout']
Safety,"In our dev environment the first query runs in 10s, but the second one runs in 77s, if it ran in 20 we'd be fine with it. As a sanity check, the first query should have 3 results and the second should have 85",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1821470558:127,sanity check,sanity check,127,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1821470558,1,['sanity check'],['sanity check']
Safety,"Increasing the executor memory per core to 20G/core seemed to help get by this memory error. . It would be useful to have some rule of thumbs for estimating memory requirements based on number of samples and variants. spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 12\; --executor-cores 4\; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 80G\; --driver-memory 60g\; --driver-cores 1\; --name ""$1"" \; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303:645,timeout,timeout,645,https://hail.is,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303,1,['timeout'],['timeout']
Safety,"Indeed 5 seconds is the default timeout we use in httpx.py. Let's do 5s. If we would prefer a different default global timeout, let's do that as a separate PR. 15s also seems reasonable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13344#issuecomment-1659273318:32,timeout,timeout,32,https://hail.is,https://github.com/hail-is/hail/pull/13344#issuecomment-1659273318,2,['timeout'],['timeout']
Safety,"Indeed in my streamify, forcing MakeArray to remain a MakeArray fixes the problem. Now to investigate why MakeStream is the wrong solution, and why the new streamify isn't handling this correctly. to be clear, this branch finds the MakeArray inside of the MakeTuple and generates a ToArray(MakeStream), which both seems not super wrong and redundant. But the fact that's it's a value issue, with an array reading garbage, also make it look like a requiredeness/ copy function issue (though this was previously tested)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511:340,redund,redundant,340,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583813511,2,['redund'],['redundant']
Safety,Is 5 seconds for all of these types of timeouts?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13344#issuecomment-1659274391:39,timeout,timeouts,39,https://hail.is,https://github.com/hail-is/hail/pull/13344#issuecomment-1659274391,1,['timeout'],['timeouts']
Safety,"It doesn't seem like a bad change. I suspect it's rare for this range to be more than 2 long, and I'd hope that the InsertFields avoids copying the entire row, so I'd be surprised if this ever made a significant difference. But I also doubt adding memory management would slow it down much, so better to be safe I guess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315:129,avoid,avoids,129,https://hail.is,https://github.com/hail-is/hail/pull/12037#issuecomment-1187948315,2,"['avoid', 'safe']","['avoids', 'safe']"
Safety,"It seems we dropped the ball on this, but I think this is old enough now to be no longer relevant. Safe to close?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/683#issuecomment-292305976:99,Safe,Safe,99,https://hail.is,https://github.com/hail-is/hail/issues/683#issuecomment-292305976,1,['Safe'],['Safe']
Safety,"It uses `hasMissingValues` now, which already has a test. I'm beefing up that test now to be safe. `test_linreg` fails if you swap the intercept with the other covariate, I can add that too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8575#issuecomment-615433780:93,safe,safe,93,https://hail.is,https://github.com/hail-is/hail/pull/8575#issuecomment-615433780,1,['safe'],['safe']
Safety,"It's flakey when we have 32 open PRs, it's not flakey if we have 10. I can adjust the timeout to be more generous.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5638#issuecomment-474580215:86,timeout,timeout,86,https://hail.is,https://github.com/hail-is/hail/pull/5638#issuecomment-474580215,1,['timeout'],['timeout']
Safety,"Ill look for closely once I get to the retreat, but first impression is that centering and normalizing are redundant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054:108,redund,redundant,108,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054,1,['redund'],['redundant']
Safety,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:355,avoid,avoid,355,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,2,['avoid'],['avoid']
Safety,"Last thing before I give up for now. There's this comment in the BlobWriterRetry code:. ```; /** Write channel implementation to upload Google Cloud Storage blobs. */; class BlobWriteChannel extends BaseWriteChannel<StorageOptions, BlobInfo> {. private final ResultRetryAlgorithm<?> algorithmForWrite;; // Detect if flushBuffer() is being retried or not.; // TODO: I don't think this is thread safe, and there's probably a better way to detect a retry; // occuring.; private boolean retrying = false;; private boolean checkingForLastChunk = false;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1564809125:306,Detect,Detect,306,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1564809125,3,"['Detect', 'detect', 'safe']","['Detect', 'detect', 'safe']"
Safety,"Let's make the change to avoid compression below a threshold, rerun benchmarks, and get this merged!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1557742021:25,avoid,avoid,25,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1557742021,1,['avoid'],['avoid']
Safety,"Let's not approve this right now. I don't think deploying will make a difference, but I'd rather not risk it while the BroadE is happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023:101,risk,risk,101,https://hail.is,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023,1,['risk'],['risk']
Safety,"Looks like it was opened and closed here #12381. I haven't taken a look at this yet, but there are two footguns that should be avoided as much as possible:. - A job waiting on a batch that it is a part of. We add the batch id into the container so we should be able to throw an error here; - Waiting on a batch in general is really wonky when there is more than 1 entity controlling it. I don't know of a good way to control this, so it might just be a ""be sure you know what you're doing thing"", but worth thinking about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229:127,avoid,avoided,127,https://hail.is,https://github.com/hail-is/hail/pull/12530#issuecomment-1334441229,1,['avoid'],['avoided']
Safety,"Looks like slightly slower in 0.1; ```; 2018-06-26 01:47:57 Hail: INFO: Number of BGEN files parsed: 1; 2018-06-26 01:47:57 Hail: INFO: Number of samples in BGEN files: 487409; 2018-06-26 01:47:57 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-06-26 01:49:08 Hail: INFO: Coerced almost-sorted dataset; 2018-06-26 01:49:08 Hail: INFO: No multiallelics detected.; 2018-06-26 01:49:09 Hail: INFO: interval filter loaded 27 of 586 partitions; ```. I'll kill what I have and run a single regression, since this will take a long time.; ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; 2018-07-18 15:43:13 Hail: WARN: 126215 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:43:13 Hail: INFO: linear_regression: running on 361194 samples for 110 response variables y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:06 Hail: WARN: 132571 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:06 Hail: INFO: linear_regression: running on 354838 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:44:59 Hail: WARN: 132781 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:44:59 Hail: INFO: linear_regression: running on 354628 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07-18 15:45:42 Hail: WARN: 133165 of 487409 samples have a missing phenotype or covariate.; 2018-07-18 15:45:42 Hail: INFO: linear_regression: running on 354244 samples for 1 response variable y,; with input variable x, intercept, and 25 additional covariates...; 2018-07",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965:372,detect,detected,372,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405979965,1,['detect'],['detected']
Safety,Looks like we're not detecting the vector extensions supported by your CPU correctly. Can you post the output of `c++ -v` and `sysctl -a | grep machdep.cpu`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274164170:21,detect,detecting,21,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274164170,1,['detect'],['detecting']
Safety,"Maybe no longer relevant, but zeroing missings *after* centering is; equivalent to using non-missing terms only rather than mean imputing,; provided you then use N_nonmissing for the final normalization. On Tue, Dec 10, 2019 at 8:50 AM Jon Bloom <notifications@github.com> wrote:. > Ill look for closely once I get to the retreat, but first impression is; > that centering and normalizing are redundant.; >; > ; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/7653?email_source=notifications&email_token=ACC577VJUORGGYMDZUE72IDQX6NDNA5CNFSM4JVAFXT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGPJKXQ#issuecomment-564041054>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC577UWWGRAMQHAOV7TGADQX6NDNANCNFSM4JVAFXTQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326:394,redund,redundant,394,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326,1,['redund'],['redundant']
Safety,Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're pure unsafe.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2089#issuecomment-322304632:110,unsafe,unsafe,110,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-322304632,1,['unsafe'],['unsafe']
Safety,"New version. In fact both uses in the code check on the chromosome and NOT in the PAR position, namely inNonParX (which is different from !inParX). And I think conceptually users like Kyle want both inParX and inNonParX in the expr langauge, rather than having to write `!inParX && contig == X` for the latter. I agree that inParXPos won't be used except through inParX and inNonParX but I wanted to avoid duplicating the intervals. as discussed in hail-dev, I also think we should change ""23"" to ""X"" on Plink import so that we are consistent on ""v.contig = X"" being the proper check throughout Hail. Then in ImputeSexPlink we can change:. ```; if (!includePar); (v.contig == ""X"" || v.contig == ""23"") && !v.inParXPos; else; v.contig == ""X"" || v.contig == ""23""; ```. to. ```; if (!includePar); v.inNonParX; else; v.contig == ""X""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499#issuecomment-235629724:400,avoid,avoid,400,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235629724,1,['avoid'],['avoid']
Safety,"Not really sure why this is coming up now (I don't see anything that changed in this PR, but the scala isn't compiling because of a redundant function definition in is.hail.expr.types and is.hail.expr.ir (coerce)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089:132,redund,redundant,132,https://hail.is,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089,1,['redund'],['redundant']
Safety,"Note that this is passing tests, except for the timeout issue we're seeing everywhere.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14517#issuecomment-2107719350:48,timeout,timeout,48,https://hail.is,https://github.com/hail-is/hail/pull/14517#issuecomment-2107719350,1,['timeout'],['timeout']
Safety,"Now passing by killing safety on readPartitions, I've updated the comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758:23,safe,safety,23,https://hail.is,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758,1,['safe'],['safety']
Safety,"Nuked `UnsafeRowBuilder` and associated tests. There was a bug in the tests that the stronger asserts in MemoryBuffer caught. Easier to nuke than fix, since we aren't planning to use it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2074#issuecomment-321102215:7,Unsafe,UnsafeRowBuilder,7,https://hail.is,https://github.com/hail-is/hail/pull/2074#issuecomment-321102215,1,['Unsafe'],['UnsafeRowBuilder']
Safety,"OK, I improved the tests two ways:. 1. I allocate a random amount of memory in the region to start so things don't always start at offset 0. 2. I test addRegionValue adding a value at the top level and and a nested level (by allocating a non-unsafe Row when t == TStruct) so it calls through to RVB.addRow. I verified it would have caught the previous errors, and it caught another error (toOff was wrong in addRegionValue because we called currentOffset before allocateRoot). Hopefully good to go now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521:242,unsafe,unsafe,242,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336949521,1,['unsafe'],['unsafe']
Safety,"OK, I think things should be working now. I asked @catoverdrive to take a look at the randomness change for sanity check.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5426#issuecomment-467240584:108,sanity check,sanity check,108,https://hail.is,https://github.com/hail-is/hail/pull/5426#issuecomment-467240584,1,['sanity check'],['sanity check']
Safety,"OK, I think this is actually ready for a real review. Almost everything was spurious (I marked as such, so hopefully we won't have to do that on every PR). There were a few real things:; 1. use integrity checks for CDN javascript libraries; 2. don't let edits to the search textbox modify the URL arbitrarily; 3. don't let the target pages of anchor tags mutate the source page's DOM (wtf, how is this the default behavior???); 4. don't send the IntegrityError from mysql back to the users. I think this is basically safe because of how restrictive we are with which error is printed, but it's also not necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860:517,safe,safe,517,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860,1,['safe'],['safe']
Safety,"OK, here's my understanding of prometheus and our use of it after chatting with @daniel-goldstein :; 1. The Python client treats Summary as just a pair of a Gauge and Counter. It's only useful if you need to track the number of times you set/increments/decrement a value and the value itself. For example, the total number of visits to a web page. Some clients for other languages treat Summaries as histogram-like things, but the Python client does not.; 2. A Histogram, with a domain-relevant array of buckets, is the right tool for visualizing a distribution of values. In this PR, we treat percent-of-cores-in-use-on-instance as a Histogram. This should let us see the distribution of instances according to what percent of the cores are revenue-generating versus not.; 3. A Gauge is the right tool for visualizing any other value. In this PR, we use a gauge to measure the total jobs, the used cores, the total free cores, the total cores, the total cost per hour (ignoring disk), and the total revenue per hour (ignoring disk). ; 4. It is important to use `remove` for metrics whose label set changes over time. For example, if all the jobs owned by a particular user finish, then the metrics labelled with that user ought to become `0`. The database query will elide such records; therefore, it is important to `remove` such labels from the `USER_CORES` and `USER_JOBS`. We prefer `remove` to `clear` so as to avoid the case where prometheus collects metrics in between a call to `clear` and a call to `set` which restores the value for a still valid label.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12253#issuecomment-1273927865:1417,avoid,avoid,1417,https://hail.is,https://github.com/hail-is/hail/pull/12253#issuecomment-1273927865,2,['avoid'],['avoid']
Safety,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1375,risk,risk,1375,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146,2,['risk'],['risk']
Safety,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:476,abort,abort,476,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['abort'],['abort']
Safety,"OK, update from Google: they suggest we check if the preemptible quota is non-zero and assume that if it is non-zero preemptible is in use and if it is zero normal quota is in use. In very rare cases, people can increase and then reduce their preemptible quota and Hail will not work properly. Google wasn't interested in providing an API to detect this case. I'll change this PR accordingly sometime next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354:342,detect,detect,342,https://hail.is,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354,1,['detect'],['detect']
Safety,"OK.; ```; [root@tele-1 ~]# pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/10 09:10:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/10 09:10:21 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/10 09:10:21 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; ```; ----------------------------; ```; >>> rdd = sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; ```; ----------------------------------; ```; >>> vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); hail: warning: `/hail/test/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071:808,detect,detected,808,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321424071,1,['detect'],['detected']
Safety,"Oh yes duh, thanks. Will fix. On Tue, Apr 13, 2021, 8:29 AM Tim Poterba ***@***.***> wrote:. > ***@***.**** commented on this pull request.; > ------------------------------; >; > In hail/src/main/scala/is/hail/annotations/UnsafeRow.scala; > <https://github.com/hail-is/hail/pull/10304#discussion_r612399889>:; >; > >; > case r: Row =>; > r.toSeq.forall(isSafe); > case a: IndexedSeq[_] =>; > a.forall(isSafe); > case i: Interval =>; > isSafe(i.start) && isSafe(i.end); > + case nd: NDArray =>; > + isSafe(nd.getRowMajorElements().forall(isSafe)); >; > oh! I see.; >; > Should be; >; > nd.getRowMajorElements().forall(isSafe)); >; > though, right? (still probably works as written, but does an extra isSafe; > on the Boolean returned by forall); >; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/10304#discussion_r612399889>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADJCWEQ7FSKURD6DFT3KGPDTIQ2JBANCNFSM422IOPNA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10304#issuecomment-818697173:223,Unsafe,UnsafeRow,223,https://hail.is,https://github.com/hail-is/hail/pull/10304#issuecomment-818697173,1,['Unsafe'],['UnsafeRow']
Safety,"Oh, huh. There's also `SafeRow` which seems to duplicate `safeFromBaseStructRegionValue`. I prefer all this functionality to be in one spot. @cseed @catoverdrive thoughts? The options seem to be:. - `Annotation.safeFromRegionValue`, `.safeFromBaseStructRegionValue`, `.safeFromArrayRegionValue`; - `SafeRow.read`, `SafeRow.apply`, `SafeIndexedSeq` (naming?). I prefer methods on `Annotation`, though I'm open to shorter names. I always found it peculiar that `UnsafeRow.read` didn't read an `UnsafeRow` (it reads an `Annotation`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3353#issuecomment-380454955:23,Safe,SafeRow,23,https://hail.is,https://github.com/hail-is/hail/pull/3353#issuecomment-380454955,10,"['Safe', 'Unsafe', 'safe']","['SafeIndexedSeq', 'SafeRow', 'UnsafeRow', 'safeFromArrayRegionValue', 'safeFromBaseStructRegionValue', 'safeFromRegionValue']"
Safety,"On my rowstore1 branch I now have it working with careful use of flock() (to steer clear of the cases; where NFS behavior diverges from local filesystems, using perl's rename command (which should; be safe assuming it uses the POSIX rename() syscall) to avoid having to lock/unlock from a Makefile. Also a bunch of Makefile changes to use commands from /bin or /usr/bin when they exist, but; otherwise to give a warning and pick up whatever might be found on $PATH. That seems a suitable; compromise between avoid-mysterious-behavior and give-best-effort-on-nonstandard-platform.; [In doing so, I noticed that I actually was picking up /Users/rcownie/anaconda2/envs/py36/bin/curl; rather than /usr/bin/curl - and I don't know whether there's any difference]. But current consensus is that we should figure out how to ship with a known-good compiler; and libraries, so I'm looking into that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413647683:201,safe,safe,201,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413647683,6,"['avoid', 'safe']","['avoid', 'avoid-mysterious-behavior', 'safe']"
Safety,"On the first sentence, RFC would be great, and yeah, having a by_time and by_unit would be generally useful. It might be nice to eventually charge a fixed per-job fee if pre-job costs begin to dominate for short lived jobs. ---. On the last sentence:. There are two major questions, the first of which is much higher priority. We probably need to do a bit of research, at least on the second question. . 1. How can we allow public Internet egress without risking untracked cost? I suspect we must track bytes and charge some, possibly very high, rate. 2. How can we allow public Internet egress at or near the real cost to us?. The second question is complex because Google's egress pricing is complex. To directly respond to your comment: I don't think we need to disaggregate by destination IP address, but we do need to disaggregate by destination ""type & location"". GeoIP _might_ allow us to do this in iptables, we should figure out what is and isn't possible and how hard it would be. ---. The following is distilled from [Network Pricing](https://cloud.google.com/vpc/network-pricing). There are six types of egress:; 1. VM-to-Internet; 2. VM-to-VM or VM-to-Google-Service (which are charged equally); 3. Spanner-to-VM; 4. VM-to-Spanner; 5. GCS-to-VM; 6. VM-to-GCS. Egress types (3) and (5) do not apply to us because hail-vdc does not have Spanner and user jobs cannot read from hail-vdc buckets. Egress types (4) and (6) are slightly ambiguous. We should create a support ticket to verify, but I believe they're charged just like (2). This means we are concerned with just two types of egress:. 1. VM-to-Internet; 2. VM-to-VM / VM-to-Google-Service. Each type has a different cost table based on the _destination location_. In these tables, the cheapest price applies, so, for example, for traffic form us-central1-a to us-central1-a the within-zone price applies, not the within-region price. 1. VM-to-Internet. Prices decrease with more usage.; 1. Standard Tier Networking. For the first 10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526:455,risk,risking,455,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692168526,1,['risk'],['risking']
Safety,"On this note, could the keys that the join was actually performed on be printed separately? Would make a nice sanity check",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951:110,sanity check,sanity check,110,https://hail.is,https://github.com/hail-is/hail/issues/4485#issuecomment-429003951,1,['sanity check'],['sanity check']
Safety,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244:43,avoid,avoid,43,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244,2,['avoid'],['avoid']
Safety,PRs having a fresh namespace for images (perhaps with some explicitly seeded ones) also avoids adding a new image that isn't in images.txt which seems like a feature.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12378#issuecomment-1291177961:88,avoid,avoids,88,https://hail.is,https://github.com/hail-is/hail/pull/12378#issuecomment-1291177961,1,['avoid'],['avoids']
Safety,"Please be picky! You can see what the new UI looks like by checking out this branch and running `make devserver SERVICE=batch`. If you can, I'd also appreciate a sanity check dev deploy to make sure the links to other apps work. I dev deployed it myself but it's hard to make sure I covered all the bases. I struggled a bit with making it mobile friendly but I hope this general approach is a good enough improvement over the current markup. I also don't have much of an opinion on colors if you have thoughts there. I was trying to go for cool and neutral and might have accidentally ended up with ""dentist office""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741:162,sanity check,sanity check,162,https://hail.is,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741,1,['sanity check'],['sanity check']
Safety,Points 2 and 3 in the description are still remaining. It should be safe to manually modify the global-config in `hail-vdc` but might want to just poke around CI afterward to check that the change is picked up successfully and that PRs are succeeding.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13545#issuecomment-1766805213:68,safe,safe,68,https://hail.is,https://github.com/hail-is/hail/issues/13545#issuecomment-1766805213,1,['safe'],['safe']
Safety,RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2511,abort,abortStage,2511,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['abort'],['abortStage']
Safety,"Re. the questions about the PCA step, I think you'll be beter off modifying `_hwe_normalized_blanczos`. For one thing, this ensures that PC-AiR always returns results in the same form as normal PCA. More importantly, `_hwe_normalized_blanczos` performs the SVD using a ""tall-skinny matrix"" representation, which is just a table of matrices (2d ndarrays). This is more efficient than using block matrices for several reasons that aren't directly relevant here. The result of the SVD is computed as local numpy ndarrays. Given these forms of the data, projecting the related sampled onto the computed PCs should be straightforward and efficient. But once everything is converted to tables and matrixtables, it's much harder and does a lot of redundant work. Let me know if you want to schedule a time to walk through the PCA internals and where you can plug in to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230:740,redund,redundant,740,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1967533230,1,['redund'],['redundant']
Safety,"Re: this interface:; ```scala; def apply(i: Int): Option[Int] = {; setGenotype(i); if (hasGT) Some(getGT) else None; }; ```; It's entirely for performance reasons. We never want to allocate or process `Option`s anywhere, and there's some overhead we can avoid with calling `setGenotype(i)` twice if we use two methods for `hasGtIdx(i: Int): Boolean ` and `getGtIdx(I: Int): Int`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365:254,avoid,avoid,254,https://hail.is,https://github.com/hail-is/hail/pull/2368#issuecomment-340901365,1,['avoid'],['avoid']
Safety,Reducing pro because other PRs will avoid triggering this and are more important.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11384#issuecomment-1050087560:36,avoid,avoid,36,https://hail.is,https://github.com/hail-is/hail/pull/11384#issuecomment-1050087560,1,['avoid'],['avoid']
Safety,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:1185,avoid,avoid,1185,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,2,['avoid'],['avoid']
Safety,"Regex isn't used to detect the optional slash because doing so doesn't really save any lines of code, because nginx does not directly allow proxy_pass with a trailing slash inside of regex-containing locations, without a rewrite rule (and that rewrite rule costs 1 line). https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762:20,detect,detect,20,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762,1,['detect'],['detect']
Safety,"Removing a plural, for example:. movies = movies.explode('genres', name='genre'). I feel like this is natural and will be pretty common so it should be easy. Yes, I agree it is redundant. The alternative is:. movies = movies.explode('genres').rename({'genres': 'genre'}). which duplicates the column being exploded. If we're happy with the latter, I'm happy to close this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981:177,redund,redundant,177,https://hail.is,https://github.com/hail-is/hail/pull/2985#issuecomment-368348981,1,['redund'],['redundant']
Safety,"Rereading the tutorial example reminds me that `{j.counts}` gives me the basename of the resource group. So the following would work:. ```python; j = b.new_job(). j.declare_resource_group(counts={; 'tsv.gz': '{root}.counts.tsv.gz',; 'tsv.gz.tbi': '{root}.counts.tsv.gz.tbi',; }). j.command(f""""""; gatk SubCommand  --output {j.counts}.counts.tsv; bgzip {j.counts}.counts.tsv; gatk IndexFeatureFile --input {j.counts['tsv.gz']}; """"""). b.write_output(j.counts, output_base_path); ```. This is perhaps a better use of the `ResourceGroup` as reflecting that I want to delocalise this entire group (of two files). This version is worse in that the command string now contains duplicated hardcoded appearances of the `.counts.tsv` extension in the various commands. This is an invitation to typos that will only be detected as file-not-found errors when the job runs, rather than earlier as invalid-key-for-`j.counts` errors at batch submission time. I may convert the code to use this two-file group. However there is still a benefit in the three-file group version as it avoids this repetition of filenames, so it would still be good in general to address the bug mentioned in the last paragraph of the initial comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192:809,detect,detected,809,https://hail.is,https://github.com/hail-is/hail/issues/13191#issuecomment-1599679192,2,"['avoid', 'detect']","['avoids', 'detected']"
Safety,"Reviving this as would be great to have! I haven't added it myself since we discussed using the python way using `{}` (e.g. `{ 1 , 2 ,3}`). When I looked at implementing it based on the Array constructor, I saw some of the compiler things @danking added, so wasn't sure how safe it was to just mimic it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/598#issuecomment-284758059:274,safe,safe,274,https://hail.is,https://github.com/hail-is/hail/issues/598#issuecomment-284758059,1,['safe'],['safe']
Safety,"Right, I was trying to suggest that run_forever appears to catches the thrown errors, and that the issue happens because we hit read_timeout (120s), which is an upstream issue. . Tests: #5065. What I think is happening: Flask is a blocking server, so while it waits that 120s, nothing else can happen. The upstream issue isn't resolved, and it keeps blocking. You see this in the 2nd set in particular (~10 timeout requests happen). While the upstream issue should be solved, I think we should also follow Flask best practices, and preferably use Gunicorn + async/green-thread workers + N kernel threads/processes. . Additionally, I would recommend we test a move to Falcon (keeping Gunicorn). It should be far faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690:407,timeout,timeout,407,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-451216690,1,['timeout'],['timeout']
Safety,"Root cause found. Each time a batch test runs it generates a bunch of garbage because batch unsafely handles `cancel`. Here's the bad sequence:. - batch adds a job to `job_id_job`; - batch makes an HTTP request to k8s to create the pod, THREAD IS NOW PAUSED WAITING FOR RESULT; - flask handles a new request to cancel said job, tries to delete the pod; - [_delete_pod says: if `_pod_name` is `None`, don't do anything](https://github.com/hail-is/hail/blob/master/batch/batch/server/server.py#L83), so it does nothing but tells the client 200 OK!; - THREAD WAITING ON k8s WAKES UP: oh good, pod created. I think the fix is to check after pod creation if our state was set to canceled. If yes, delete said pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5168#issuecomment-456618542:92,unsafe,unsafely,92,https://hail.is,https://github.com/hail-is/hail/issues/5168#issuecomment-456618542,1,['unsafe'],['unsafely']
Safety,"Same problem as the issue. The log file is for a job that never got started. By default, hail writes the log file to `hail.log` when it runs. To avoid overwriting the log file, you can use `-l /path/to/my.log` to write a different file name or `-a` to append to the log file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/304#issuecomment-211168558:145,avoid,avoid,145,https://hail.is,https://github.com/hail-is/hail/issues/304#issuecomment-211168558,1,['avoid'],['avoid']
Safety,"Search `.zipWithIndex()` and you'll see five places we used Spark's zipWithIndex, which triggers a job. When partitionCounts are available I think we could avoid that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408:156,avoid,avoid,156,https://hail.is,https://github.com/hail-is/hail/pull/2547#issuecomment-350560408,1,['avoid'],['avoid']
Safety,"See for example https://cloudlogging.app.goo.gl/ziaRD9HKxxca8Nd3A. in which ~15 MJCs have to retry because of `ServerDisconnectedError` or `TimeoutError`. With this PR, I think we would have seen just the three ""two errors observed"" warning messages. Here's a possible extension to this PR that fuses the thinking of both PRs (this one and #12505): use the total delay instead of `errors = 2`. We retry really quickly, so two errors could occur in ~500ms which really isn't enough time for batch driver to fix itself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106:140,Timeout,TimeoutError,140,https://hail.is,https://github.com/hail-is/hail/pull/12712#issuecomment-1434824106,1,['Timeout'],['TimeoutError']
Safety,Seems like monkey patching with event can somehow override the timeout. We're not using event though. https://github.com/kennethreitz/requests/issues/3924#issuecomment-307502871,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575:63,timeout,timeout,63,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575,1,['timeout'],['timeout']
Safety,"Seems to average 60MB/s. No clear culprits. Zstd decoding is the top hit right now. The hottest generated code is inplace decoding of an optional array of optional int32. Really sucks because things like `LA` are somehow getting written as element optional, even though, by construction their elements are not optional. ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:; +EArray[EBaseStruct{; LA:EArray[EInt32]; ,LGT:EInt32; ,LAD:EArray[EInt32]; ,LPGT:EInt32; ,LPL:EArray[EInt32]; ,RGQ:EInt32,; gvcf_info: EBaseStruct{; AC:EArray[EInt32]; ,AF:EArray[EFloat64]; ,AN:EInt32,AS_BaseQRankSum:EArray[EFloat64]; ,AS_FS:EArray[EFloat64]; ,AS_InbreedingCoeff:EArray[EFloat64]; ,AS_MQ:EArray[EFloat64]; ,AS_MQRankSum:EArray[EFloat64]; ,AS_QD:EArray[EFloat64]; ,AS_QUALapprox:EArray[EInt32]; ,AS_RAW_BaseQRankSum:EBinary,AS_RAW_MQ:EArray[EFloat64]; ,AS_RAW_MQRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_RAW_ReadPosRankSum:EArray[EBaseStruct{`0`:EFloat64,`1`:EInt32}]; ,AS_ReadPosRankSum:EArray[EFloat64]; ,AS_SB_TABLE:EArray[EArray[EInt32]]; ,AS_SOR:EArray[EFloat64]; ,AS_VarDP:EArray[EInt32]; ,BaseQRankSum:EFloat64,ExcessHet:EFloat64,FS:EFloat64,InbreedingCoeff:EFloat64,MQ:EFloat64,MQRankSum:EFloat64,MQ_DP:EInt32,QD:EFloat64,QUALapprox:EInt32,RAW_GT_COUNT:EArray[EInt32]; ,RAW_MQandDP:EArray[EInt32]; ,ReadPosRankSum:EFloat64,SOR:EFloat64,VarDP:EInt32}; ,DP:EInt32; ,GQ:EInt32; ,MIN_DP:EInt32; ,PID:EBinary; ,PS:EInt32; ,SB:EArray[EInt32]; }; ]; }; ```. Async profiler periodic sampling:; <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 06 38"" src=""https://github.com/hail-is/hail/assets/106194/ee5df1c7-9c4a-4a4c-9ff4-caf599f1883b"">; <img width=""517"" alt=""Screenshot 2023-10-10 at 18 07 10"" src=""https://github.com/hail-is/hail/assets/106194/2bb5ba37-dab4-4b29-bb03-6cc2b08dafb9"">; Sync profiler (note safe point bias); <img width=""2032"" alt=""Screenshot 2023-10-10 at 18 32 14"" src=""https://github.com/hail-is/hail/assets/106194/85f1c1b6-3ac1-4b87-9e32-e6abdd02bb49"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633:1835,safe,safe,1835,https://hail.is,https://github.com/hail-is/hail/pull/13787#issuecomment-1756358633,2,['safe'],['safe']
Safety,"Seems to have passed that PR now. I'm really not sure why our timeout isn't respected. I trolled through the requests issue tracker and didn't find anything relevant. Honestly, requests feels like a huge pile of indirection on top of urllib3 and it's really hard to understand. I'd prefer a library that was a bit cleaner.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5566#issuecomment-471059257:62,timeout,timeout,62,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471059257,1,['timeout'],['timeout']
Safety,Should `group_by_key` be `implode`? To avoid confusion with `group_by`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3080#issuecomment-370538411:39,avoid,avoid,39,https://hail.is,https://github.com/hail-is/hail/pull/3080#issuecomment-370538411,1,['avoid'],['avoid']
Safety,"Since RegionPool cleans its memory via PhantomReferences now, it is not AutoCloseable. In fact, I'm not sure how we avoided double free errors in the past. I made all the tests not use `using`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446:116,avoid,avoided,116,https://hail.is,https://github.com/hail-is/hail/pull/8203#issuecomment-592744446,1,['avoid'],['avoided']
Safety,"So I don't think this is quite correct. malloc(0) is implementation defined, it either returns a valid pointer which cannot be dereferenced, or 0: https://stackoverflow.com/a/2022402. memcpy with a 0 source is undefined: https://stackoverflow.com/questions/5243012/is-it-guaranteed-to-be-safe-to-perform-memcpy0-0-0. There is some debate in the comments to the first answer about whether any reasonable memcpy implementation would do anything if the length is 0. However, see the second answer in the link above: gcc improves optimization by assuming the undefined behavior cannot happen. So our options seem to be:. 1. Leave this change, but change Memory.malloc to allocate at least one byte (guaranteeing it never returns 0). 2. Revert this change, and assume memcpy with 0 length is safe and weaken the memcpy check. 3. Revert this change, don't assume memcpy of 0 length is safe, and have Memory.memcpy check the length before calling into the Unsafe memcpy implementation. I'm inclined to do 3 in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713:288,safe,safe-to-perform-,288,https://hail.is,https://github.com/hail-is/hail/pull/8970#issuecomment-645082713,4,"['Unsafe', 'safe']","['Unsafe', 'safe', 'safe-to-perform-']"
Safety,"So after chatting with @tpoterba last week, I think the conclusion we came to is that `per_y_list` was probably being recomputed for each row of output, which is why changing it to a python list comprehension helped. I tried changing things to avoid that by adding an `rbind` to the old version:. ```; def build_row(per_y_list, row_idx):; # For every field we care about, map across all y's, getting the row_idxth one from each.; idxth_keys = {field_name: block[field_name][row_idx] for field_name in key_field_names}; computed_row_field_names = ['n', 'sum_x', 'y_transpose_x', 'beta', 'standard_error', 't_stat', 'p_value']; computed_row_fields = {; #field_name: [one_y[field_name][row_idx] for one_y in per_y_list] for field_name in computed_row_field_names; field_name: per_y_list.map(lambda one_y: one_y[field_name][row_idx]) for field_name in computed_row_field_names; }; pass_through_rows = {; field_name: block[field_name][row_idx] for field_name in row_field_names; }. if not is_chained:; computed_row_fields = {key: value[0] for key, value in computed_row_fields.items()}. return hl.struct(**{**idxth_keys, **computed_row_fields, **pass_through_rows}). new_rows = hl.rbind(per_y_list, lambda l_per_y_list: hl.range(rows_in_block).map(lambda inner_i: build_row(l_per_y_list, inner_i))); ```. and had no luck, still incredibly slow. Doesn't seem to change the IR size at all if I just add an `rbind` to main branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9666#issuecomment-724070179:244,avoid,avoid,244,https://hail.is,https://github.com/hail-is/hail/pull/9666#issuecomment-724070179,1,['avoid'],['avoid']
Safety,So google just doesn't ever retry Read Timeouts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8053#issuecomment-583445035:39,Timeout,Timeouts,39,https://hail.is,https://github.com/hail-is/hail/issues/8053#issuecomment-583445035,1,['Timeout'],['Timeouts']
Safety,"So there's a double regex substitution now in this version. I couldn't figure out how to avoid this without having nice error checking at the exact line there's a problem. For example, `j.command(f'{b}')` right now immediately errors with a nice error message. But if the error checking doesn't come until the massive parallel `_compile` in `Backend.run`, then it will be harder to tell where the error is. I thought about having a `debug_mode` which is on by default that does the double check while the `debug_mode` being off is more efficient.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694:89,avoid,avoid,89,https://hail.is,https://github.com/hail-is/hail/pull/10716#issuecomment-888459694,1,['avoid'],['avoid']
Safety,"So, this is passing locally for me now. The credentials change removed the race condition with my kubernetes permissions being overridden. Two questions:. 1. The logs are written to this path: `f'{instance_id}/{job_id}/{task_name}/job.log'` where `instance_id` is the Batch instance id. I did this to avoid naming conflicts between different batch instances running in the CI, locally, and the production batch instance. However, this makes it difficult to find a particular log file in the browser. It is also based on the instance ID which is printed in a log file that is not persistent. I think this problem will go away once the SQL changes go in. But thought it was something to bring up. 2. I don't do any cleanup of the test logs output. I think I should probably add this before this PR goes in. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732:301,avoid,avoid,301,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485458732,1,['avoid'],['avoid']
Safety,Some Spark tests timed out (e.g. in https://ci.hail.is/batches/7644244/jobs/74 it was `test_spectral_moments_4`) which often crashes the JVM leading to the other errors. Retry and post links into Query Dev to alert them that QoS can timeout on spectral moments.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229:233,timeout,timeout,233,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1638682229,1,['timeout'],['timeout']
Safety,"Some things we should try first:. 1. Set default delay in Azure to 1s.; 2. Investigate the AzureBlobStorage Logs [1]; perhaps we are misbehaving?; 3. Can we avoid issuing ""list"" commands, these are known to be particularly slow/taxing.; 4. There might be a threading issue with BlobServiceClient. Maybe we need the client to be thread local?. [1] e.g. go [here]( [here](https://portal.azure.com/#view/Microsoft_OperationsManagementSuite_Workspace/Logs.ReactView/initiator/ActivityLogBlade/source/ActivityLogBlade/scope~/%7B%22resources%22%3A%5B%7B%22resourceId%22%3A%22%2Fsubscriptions%2F22cd45fe-f996-4c51-af67-ef329d977519%2FresourceGroups%2Fhaildev%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Fhaildevtest%22%7D%5D%7D)) and try:; ```; StorageBlobLogs; | where MetricResponseType != ""Success"" and StatusCode != 404; | order by TimeGenerated desc; | limit 5000; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13351#issuecomment-1660689133:157,avoid,avoid,157,https://hail.is,https://github.com/hail-is/hail/issues/13351#issuecomment-1660689133,1,['avoid'],['avoid']
Safety,Sorry about that! We changed how input files are stored such that we copy the file to `inputs/{token}/{filename}` to avoid duplicates while not needing to specify extensions. We can't do the same thing for JobResourceFile so it's still there. I'll fix the tutorial.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9645#issuecomment-717257900:117,avoid,avoid,117,https://hail.is,https://github.com/hail-is/hail/issues/9645#issuecomment-717257900,1,['avoid'],['avoid']
Safety,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:588,avoid,avoid,588,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,1,['avoid'],['avoid']
Safety,"Sorry to be naive, I haven't really bumped into this method much. We can still avoid python lists by returning a table with one column instead of a set!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2975#issuecomment-368139620:79,avoid,avoid,79,https://hail.is,https://github.com/hail-is/hail/pull/2975#issuecomment-368139620,1,['avoid'],['avoid']
Safety,"Sorry! I didn't see this because of the review. The root problem is that `raise_for_status` ignores the response body. This is a [known issue in aiohttp](https://github.com/aio-libs/aiohttp/issues/4600). It will be fixed in 4.0.0, but development on that seems slow. There's a variety of solutions to this problem. Every solution avoids aiohttp's raise_for_status and replaces it with something that includes the response body in the error message. A thorough fix to this is to finish the work I started in `httpx.py`. Instead of returning an `aiohttp.ClientSession` we could return a shim class that wraps `aiohttp.ClientSession` and checks the status code itself and raises an error *with the response body*. A smaller fix that only addresses aiogoogle would be to modify `aiogoogle.auth.Session` to:; 1. Not pass `raise_for_status` on to `aiohttp.ClientSession`.; 2. Store raise_for_status as a field on `aiogoogle.auth.Session`.; 2. In `aiogoogle.auth.Session.request`, if `self.raise_for_status` is true and the response status is greater than or equal to 400, retrieve the response body and raise an exception (maybe `HailHTTPError`) that includes the body.; 3. Ensure `is_transient_error` properly handles whatever exception we raise.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289:330,avoid,avoids,330,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-852213289,1,['avoid'],['avoids']
Safety,Sorry. I need to get my head back into this again and I need to do the billing fixes from the redundant resource prices first. Unassigning you for now until it's ready.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316:94,redund,redundant,94,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1438856316,1,['redund'],['redundant']
Safety,"Sorta fixed it in that it revealed an Assertion. Log coming over other medium. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 28.0 failed 20 times, most recent failure: Lost task 3.19 in stage 28.0 (TID 671, exomes-w-0.c.broad-mpg-gnomad.internal, executor 4): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:417); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:345); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:351); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:1024); 	at is.hail.expr.ir.MatrixAggregateColsByKey$$anonfun$33$$anonfun$apply$15.apply(MatrixIR.scala:980); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1087); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$22$$anon$3.next(OrderedRVD.scala:1081); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$class.foreach(Iterator.sc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:140,abort,aborted,140,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['abort'],['aborted']
Safety,"Sounds good Dan, and agreed it's a long term issue. Regarding point 2, I also don't really like the idea of non-preemtible nodes from a resource utilization standpoint. I think we could probably write our own peak load predictor, or use one of the existing tools, outside of the kube ecosystem. There has been some interesting work using some relatively simple learning models to predict load. It would be interesting to use an RNN for this, but linear regression seems to work pretty well. This could be an interesting topic to investigate. https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-part-2-bb9c4f9b9385",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683:219,predict,predictor,219,https://hail.is,https://github.com/hail-is/hail/issues/5269#issuecomment-461549683,6,['predict'],"['predict', 'predictive-auto-scaling-engine-part-', 'predictor']"
Safety,"Sounds good! I'll PR it separately. Mostly a question of whether it's a safe/desirable thing to do, I think?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691238047:72,safe,safe,72,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691238047,1,['safe'],['safe']
Safety,"Starting with an underscore is safe, isn't it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10829#issuecomment-911708835:31,safe,safe,31,https://hail.is,https://github.com/hail-is/hail/pull/10829#issuecomment-911708835,1,['safe'],['safe']
Safety,"Stepping back a little bit, there might be a reasonable (if unsatisfying) middle ground. Presumably the operations most at risk are long streams that we always do in chunks anyway, and in that case we can create new downloaders on `AzureReadableStream.read` if the SAS token expires. That would probably solve most of these problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732:123,risk,risk,123,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930492732,1,['risk'],['risk']
Safety,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:101,risk,risks,101,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,6,['risk'],"['risk', 'risks']"
Safety,"Sure thing. Planning on implementing three top-level convenience methods for converting between relational IRs:; - `t.to_matrix_table` which essentially wraps the python approach you laid out in the creation of this issue; - `bm.to_table` which produces a table where each row corresponds to a row of the original BlockMatrix (will do a write and a read to avoid shuffling, actually have to dig into the RDDs for this one); - `bm.to_matrix_table` which will just compose the previous two methods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897:357,avoid,avoid,357,https://hail.is,https://github.com/hail-is/hail/issues/5504#issuecomment-470199897,1,['avoid'],['avoid']
Safety,Tests failed unrelatedly on `test_ci` due to a timeout on pulling an image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13949#issuecomment-1787517445:47,timeout,timeout,47,https://hail.is,https://github.com/hail-is/hail/pull/13949#issuecomment-1787517445,1,['timeout'],['timeout']
Safety,"Thank you for reporting this. I've fixed the issue. You could lower the batch size to 50, and that will avoid this problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11891#issuecomment-1167713936:104,avoid,avoid,104,https://hail.is,https://github.com/hail-is/hail/issues/11891#issuecomment-1167713936,1,['avoid'],['avoid']
Safety,"Thank you for the quick response, @tpoterba . I understand, I think we are on the same page. I was mainly curious what would happen if overwrite was True. Whether it would start from scratch or ignore overwrite, or something else. That being said, a possible use case re overwrite could be to actually remove partial files re tasks being recovered. Re a single task, if it would fail, then it would be possible for there to exist a partial task file. Usually if a task fails, then Spark would retry it. And if the task's next attempt passes, then there would exist a full task file plus a partial task file. So, if Hail would recover the task, then I imagine there would also exist a full task file plus a partial task file. I have not seen partial task files affecting downstream processing yet, but I am generally wary of ""duplicate"" files. Just a thought",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485:338,recover,recovered,338,https://hail.is,https://github.com/hail-is/hail/pull/10215#issuecomment-822979485,2,['recover'],"['recover', 'recovered']"
Safety,"Thanks @danking for helping with this, and for the nudge to finally get it implemented!. Some more next steps:; * Make a benchmarking setup to collect data on performance and accuracy. I want to use the same metrics for measuring error as [here](http://quantiles.github.io/) (Kolmogorov-Smirnov divergence and total variation distance between the true and estimated CDFs, which are both simple to compute). As a first sanity check, we should get numbers in the same ballpark as they did (worse at first, because we started with a very naive algorithm).; * Make improvements to the algorithm, evaluating the gains in the benchmarks.; * When we feel we understand the behavior of the accuracy measures, we can pick a conservative upper error bound and turn that into an automated test.; * Also once we understand the behavior of the accuracy measures, figure out how to communicate that to users. One thought is to offer a few default choices of parameters, and for each document memory usage, empirical error from our benchmarks, and a theoretical upper bound on the error (of the form ""with probability > .99 the estimated q quantile element will have true quantile q  .001""). This allows users to make an informed decision based on their use case. That looks like a lot, but I think those are all relatively simple. I'm also using this as a test case for thinking about how to implement approximate/randomized methods in Hail in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410:418,sanity check,sanity check,418,https://hail.is,https://github.com/hail-is/hail/pull/5332#issuecomment-463200410,2,['sanity check'],['sanity check']
Safety,"That was nasty to debug. For some reason, Scala has both `Ordering.on` (on the `Ordering` class) and `Ordering.by` (on the companion object) to translate an ordering on `U` to an ordering on `T` along a function `f: T => U`. `on` defines `compare`:; ```; def on[U](f: U => T): Ordering[U] = new Ordering[U] {; def compare(x: U, y: U) = outer.compare(f(x), f(y)); }; ```; but `by` mysteriously bases the new ordering only on the original `lt`:; ```; def by[T, S](f: T => S)(implicit ord: Ordering[S]): Ordering[T] =; fromLessThan((x, y) => ord.lt(f(x), f(y))). def fromLessThan[T](cmp: (T, T) => Boolean): Ordering[T] = new Ordering[T] {; def compare(x: T, y: T) = if (cmp(x, y)) -1 else if (cmp(y, x)) 1 else 0; // overrides to avoid multiple comparisons; override def lt(x: T, y: T): Boolean = cmp(x, y); override def gt(x: T, y: T): Boolean = cmp(y, x); override def gteq(x: T, y: T): Boolean = !cmp(x, y); override def lteq(x: T, y: T): Boolean = !cmp(y, x); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8272#issuecomment-597259336:728,avoid,avoid,728,https://hail.is,https://github.com/hail-is/hail/pull/8272#issuecomment-597259336,1,['avoid'],['avoid']
Safety,"That's a good question, do the semantics of the client library intend to support two different users modifying the objects simultaneously. We don't have an intended use case for it, so I'd say no. So `_status` cannot be out of date, it can only be stale. I'm only checking if it is complete. Once a Job is complete, its status cannot change. In the case it was deleted by someone else, if I didn't cache it would return 404, but we ruled that out. So I think this code is currently safe. Does that clarify?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033:482,safe,safe,482,https://hail.is,https://github.com/hail-is/hail/pull/5812#issuecomment-481006033,1,['safe'],['safe']
Safety,"That's fair, although I would add predictable behavior to `interval_table[ht.key]` => Either first or last overlapping record of `interval_table`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4270#issuecomment-419929482:34,predict,predictable,34,https://hail.is,https://github.com/hail-is/hail/issues/4270#issuecomment-419929482,1,['predict'],['predictable']
Safety,"That's probably more code complication for CI that could be easily avoided by just sticking to the ""WIP"" tag.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8552#issuecomment-613639601:67,avoid,avoided,67,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613639601,1,['avoid'],['avoided']
Safety,"Thats a great point; ________________________________; From: Patrick Schultz ***@***.***>; Sent: Tuesday, October 4, 2022 7:27 AM; To: hail-is/hail ***@***.***>; Cc: Emma S Kelminson ***@***.***>; Author ***@***.***>; Subject: Re: [hail-is/hail] geom_boxplot (PR #11720). CAUTION: EXTERNAL SENDER. I haven't had a chance to look into this and understand how precomputed and faceting work and interact. If Iris wants to give that some thought, I'd be happy to advise. If there is a real obstruction to doing this in the current design, it would be good for both Iris and I to understand it before starting a redesign. ; Reply to this email directly, view it on GitHub<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fhail-is%2Fhail%2Fpull%2F11720%23issuecomment-1266817118&data=05%7C01%7Cemma.kelminson001%40umb.edu%7Cf12d7220658149a9bc7a08daa5fb7ba4%7Cb97188711ee94425953c1ace1373eb38%7C0%7C0%7C638004796790915105%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=dry2ZdLcd1beO2WrdhCRT9sV0kjtFm4lKsy2nL0iqnU%3D&reserved=0>, or unsubscribe<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FARVGM3NWM6AEALMGZIZ2WS3WBQIDVANCNFSM5SGPPZUA&data=05%7C01%7Cemma.kelminson001%40umb.edu%7Cf12d7220658149a9bc7a08daa5fb7ba4%7Cb97188711ee94425953c1ace1373eb38%7C0%7C0%7C638004796791071348%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=iR6L6EoWRp6CivnjxCyKhe%2Bjs%2FYl2p29e%2FVbCxYxgvQ%3D&reserved=0>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11720#issuecomment-1266858564:682,safe,safelinks,682,https://hail.is,https://github.com/hail-is/hail/pull/11720#issuecomment-1266858564,2,['safe'],['safelinks']
Safety,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:174,recover,recover,174,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189,1,['recover'],['recover']
Safety,"The behavior of this PR has been restored to my initial intentions: in the case of a `TableKeyBy(..., isSorted=true)`, if the partitions are 1:1, we avoid a costly shuffle, perhaps at the risk of loss of parallelism.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864#issuecomment-637177972:149,avoid,avoid,149,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177972,2,"['avoid', 'risk']","['avoid', 'risk']"
Safety,"The failure here:; ```; E org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (hostname-c5956f6f02 executor driver): java.io.EOFException: Invalid seek offset: position value (6) must be between 0 and 6 for 'gs://hail-services-requester-pays/hello'; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.validatePosition(GoogleCloudStorageReadChannel.java:665); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.position(GoogleCloudStorageReadChannel.java:546); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFSInputStream.seek(GoogleHadoopFSInputStream.java:178); E 	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65); ```. I hit this same error in Avro/GVS work recently -- I think the Google Hadoop API connector is wrong in that you cannot seek to the end of a file (N where N is the number of bytes in the file).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700:63,abort,aborted,63,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700,1,['abort'],['aborted']
Safety,The foreign key constraints are also a safety belt which protect us from a corrupted database. How much does this improve performance? Let's explicitly record the cost-benefit analysis in the PR comments so we can refer back to it if necessary.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163219995:39,safe,safety,39,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163219995,1,['safe'],['safety']
Safety,The inner docker retry function is catching the outer wait_for timeout :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8160#issuecomment-591515885:63,timeout,timeout,63,https://hail.is,https://github.com/hail-is/hail/pull/8160#issuecomment-591515885,1,['timeout'],['timeout']
Safety,"The issue is `j.wait()` will trigger when the job is complete which is set before the callback occurs. However, even if we change the order `set_state` and `callback` are called (change in interface), there's still the possibility that the callback won't complete before the wait is terminated. Therefore, the correct solution should wait for `d` to be non-empty with a timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121:370,timeout,timeout,370,https://hail.is,https://github.com/hail-is/hail/issues/5817#issuecomment-482597121,1,['timeout'],['timeout']
Safety,"The issue seems to be that ci's `/refresh_batch_state` POST route broke. ```; INFO	| 2019-03-02 16:16:17,392 	| ci.py 	| <lambda>:409 | 127.0.0.1 ""POST /refresh_batch_state HTTP/1.1"" 500 -; ERROR	| 2019-03-02 16:16:17,394 	| ci.py 	| polling_event_loop:400 | Could not poll due to exception: 500 Server Error: INTERNAL SERVER ERROR for url: http://127.0.0.1:5000/refresh_batch_state; ```. edit:. These appear to be the relevant parts of the stack trace:. ```; File ""/hail-ci/ci/ci.py"", line 144, in refresh_batch_state; jobs = batch_client.list_jobs(); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/client.py"", line 202, in list_jobs; jobs = self.api.list_jobs(self.url); File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/api.py"", line 41, in list_jobs; response = requests.get(url + '/jobs', timeout=self.timeout). #... requests.exceptions.ReadTimeout: HTTPConnectionPool(host='batch.default', port=80): Read timed out. (read timeout=5); ```. The batch /jobs endpoint appears to be having an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883:819,timeout,timeout,819,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468934883,3,['timeout'],['timeout']
Safety,The previous PR has merged. Could you remove the Stacked PR label and rebase / drop any redundant commits? I thought GitHub wouldn't show the extra diff at this point but *shrug*,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699:88,redund,redundant,88,https://hail.is,https://github.com/hail-is/hail/pull/11325#issuecomment-1033961699,1,['redund'],['redundant']
Safety,"The problem I saw was this:. ToStream's invariant is that its children must be TIterable. Given this invariant, in boundary it is not safe to call ToArray on streamified when streamified.isInstanceOf[TStream] and node.typ.isInstanceOf[TArray], because this will miss cases (potentially) when node is a different TIterable, and likewise it is not safe to call ToArray on streamified when node.typ.isInstanceOf[TIterable], because we may inadvertently cast a non-array TIterable to TArray, and thereby break boundary's type invariance. So everywhere that we add a ToStream, we need to perform a check on the child: if it's a non-TArray TIterable, return it, else wrap in ToArray, unless we can be sure we never perform said wrap on a TIterable when streamify is called from boundary. . In the latest commit, I simplified the toStream code, and improved the type check to check not TContainer, but (TIterable && !TStream). This is more precise that checking TContainer alone. That being said I haven't created a convincing test yet (though it's trivially easy to make *a* test: pass a ToStream wrapping a node with typ TDict to boundary, with the old check on boundary, and a TIterable check-before-wrap-in-ToStream in the base case of streamify). However, I don't think we can avoid the condition you don't like in `boundary` without changing ToStream's child-type invariant to TArray.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403:134,safe,safe,134,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586553403,6,"['avoid', 'safe']","['avoid', 'safe']"
Safety,"The reason I didn't put it all into Python is because I thought we wanted to avoid transferring a huge array with py4j from Scala to Python to just use `annotateGlobals` to add it back to the table in Python, which will transfer it back to Scala. Am I missing something?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3425#issuecomment-383929900:77,avoid,avoid,77,https://hail.is,https://github.com/hail-is/hail/pull/3425#issuecomment-383929900,1,['avoid'],['avoid']
Safety,The sun lint thing is irrelevant in Scala because we have a Java-based shim around all the unsafe calls.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6279#issuecomment-499672902:91,unsafe,unsafe,91,https://hail.is,https://github.com/hail-is/hail/pull/6279#issuecomment-499672902,1,['unsafe'],['unsafe']
Safety,"The test that lists batches timed out. The main problem is the limit in the aioclient used by the test_batch tests was passing a string rather than an integer. I assumed downstream the function was passing an integer. Therefore, we were doing this:. ```; batch_id < ""137""; ```. and not `batch_id < 137`. So the query was running forever and scanning all batches from the `test` user. I also was missing a tag annotation on the queries, but that was not causing the timeout.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046:465,timeout,timeout,465,https://hail.is,https://github.com/hail-is/hail/pull/13237#issuecomment-1631270046,1,['timeout'],['timeout']
Safety,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:470,avoid,avoid,470,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,2,['avoid'],['avoid']
Safety,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:518,risk,risk,518,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448,1,['risk'],['risk']
Safety,"There's a 30m timeout. Also things are a bit slower on batch than on your laptop, I'd shoot for a couple minutes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962#issuecomment-578324936:14,timeout,timeout,14,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578324936,1,['timeout'],['timeout']
Safety,"There's a pretty sizeable amount of changes that had to be made manually, so the merge conflicts might be a lot more painful with this one than https://github.com/hail-is/hail/pull/14119; not sure if that's a case for delaying this or not. I think many of those manual changes could also be made using the `--unsafe-fixes` flag to `ruff`, but it seemed better to try doing them manually to try and avoid introducing too many issues. The lint config changes, automatic changes from `ruff`, and manual changes required to make the `ruff` check pass are split into separate commits, so hopefully that will make the review easier",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883315070:309,unsafe,unsafe-fixes,309,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883315070,2,"['avoid', 'unsafe']","['avoid', 'unsafe-fixes']"
Safety,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying  . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998:589,avoid,avoid,589,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998,2,['avoid'],['avoid']
Safety,"This approach to joining doesn't work for me:; ```; In [1]: import hail as hl . In [2]: t = hl.utils.range_table(1) . In [3]: t2 = t.key_by(idx=t.idx, idx2=t.idx) . In [4]: t.annotate(foo=t2[t.key]) ; Traceback (most recent call last):; File ""<ipython-input-4-85e676382c80>"", line 1, in <module>; t.annotate(foo=t2[t.key]); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 368, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32, int32; Index Expressions: int32; ```. And since the annotation db is just built on joins, it wouldn't work in that setting either. Moreover, the annotation db needs to be careful with uniqueness of the key-row relationship. I try to avoid unnecessarily using `all_matches=True` which introduces arrays that complicate downstream work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587:963,avoid,avoid,963,https://hail.is,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587,1,['avoid'],['avoid']
Safety,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:1133,safe,safe,1133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139,2,['safe'],['safe']
Safety,This is a limitation of the Hadoop library that we cannot avoid.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9607#issuecomment-1145283347:58,avoid,avoid,58,https://hail.is,https://github.com/hail-is/hail/issues/9607#issuecomment-1145283347,1,['avoid'],['avoid']
Safety,"This is a result of accidental mutation of the `params` dictionary that's passed into this generator. Notice that the same `params` dictionary is used for each listing per zone https://github.com/hail-is/hail/blob/de0f7a8d54f29f601b7321950ca3bc2425befecf/batch/batch/cloud/gcp/driver/disks.py#L27-L31; and the dictionary is mutated here https://github.com/hail-is/hail/blob/de0f7a8d54f29f601b7321950ca3bc2425befecf/hail/python/hailtop/aiocloud/aiogoogle/client/compute_client.py#L73. This means that on the second iteration (second zone), `self._request_params` will contain the last `nextPageToken` from the previous iteration, triggering this assertion. Avoiding this accidental mutation should resolve this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14613#issuecomment-2226334726:656,Avoid,Avoiding,656,https://hail.is,https://github.com/hail-is/hail/issues/14613#issuecomment-2226334726,1,['Avoid'],['Avoiding']
Safety,"This is approved but needs rebase. I'm making some more API changes. If you can rebase soon, I will wait until this goes in to avoid further conflicts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5797#issuecomment-480980256:127,avoid,avoid,127,https://hail.is,https://github.com/hail-is/hail/pull/5797#issuecomment-480980256,1,['avoid'],['avoid']
Safety,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187:530,risk,risk,530,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187,4,['risk'],"['risk', 'risky']"
Safety,"This is kind of an annoying problem to have because these pip installed versions are frozen in time. I feel like these steps are either redundant (nothing has changed), or will fail because we've updated the checks to improve on what we had at last release. What do these steps really do for us?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165:136,redund,redundant,136,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165,1,['redund'],['redundant']
Safety,"This is really great. . I have some thoughts below, mostly brain storming. Don't take any of it too seriously. Some thoughts:. 1. I thought you wanted to support f-strings. By making the batch file quote double curly parens, that means if you use them in an f-string, you need to write `f'{{{{foo}}}}'` which is a bit much. But that means that non-batch uses of `{}` need to be double-quoted, so `awk '{{ ... }}'` and `f""awk '{{{{ ... }}}}'""`. Hmm. Maybe using the same escape syntax as f-strings is not ideal. . I don't have a no-brainer suggestion. Happy to brainstorm ideas offline. I think ultimately this is a minor syntactic choice. 2. Inputs seem ... almost redundant, because they also appear in the command strings. What about:. ```; .command('shapeit --bed-file {{<subset.ofile}} --chr ' + contig + ' --out {{>ofile}}'); ```. Then the question becomes, how do associate `subset` with the corresponding Python variable? You could use the task label, but then the user has to maintain two sets of names, which isn't ideal. Hmm, maybe this doesn't work. 3. I like arrays of resources!. > `.command('cat {{files}} >> {{ofile}}')`. I wonder, will we ever want arrays to be formatted other than joined with spaces? I worry the user will want more flexibility in formatting, and we'll want that in Python. What about if the argument is a function, it takes a dictionary from resource names to their string representation, and you can format however you want? Then you could write the last command as:. ```; .command(lambda rs: f'cat {' '.join(rs['files'])} >> {rs['ofile']}'); ```. 4. I was confused by this:. > `p.write_output(merger.ofile + "".haps"", ...)`. What's the left hand side? Why isn't this just `merger.ofile`?. This suggests another issue: what if you want to use `ofile` in a plink command, but plink outputs some files with various extensions with `ofile` as the base? We might need an `outputs` that lists (docker local) output files based on a base path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609:665,redund,redundant,665,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-446733609,1,['redund'],['redundant']
Safety,"This is untested, but almost certainly safe. Waiting on CI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6270#issuecomment-499320804:39,safe,safe,39,https://hail.is,https://github.com/hail-is/hail/pull/6270#issuecomment-499320804,1,['safe'],['safe']
Safety,"This looks OK to me. The first bit is the stack trace to the retry_transient_errors. The second bit is the stacktfrace for the timeout error. The last part of a Python stack trace is always the name of the exceptional class, e.g.:; ```. In [13]: raise ValueError() ; Traceback (most recent call last):; File ""<ipython-input-13-4954757c312d>"", line 1, in <module>; raise ValueError(); ValueError. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649524:127,timeout,timeout,127,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649524,1,['timeout'],['timeout']
Safety,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:116,avoid,avoid,116,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428,1,['avoid'],['avoid']
Safety,"This will also retry things that are our fault, but that might just be worth it if it avoids most transient errors",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11666#issuecomment-1077884202:86,avoid,avoids,86,https://hail.is,https://github.com/hail-is/hail/pull/11666#issuecomment-1077884202,1,['avoid'],['avoids']
Safety,"This will be fixed in the next release.; If you can't wait until then and you're comfortable patching this yourself, replace the contents of the affected file with the following:; ```yaml; dataproc:; init_notebook.py: gs://hail-common/hailctl/dataproc/0.2.129/init_notebook.py; vep-GRCh37.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh37.sh; vep-GRCh38.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh38.sh; wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl; pip_dependencies: aiodns==2.0.0|aiohttp==3.9.3|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.2.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.30.1|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.4|boto3==1.34.55|botocore==1.34.55|cachetools==5.3.3|certifi==2024.2.2|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==42.0.5|decorator==4.4.2|deprecated==1.2.14|dill==0.3.8|frozenlist==1.4.1|google-auth==2.28.1|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.3|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.5|msal==1.27.0|msal-extensions==1.1.0|msrest==0.7.1|multidict==6.0.5|nest-asyncio==1.6.0|numpy==1.26.4|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.2.1|parsimonious==0.10.0|pillow==10.2.0|plotly==5.19.0|portalocker==2.8.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.9.0.post0|python-json-logger==2.0.7|pytz==2024.1|pyyaml==6.0.1|regex==2023.12.25|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.10.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.4|typer==0.9.0|typing-extensions==4.10.0|tzdata==2024.1|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.4|; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651:572,timeout,timeout,572,https://hail.is,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651,1,['timeout'],['timeout']
Safety,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:49,redund,redundant,49,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['redund'],['redundant']
Safety,"Update to this, tried running the same script with the bgen file as v1.2 instead (was v1.1 in initial posted issue), but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:184,abort,aborted,184,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,3,['abort'],"['abortStage', 'aborted']"
Safety,Updated so PCA avoids select when the expression is a field (similar to BlockMatrix.write_from_entry_expr). Hopefully good to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377693701:15,avoid,avoids,15,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377693701,1,['avoid'],['avoids']
Safety,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:543,Safe,SafeRow,543,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174,1,['Safe'],['SafeRow']
Safety,"Verified it works. On Monday, I'll think about how to avoid this kind of bug.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9871#issuecomment-757514725:54,avoid,avoid,54,https://hail.is,https://github.com/hail-is/hail/pull/9871#issuecomment-757514725,1,['avoid'],['avoid']
Safety,"Verified that before #3510 was merged into master, if this PR had been added to master, it would have caught the issue. The stack trace directly fingers the EmitPackDecoder, which is exactly what one would hope to happen:. ```; FatalError: AssertionError: assertion failed: PackDecoder compilation should happen on master, but happened on worker. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 18, localhost, executor driver): java.lang.AssertionError: assertion failed: PackDecoder compilation should happen on master, but happened on worker; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:291); 	at is.hail.io.EmitPackDecoder$.apply(RowStore.scala:637); 	at is.hail.io.PackCodecSpec.buildDecoder(RowStore.scala:110); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:550); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:546); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:282); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:282); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3521#issuecomment-387199916:403,abort,aborted,403,https://hail.is,https://github.com/hail-is/hail/pull/3521#issuecomment-387199916,1,['abort'],['aborted']
Safety,"WIP, to avoid merging while azure isn't required.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11602#issuecomment-1071719195:8,avoid,avoid,8,https://hail.is,https://github.com/hail-is/hail/pull/11602#issuecomment-1071719195,1,['avoid'],['avoid']
Safety,"Was there a reason you unassigned me? Happy to write a draft, but might be better to wait until we want to actually start billing for this new fee to avoid stale code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13526#issuecomment-1701289632:150,avoid,avoid,150,https://hail.is,https://github.com/hail-is/hail/issues/13526#issuecomment-1701289632,1,['avoid'],['avoid']
Safety,"We could follow the style of the rest of the IR with `CallStatsAggOp(a: IR, name: String, body: IR)` (this avoids an IR lambda). We'd still need to compile and emit the `body` and pass it as a JVM object to the aggregator.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3486#issuecomment-386411966:107,avoid,avoids,107,https://hail.is,https://github.com/hail-is/hail/issues/3486#issuecomment-386411966,1,['avoid'],['avoids']
Safety,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:123,avoid,avoid,123,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761,1,['avoid'],['avoid']
Safety,"We don't really have the infrastructure to create arbitrary values in c++ right (without translating into potentially large trees of IR). Since they already exist as Scala annotations in the IR, I decided that it was easier to serialize them in scala and decode them in the function rather than try to create logic and encode them in the function itself (we can't create them at compile-time and pass around naked hail-values, since they're region-backed and the functions can get serialized and shipped to workers, which don't have access to those). I put the decoded literals on a SparkFunctionContext in the hopes that it will be easier to plug in another literal broadcaster if/when the `Literal` IR no longer holds a Scala annotation, but in the meantime this was a reasonably straightforward way to get them into the c++ Emit code. I think we may not need to serialize literals like this for the `cxx.Compile.apply` methods, since they should all execute on the master node, but we don't currently enforce that so I was serializing there just to be safe. The alternative is to enforce execution of these functions on the master node, which I don't see a huge problem with off the top of my head? @cseed do you have feelings about this? All the distributed computation is going to go through `CollectDistributedArray` and friends, which should create their own entrypoints.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874:1055,safe,safe,1055,https://hail.is,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874,1,['safe'],['safe']
Safety,"We have a difference of opinion about the risks involved in using whatever; compiler happens to show up as $(CXX); to try to compile arbitrarily large auto-generated C++ files, and maybe; about what happens when that fails; and gives an error message about something in the middle of 12000 lines of; code that bears no obvious relationship; to what the user is doing. Or when that compiler takes 15 minutes to; compile it. It's the C++ equivalent of; the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; it but the code gives the wrong answers; because that particular compiler has a bug, and we never tested the; combination of our codegen with *that*; compiler/version. A couple of years ago I was seeing g++ take 40-60 seconds to compile; something that clang did in 2 seconds; (fairly heavily templated code generated for an SQL query, so very much in; the same ballpark as parts of Hail),; which contributes to my concern about this, especially on linux where g++; is the default. So in the long run I expect we'll ship a compiler, or specify a compiler.; But that becomes a problem in itself; if we want the shipped compiler to work on a variety of OS'es. When I did; that before it was all Ubuntu-14.04; and Ubuntu-16.04, and it was manageable to build it for two platforms. On Thu, Aug 2, 2018 at 9:59 PM cseed <notifications@github.com> wrote:. > *@cseed* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207422997>:; >; > > +}; > +; > +std::string strip_suffix(const std::string& s, const char* suffix) {; > + size_t len = s.length();; > + size_t n = strlen(suffix);; > + if ((n > len) || (strncmp(&s[len-n], suffix, n) != 0)) return s;; > + return std::string(s, 0, len-n);; > +}; > +; > +std::string get_cxx_name() {; > + char* p = ::getenv(""CXX"");; > + if (p) return std::string(p);; > + // We prefer clang because it has faster compile; > + auto s = run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709:42,risk,risks,42,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410127709,1,['risk'],['risks']
Safety,"We may want to use NumPy ndarray as local matrix now, to avoid interface duplication and grab all its functionality, even if there's a performance hit in moving between Python and Java (worst case, we go through disk). I'm going to close this while we strategize, will PR the BlockMatrix updates separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825:57,avoid,avoid,57,https://hail.is,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825,1,['avoid'],['avoid']
Safety,"We now use an API token linked to hailgenetics instead of password auth. In order to do the org request, Daniel set up 2FA. There's no way to set up 2FA. There are recovery codes in the ""usual place"". No one besides Daniel can currently log into hailgenetics PyPI account as a result (other than using recovery codes).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767:164,recover,recovery,164,https://hail.is,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767,2,['recover'],['recovery']
Safety,"We test against GCP's dataproc product, which, afaik, is using google's [""container optimized OS""](https://cloud.google.com/container-optimized-os/docs/). We also test local-mode with Debian 9.5 (for no particular reason). We don't currently have plans to test other distributions. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); ```. This means the JVM process on the worker node is not successfully starting. My first guess is that we still have not resolved your hail native library issues. What does `ldd --version` return on your leader node and on all of your worker nodes? If those versions are not equal, then hail will not work properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838:324,abort,aborted,324,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-454163838,1,['abort'],['aborted']
Safety,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:325,detect,detected,325,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345,1,['detect'],['detected']
Safety,"We're moving in the other direction, actually -- we're removing the nice mirrored interfaces in python/scala and replacing them with super fast `sun.misc.unsafe`-based infrastructure and native routines. It's not a good time investment to make a scala interface as nice as the Python interface right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2378#issuecomment-349722748:154,unsafe,unsafe,154,https://hail.is,https://github.com/hail-is/hail/issues/2378#issuecomment-349722748,1,['unsafe'],['unsafe']
Safety,"We, unfortunately, have no satisfactory performance measurement, target, or monitoring story. Currently, when someone makes a change that risks changing the performance of Hail, we first do local timings on large files (I have a 1GB and a 30GB file). If those are satisfactory, we additionally run some timings using a cluster on larger files. Generally, we are only comparing against latest master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009:138,risk,risks,138,https://hail.is,https://github.com/hail-is/hail/pull/5075#issuecomment-454043009,1,['risk'],['risks']
Safety,"What do we want the timeouts to be?. ```scala; private lazy val httpClientOptions = new HttpClientOptions(); .setReadTimeout(Duration.ofSeconds(5)); .setConnectTimeout(Duration.ofSeconds(5)); .setConnectionIdleTimeout(Duration.ofSeconds(5)); .setWriteTimeout(Duration.ofSeconds(5)); ```. Also, python has ``read_timeout`` as well. Should I add that and what should the timeout be? The default limit is 1 minute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13344#issuecomment-1659268538:20,timeout,timeouts,20,https://hail.is,https://github.com/hail-is/hail/pull/13344#issuecomment-1659268538,2,['timeout'],"['timeout', 'timeouts']"
Safety,"When I run the current GRM with BlockMatrix locally on profile225.hardcalls.vds (2535 samples, 225k variants), I get:. ```; java.lang.ArrayIndexOutOfBoundsException: 1048578; ```. When I run on profile.hardcalls.vds (only 25k variants), I get:. ```; java.lang.OutOfMemoryError: Java heap space; ```. When I cut profile down to only 10k variants, grm takes about 66s:. ```; read: 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:901,avoid,avoiding,901,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['avoid'],['avoiding']
Safety,"When I use python 2.7.5, it still can't work: ; ```; [root@tele-1 ~]# pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer; ****Python 2.7.5 (default, Nov 6 2016, 00:28:07)**** ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 18:18:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 18:18:30 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 18:18:30 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(sc); hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583:859,detect,detected,859,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321215583,1,['detect'],['detected']
Safety,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:763,sanity check,sanity checking,763,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530,2,['sanity check'],['sanity checking']
Safety,Why would we want the behavior to be that a user has to explicitly cancel batches on a failure? This code already does that:. ```python3; async def async_result_or_cancel_all(future):; try:; return await future.async_result(timeout=timeout); except Exception as err:; for fut in futures:; fut.cancel(); raise err; if chunksize > 1:; return (val; for future in futures; for val in await async_result_or_cancel_all(future)); return (await async_result_or_cancel_all(future); for future in futures); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10761#issuecomment-894501440:224,timeout,timeout,224,https://hail.is,https://github.com/hail-is/hail/pull/10761#issuecomment-894501440,2,['timeout'],['timeout']
Safety,"With `hailtop` installed, I get:; ```; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources ; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 211, in <module>; asyncio.run(main()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 182, in main; files = json.loads(args.files); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/__init__.py"", line 346, in loads; return _default_decoder.decode(s); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1); make:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163:723,timeout,timeout,723,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163,1,['timeout'],['timeout']
Safety,"Yeah but it would really mess with debugging. I think probably still worth keeping the `unify_exprs` and `TypeError` lines (it'll be a little redundant, but it's just type checking so shouldn't be slow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406:142,redund,redundant,142,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406,1,['redund'],['redundant']
Safety,"Yeah, I think I agree that we have a unique UUID. What I'm more skeptical of is: what if we throw an exception in a `write`? Do we clean up all the open resources? If not, that could totally leave a writer open that will conflict when we retry (even if we retried on a different VM!). ---. `retryTransientErrors` expects partition code to be safe to execute twice, but otherwise it's quite simple:. ```scala; def retryTransientErrors[T](f: => T): T = {; var delay = 0.1; var errors = 0; while (true) {; try {; return f; } catch {; case e: Exception =>; errors += 1; if (errors == 1 && isRetryOnceError(e)); return f; if (!isTransientError(e)); throw e; if (errors % 10 == 0); log.warn(s""encountered $errors transient errors, most recent one was $e""); }; delay = sleepAndBackoff(delay); }. throw new AssertionError(""unreachable""); }; ```; and this is the call site:; ```scala; val htc = new ServiceTaskContext(i); var result: Array[Byte] = null; var userError: HailException = null; try {; retryTransientErrors {; result = f(context, htc, theHailClassLoader, fs); }; } catch {; case err: HailException => userError = err; }; htc.close(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331:342,safe,safe,342,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1531592331,2,['safe'],['safe']
Safety,"Yeah, I'd like to make it required. It's one of the things that NIST 800-53 asks for: automated vulnerability detection. To be honest, I'm not impressed by a tool that finds variables whose names include the character string ""secret"" and warns if they're printed, but, .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1272063880:110,detect,detection,110,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1272063880,1,['detect'],['detection']
Safety,"Yeah, UnsafeOrdering feels like a separate PR. I'll focus on the IR sets for now, so you can give that half to me. You can give the UnsafeOrdering half to me too, but it would still be helpful to me to keep them separate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2516#issuecomment-348696868:6,Unsafe,UnsafeOrdering,6,https://hail.is,https://github.com/hail-is/hail/pull/2516#issuecomment-348696868,2,['Unsafe'],['UnsafeOrdering']
Safety,"Yeah, that's a good question, and probably something I should research, address on a Thursday. It would be nice if the structure were flatter. There is an open issue related to this: https://github.com/npm/npm/issues/19770. The file is a bit ridiculous; I should explore using npm 5.5.1 or yarn at some point. Not sure if yarn behavior is better; avoided yarn in this pull request because I want to minimize our use of third party packages to reduce complexity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632:347,avoid,avoided,347,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-456642632,1,['avoid'],['avoided']
Safety,"Yes, when I do :; ```; pyspark; sc.textFile('/hail/test/BRCA1.raw_indel.vcf') ; ```; The following information is shown:; ```; [root@tele-1 ~]# pyspark; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 17/08/09 19:16:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 17/08/09 19:16:02 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/opt/Software/hail/build/libs/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:697,detect,detected,697,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['detect'],['detected']
Safety,"You can push back on the extra flush to align the block row with the buffer. It shouldn't matter too much either way, but to the extent there is redundancy per row (as there will be in UKBB LD case) I figure alignment is a net positive.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2936#issuecomment-367163174:145,redund,redundancy,145,https://hail.is,https://github.com/hail-is/hail/pull/2936#issuecomment-367163174,1,['redund'],['redundancy']
Safety,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:414,timeout,timeout,414,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393,1,['timeout'],['timeout']
Safety,"Your diff includes the Genotype class (which changed in master yesterday). Try fetching the current master, rebasing, and force pushing (you might want to copy your branch first to ease recovery in case you have trouble with rebasing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856:186,recover,recovery,186,https://hail.is,https://github.com/hail-is/hail/pull/1288#issuecomment-274950856,1,['recover'],['recovery']
Safety,"[batch.log](https://github.com/hail-is/hail/files/2504453/batch.log). Here's a snippet:; ```; INFO | 2018-10-23 03:27:48,536 | server.py | mark_complete:184 | job 151 complete, exit_code 2; INFO | 2018-10-23 03:27:48,541 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:27:48] ""POST /pod_changed HTTP/1.1"" 204 -; ...skipping...; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 600, in urlopen; chunked=chunked); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 386, in _make_request; self._raise_timeout(err=e, url=url, timeout_value=read_timeout); File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout; raise ReadTimeoutError(self, url, ""Read timed out. (read timeout=%s)"" % timeout_value); urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:757,timeout,timeout,757,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,2,['timeout'],['timeout']
Safety,`SafeIndexedSeq` would need to be added.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3353#issuecomment-380507900:1,Safe,SafeIndexedSeq,1,https://hail.is,https://github.com/hail-is/hail/pull/3353#issuecomment-380507900,1,['Safe'],['SafeIndexedSeq']
Safety,```; TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E reactor.core.Exceptions$ReactiveException: java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.Exceptions.propagate(Exceptions.java:392); E 	at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:97); E 	at reactor.core.publisher.Flux.blockLast(Flux.java:2519); E 	at com.azure.core.util.paging.ContinuablePagedByIteratorBase.requestPage(ContinuablePagedByIteratorBase.java:94); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable$ContinuablePagedByItemIterator.<init>(ContinuablePagedByItemIterable.java:50); E 	at com.azure.core.util.paging.ContinuablePagedByItemIterable.iterator(ContinuablePagedByItemIterable.java:37); E 	at com.azure.core.util.paging.ContinuablePagedIterable.iterator(ContinuablePagedIterable.java:106); E 	at java.lang.Iterable.forEach(Iterable.java:74); E 	at is.hail.io.fs.AzureStorageFS.delete(AzureStorageFS.scala:203); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1(ExecuteContext.scala:27); E 	at is.hail.backend.OwningTempFileManager.$anonfun$cleanup$1$adapted(ExecuteContext.scala:26); E 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); E 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); E 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); E 	at is.hail.backend.OwningTempFileManager.cleanup(ExecuteContext.scala:26); E 	at is.hail.backend.ExecuteContext.close(ExecuteContext.scala:148); E 	at is.hail.utils.package$.using(package.scala:660); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:70); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.Ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:5,Timeout,TimeoutException,5,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,2,['Timeout'],['TimeoutException']
Safety,"```; hail: info: SparkUI: http://10.131.101.159:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-f69b497; >>> print sc; <SparkContext master=yarn appName=PySparkShell>; >>> print hc; <hail.context.HailContext object at 0x1f15350>; >>> hc.import_vcf(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; TypeError: import_vcf() takes at least 2 arguments (1 given); >>> hc.import_vcf('/hail/sample.vcf'); [Stage 0:> (0 + 1) / 2]Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-313>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, com2, executor 1): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:900,abort,aborted,900,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['abort'],['aborted']
Safety,```text; In [4]: hc.import_vcf('src/test/resources/sample.vcf'); hail: info: No multiallelics detected.; hail: info: Coerced sorted dataset; Out[4]: <hail.dataset.VariantDataset at 0x10b0d8ad0>. In [5]: hc.import_vcf('src/test/resources/sample2.vcf'); hail: info: Multiallelics detected. Some methods cannot be run without splitting or filtering multiallelics first.; hail: info: Coerced sorted dataset; Out[5]: <hail.dataset.VariantDataset at 0x10b0d8cd0>; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636:94,detect,detected,94,https://hail.is,https://github.com/hail-is/hail/pull/1632#issuecomment-290863636,2,['detect'],['detected']
Safety,"`db` sounds like a connection. I used dbpool. Added a check in _start_build. I had debated about this, but I agree, certainly safer. I put the yaml fields in alphabetical order, which is what the K8s docs usually do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6308#issuecomment-500908659:126,safe,safer,126,https://hail.is,https://github.com/hail-is/hail/pull/6308#issuecomment-500908659,1,['safe'],['safer']
Safety,"`large_range_matrix_table_sum()` failed in the benchmarks, looking into that. When I ran it locally, seemed to be allocating more memory than I would think, so there's probably a leak there. Otherwise, I think this is safe to review while I track this one down (and maybe you'll catch the cause of this). ```; 2020-03-26 12:41:14 root: INFO: RegionPool: REPORT_THRESHOLD: 16.0M allocated (792.0K blocks / 15.3M chunks), thread 70: Executor task launch worker for task 10; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228:218,safe,safe,218,https://hail.is,https://github.com/hail-is/hail/pull/8315#issuecomment-604542228,1,['safe'],['safe']
Safety,"`pip install -e .`; Defaulting to user installation because normal site-packages is not writeable; Obtaining file:///home/skr/hail2/hail; Installing build dependencies ... done; Checking if build backend supports build_editable ... done; Getting requirements to build editable ... error; error: subprocess-exited-with-error; ;  Getting requirements to build editable did not run successfully.;  exit code: 1; > [14 lines of output]; error: Multiple top-level packages discovered in a flat-layout: ['tls', 'gear', 'hail', 'auth', 'blog', 'infra', 'batch', 'query', 'docker', 'memory', 'devbin', 'gateway', 'website', 'grafana', 'notebook', 'graphics', 'datasets', 'monitoring', 'web_common', 'prometheus', 'letsencrypt'].; ; To avoid accidental inclusion of unwanted files or directories,; setuptools will not proceed with this build.; ; If you are trying to create a single distribution with multiple packages; on purpose, you should not rely on automatic discovery.; Instead, consider the following options:; ; 1. set up custom discovery (`find` directive with `include` or `exclude`); 2. use a `src-layout`; 3. explicitly set `py_modules` or `packages` with a list of names; ; To find more information, look for ""package discovery"" on setuptools docs.; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: subprocess-exited-with-error.  Getting requirements to build editable did not run successfully.;  exit code: 1; > See above for output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290:731,avoid,avoid,731,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290,1,['avoid'],['avoid']
Safety,able.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurren,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217388,recover,recover,217388,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Safety,"ah, I should have enumerated the changes besides motion. . I renamed utils to helpers to avoid a name conflict. ; I moved a few things like get_dataset to helpers, which deleted a bunch of duplicated code. I also deleted a few of the ColumnTests in test_api that were already covered by test_expr, and moved the rest into that module.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3887#issuecomment-402121286:89,avoid,avoid,89,https://hail.is,https://github.com/hail-is/hail/pull/3887#issuecomment-402121286,1,['avoid'],['avoid']
Safety,ain.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.Th,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4690,Timeout,TimeoutMainSubscriber,4690,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['Timeout'],['TimeoutMainSubscriber']
Safety,"al: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact and total control over the memory available to the driver and the workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:2733,safe,safe,2733,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,2,['safe'],['safe']
Safety,"alhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5261,timeout,timeout,5261,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,"ams that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4274,timeout,timeout,4274,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,"an 28 18:37:39 UTC 2016; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@733: Client environment:user.name=schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@741: Client environment:user.home=/home/users/schoi; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@log_env@753: Client environment:user.dir=/mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:1893,timeout,timeout,1893,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['timeout'],['timeout']
Safety,"analysis_type=VariantFiltration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.unfiltered.vcf) mask=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub filterExpression=[FS>200.0, QD<2.0, ReadPosRankSum<-20.0, InbreedingCoeff<-0.8] filterName=[Indel_FS, Indel_QD, Indel_ReadPosRankSum, Indel_InbreedingCoeff] genotypeFilterExpression=[] genotypeFilterName=[] clusterSize=3 clusterWindowSize=0 maskExtension=0 ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:19927,unsafe,unsafe,19927,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,ancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 Y,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201744,abort,abortStage,201744,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['abort'],['abortStage']
Safety,"anche level for SNP model at VQS Lod < -502107.0516"">; ##FILTER=<ID=VQSRTrancheSNP99.95to100.00,Description=""Truth sensitivity tranche level for SNP model at VQS Lod: -502107.0516 <= x < -22.1967"">; ##FORMAT=<ID=AD,Number=.,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block"">; ##FORMAT=<ID=PGT,Number=.,Type=String,Description=""PGT"">; ##FORMAT=<ID=PID,Number=.,Type=String,Description=""PID"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Normalized, Phred-scaled likelihoods for genotypes as defined in the VCF specification"">; ##FORMAT=<ID=RGQ,Number=1,Type=Integer,Description=""Unconditional reference genotype confidence, encoded as a phred quality -10*log10 p(genotype call is wrong)"">; ##FORMAT=<ID=SB,Number=4,Type=Integer,Description=""Per-sample component statistics which comprise the Fisher's Exact Test to detect strand bias."">; ... ~ 3000 lines of header + decoys; ##contig=<ID=HLA-DRB1*12:17,length=11260>; ##contig=<ID=HLA-DRB1*13:01:01,length=13935>; ##contig=<ID=HLA-DRB1*13:02:01,length=13941>; ##contig=<ID=HLA-DRB1*14:05:01,length=13933>; ##contig=<ID=HLA-DRB1*14:54:01,length=13936>; ##contig=<ID=HLA-DRB1*15:01:01:01,length=11080>; ##contig=<ID=HLA-DRB1*15:01:01:02,length=11571>; ##contig=<ID=HLA-DRB1*15:01:01:03,length=11056>; ##contig=<ID=HLA-DRB1*15:01:01:04,length=11056>; ##contig=<ID=HLA-DRB1*15:02:01,length=10313>; ##contig=<ID=HLA-DRB1*15:03:01:01,length=11567>; ##contig=<ID=HLA-DRB1*15:03:01:02,length=11569>; ##contig=<ID=HLA-DRB1*16:02:01,length=11005>; ##reference=file:///cromwell_root/broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; #CHROM	POS	ID	RE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916114:2310,detect,detect,2310,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916114,1,['detect'],['detect']
Safety,"and.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). ```. I instead tried to run the same code in two separate jupyter notebooks, with the same code inside but different ways to initialize the hailcontext, one like this (works and exports):. ```; from hail import *; hc = HailContext(); ```; With startup messages looking like this:. ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 13:51:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 13:51:03 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 13:51:03 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 13:51:03 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 13:51:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; ```. And the other initialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:6273,detect,detected,6273,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['detect'],['detected']
Safety,anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2413,abort,abortStage,2413,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['abort'],['abortStage']
Safety,"are you sure it's a dataproc problem?. If scalding is using java unsafe in a not-guaranteed-to-work way, then a core dump s totally possible. Example - the JVM will sometimes tolerate misaligned floats/ints, and sometimes will crash.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419972098:65,unsafe,unsafe,65,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419972098,1,['unsafe'],['unsafe']
Safety,"arrayCopyTest looks like it needs requiredeness upcast:. UnsafeIndexedSeq(8589934593, 12884901890, 17179869187, 21474836484, 25769803781, 30064771078, 34359738375, 38654705672, 9) did not equal Vector(1, 2, 3, 4, 5, 6, 7, 8, 9)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8099#issuecomment-586471938:57,Unsafe,UnsafeIndexedSeq,57,https://hail.is,https://github.com/hail-is/hail/pull/8099#issuecomment-586471938,1,['Unsafe'],['UnsafeIndexedSeq']
Safety,"ate. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2782,Timeout,Timeout,2782,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,3,"['Timeout', 'timeout']","['Timeout', 'timeout']"
Safety,ava:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:5094,recover,recover,5094,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Safety,ava:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4901,recover,recover,4901,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Safety,"but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1105,abort,abortStage,1105,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['abort'],['abortStage']
Safety,cala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12325,abort,abortStage,12325,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['abort'],['abortStage']
Safety,"ce for Amanda:; ```; Traceback (most recent call last):; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 157, in <module>; main(args); File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/assign_subpops.py"", line 98, in main; pca_mt = hl.ld_prune(pca_mt, r2=0.1, n_cores=args.num_cores); File ""<decorator-gen-788>"", line 2, in ld_prune; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/typecheck/check.py"", line 490, in _typecheck; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/methods/statgen.py"", line 2918, in ld_prune; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/3c5f402fed564ccd85257c0919d4bffb/hail-devel-38dbf156b630.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 530 in stage 9.0 failed 20 times, most recent failure: Lost task 530.19 in stage 9.0 (TID 19101, gt1-w-78.c.broad-mpg-gnomad.internal, executor 199): java.lang.ClassCastException: is.hail.codegen.generated.C14 cannot be cast to is.hail.asm4s.AsmFunction2; 	at is.hail.io.CompiledPackDecoder.readRegionValue(RowStore.scala:643); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:222); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9$$anonfun$apply$10.apply(RVD.scala:221); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:221); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8$$anonfun$apply$9.apply(RVD.scala:220); 	at is.hail.utils.package$.using(package.scala:577); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:220); 	at is.hail.rvd.RVD$$anonfun$7$$anonfun$apply$8.apply(RVD.scala:218); 	at scala.collection.I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:1003,abort,aborted,1003,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['abort'],['aborted']
Safety,"cent call last):; File ""/batch/server.py"", line 417, in run_forever; target(*args, **kwargs); File ""/batch/server.py"", line 441, in kube_event_loop; requests.post('http://127.0.0.1:5000/pod_changed', json={'pod_name': name}, timeout=120); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 116, in post; return request('post', url, data=data, json=json, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/api.py"", line 60, in request; return session.request(method=method, url=url, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 524, in request; resp = self.send(prep, **send_kwargs); File ""/usr/lib/python3.6/site-packages/requests/sessions.py"", line 637, in send; r = adapter.send(request, **kwargs); File ""/usr/lib/python3.6/site-packages/requests/adapters.py"", line 529, in send; raise ReadTimeout(e, request=request); requests.exceptions.ReadTimeout: HTTPConnectionPool(host='127.0.0.1', port=5000): Read timed out. (read timeout=120); INFO | 2018-10-23 03:58:08,124 | server.py | run_forever:416 | run_forever: run target kube_event_loop; INFO | 2018-10-23 03:59:30,729 | server.py | mark_complete:175 | wrote log for job 153 to logs/job-153.log; INFO | 2018-10-23 03:59:30,730 | server.py | set_state:141 | job 153 changed state: Created -> Complete; INFO | 2018-10-23 03:59:30,730 | server.py | mark_complete:184 | job 153 complete, exit_code 2; INFO | 2018-10-23 03:59:30,737 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,806 | _internal.py | _log:88 | 10.56.142.33 - - [23/Oct/2018 03:59:30] ""GET /jobs HTTP/1.1"" 200 -; INFO | 2018-10-23 03:59:30,815 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,816 | _internal.py | _log:88 | 127.0.0.1 - - [23/Oct/2018 03:59:30] ""POST /pod_changed HTTP/1.1"" 204 -; INFO | 2018-10-23 03:59:30,817 | server.py | refresh_k8s_state:385 | started k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038:1974,timeout,timeout,1974,https://hail.is,https://github.com/hail-is/hail/issues/4608#issuecomment-432083038,1,['timeout'],['timeout']
Safety,chRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5247,Unsafe,UnsafeRow,5247,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety,"ched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadins",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2563,detect,detected,2563,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['detect'],['detected']
Safety,"cker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:42925,timeout,timeout-,42925,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['timeout'],['timeout-']
Safety,"closing in favor of https://github.com/hail-is/hail/pull/14415, which starts from scratch on the current `main` and applies all the temporarily-ignored ruff rules manually to avoid bugs being introduced by automatic fixes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14150#issuecomment-1998147422:175,avoid,avoid,175,https://hail.is,https://github.com/hail-is/hail/pull/14150#issuecomment-1998147422,1,['avoid'],['avoid']
Safety,cutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:306); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:6953,recover,recover,6953,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['recover'],['recover']
Safety,cutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:326); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at co,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:7144,recover,recover,7144,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['recover'],['recover']
Safety,d caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5644,abort,abortStage,5644,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['abortStage']
Safety,"definitely has an infinite loop, job was running for 13 hours. I manually killed it. Issue https://github.com/hail-is/ci/issues/91 created to track timeouts on jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4320#issuecomment-420689376:148,timeout,timeouts,148,https://hail.is,https://github.com/hail-is/hail/pull/4320#issuecomment-420689376,1,['timeout'],['timeouts']
Safety,"duh, my issue is that my copy stuff only runs on MakeArray, so I'm just seeing requiredeness offset issues when I'm lowering to MakeStream. . toEmitTriplet in Emit needs to be modified to read the actual element types of the child IRs. It currently assumes uniform element pType's. edit: Personal notes to avoid comments in code. Currently trying version that only affects toEmitTriplet, note also however:. ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ````. assumes all element types are same, which can be trivially untrue (MakeStream(MakeTuple())",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583897947:306,avoid,avoid,306,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583897947,1,['avoid'],['avoid']
Safety,"e a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, good point!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:3416,abort,aborted,3416,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,2,"['abort', 'redund']","['aborted', 'redundant']"
Safety,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:3442,avoid,avoid,3442,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['avoid'],['avoid']
Safety,"e! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(dige",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:2439,avoid,avoid,2439,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['avoid'],['avoid']
Safety,e=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattered.intervals] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=75 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false genotype_likelihoods_model=BOTH pcr_error_rate=1.0E-4 computeSLOD=false annotateNDA=false pair_hmm_implementation=ORIGINAL min_base_quality_score=17 max_deletion_fraction=0.05 min_indel_count_for_genotyping=5 min_indel_fraction_per_sample=0.25 indel_heterozygosity=1.25E-4 indelGapContinuationPenalty=10 indelGapOpenPenalty=45 indelHaplotypeSize=80 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:14667,unsafe,unsafe,14667,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,"eContext.scala:18); at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229); at is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:303); at is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:323); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:282); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:9564,abort,aborted,9564,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['abort'],['aborted']
Safety,"ecur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2244,safe,safe,2244,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,2,['safe'],['safe']
Safety,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:112,timeout,timeout,112,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650,2,['timeout'],['timeout']
Safety,"eepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:4430,abort,aborted,4430,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['aborted']
Safety,"eles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFaile",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7985,abort,aborted,7985,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['aborted']
Safety,"elet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20494,timeout,timeout,20494,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['timeout'],['timeout']
Safety,"et that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:1144,timeout,timeout,1144,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['timeout'],['timeout']
Safety,"except in 3rd handler:200 (another response.post) hangs first... `ConnectionResetError` errorNum=104.; * May be related: https://github.com/kubernetes/kubernetes/pull/53947. Batch: `kube_event_loop` is always involved. Always:; ```log; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 384, in _make_request; six.raise_from(e, None); File ""<string>"", line 2, in raise_from; File ""/usr/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 380, in _make_request; httplib_response = conn.getresponse(); File ""/usr/lib/python3.6/http/client.py"", line 1331, in getresponse; response.begin(); File ""/usr/lib/python3.6/http/client.py"", line 297, in begin; version, status, reason = self._read_status(); File ""/usr/lib/python3.6/http/client.py"", line 258, in _read_status; line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1""); File ""/usr/lib/python3.6/socket.py"", line 586, in readinto; return self._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:1054,timeout,timeout,1054,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,2,['timeout'],['timeout']
Safety,"executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at or",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8787,abort,abortStage,8787,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['abortStage']
Safety,ext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4313,abort,abortStage,4313,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['abort'],['abortStage']
Safety,"fmtlib looks promising, but. a) I'm not keen to mix in non-standard third-party source code into our; codebase; unless it's a significant win over the functionality that is; available everywhere; and familiar to everyone. printf has been getting the job done for; decades. b) the speed test shows it still slower than printf (iirc 1.56s vs; 1.35s), just not a; *lot* slower (whereas C++ iostreams are very slow). On Fri, Aug 3, 2018 at 10:45 AM Patrick Schultz <notifications@github.com>; wrote:. > *@patrick-schultz* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207566438>:; >; > > +#include <sys/stat.h>; > +#include <sys/time.h>; > +#include <unistd.h>; > +#include <atomic>; > +#include <memory>; > +#include <mutex>; > +#include <iostream>; > +#include <sstream>; > +#include <string>; > +#include <vector>; > +; > +#if 0; > +#define D(fmt, ...) { \; > + char buf[1024]; \; > + sprintf(buf, fmt, ##__VA_ARGS__); \; > + fprintf(stderr, ""DEBUG: %s,%d: %s"", __FILE__, __LINE__, buf); \; >; > @cseed <https://github.com/cseed> suggested using the fmt library (; > https://github.com/fmtlib/fmt), which I believe is being proposed for; > standardization. That is my preference also. It appears to be as safe as; > IOstreams and as fast as printf. It supports both Python style and C; > style formatting, e.g.; >; > fmt::print(""Hello, {}!"", ""world""); // uses Python-like format string syntaxfmt::printf(""Hello, %s!"", ""world""); // uses printf format string syntax; >; > Format strings are checked at compile time, as are argument types.; >; > ; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207566438>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExrljAAlcA6ksqZiNPyOw8wgC0hheks5uNGH7gaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410319504:1319,safe,safe,1319,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410319504,1,['safe'],['safe']
Safety,"gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8448,abort,abortStage,8448,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['abortStage']
Safety,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19403,timeout,timeout,19403,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['timeout'],['timeout']
Safety,gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at __C796collect_distributed_array_table_native_writer.__m872SKIP_o_int32(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m869INPLACE_DECODE_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_o_struct_of_o_callANDo_int32ANDo_stringANDo_array_of_o_int32END(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m868INPLACE_DECODE_r_array_of_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_r_array_of_o_struct_of_o_callANDo_int32ANDo_stringANDo_array_of_o_int32END(Unknown Source) ~[?:?]; 	at __C796collect_distributed_array_table_native_writer.__m867DECODE_r_struct_of_r_array_of_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32ENDEND_TO_SBaseStructPointer(Unknown Source) ~[?:?]; ```. Error for run 2; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:655) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readBytes(InputBuffers.scala:444) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.LEB128InputBuffer.readBytes(InputBuffers.scala:253) ~[gs:__hail-query-ger0g_jars_ee77707f4fab22b1c253321b082a70aff3f44d1c.jar.jar:0.0.1-SNAPSHOT]; 	at __C816collect_distributed_array_table_native_writer.__m893INPLACE_DECODE_o_binary_TO_o_string(Unknown Source) ~[?:?]; 	at __C816collect_distributed_array_table_native_writer.__m889INPLACE_DECODE_o_struct_of_o_int32ANDo_int32ANDo_int32ANDo_binaryANDo_array_of_o_int32END_TO_o_struct_of_o_callANDo_int32ANDo_stringANDo_array_of_o_int32END(Unknown Source) ~[?:?]; 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:4103,detect,detected,4103,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['detect'],['detected']
Safety,hedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionCon,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:216037,recover,recover,216037,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Safety,hedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215966,recover,recover,215966,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Safety,here's a timeout:; ```; # time curl -vvv https://ci.hail.is; * Rebuilt URL to: https://ci.hail.is/; * Trying 35.188.91.25...; * TCP_NODELAY set; * Connection failed; * connect to 35.188.91.25 port 443 failed: Operation timed out; * Failed to connect to ci.hail.is port 443: Operation timed out; * Closing connection 0; curl: (7) Failed to connect to ci.hail.is port 443: Operation timed out; curl -vvv https://ci.hail.is 0.01s user 0.01s system 0% cpu 1:15.36 total; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8047#issuecomment-582646516:9,timeout,timeout,9,https://hail.is,https://github.com/hail-is/hail/issues/8047#issuecomment-582646516,1,['timeout'],['timeout']
Safety,"hich region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; ---------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:4334,Abort,Aborted,4334,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['Abort'],['Aborted']
Safety,"his pull request, linked above, I chose Starlette, and it is a thin abstraction, nearly identical performance, over Uvicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3017,timeout,timeouts,3017,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeouts']
Safety,https://ci.hail.is/batches/3823676/jobs/139 looks like an unrelated timeout? Can I trigger a rerun of the `ci-test` somehow?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12027#issuecomment-1186643880:68,timeout,timeout,68,https://hail.is,https://github.com/hail-is/hail/pull/12027#issuecomment-1186643880,1,['timeout'],['timeout']
Safety,"i went through and manually fixed everything that `--unsafe-fixes` had initially addressed (after undoing the unsafe fixes, obv), and i broke all my changes up into separate commits based on which linter rule they were addressing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14150#issuecomment-1894479704:53,unsafe,unsafe-fixes,53,https://hail.is,https://github.com/hail-is/hail/pull/14150#issuecomment-1894479704,4,['unsafe'],"['unsafe', 'unsafe-fixes']"
Safety,ils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.IndexOutOfBoundsException: 3; 	at is.hail.annotations.UnsafeRow.isNullAt(UnsafeRow.scala:294); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:254); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:871); 	at is.hail.variant.MatrixTable$$anonfun$59$$anonfun$apply$42.apply(MatrixTable.scala:860); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:637); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:631); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.Ri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:5201,Unsafe,UnsafeRow,5201,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['Unsafe'],['UnsafeRow']
Safety,"imits; await _handle_api_error(_edit_billing_limit, db, billing_project, limit); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 212, in _handle_api_error; await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2227, in _edit_billing_limit; await insert() # pylint: disable=no-value-for-parameter; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 34, in wrapper; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 64, in wrapper; return await fun(tx, *args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/batch/front_end/front_end.py"", line 2212, in insert; (billing_project,),; File ""/usr/local/lib/python3.7/dist-packages/gear/database.py"", line 209, in execute_and_fetchone; await cursor.execute(sql, args); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 239, in execute; await self._query(query); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/cursors.py"", line 457, in _query; await conn.query(q); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 469, in query; await self._read_query_result(unbuffered=unbuffered); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 672, in _read_query_result; await result.read(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 1153, in read; first_packet = await self.connection._read_packet(); File ""/usr/local/lib/python3.7/dist-packages/aiomysql/connection.py"", line 641, in _read_packet; packet.raise_for_error(); File ""/usr/local/lib/python3.7/dist-packages/pymysql/protocol.py"", line 221, in raise_for_error; err.raise_mysql_exception(self._data); File ""/usr/local/lib/python3.7/dist-packages/pymysql/err.py"", line 143, in raise_mysql_exception; raise errorclass(errno, errval); pymysql.err.OperationalError: (1205, 'Lock wait timeout exceeded; try restarting transaction'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586:2337,timeout,timeout,2337,https://hail.is,https://github.com/hail-is/hail/pull/12652#issuecomment-1416434586,1,['timeout'],['timeout']
Safety,"it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I j",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1187,timeout,timeouts,1187,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,4,['timeout'],['timeouts']
Safety,"ittle unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we *are* creating for hail use, like the temp bucket, ask for the name first and then ask which project it should be created in, using the projects listed in gcloud as choices with the option to write in your own. ### Regarding number of checks; I think it'd be good to avoid warnings when possible. From looking at this I see a pattern of; 1. Ask a leading question; 2. Emit a warning if the user selects the alternative option instead of the suggested option. I think I would prefer instead to ask a leading question and in the prompt explain why the alternative option might be undesirable. Then when they make a decision just move on. On a broader note, I think we should focus on having good documentation and linking to it over having perfectly thorough ; explanations in the CLI. At some point in an interactive setup if it gets longwinded I start spamming enter, but if it was quick and at the end it said something to the effect of: ""Your current configuration could result in excess cloud cost. See the documentation <here> about common pitfalls and how to avoid them"", I might decide to read through that FAQ with a more discerning eye. This is just my opinion though, I would be curious to see if other folks disagree regarding the UX.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:2269,avoid,avoid,2269,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,4,['avoid'],['avoid']
Safety,"ld/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16020,risk,risk,16020,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['risk'],['risk']
Safety,"les`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail inst",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2644,Timeout,Timeout,2644,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['Timeout'],['Timeout']
Safety,locks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3552,abort,abortStage,3552,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['abort'],['abortStage']
Safety,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355:38,unsafe,unsafe-fixes,38,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355,2,['unsafe'],['unsafe-fixes']
Safety,"loop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3760,timeout,timeouts,3760,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeouts']
Safety,"ls.package$.using(package.scala:635); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:59); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:339); 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:483); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:482); 	at sun.reflect.GeneratedMethodAccessor88.invoke(Unknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:238); 	at java.lang.Thread.run(Thread.java:750). Hail version: 0.2.107-2387bb00ceee; Error summary: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. ```; [hail-20230803-1955-0.2.107-2387bb00ceee.log](https://github.com/hail-is/hail/files/12254716/hail-20230803-1955-0.2.107-2387bb00ceee.log); Log file attached.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:10881,abort,aborted,10881,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['aborted']
Safety,ly$27.apply(ContextRDD.scala:355); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5193,abort,abortStage,5193,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['abortStage']
Safety,ly(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6426,abort,abortStage,6426,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['abort'],['abortStage']
Safety,n$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:222); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 11 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:3209,abort,abortStage,3209,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['abort'],['abortStage']
Safety,nd.scala:430); E 	at is.hail.backend.service.Main$.main(Main.scala:33); E 	at is.hail.backend.service.Main.main(Main.scala); E 	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4521,Timeout,TimeoutException,4521,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['Timeout'],['TimeoutException']
Safety,"nect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg St",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5107,timeout,timeout,5107,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,"nformation. Nit: the doc doesn't say the instance monitor monitors instances, it just monitors and handles *events*. Let me be explicit: I think the doc is wrong about the monitor doing health checking because that requires it to track the instances, which I just said should be owned by the pools and the JPIM. That didn't occur to me when we were writing the doc, my apologies. I still think the monitor should:; - route events to the right pool or to the JPIM, and; - aggregate summaries up for the web UI. ---. Let me try to be more specific in my critique:. I think of the system as three layers: the top most is the driver, the middle layer is the monitor, and the bottom layer is the pool or JobPrivateInstanceManager (JPIM). I don't want control flow to go down, up, and back down again. If that happens, then we can't reason about our system as separate layers, we necessarily have to think about the middle and bottom layer together. Very specifically, this flow worries me: (instance pool) create_instance -> (instance monitor) add_instance -> adjust_for_add_instance -> (instance pool) adjust_for_add_instance. We move from low to mid *back to low*. I want information to flow in one direction: either its downward information or its upward information. ---. I'm guessing you're also concerned about code organization / code duplication. I'm not that worried about this. The JPIM and the Pool are similar things and we might inevitably produce some duplication. That's OK with me. To be honest, I think a few stand-alone functions that both of them use will eliminate any code duplication. Both pools and the JPIM will have a `name_instances` and `instance_by_last_updated`. If the duplication gets hard to manage, we might pack that up into another class like InstanceCollection. I realize this means we have several monitoring loops. I'm not very worried about that. I think it's fine and it helps simplify the architecture. It avoids entangling the monitor with the pools and the JPIM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358:2081,avoid,avoids,2081,https://hail.is,https://github.com/hail-is/hail/pull/9772#issuecomment-738515358,2,['avoid'],['avoids']
Safety,"oh, interesting.. I thought liftover processed each variant independently. Just out of curiosity, what's the purpose of the special while-loop infrastructure?; and will it be possible to avoid shuffling when sorting lifted-over variants?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4895#issuecomment-444706692:187,avoid,avoid,187,https://hail.is,https://github.com/hail-is/hail/pull/4895#issuecomment-444706692,1,['avoid'],['avoid']
Safety,ompile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32268,timeout,timeout,32268,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['timeout'],['timeout']
Safety,"on-autosomes for `concordance`. Importing two VCFs, splitting multi, and `describe()`, `count()`, and `_force_count_rows()` all behave as expected. ```; import hail as hl. giab_gs_path = 'gs://xxxxxxxx'; giab_ds = hl.import_vcf(giab_gs_path, reference_genome='GRCh38'); giab_sm = hl.SplitMulti(giab_ds); giab_ds = giab_sm.result(); giab_ds.describe(); gaib_ds.count(); gaib_ds._force_count_rows(); # all good. sent_gs_path = 'gs://xxxxxxxxxxx'; sent_ds = hl.import_vcf(sent_gs_path, reference_genome='GRCh38'); sent_sm = hl.SplitMulti(sent_ds); sent_ds = sent_sm.result(); sent_ds.describe(); sent_ds.count(); sent_ds._force_count_rows(); # all good. # then this gives the stack trace below. giab_22_ds = hl.filter_intervals(giab_ds, [hl.parse_locus_interval('chr22', reference_genome='GRCh38')]); ```. Stack trace:. ```; FatalError: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 20 times, most recent failure: Lost task 0.19 in stage 19.0 (TID 312, gilson-validation-test-2-w-4.c.perfect-atrium-179917.internal, executor 2): java.lang.ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1641); 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1637); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.Con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:1065,abort,aborted,1065,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['abort'],['aborted']
Safety,"or linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /Users/Jon/data/plinkTest.assoc.linear ... done. real 0m13.167s; user 0m13.071s; sys 0m0.080s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1296,detect,detected,1296,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,1,['detect'],['detected']
Safety,"order as listed"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##INFO=<ID=BaseQRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt Vs. Ref base qualities"">; ##INFO=<ID=DB,Number=0,Type=Flag,Description=""dbSNP Membership"">; ##INFO=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth; some reads may have been filtered"">; ##INFO=<ID=DS,Number=0,Type=Flag,Description=""Were any of the samples downsampled?"">; ##INFO=<ID=Dels,Number=1,Type=Float,Description=""Fraction of Reads Containing Spanning Deletions"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=FS,Number=1,Type=Float,Description=""Phred-scaled p-value using Fisher's exact test to detect strand bias"">; ##INFO=<ID=HaplotypeScore,Number=1,Type=Float,Description=""Consistency of the site with at most two segregating haplotypes"">; ##INFO=<ID=InbreedingCoeff,Number=1,Type=Float,Description=""Inbreeding coefficient as estimated from the genotype likelihoods per-sample when compared against the Hardy-Weinberg expectation"">; ##INFO=<ID=MLEAC,Number=A,Type=Integer,Description=""Maximum likelihood expectation (MLE) for the allele counts (not necessarily the same as the AC), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:7287,detect,detect,7287,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['detect'],['detect']
Safety,org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12664,abort,abortStage,12664,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['abort'],['abortStage']
Safety,"p.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 TaskSetManager: ERROR: Task 35 in stage 0.0 failed 4 times; aborting job; 2019-01-22 13:12:06 TaskSetManager: WARN: Lost task 5.3 in stage 0.0 (TID 61, scc-q03.scc.bu.edu, executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0174_01_000022 on host: scc-q03.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0174_01_000022; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:194375,abort,aborting,194375,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['abort'],['aborting']
Safety,"path.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1714,detect,detected,1714,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['detect'],['detected']
Safety,"plementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBufferSpec""}}}}; ```; Error for run 1.; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:655) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:385) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readByte(InputBuffers.scala:403) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.LEB128InputBuffer.readByte(InputBuffers.scala:220) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.LEB128InputBuffer.skipInt(InputBuffers.scala:260) ~[gs:__hail-query-ger0g_jars_fcaafc533ec130ae210b152afa81c0b5ac04592b.jar.jar:0.0.1-SNAPSHOT]; 	at __C796collect_distributed_array_table_native_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:2244,detect,detected,2244,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['detect'],['detected']
Safety,"pper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:194); at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:147); at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:206); at org.apache.spark.util.collection.ExternalAppendOnlyMap.spill(ExternalAppendOnlyMap.scala:55); at org.apache.spark.util.collection.Spillable$class.m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:2079,abort,aborted,2079,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,2,['abort'],['aborted']
Safety,"putting this back in the WIP stack while i try using `--unsafe-fixes` and see if it differs from the manual changes significantly, and break out the manual fixes into separate commits based on which rule they address",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883347616:56,unsafe,unsafe-fixes,56,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883347616,1,['unsafe'],['unsafe-fixes']
Safety,r$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201986,abort,abortStage,201986,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['abort'],['abortStage']
Safety,reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:4790,Timeout,TimeoutMainSubscriber,4790,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['Timeout'],['TimeoutMainSubscriber']
Safety,"rg.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from container-launch.; Container id: container_e2435_1542127286896_0109_02_000004; Exit code: 1; Stack trace: ExitCodeException exitCode=1:; at org.apache.hadoop.util.Shell.runCommand(Shell.java:576); at org.apache.hadoop.util.Shell.run(Shell.java:487); at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:753); at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:371); at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:303); at org.apache.hadoop.yarn.server.nodemanager.containermanager.lau",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:3180,abort,aborted,3180,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['abort'],['aborted']
Safety,"rom /192.168.18.203:44844 is closed; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: WARN: Attempted to get executor loss reason for executor id 14 at RPC address 192.168.18.189:50356, but got no response. Marking as slave lost.; java.io.IOException: Connection from /192.168.18.203:44844 closed; at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:278); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:211331,timeout,timeout,211331,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['timeout'],['timeout']
Safety,"s/TOPMed/BROAD/Chr22; 2016-08-24 16:42:27,052:8532(0x7fef97fff700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=192.168.0.1:2181,192.168.0.9:2181,192.168.0.17:2181 sessionTimeout=10000 watcher=0x3b9c8c00a0 sessionId=0 sessionPasswd=<null> context=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2248,detect,detector,2248,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['detect'],['detector']
Safety,"s=""analysis_type=SelectVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL kee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:12239,unsafe,unsafe,12239,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['unsafe'],['unsafe']
Safety,se$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63); at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:217459,recover,recover,217459,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['recover'],['recover']
Safety,similar:; ```; >>> mt.aggregate_entries(hl.agg.fraction(mt.GQ > 20)); 0.97; >>> mt = mt.filter_entries(mt.GQ > 20); >>> mt.aggregate_entries(hl.agg.fraction(mt.GQ > 20)) # sanity check; 0.97; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8041#issuecomment-582174317:172,sanity check,sanity check,172,https://hail.is,https://github.com/hail-is/hail/issues/8041#issuecomment-582174317,1,['sanity check'],['sanity check']
Safety,spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6184,abort,abortStage,6184,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['abort'],['abortStage']
Safety,"st-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip', '--properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false', '--initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh', '--master-machine-type=n1-standard-1', '--master-boot-disk-size=40GB', '--num-master-local-ssds=0', '--num-preemptible-workers=0', '--num-worker-local-ssds=0', '--num-workers=2', '--preemptible-worker-boot-disk-size=40GB', '--worker-boot-disk-size=40', '--worker-machine-type=n1-standard-1', '--zone=us-central1-b', '--initialization-action-timeout=20m', '--bucket=hail-ci-0-1-dataproc-staging-bucket', '--max-idle=10m']' returned non-zero exit status 1. real	20m34.381s; user	0m6.329s; sys	0m1.522s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:11345,timeout,timeout,11345,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['timeout'],['timeout']
Safety,"stributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8545,abort,abortStage,8545,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['abort'],['abortStage']
Safety,"t look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, ti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4122,timeout,timeout,4122,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,"t: sigmaG2 = 0.13829390418697945; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: sigmaE2 = 0.8304138510277874; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: delta = 6.004703214575758; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: h2 = 0.1427612233333665; 2017-08-28 21:47:47 Hail: INFO: lmmreg: global model fit: seH2 = 0.13770872661270844; 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Cov1' as type `Float64' (user-specified); Loading column `Cov2' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: Reading table with no type imputation; Loading column `Sample' as type `String' (type not specified); Loading column `Pheno' as type `Float64' (user-specified). 2017-08-28 21:47:48 Hail: INFO: No multiallelics detected.; 2017-08-28 21:47:48 Hail: INFO: Coerced sorted dataset; 2017-08-28 21:47:48 Hail: WARN: called redundant `filtermulti' on an already split or multiallelic-filtered VDS; 2017-08-28 21:47:48 Hail: INFO: rrm: Computing Realized Relationship Matrix...; 2017-08-28 21:47:49 Hail: INFO: rrm: RRM computed using 3 variants.; 2017-08-28 21:47:49 Hail: WARN: 1 of 8 samples have a missing phenotype or covariate.; 2017-08-28 21:47:49 Hail: INFO: lmmreg: running lmmreg on 7 samples with 3 sample covariates including intercept...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Computing eigendecomposition of kinship matrix...; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Estimating delta using REML... ; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 1 to 7: 3.09757, 2.66667, 2.23576, 0.00000, 0.00000, -0.00000, -0.00000; 2017-08-28 21:47:49 Hail: INFO: lmmreg: Evals 7 to 1: -0.00000, -0.00000, 0.00000, 0.00000, 2.23576, 2.66667, 3.09757; 2017-08-28 21:47:50 Hail: INFO: lmmreg: global model fit: beta = Map(intercept -> 0.48721539559123606, sa.cov.Cov1 -> 0.5924758486080468, sa.cov.Cov2 -> -0.23132255249379874); 2017-08-28 21:47:50 Ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835:5741,redund,redundant,5741,https://hail.is,https://github.com/hail-is/hail/pull/2132#issuecomment-325495835,1,['redund'],['redundant']
Safety,"ter to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2889,avoid,avoid,2889,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['avoid'],['avoid']
Safety,"terface is on me as we tried to not break 0.1. I'd appreciate discussing in person what makes the most sense for 0.1. Here are the pieces I think we should separate for devel, though we could consider providing a meta-interface as well that combines some of them for usability. I'm writing (U, S, L) for a local matrix of eigenvectors U, an array of eigenvalues S, and an array of labels L on the rows of U (as with labels for SymmetricMatrix). 1) VDS to a (labeled) symmetric matrix (we have these: GRM, RRM, LD matrix, and Dan is working on a way to read and write them). 2) Symmetric matrix to (U, S, L), which we'll want to write and read. This modularizes the single-core eigen-decomposition bottleneck. 3) Variant-labeled (V, S, L) and VDS with those variants to transport (V, S, L) to sample-labeled (U, S, L). Currently this also requires knowing the number of samples used to make the LDMatrix since that number is used in its normalization. I agree it feels unnatural to need to remember this; to avoid it we'd need an unnormalized version. 4) Sample-labeled (U, S, L) and VDS to global fit of LMM including delta. This is currently a local computation that's been pretty fast in practice but as sample sizes increase we will want to distribute evaluating many values of delta in parallel. Note this step only uses the sample annotations on the VDS, so logically it could also be on KeyTable (which would be the sample KeyTable of the VDS). 5) Sample-labeled (U, S, L), VDS, and delta to per-variant-fit of LMM. This VDS can now contain exactly the variants one wants to fit. (5) should eventually be decomposed as well. The first command should project from Matrix to Matrix (projecting both numeric cells and a list of numeric sample annotations) and some additional small data. Then (6) will do per variant tests starting from after this projection (that is, after what is the BIG computation when you have tens of millions of variants). That way users can quickly iterate on exploration",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210:1039,avoid,avoid,1039,https://hail.is,https://github.com/hail-is/hail/pull/1984#issuecomment-319971210,2,['avoid'],['avoid']
Safety,"ternal signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.u",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5324,abort,abortStage,5324,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['abortStage']
Safety,"text=0x7fed6c000960 flags=0; I0824 16:42:27.052752 8752 sched.cpp:164] Version: 0.25.0; 2016-08-24 16:42:27,053:8532(0x7fef76bfd700):ZOO_INFO@check_events@1703: initiated connection to server [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cann",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2500,detect,detected,2500,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['detect'],['detected']
Safety,"thank you very much for the reply,; when I upgrade the decorator from 3.4.0 to 4.1.2 , this error disappears. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (bui",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:510,detect,detected,510,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,1,['detect'],['detected']
Safety,"the sidecars are going away in another pull request, so I'd prefer to avoid the merge conflict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6797#issuecomment-517734418:70,avoid,avoid,70,https://hail.is,https://github.com/hail-is/hail/pull/6797#issuecomment-517734418,1,['avoid'],['avoid']
Safety,"tialize hail like this (crashes with the stack trace/error in the issue):; ```; from pyspark import *; from hail import *; conf = SparkConf(); conf.set('spark.sql.files.maxPartitionBytes','60000000000') ; conf.set('spark.sql.files.openCostInBytes','60000000000') ; conf.set('spark.driver.cores','1') #test with 1 core; sc = SparkContext(conf=conf); hc = HailContext(sc); ```. With startup messages looking like this:; ```; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/01/08 15:16:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/01/08 15:16:23 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).; 18/01/08 15:16:23 WARN SparkConf: ; SPARK_CLASSPATH was detected (set to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar').; This is deprecated in Spark 1.0+. Please instead use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/home/ludvig/Programs/hail/jars/hail-all-spark.jar' as a work-around.; 18/01/08 15:16:23 WARN Utils: Your hostname, <my computer name> resolves to a loopback address: <my local IP>; using <my IP> instead (on interface enp3s0); 18/01/08 15:16:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; 18/01/08 15:16:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.; 18/01/08 15:16:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting por",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:8105,detect,detected,8105,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['detect'],['detected']
Safety,"timed); 97 # print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1)); 98 try:; ---> 99 result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); 100 (result, timings) = (result_tuple._1(), result_tuple._2()); 101 value = ir.typ._from_encoding(result). /opt/conda/lib/python3.7/site-packages/py4j/java_gateway.py in __call__(self, *args); 1321 answer = self.gateway_client.send_command(command); 1322 return_value = get_return_value(; -> 1323 answer, self.gateway_client, self.target_id, self.name); 1324 ; 1325 for temp_arg in temp_args:. /opt/conda/lib/python3.7/site-packages/hail/backend/py4j_backend.py in deco(*args, **kwargs); 29 tpl = Env.jutils().handleForPython(e.java_exception); 30 deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); ---> 31 raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; 32 except pyspark.sql.utils.CapturedException as e:; 33 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:3723,abort,aborted,3723,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['abort'],['aborted']
Safety,tor.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6087,abort,abortStage,6087,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['abort'],['abortStage']
Safety,"tors and matrices with primary dimension the number of samples (as in QR), and because BLAS3 matrix multiplication is fast. I also checked that upping to 8 covariates didn't balance things out. It didn't. The fancy approach basically trades X.t * X and generic k-dim solve for a QR on X and triangular k-dim solve...better for larger k and smaller n. ```; Standard. 2 cov; Lin 7s; Score 54.5s; LRT 93s; Wald 90s. 2 cov, QR / TriSolve; Lin 7.42s; Score 53.6s, 53.1s; LRT 2m06s, 1m59s; Wald 1m53s, 1m54s. 8 cov; Lin 7.16s; Score 59.1s; LRT 2m25s, 2m20s, 2m26s; Wald 2m27s, 2m27s, 2m25s. 8 cov, QR / TriSolve; Lin 7.76s; Score 52.7s; LRT 3m30s; Wald 3m26s; ```. For Firth, since I'm using QR anyway, may as will use TriSolve (though the timing is not particularly effected even with 8 covariates):. ```; 2 cov:; Firth 5m 10s, 4m55s, 5m7s. 8 cov:; Firth 10m37s, 10m50s, 10m28s; ```. For reference, here's the core logic of the QR approach. This corresponds to another version of LogisticRegressionFit where I tried to reduce unnecessary computation, see below. ```; while (!converged && !exploded && iter <= maxIter) {; try {; val mu = sigmoid(X * b); val sqrtW = sqrt(mu :* (1d - mu)); val QR = qr.reduced(X(::, *) :* sqrtW). deltaB = TriSolve(QR.r, QR.q.t * ((y - mu) :/ sqrtW)). if (max(abs(deltaB)) < tol) {; converged = true; if (computeScoreR) {; optScore = Some(X.t * (y - mu)); optR = Some(QR.r); }; if (computeSe) {; val invR = inv(QR.r) // could speed up inverting as upper triangular, or avoid altogether as 1 / se(-1) = fit.fisherSqrt(-1, -1); optSe = Some(norm(invR(*, ::))); }; if (computeLogLkld); optLogLkhd = Some(sum(breeze.numerics.log((y :* mu) + ((1d - y) :* (1d - mu))))); } else {; iter += 1; b += deltaB; }; }; ```. ```; case class LogisticRegressionFit(; b: DenseVector[Double],; optScore: Option[DenseVector[Double]],; optR: Option[DenseMatrix[Double]],; optSe: Option[DenseVector[Double]],; optLogLkhd: Option[Double],; nIter: Int,; converged: Boolean,; exploded: Boolean); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833:1720,avoid,avoid,1720,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279532833,1,['avoid'],['avoid']
Safety,tractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4213,abort,abortStage,4213,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['abort'],['abortStage']
Safety,"ts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4794,timeout,timeout,4794,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,ugh. the printed form of UnsafeRow is a public API? surely no one actually depends on this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143:25,Unsafe,UnsafeRow,25,https://hail.is,https://github.com/hail-is/hail/pull/8191#issuecomment-592571143,1,['Unsafe'],['UnsafeRow']
Safety,"ugh.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more informat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2500,timeout,timeout,2500,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['timeout'],['timeout']
Safety,ughhhhhhhhh `index_bgen` is very not thread safe.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6863#issuecomment-524927093:44,safe,safe,44,https://hail.is,https://github.com/hail-is/hail/pull/6863#issuecomment-524927093,1,['safe'],['safe']
Safety,un$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:4311,abort,abortStage,4311,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['abort'],['abortStage']
Safety,un$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:231); at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:78); at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:117); at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146); at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:108); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:278); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:241); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:227); at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:220); at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75); at org.apache.spark.network.util.TransportFrameDecode,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:220513,timeout,timeout,220513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['timeout'],['timeout']
Safety,"ver, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:8111/; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. # vim: syntax=apache ts=4 sw=4 sts=4 sr noet; </IfModule>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2589,timeout,timeout,2589,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,2,['timeout'],['timeout']
Safety,"vicorn + httptools, which were both written by Yury Selivanov, the asyncio person I mention above. Starlette and Uvicorn are currently the fastest options, (Sanic isn't tested), by a relatively large margin, on Techempower's benchmarks. If there is a reference standard benchmark of http library performance, Techempower is it: https://www.techempower.com/benchmarks/#section=data-r17 . Starlette is something like base Go performance (though 1/5-1/10th the performance of Go's fasthttp library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3163,timeout,timeouts,3163,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeouts']
Safety,"wer than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3795,timeout,timeouts,3795,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeouts']
Safety,"what the hell, bytecode verify errors from using locals? i saw another code-function creating locals so i assumed it was safe to do so, but apparently it isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768:121,safe,safe,121,https://hail.is,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768,1,['safe'],['safe']
Safety,xt.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:3472,abort,abortStage,3472,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,3,['abort'],['abortStage']
Safety,"y curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, tim",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:4587,timeout,timeout,4587,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['timeout'],['timeout']
Safety,y$5.apply(ContextRDD.scala:135); at is.hail.sparkextras.ContextRDD$$anonfun$run$1$$anonfun$apply$5.apply(ContextRDD.scala:135); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5290,abort,abortStage,5290,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['abort'],['abortStage']
Safety,"yeah, we don't have a way to avoid constructing the full array right now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9474#issuecomment-694929657:29,avoid,avoid,29,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694929657,1,['avoid'],['avoid']
Safety,"ylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:42156,timeout,timeout,42156,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['timeout'],['timeout']
Safety,"~~Performance note: Was able to do 100k variants by 500k samples with 1000 partitions on 800 cores in 5m4s. We've never successfully done such a multiply on gCloud, but even 50k by 100k examples with the old method took a little more than 20 minutes, so safe to say this is an improvement.~~. Unfortunately, there was a bug that resulted in not all the work getting done and as such the method is not actually as performant as initial tests suggested.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520:254,safe,safe,254,https://hail.is,https://github.com/hail-is/hail/pull/1884#issuecomment-304080520,1,['safe'],['safe']
Security," (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write up instructions for adding a new domain to get certs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1701,password,password,1701,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['password'],['password']
Security," 0.2.128-ce3ca9c77507; Error summary: SocketTimeoutException: connect timed out; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/py4j_backend.py"", line 223, in _rpc; raise fatal_error_from_java_error_triplet(; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/backend/backend.py"", line 190, in execute; raise e.maybe_user_error(ir) from None; File ""/home/edmund/.local/src/hail/hail/python/hail/table.py"", line 2002, in write; Env.backend().execute(; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/test.py"", line 6, in <module>; ht.write('gs://ehigham-hail-tmp/test_hail_in_notebook.ht'); hail.utils.java.FatalError: SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:11663,access,access,11663,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['access'],['access']
Security," account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have suffici",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:3043,access,access,3043,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security, at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at sca,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200392,Hash,HashTable,200392,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashTable']
Security," push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't help).; 4. Pushing to something like `cache-11907` would allow force pushes to still access the last build's images. What do you think of the #3 proposal? . ---. [1]: I had two files:; ```; # cat sleep/Dockerfile; FROM ubuntu:18.04; RUN sleep 10; # cat touch/Dockerfile; FROM ubuntu:18.04; RUN touch foo; ```; To build I use this command (slightly different syntax from the buildctl syntax, but, AFAIK, uses the same backend):; ```; docker buildx \ ; build \; DIRECTORY_NAME_HERE \; --output 'type=image,""name=gcr.io/hail-vdc/dktest,gcr.io/hail-vdc/dktest:cache"",push=true' \; --cache-to type=inline \; --cache-from type=registry,ref=gcr.io/hail-vdc/dktest; ```; Before every build I clear the _local_ cache with:; ```; docker system prune -a; ```; I can clear the remote cache with:; ```; gcloud container images list-tags gcr.io/hail-vdc/dktest --format=""get(digest)"" \; | awk '{print ""gcr.io/hail-vdc/dktest@"" $1}' \; | xargs gcloud container images delete --force-delete-tags; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:2657,access,access,2657,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,2,['access'],['access']
Security," recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10631,Checksum,ChecksumFileSystem,10631,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security," str, sampleType: str, individualGuid: str, familyGuid: str, affected_id: int32}>>, struct{z_score: float32}, struct{region_type_ids: array<int32>}, locus<GRCh37>, str, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, str, int64, struct{PHRED: float32}, struct{alleleId: int32, conflictingPathogenicities: array<struct{pathogenicity_id: int32, count: int32}>, goldStars: int32, pathogenicity_id: int32, assertion_ids: array<int32>}, struct{REVEL_score: float32, VEST4_score: float32, MutPred_score: float32, SIFT_pred_id: int32, Polyphen2_HVAR_pred_id: int32, MutationTaster_pred_id: int32, fathmm_MKL_coding_pred_id: int32}, struct{Eigen_phred: float32}, struct{AF_POPMAX: float32, AF: float32, AC_Adj: int32, AC_Het: int32, AC_Hom: int32, AC_Hemi: int32, AN_Adj: int32}, struct{AF: float32, AN: int32, AC: int32, Hom: int32, AF_POPMAX_OR_GLOBAL: float32, FAF_AF: float32, Hemi: int32}, struct{AF: float32, AN: int32, AC: int32, Hom: int32, AF_POPMAX_OR_GLOBAL: float32, FAF_AF: float32, Hemi: int32}, struct{MPC: float32}, struct{score: float32}, struct{delta_score: float32, splice_consequence_id: int32}, struct{AC: int32, AF: float32, AN: int32, Hom: int32, Het: int32}, struct{accession: str, class_id: int32}, struct{AC: int32, AN: int32, AF: float32, hom: int32}, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, bool, array<struct{amino_acids: str, canonical: int32, codons: str, gene_id: str, hgvsc: str, hgvsp: str, transcript_id: str, biotype_id: int32, consequence_term_ids: array<int32>, is_lof_nagnag: bool, lof_filter_ids: array<int32>, transcript_rank: int32}>, bool, str""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465:2504,access,accession,2504,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1830257465,1,['access'],['accession']
Security,"""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000263119.5:c.2558G>A"",""hgvsp"":""ENSP00000263119.5:p.(Arg853Gln)"",""isCanonical"":null,""polyPhenScore"":0.625,""polyPhenPrediction"":""possibly damaging"",""proteinId"":""ENSP00000263119.5"",""proteinPos"":""853"",""siftScore"":0.01,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000405822.2"",""bioType"":""protein_coding"",""aminoAcids"":""R/Q"",""cDnaPos"":""2502"",""codons"":""cGg/cAg"",""cdsPos"":""2408"",""exons"":""17/36"",""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000405822.2:c.2408G>A"",""hgvsp"":""ENSP00000384694.2:p.(Arg803Gln)"",""isCanonical"":null,""polyPhenScore"":0.94,""polyPhenPrediction"":""probably damaging"",""proteinId"":""ENSP00000384694.2"",""proteinPos"":""803"",""siftScore"":0.02,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000398319.2"",""bioType"":""protein_coding"",""aminoAcids"":""R/Q"",""cDnaPos"":""2943"",""codons"":""cGg/cAg"",""cdsPos"":""2558"",""exons"":""18/37"",""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000398319.2:c.2558G>A"",""hgvsp"":""ENSP00000381364.2:p.(Arg853Gln)"",""isCanonical"":true,""polyPhenScore"":0.625,""polyPhenPrediction"":""possibly damaging"",""proteinId"":""ENSP00000381364.2"",""proteinPos"":""853"",""siftScore"":0.01,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000484593.1"",""bioType"":""retained_intron"",""aminoAcids"":null,""cDnaPos"":null,""codons"":null,""cdsPos"":null,""exons"":null,""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""downstream_gene_variant""],""hgvsc"":null,""hgvsp"":null,""isCanonical"":null,""polyPhenScore"":null,""polyPhenPrediction"":null,""proteinId"":null,""proteinPos"":null,""siftScore"":null,""siftPrediction"":null}]},""genes"":null}]}}; ```. I've added the experimental warning until there is some form of automated testing in place (perhaps validating that consistency is maintained against a fixed output file that's been otherwise reviewed by Nirvana). Note that this method is modeled on VEP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:6447,validat,validating,6447,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['validat'],['validating']
Security,"# Notes On Container Security. Every pod has a [`PodSecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod y",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:21,Secur,Security,21,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573,5,"['Secur', 'access', 'secur']","['Security', 'SecurityContext', 'access', 'securitycontext-']"
Security,"$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205688,Hash,HashSet,205688,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""http",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2842,HASH,HASH,2842,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['HASH'],['HASH']
Security,'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB);  53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB);  97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:28824,validat,validate,28824,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['validat'],['validate']
Security,(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200312,Hash,HashMap,200312,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,"(i) -> O(i, i)` makes a square matrix whose diagonal is T. * `T(i) -> O(i, j)` and `T(i) -> O(j, i)` broadcast T over a matrix in the two possible directions. Now let T be a 2-tensor. * `T(i, j) -> O(i)` is the vector of row-sums of T. * `T(i, i) -> O(i)` is the diagonal of T. * `T(i, i) -> O()` is the trace of T. * `T(i, j) -> O(j, i)` is transposition. How do we represent matrix multiplication? Let T1 and T2 be 2-tensors. Then letting `T = Out(T1, T2, ""and"").map((x, y) => x * y)`, the matrix product is given by. * `T(i, j, j, k) -> O(i, k)`. In general, an index operation on T requires specifying an output tensor O (including its shape, though you can deduce that in non-broadcasting cases), a set of index variables (eg. ""i, j, k""), and an assignment of a variable to each dimension of T and O. . More abstractly, let DT and DO be the sets of dimensions of T and O. An index operation consists of a set I and two functions DT -> I <- DO, a ""cospan"". These operations compose by cospan composition, which involves a pushout (a disjoint union and a quotient). Let i: DT -> I and o: DO -> I be the two maps assigning index variables. It helps to consider some special cases (compare to the examples above):. * If i is surjective, and o is identity, this is extracting a diagonal from T. * If i is injective, and o is identity, this is a broadcast. * If i is identity, and o is surjective, this embeds T as a diagonal of a higher-dimensional output tensor. * If i is identity, and o is injective, this is a pure aggregation, summing out some dimensions of T. To make composition easy to compute, we could represent an index operation using a union-find structure for I. In other words, an index operation consists of a union-find structure I, and two arrays of points of I, encoding the two functions above. Then the composition of (T, I1, M) and (M, I2, O) is (T, I', O), where I' is computed by taking the union I1+I2, then for each dimension of M, unioning the assigned points of I1 and I2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772:3583,inject,injective,3583,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-457598772,4,['inject'],['injective']
Security,"(that's not to say we shouldn't do dependency audits every few months, bumping the pinned version to latest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189:46,audit,audits,46,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189,1,['audit'],['audits']
Security,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:421,access,accessed,421,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674,2,['access'],"['accessed', 'accessing']"
Security,"--------------------------------------------------------------------------------------------------------------------+; | batch_id | job_group_id | resources |; +----------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | 0 | {""service-fee/1"": 39973000, ""ip-fee/preemptible/1024/1"": 2558272, ""ip-fee/nonpreemptible/1024/1"": 3449280, ""disk/pd-ssd/us-central1/1707831761013"": 51165440, ""gcp-support-logs-specs-and-firewall-fees/1"": 39973000, ""memory/n1-preemptible/us-central1/1707831761013"": 153496320, ""compute/n1-preemptible/us-central1/1707831761013"": 39973000, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 206956800, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 53895000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 959352000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1293480000} |; +----------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:5469,firewall,firewall-fees,5469,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['firewall'],['firewall-fees']
Security,"-------------------------------------------------------------------------------------------------------------------------+; | batch_id | ancestor_id | resources |; +----------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | 0 | {""service-fee/1"": 14452500, ""ip-fee/preemptible/1024/1"": 924960, ""ip-fee/nonpreemptible/1024/1"": 4239936, ""disk/pd-ssd/us-central1/1707831761013"": 18499200, ""gcp-support-logs-specs-and-firewall-fees/1"": 14452500, ""memory/n1-preemptible/us-central1/1707831761013"": 55497600, ""compute/n1-preemptible/us-central1/1707831761013"": 14452500, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 254396160, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 66249000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 346860000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1589976000} |; +----------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:2568,firewall,firewall-fees,2568,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['firewall'],['firewall-fees']
Security,"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | batch_id | resources |; +----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | {""service-fee/1"": 427000, ""ip-fee/preemptible/1024/1"": 27328, ""ip-fee/nonpreemptible/1024/1"": 4239936, ""disk/pd-ssd/us-central1/1707831761013"": 546560, ""gcp-support-logs-specs-and-firewall-fees/1"": 427000, ""memory/n1-preemptible/us-central1/1707831761013"": 1639680, ""compute/n1-preemptible/us-central1/1707831761013"": 427000, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 254396160, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 66249000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 10248000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1589976000} |; +----------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:8297,firewall,firewall-fees,8297,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['firewall'],['firewall-fees']
Security,".0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10605,Checksum,ChecksumFileSystem,10605,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,.java:196); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:470); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:251); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.Exec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:7278,access,access,7278,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['access'],['access']
Security,.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:5498,access,access,5498,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,1,['access'],['access']
Security,".onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206849,Hash,HashSet,206849,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$Scattering,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8996,secur,security,8996,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:5873,access,access,5873,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['access'],['access']
Security,.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spar,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200471,Hash,HashMap,200471,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,"/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.executor.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.serializer: org.apache.spark.serializer.KryoSerializer; spark.kryo.registrator: is.hail.kryo.HailKryoRegistrator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:3809,secur,security,3809,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,4,['secur'],['security']
Security,/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9218,Secur,SecurityManager,9218,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:16371,Secur,SecurityManager,16371,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1389,access,access,1389,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['access'],['access']
Security,"0000004: not in 7f15e9ffb1f8/00010000'; tpl = JavaObject id=o553; deepest = 'RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; full = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord_ltNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92860ord_lt(Unknown Source); E 	at __C92844etypeDecode.__m92857arraySorter_merge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3514,access,access,3514,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['access'],['access']
Security,"1. localize_entries is a no-op at runtime, it just exposes the table representation to python; 2. Good point. We agreed on ordering by column key, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13405#issuecomment-1673655493:51,expose,exposes,51,https://hail.is,https://github.com/hail-is/hail/pull/13405#issuecomment-1673655493,1,['expose'],['exposes']
Security,"21); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_contr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2749,HASH,HASH,2749,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['HASH'],['HASH']
Security,"26574/00000004: not in 7f15e9ffb1f8/00010000'; full = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.hail.annotations.Memory.loadInt(Memory.java:140); E 	at is.hail.annotations.Region$.loadInt(Region.scala:20); E 	at __C92844etypeDecode.__m92863ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92862ord_compareNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92861ord_ltNonnull(Unknown Source); E 	at __C92844etypeDecode.__m92860ord_lt(Unknown Source); E 	at __C92844etypeDecode.__m92857arraySorter_merge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source); E 	at __C92844etypeDecode.__m92864arraySorter_splitMerge(Unknown Source",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:3649,access,access,3649,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['access'],['access']
Security,30); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201438,Hash,HashSet,201438,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"30); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205085,Hash,HashSet,205085,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"4, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.ex",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10685,Checksum,ChecksumFileSystem,10685,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,"5). org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10502,Checksum,ChecksumFileSystem,10502,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,2,['Checksum'],"['ChecksumFSOutputSummer', 'ChecksumFileSystem']"
Security,"74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; Proxy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1518,access,access,1518,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['access'],['access']
Security,7bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:2086,access,access,2086,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['access'],['access']
Security,; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199937,Hash,HashSet,199937,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"> > > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > > ; > > ; > > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py; > ; > There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook. They are different enough at this point that tying one to the other doesn't make much sense to me. But the bigger reason is that if needed, it will be easier to restore notebook from a pristine file, then look back through git commit history to the first breaking change. The goal should always be to introduce as few breaking changes as possible before a public demonstration. Once that is done, I don't mind doing something else. I've had the unfortunate pleasure of doing this with large-ish user-facing deployments (thousands of users), and it's not a fun experience. Git works well, but under the pressure of public-facing issues, entropy is not a friend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939:569,authenticat,authentication,569,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939,1,['authenticat'],['authentication']
Security,"> > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > ; > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py. There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459893109:549,authenticat,authentication,549,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459893109,1,['authenticat'],['authentication']
Security,"> Almost by definition I'd suspect that adding a system administrator is a high security impact (that's not a judgement on you, just a statement about the security boundary getting wider).; > ; > This is obviously fine in this case because we want you to be a system administrator, but we should let appsec know regardless. They'll also probably want to send you some standard trainings (and maybe background check forms?). I'll ping them. Ah right that's an extremely fair point. Should I put ""high"" back in this PR description, or just sit tight until I hear from appsec?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400720230:80,secur,security,80,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400720230,2,['secur'],['security']
Security,"> Am I strange in that I want to name something what it is (ci, batch, etc.); > rather than give everything codenames? The purpose of codenames is to hide and; > obscure, you know. Good point. > I think this should be called tutorial. And when it becomes a notebook; > service, notebook. And when it becomes the Hail service, it should just be the; > main website. I almost called it notebook-service but all our other names were single; words. I'll call it notebook so we can avoid a rename. > The landing page should be password protected. We should think about whether; > we want to collect additional information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:522,password,password,522,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,4,['password'],['password']
Security,"> As far as I can tell, there's never a reason to use dgesvd. It appears dgesvd uses less memory and is more accurate, particularly on very small singular values, but is potentially several times slower. I agree dgesdd is the right default, but it's possible (though I'm guessing unlikely) we'll eventually have reason to expose dgesvd as an option. Here's a thread of Julia devs discussing which to use: https://groups.google.com/u/1/g/julia-dev/c/mmgO65i6-fA?pli=1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9303#issuecomment-677708620:322,expose,expose,322,https://hail.is,https://github.com/hail-is/hail/pull/9303#issuecomment-677708620,1,['expose'],['expose']
Security,> Closing because; > ; > 1. The change is purely cosmetic; > 2. Breaking previously-run migration checksums feels like a bad idea. Do python files contribute to migration checksums?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14669#issuecomment-2318144085:98,checksum,checksums,98,https://hail.is,https://github.com/hail-is/hail/pull/14669#issuecomment-2318144085,2,['checksum'],['checksums']
Security,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:36,password,password,36,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,2,['password'],['password']
Security,"> Forks would indeed need to overwrite ours, but since the file wouldn't change much it seems like that's not much of a hassle to maintain, right Leo? And ya this seems like a fine change but we would need to follow up right afterward with our own credentials. Yes, alternatively we could also use the GitHub organization name or something similar when constructing the file path to encrypted credentials, to avoid collisions completely. (Forks like the CPG one would only add files to their deployments.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130:383,encrypt,encrypted,383,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130,1,['encrypt'],['encrypted']
Security,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:197,hash,hash,197,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314,1,['hash'],['hash']
Security,"> Having the Auth service ping the Batch API with it to verify the token is valid. I believe the way our CRSF is implemented, we don't actually ever ""validate"" the tokens, we only check that the token in the formdata matches the token in the cookie. > Or perhaps we could just make this UI a single page application instead of a bunch of pages on different subdomains that resemble one. . This would be wonderful! Sort of similar-but-better to my thought of hosting the ""top menu bar"" as a separate iframe that always comes from auth. For the same reason (in particular, the apparently lack of regular usage of the logout button), that kind of change is probably larger than the scope of getting this bug fixed... but would cool to look into some day!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369:150,validat,validate,150,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369,1,['validat'],['validate']
Security,"> High level question: is the state stored in MySQL the same as the k8s notebook pod state? If so, should we just query k8s rather than duplicating the notebook state in MySQL?. Not entirely the same I think, but I also prefer tools that are well-designed for the purpose, and have limited k8 / etcd experience / survivor bias. One interesting fact (spoke with Dan), is that we don't have direct access to etcd, so are limited to k8 client operations. * We had talked about, in batch context, of having our own master state, against which k8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there eith",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:396,access,access,396,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['access'],['access']
Security,"> I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:; > ; > > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: [user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com](mailto:user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com) does not have storage.buckets.get access to untitled-folder.; > ; > That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?. Right, I mentioned this above. It appears to be trying to create, or trying to read, untitled_folder as a bucket. If you trick it into using /your_bucket_name as the cwd, folder creation works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353:577,access,access,577,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480381353,1,['access'],['access']
Security,"> I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is. I agree. Looks like there is an issue, hello isn't showing up. I'll let you investigate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7466#issuecomment-551133834:57,access,access,57,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551133834,2,['access'],['access']
Security,"> I think port 443 is so we don't need root privileges in Envoy?. This is related to the way headless services expose the pod itself, but as I'm writing this I feel like I want more clarity on exactly why, so I will do a bit of digging and come back with a better response.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1283003577:111,expose,expose,111,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1283003577,1,['expose'],['expose']
Security,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1331,expose,exposed,1331,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940,4,"['expose', 'validat']","['exposed', 'validation']"
Security,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:43,audit,audit,43,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446,1,['audit'],['audit']
Security,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:1208,password,password,1208,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,1,['password'],['password']
Security,"> In particular, the matrixtable available in doc examples as `ds` lives at `hail/hail/python/hail/docs/data/example.mt`. Thanks, this is exactly what I was looking for. I am having trouble testing my new example locally though. When I run `make -C hail doctest-query`, the tests fail with a checksum error. I tried running `make -C hail clean` and retrying, but I still get the same error. ```; E hail.utils.java.FatalError: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E; E Java stack trace:; E org.apache.hadoop.fs.ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E 	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347); E 	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303); E 	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252); E 	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at is.hail.io.fs.HadoopFS$$anon$2.read(HadoopFS.scala:58); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at org.apache.commons.compress.utils.CountingInputStream.read(CountingInputStream.java:56); E 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252); E 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:185); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:168); E 	at is.hail.io.fs.GZipCompressionCodec$.makeInputStream(FS.scala:125); E 	at is.hail.io.fs.FS.open(FS.scala:563); E 	at is.hail.io.fs.FS.open$(FS.scala:560); E 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:292,checksum,checksum,292,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,5,"['Checksum', 'checksum']","['Checksum', 'ChecksumException', 'checksum']"
Security,"> Maybe lazy fields should not subclass Value, and to access a lazy field requires an explicit load(cb: CodeBuilder): Value[T]. I think this is probably the right model.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10907#issuecomment-961973892:54,access,access,54,https://hail.is,https://github.com/hail-is/hail/pull/10907#issuecomment-961973892,1,['access'],['access']
Security,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:946,access,access,946,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467,1,['access'],['access']
Security,"> Read through and looks good. Just checking: this is just intended to be an internal registered function, not something for users to call, right?. Users may sometimes use this -- it was exposed through `hl.experimental` previously as part of the vcf combiner utilities, I just moved it to `hl.vds` (though it still exists at the old location for compatibility)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10703#issuecomment-887575834:187,expose,exposed,187,https://hail.is,https://github.com/hail-is/hail/pull/10703#issuecomment-887575834,1,['expose'],['exposed']
Security,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:261,expose,expose,261,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,2,"['authoriz', 'expose']","['authorization', 'expose']"
Security,"> Second, StructExpression.init creates expressions for each field in the constructor. That seems excessive, we should construct the field expressions on demand when they are accessed. We lose dot-completion, then. I think the current design is correct (constructing 3k things shouldn't be a problem, anyhow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4153#issuecomment-413194946:175,access,accessed,175,https://hail.is,https://github.com/hail-is/hail/issues/4153#issuecomment-413194946,1,['access'],['accessed']
Security,"> Should I get rid of that option?. Yes. For security, I don't think we should ever make our production database public, even in a limited way. For testing, we have a few options: use a test one as you say (in a non-production project?), use Cloud SQL proxy, spin up one locally, or make in-cluster testing easier. You can grant specific privileges to a user to a database in MySQL. I'm guessing you granted all on *.* which will allow them to create tables (among everything else). Basically, we should have an admin user that can create databases, and then individual users for each role that have read or read/write access to specific databases. Yeah, let's talk about it more today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373:45,secur,security,45,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473946373,2,"['access', 'secur']","['access', 'security']"
Security,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:895,secur,secure,895,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,2,"['access', 'secur']","['access', 'secure']"
Security,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:224,expose,expose,224,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,4,"['expose', 'threat']","['expose', 'threat']"
Security,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:741,encrypt,encrypt,741,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569,1,['encrypt'],['encrypt']
Security,"> Thanks, added with one tweak. Sadly I don't know how to convince your code analyser that using `randint` to make test cases in test code is not a security issue. Me neither :shrug: . > Feel free to push to PR branches directly, or just to add things while merging. . I don't have write access to the `populationgenomics` fork, hence the PR :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2402897043:148,secur,security,148,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2402897043,2,"['access', 'secur']","['access', 'security']"
Security,"> The approach in this PR doubles down on the functional Code[T] structure. I don't see anything in the design that prevents us from moving away from `Code[T]`, but it does have to support it for now. > I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. This is also the interface I would like to see for CodeBuilder. And you're right, making that change would allow methods taking a `CodeBuilder` to not need a line number argument. I agree that's better. I may have gotten a bit of tunnel vision in the middle of the giant mechanical refactoring :) I will make this change. > I think part of my concern is that Im not entirely sold by the need to have a whole stack of IR printouts and associated line numbers  right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. I think most of this PR is necessary for debug information with the fully lowered IR line numbers. It doesn't do anything to propagate line numbers through IR lowerings, which is what would be needed to support line numbers at earlier compiler stages. > Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration. My main reason for that approach was to manage the number of changes required in this PR. We could follow up on this with making line number arguments explicit in manageable chunks. But personally this seems like the ideal use case for implicit arguments. And as `Code` goes away, the number of places with implicit line number arguments should go down significantly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975:246,inject,injecting,246,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742042975,1,['inject'],['injecting']
Security,"> The first usage of this feature is to move the FS off of the class itself and onto a container class. Is the container class generated globally? That's going to be a problem. The service backend has multiple FSes for multiple, different users. Keeping those FSes separate is **critical** for security. The reference genomes are the same story. Each service query will come with its own, distinct context of reference genomes, and they can't get mixed up. In particular, a reference genome sequence may be sensitive data and can't be exposed to other users. The reference genome global state will only be stored in the Python client. Removing the global reference state from the Scala side is a pending Query service project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652673222:294,secur,security,294,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652673222,2,"['expose', 'secur']","['exposed', 'security']"
Security,"> The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?. The problem I believe is that they would need project-wide read/list permissions. The blob (folder) is not being created in their bucket, but as a new bucket in the project. edit: You can clearly see the difference if you click on the checkpoint folder, back up to the folder /bucket_name and try to create a folder. No additional permissions needed (it's being made in their bucket)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549:157,access,access-control,157,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480375549,2,['access'],['access-control']
Security,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:23,expose,expose,23,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109,1,['expose'],['expose']
Security,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:191,secur,secure,191,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805,10,"['certificate', 'secur']","['certificate', 'certificates', 'secure']"
Security,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:506,certificate,certificate,506,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945,1,['certificate'],['certificate']
Security,"> This doesn't track the length if known like the old code. That will make some things e.g. ToArray much more expensive. I think you should finish all the stream processing nodes to unblock lowering and then return to this. Right, I was waiting to get the hard parts right first. This should be easy to add back. > Finally, this seems a silent on the the question of region management. What's your picture here? The region management write had consumers passing regions to producers, but I don't see how that fits in here. I put the region management on hold when I realized there was no way to make a stream free any regions it owns when its consumer stops pulling from it. So the new stream design is just to expose those setup and finalization hooks, where creating and freeing regions can go. When I start moving the region management logic I had over to the new design, I'll have to see if regions have to be baked into streams, or if they can be orthogonal. My plan was to first try making `EmitStream` return `Stream[Region => EmitTriplet]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8129#issuecomment-589676137:711,expose,expose,711,https://hail.is,https://github.com/hail-is/hail/pull/8129#issuecomment-589676137,1,['expose'],['expose']
Security,"> What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation. It would claim deletion without deleting. Must have been resolved or caused by a separate issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7162#issuecomment-536318084:154,validat,validation,154,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536318084,1,['validat'],['validation']
Security,"> agree: api is in GH, ergo public, so only point of contention is:. It's public to people who read GitHub and hail docs. It isn't really public to someone who is probing around for endpoints to exploit. > Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. Sure. > The risk is that an attacker may learn /jobs exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch. I agree in general, except I think of the problem seemingly inversely. If providing 401/403 responses to the end user substantially improves their experience, then we should do it. If not we shouldn't, because the degree to which an attacker is ""substantially"" enabled, is in my mind anything other than 0. Battles can be lost by small degrees. The choices should be user driven. . I think you told me that your system benefits, so let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170:353,attack,attacker,353,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483797170,8,"['attack', 'secur']","['attacker', 'securing']"
Security,"> but not the thunk of thunking. You mean the dependent aggregators? The issue is the copy code needs access to the type that was matched in the TVariable. The TVariable binding is cleaned on every function match, which is long before the code is executed on a worker. Therefore, we need to run some code to capture the TVariable binding just after the match happens. That's what the extra level of thunking is about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2360#issuecomment-340047873:102,access,access,102,https://hail.is,https://github.com/hail-is/hail/pull/2360#issuecomment-340047873,1,['access'],['access']
Security,"> if MathJax changes the vertical layout of the page, we might have scrolled too far or not far enough to have the anchor located just below the header. I haven't seen the issue in practice, but it would be better to listen to mathjaxj onload. Anchor tags / clicks of course wouldn't be affected. > if we navigate to a new URL using any means other than clicking on an anchor tag, our history will have the URL with the special hash. I don't think there's anything special about the hash; without this solution clicking on an anchor tag should produce exactly the same behavior (a #id appended).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544692099:428,hash,hash,428,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544692099,2,['hash'],['hash']
Security,"> if there's a problem with the expression, I don't want to get a crash from a requirementError from the Variant constructor without any context. You need to do validation in the expr code. User could isn't allowed to fail with a requirement error. I'd solve the error message problem by carrying it along with the annotation. Line can be generalized to carry line information about any type. > My CNV work involves parallelizing file parsing, and this interface wouldn't be compatible with that use case. I don't understand, can you elaborate on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233005833:161,validat,validation,161,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233005833,1,['validat'],['validation']
Security,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:80,encrypt,encrypt,80,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868,2,['encrypt'],['encrypt']
Security,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:12,Authoriz,Authorization,12,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942,5,"['Authoriz', 'access', 'authoriz']","['Authorization', 'access', 'access-authn-authz', 'authorization']"
Security,"> security. for kubernetes/flask/those things? I don't see much security risk for the other stuff right now (not including malicious packages as security risks, those seem to be in their own category). Agree to punt on this for now though ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370:2,secur,security,2,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435205370,3,['secur'],['security']
Security,"> validation. Checking the arguments to the `Variant` constructor and generating nice error messages. For CNV work, you want to parallelize over files and not lines within files? You need to process the each file serially?. I'm not against having an additional interface like ParseContext that you can use both for the RDD interface and for the CNV stuff. Another option might be to make `TableReader[C[_]](...): (TStruct, C[Annotation])` but you'll have to do some work to define a `C` that knows how to load itself from a file, for example. It would be useful to be able to write code that can be used with either RDDs or local collections. > For the 'annotation line' are you suggesting a general error-catching wrapper?. Yep! I'll look over your proposed interface. Letting my mind wander a little here. One of the challenges with Spark error handling is propagating errors from the workers back to the master. RDDs are naturally used functionally, so functional error handling might be a better approach. The `Try` monad is the normal way to do functional error handling in Scala. Since we have concurrency, we have multiple errors and we want to preserve them all. In addition, it would be nice if the new generalized `Try` monad tracked warnings (like VCFReport). The main thing it isn't clear how to handle is writing RDDs. You basically want a write to write out the values and return an RDD, but just with the errors and warnings, which you could transfer to the driver at the end with collect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233041581:2,validat,validation,2,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233041581,2,['validat'],['validation']
Security,"> wait, you force-pushed current master as this branch. Yep, I put the wrong hash in a rebase --onto and when it was the same as master, Github closed it and marked it as merged. Surprising! Fixed!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5004#issuecomment-448650369:77,hash,hash,77,https://hail.is,https://github.com/hail-is/hail/pull/5004#issuecomment-448650369,1,['hash'],['hash']
Security,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1024,hash,hash,1024,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748,2,['hash'],['hash']
Security,">Explicitly specify the subnet in the VM creation. This was optional before but BITS changed some settings in GCP such that you have to specify subnets when creating a VM. Nah that was me (to comply with security requirements, but still me). My bad for not updating the script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13404#issuecomment-1673480429:204,secur,security,204,https://hail.is,https://github.com/hail-is/hail/pull/13404#issuecomment-1673480429,1,['secur'],['security']
Security,">We only have to support two platforms: recent Linux and OSX. OSX is not a problem, all recent versions are based on libc++ (rather than libstdc++) which has; had a more stable ABI. The problem is precisely that ""recent Linux"" encompasses both; systems based on g++-4.8.x/4.9.x with only old-ABI std::string's (e.g. debian8), and systems; based on g++-5.x and later with new-ABI std::string by default (e.g. debian9). That; incompatibility is the problem. >Not having access to the standard library seems problematic. You absolutely have access to the full C++11 standard library in dynamically-generated code -; you're compiling with the master node's default compiler, and header files, and using; the default libstdc++.o (libc++.dylib), and it's all fine. And you'll be using whichever flavor; of std::string is the default for that system, which will presumably also be interoperable with any; third-party library packages on that system. The issues we're getting round are:. a) If libhail.so is prebuilt on a new-ABI system *and* uses std::string, then it can't run against; the libstdc++ on an old-ABI system. b) If libhail.so is prebuilt on an old-ABI system, then it can run against either old-ABI or new-ABI; libstdc++, but if it then gets linked against third-party libraries compiled against new-ABI; headers, you'll have two different flavors of std::string floating around in the same program,; which causes confusion at any interfaceswhich pass std::string around. Now if you want to go further in shipping more of the system, then the question of whether to; ship your own libstdc++ (or libc++) is independent of the choice of compiler version. *If* you ; ship your own libstdc++, then you potentially introduce the problems of interoperability with; other libraries on the system). And there's a slight question about whether your libstdc++ will; work against the other systems libc.so, though I think the ABI at that level has been stable enough for long enough that it probably doesn",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941:468,access,access,468,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424787941,2,['access'],['access']
Security,@ce-carey Do you know which hail version hash you were running?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-388847560:41,hash,hash,41,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-388847560,1,['hash'],['hash']
Security,"@chrisvittal not sure what this error is. Doesnt happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions Im interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:272,access,accessing,272,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219,1,['access'],['accessing']
Security,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:434,certificate,certificate,434,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,2,['certificate'],['certificate']
Security,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:14,Access,AccessDeniedException,14,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023,2,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:259,EXPOSE,EXPOSE,259,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832,1,['EXPOSE'],['EXPOSE']
Security,"@cseed One small annoyance is that k8 has restrictions on characterset in labels tighter than ascii. Affects user ids: auth0 concatenates id and social service with a pipe, ex: google-oauth2|something. Also affects notebook naming (smaller concern): I want people to be able to name notebooks, with some human-readable default value, because strings of random numbers and non-alphanumeric characters feel intimidating to non-cs / data-science / similar people, and I don't want them to feel intimidated in anything so trivial, since those intimidated will project that the core product is inaccessible. I can obviously str.replace unwanted characters, and do the transformation on the other end, but this will be fragile and a bit awkward. If we want this kind of thing and want to stay with k8 label-based storage for such things. Not a concern for the demo. Wondering what is better, hash, base64, or move to sql. Regarding hash, auth0 user names are globally unique, so I imagine, but am not certain, that a so-called ""cryptographically secure"" algorithm would be exceedingly unlikely to result in name collisions. For demo, this isn't a problem; I will just str.replace(|, '--_--'), collisions won't happen unintentionally.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459208736:886,hash,hash,886,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459208736,3,"['hash', 'secur']","['hash', 'secure']"
Security,"@cseed This should now be sufficient for Wed. I will probably add 2 features before then: reconnecting web sockets (though connection should infrequently drop, shouldn't affect workshop), and an admin panel to blow out users / delete notebooks. And maybe SSL. Edit: my commit https://github.com/hail-is/hail/pull/5162/commits/bac155c9713d99c68cd7d0605eb59585656c14ea makes reference to csrf. This may not be necessary: inspecting the login request I notice a nonce, generated by the auth0-js lib, sent with login and silent token rotation / refresh, which is nice. Assuming it's used to prevent replay attacks, in this case it has the same purpose as a CSRF token. Yay auth0. Edit2: Regarding performance. Scorecard page, before server-side caching takes 100ms on refresh, and 50ms on navigation from another page. This effectively means no overhead from my web implementation. Why? It takes ~50ms to return *anything* (including favicon.ico of 0 bytes), i.e 50ms is the time it takes from my computer to kubernetes and back, with no additional work done. We have 2 such requests currently when visiting app.hail.is/scorecard: one to to the web app server, one to scorecard/json. After caching (which is invalidated every 3 minutes), takes 50ms. So, if current scorecard.hail.is needed to hit a json endpoint to get data for its template, we would expect it to be no faster. Alternatively if we placed the json-generating function in the web-app's nodejs server it's response time would drop by ~50ms.; * I also am trying to use the internal DNS (SSR phase routes to http://scorecard/json, so should use kubernetes DNS; still take ~50ms to get the json... before caching).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524:602,attack,attacks,602,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460071524,1,['attack'],['attacks']
Security,@cseed an issue remains: not sure if this is the cause:. `ls: cannot access 'PARK_HOME/python/lib/py4j-*-src.zip': No such file or directory` as a result of `make test`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5929#issuecomment-485140905:69,access,access,69,https://hail.is,https://github.com/hail-is/hail/pull/5929#issuecomment-485140905,1,['access'],['access']
Security,"@cseed that seems good enough for our purposes. We can run it, unprivileged, on a VM outside of k8s. We have a thin service in k8s that has privilege to talk to that VM. It streams build context and docker file to said VM. That VM invokes `img`. That VM is secure as long as `img` doesn't allow builds to escalate their privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474448122:257,secur,secure,257,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474448122,1,['secur'],['secure']
Security,"@cseed: One thing to note. Safari currently has an issue with silent refresh of auth token, due to the way it prevents cross-site tracking. This can be disabled (we could certainly mention this to participants), and other browsers don't appear to have this problem. The solution I recommend for the moment is to pay auth0 ~$15-20/month for a custom domain (we get some additional features as well: https://auth0.com/pricing, including built-in account linking across social providers, which would be nice to not have to write ourselves) ... which will mean that no cross-site cookies are needed. The other approaches involve putting more authentication functionality on our servers, which will be more work for us, and cost much more than $250/year in developer time. Auth0 is also working on a more community-friendly fix (an authentication standard that is exempt from this kind of cross-site cookie block). I've also posted a request for their opinion on the matter in our case specifically.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-460040357:638,authenticat,authentication,638,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-460040357,2,['authenticat'],['authentication']
Security,"@daniel-goldstein After this lands in main, can we do a lets encrypt rotation just to smoke test that flow while this is all front of find? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1255548750:61,encrypt,encrypt,61,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1255548750,1,['encrypt'],['encrypt']
Security,"@daniel-goldstein Can you take a look at the changes in the first query and see if the addition of the audit helps at all? I didn't change the second one as it was a lot of work to do the first and if it's not the right approach, then no need to waste time rewriting that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1613498289:103,audit,audit,103,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1613498289,1,['audit'],['audit']
Security,@daniel-goldstein I added an exit 1 after argument validation and removed the test-dataproc and wheel dependencies in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:51,validat,validation,51,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['validat'],['validation']
Security,@daniel-goldstein I found an error in async_cancel that was exposed by the new version of nest_asyncio. I'm 99% sure the change is correct. Feel free to ask Dan to double check it. The issue was that `fetch_coro` is a Task and not a Future. Cancelling a task just adds the cancellation event to the event loop. You have to actually wait for it to finish before the state of the task will be cancelled. Dan wrote a bunch of tests that asserted `cancel` results in `cancelled == True`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10705#issuecomment-888513583:60,expose,exposed,60,https://hail.is,https://github.com/hail-is/hail/pull/10705#issuecomment-888513583,1,['expose'],['exposed']
Security,"@daniel-goldstein sorry I was a dummy, what I had didn't actually do what I thought it did. It's a bit complex to get access to test information in a fixture, but there's some docs on how to do it. I did that. Now the fixture checks and only tears down a batch when the test failed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13326#issuecomment-1664664775:118,access,access,118,https://hail.is,https://github.com/hail-is/hail/pull/13326#issuecomment-1664664775,1,['access'],['access']
Security,"@danking : added security flags (httponly, secure, samesite). should be ready +/- if Cotton wants me to add a cookie field that stores the Kubernetes secret path, or whether we can have that stored under the user's Kubernetes namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5633#issuecomment-474511788:17,secur,security,17,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474511788,2,['secur'],"['secure', 'security']"
Security,@danking @daniel-goldstein Do we want this endpoint to be public? I think we should add the authorized users only decorator.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11559#issuecomment-1065308519:92,authoriz,authorized,92,https://hail.is,https://github.com/hail-is/hail/pull/11559#issuecomment-1065308519,1,['authoriz'],['authorized']
Security,@danking Can you let me know what I'm supposed to do here with these security errors? Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12794#issuecomment-1505755543:69,secur,security,69,https://hail.is,https://github.com/hail-is/hail/pull/12794#issuecomment-1505755543,1,['secur'],['security']
Security,"@danking Could you please look at the comments and changes made to this repo? I would also like to focus on making the smallest set of changes necessary. Some of the comments appear to be better suited for future PRs, for instance differences in architectural preferences (whether or not a user_id should be validated at this layer, whether the marshaling function could be written to more resemble something you find idiomatic) that don't affect the ability of the web client to consume a valid response. . This PR is getting quite hard to follow, so I'd like to get a summary of what remaining needs to be addressed for you to accept.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464232124:308,validat,validated,308,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464232124,1,['validat'],['validated']
Security,"@danking Curious for your thoughts on this refactor. I was getting pretty turned around myself with the various credential classes and I think this is closer to what we want in a keyless world. Ideally the batch worker should just be able to request credentials (in the form of an access token) for a given identity with just the identity's ID. LMK if you're in favor of this or not, or if you would like to see it folded into the metadata server PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756:281,access,access,281,https://hail.is,https://github.com/hail-is/hail/pull/14125#issuecomment-1879054756,1,['access'],['access']
Security,"@danking I can now access https://auth.hail.is/user successfully (for the `lgruensc` user), but https://ci.hail.is/batches/3654512 or similar still results in a ""redirected you too many times"" followed by a 401.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11954#issuecomment-1178404584:19,access,access,19,https://hail.is,https://github.com/hail-is/hail/pull/11954#issuecomment-1178404584,1,['access'],['access']
Security,"@danking I noticed the hail-ci-azure automated check failed.; I don't have access to see the root cause, but let me know if I need to change anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12951#issuecomment-1531772729:75,access,access,75,https://hail.is,https://github.com/hail-is/hail/pull/12951#issuecomment-1531772729,1,['access'],['access']
Security,@danking I'm getting this error. Do you see any problem with granting that capability to the test service account?. ```; + retry gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; + gcloud -q auth activate-service-account '--key-file=/gsa-key/key.json'; Activated service account credentials for: [test-665@hail-vdc.iam.gserviceaccount.com]; + mkdir -p /io/batch/27b395/inputs/wjDTI; + retry gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 2; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; + sleep 5; + gsutil -u hail-vdc -m cp -R gs://hail-services-requester-pays/hello /io/batch/27b395/inputs/wjDTI/hello; AccessDeniedException: 403 test-665@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; CommandException: 1 file/object could not be transferred.; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952:625,Access,AccessDeniedException,625,https://hail.is,https://github.com/hail-is/hail/pull/9096#issuecomment-662085952,6,"['Access', 'access']","['AccessDeniedException', 'access']"
Security,@danking Looks like there are still some failing CDN fetches and a kind of annoying false-positive complaint about URL sanitization.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1270381357:119,sanitiz,sanitization,119,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1270381357,1,['sanitiz'],['sanitization']
Security,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:507,authenticat,authenticated,507,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071,1,['authenticat'],['authenticated']
Security,"@danking So would be curious to get your thoughts. I was initially going to make this change such that instead of these activation tokens the batch worker authenticates with a cloud access token. I had a minor pause though because this means that other workers (even from other namespaces) could potentially impersonate each other, whereas they cannot in our current token system. Is that a concern to you? I suppose we are already pretty compromised if someone gets control of the batch worker's identity, considering the buckets that the batch worker has access to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13071#issuecomment-1551928272:155,authenticat,authenticates,155,https://hail.is,https://github.com/hail-is/hail/pull/13071#issuecomment-1551928272,6,"['access', 'authenticat']","['access', 'authenticates']"
Security,"@danking This doesn't forbid clashing field names, but accessing the field must use the `struct['items']` syntax. This appears to be what we meant to be doing, but we had a bug. Open to discussion on whether we should just prevent the clashing entirely (but what happens when importing data with a bad field name?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13498#issuecomment-1693437508:55,access,accessing,55,https://hail.is,https://github.com/hail-is/hail/pull/13498#issuecomment-1693437508,1,['access'],['accessing']
Security,"@danking Unfortunately I didn't get the hash, but I was running whatever version was the default 6 days ago. Started the cluster using `--version devel --spark 2.2.0`, no specific hash specified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-388871629:40,hash,hash,40,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-388871629,2,['hash'],['hash']
Security,@danking it seems to me that this is exactly the situation for which they exposed `unsafeValueAt`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194:74,expose,exposed,74,https://hail.is,https://github.com/hail-is/hail/pull/1566#issuecomment-287549194,1,['expose'],['exposed']
Security,"@danking, @cseed An alternative: [as mentioned in the ticket Dan linked] the acl boundary for pod creation is a namespace. If we scope all user resources to their namespace, and during user resource creation give notebook service account 'create-pod' permissions in the user's namespace, and also remove create pod permissions in the default namespace, we reduce the likelihood that a compromised notebook leader could expose user secrets and other data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234:419,expose,expose,419,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234,1,['expose'],['expose']
Security,"@gregsmi Looks like the scopes we currently have set are insufficient for reading the storage account keys. Do you know best practice permissions for creating SAS tokens?. ```; The client '96fe73da-25e0-4a69-9cd8-0043e56d0d0a' with object id '96fe73da-25e0-4a69-9cd8-0043e56d0d0a' does not have authorization to perform action 'Microsoft.Storage/storageAccounts/listKeys/action' over scope '/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/haildev/providers/Microsoft.Storage/storageAccounts/hailtest' or the scope is invalid. If access was recently granted, please refresh your credentials.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13140#issuecomment-1577254412:295,authoriz,authorization,295,https://hail.is,https://github.com/hail-is/hail/pull/13140#issuecomment-1577254412,2,"['access', 'authoriz']","['access', 'authorization']"
Security,"@iris-garden I think you make great points! And I agree, across many PRs we probably do want to be analyzing the security impacts at every stage, not just as a one-off ""when we're done it will be X"" analysis in the ticket... So I guess in my mind the _only_ real reason for using the issue-level review would be for tracking the impact of non-code changes (like configuration updates to production). I will try to make the templates reflect that distinction",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981:113,secur,security,113,https://hail.is,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981,1,['secur'],['security']
Security,"@jbloom22 sorry, I meant could you include the commit hash in the VDS's name in case we ever have to go back to the commit used to generate it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2173#issuecomment-327467484:54,hash,hash,54,https://hail.is,https://github.com/hail-is/hail/pull/2173#issuecomment-327467484,1,['hash'],['hash']
Security,"@jigold that's a great suggestion. To test that this code is acutally idempotent, I wanted to do this:. Duplicate the test database createDatabase task in `ci/test/resources/build.yaml` (but with a new step name). The jobs have the same parents so they race to run first. They have distinct passwords, so the resulting secret will non-deterministically have the wrong password (e.g. A creates the user, A writes its password first, B ignores already created user, then B writes its password second), but if everything but the secret works, then I'm confident repeated attempts sharing the same secret should work!. Unfortunately, I can't test CI itself in this way because test CIs (whether in dev or in a PR) are not permitted to create databases. I took your suggestion and copied the SQL query out and ran it twice instead. That worked fine. Everything was created once.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719:291,password,passwords,291,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631672719,4,['password'],"['password', 'passwords']"
Security,"@konradjk This ended up being more work than expected since we had not yet had a function that took a lambda (only had methods that did so). I have it exposed on a branch, but it seems our uniroot functionality is a little bit unexpected. It always returns the right root when there is one, but it fails to identify situations where no root exists. Will require some investigation",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-305575523:151,expose,exposed,151,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-305575523,1,['expose'],['exposed']
Security,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:763,expose,expose,763,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344,4,"['access', 'expose']","['accessible', 'expose', 'exposed']"
Security,"@sjparsa I'll take a look! In the future, if you set the ""Assignee"" it shows in my Hail CI queue. You should have access to the Hail CI page now: https://ci.hail.is Your ""queue"" is at https://ci.hail.is/me . It shows you just the PRs you're working on or reviewing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258#issuecomment-1644475278:114,access,access,114,https://hail.is,https://github.com/hail-is/hail/pull/13258#issuecomment-1644475278,1,['access'],['access']
Security,"@tpoterba @danking We at Databricks are still interested in this. Although Hail's frontend is in Python, it's still useful to publish to maven central. First, it makes the dependency information available. I've seen people write pipelines that are partly in Hail and partly in PySpark and can include Java libraries for things like data sources. There's a lot of tooling for resolving dependency conflicts between different libraries, but they're not very accessible unless all your dependencies are published to maven repos and have dependency poms available. It's also easier to update pipelines to the latest Hail version if the artifacts published to a standard location.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319:456,access,accessible,456,https://hail.is,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319,1,['access'],['accessible']
Security,"@tpoterba I don't quite see why anyone should use `hadoop_copy` if their tool supports exporting to a file handle. Unless they're doing some random access, but I think that's much rarer than `df.to_csv(f)` and using `hadoop_copy` for that is silly (AFAIK?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4644#issuecomment-433528370:148,access,access,148,https://hail.is,https://github.com/hail-is/hail/pull/4644#issuecomment-433528370,1,['access'],['access']
Security,@tpoterba I just realized I forgot to propagate the FUSE config through to worker jobs. Should I be and I got lucky that the singular test is just doing everything driver-side? Or is there a test we can write to ensure that worker jobs access the FASTA data?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495:236,access,access,236,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1478404495,1,['access'],['access']
Security,"@tpoterba I realized that hl.index is actually exposed in python, so I added it back in. I don't think it's documented, though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3662#issuecomment-392173394:47,expose,exposed,47,https://hail.is,https://github.com/hail-is/hail/pull/3662#issuecomment-392173394,1,['expose'],['exposed']
Security,"@tpoterba I think write access is necessary to dismiss reviews. I rebased this branch, hopefully should be good to go now. Will probably be making some changes in the near future, but I think it should be fine in experimental for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4247#issuecomment-423306621:24,access,access,24,https://hail.is,https://github.com/hail-is/hail/pull/4247#issuecomment-423306621,1,['access'],['access']
Security,"@ttbek, thanks for the comment and concern,. > ""the implementations should rely directly on java.util.Random"" Umm, why? From my outsiders perspective I would have assumed that high quality software worked on by the Broad Institute would use a half decent Random Number Generator (RNG). The phrase ""should rely directly on `java.util.Random`"" was referring to not accepting a source of Randomness as a parameter. It was unnecessarily specific, we're sorry that lead to your confusion. We would be happy to accept a pull request that resolves this issue by building an RNG on more theoretically sound primitives as we have done for [hash functions](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/utils/HashMethods.scala) or by using an existing efficient random number generator, such as the ones provided by Apache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314#issuecomment-384139281:631,hash,hash,631,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-384139281,2,"['Hash', 'hash']","['HashMethods', 'hash']"
Security,"A checklist of things to make this robust:. - [x] https://github.com/Nealelab/cloudtools/issues/72; - [x] we need more permissions:; ```; ++ cluster start ci-test-4d8a9b262c3687f33359d92afdae693c819dfb09-e9e8a40bb4f0c2337e5088c26186a4da4948bed2 --version devel --spark 2.2.0 --jar build/libs/hail-all-spark.jar --zip build/distributions/hail-python.zip; ERROR: (gcloud.dataproc.clusters.create) PERMISSION_DENIED: Request had insufficient authentication scopes.; ```; - [x] be certain clusters don't stick around. I am not too concerned about the latter. We should look carefully, but it appears that, by default, processes on pods [get 30s notice via TERM](https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods) before they're killed. All `cluster` needs to do is to send google a termination request. Although the command takes forever to exit after `cluster stop`, this is because it waits for the cluster to shut down before returning. I regularly issue `cluster stop` and then force-kill the `cluster` command instead of waiting for the cluster to shutdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146:439,authenticat,authentication,439,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417653146,1,['authenticat'],['authentication']
Security,A stack trace for posterity:; ```; Caused by: javax.net.ssl.SSLException: Tag mismatch!; 	at sun.security.ssl.Alert.createSSLException(Alert.java:133) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:331) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:274) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:269) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:119) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:97,secur,security,97,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,10,"['access', 'secur']","['access', 'security']"
Security,"A way to do this: https://pypi.org/project/secure/0.1.6/. ```python; secure.SecureCookie.aiohttp(; response,; value=""ABC123"",; samesite=False,; path=""/secure"",; expires=24,; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533:43,secur,secure,43,https://hail.is,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533,4,"['Secur', 'secur']","['SecureCookie', 'secure']"
Security,AFAIK it is only a matter of deleting old code paths and secrets. I don't believe there is anything blocked on the old authentication tokens existing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767090181:119,authenticat,authentication,119,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767090181,1,['authenticat'],['authentication']
Security,"AFAIK, building images should not require access to our private network. Neither should creating passwords. I put both of those on the private network. Eventually, I'd prefer that CI jobs explicitly opt-in to the private network, but for now I put them all on the private network.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9380#issuecomment-686655669:42,access,access,42,https://hail.is,https://github.com/hail-is/hail/pull/9380#issuecomment-686655669,2,"['access', 'password']","['access', 'passwords']"
Security,"AFAIK, none of our CI image builds should need to contact the k8s cluster. That and the create passwords step are the only steps of CI that are not on the private network. Everything else uses k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9380#issuecomment-684063947:95,password,passwords,95,https://hail.is,https://github.com/hail-is/hail/pull/9380#issuecomment-684063947,1,['password'],['passwords']
Security,"Action items for getting this in:; - Delete Docker's copy of an image when deleting an expanded root filesystem; - Try docker save instead of docker export so docker doesn't create a container. If this doesn't work, delete the container before deleting the image; - Pull every time a job is run for authentication purposes; - Up the reserved image cache size to 30",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10648#issuecomment-879462826:299,authenticat,authentication,299,https://hail.is,https://github.com/hail-is/hail/pull/10648#issuecomment-879462826,1,['authenticat'],['authentication']
Security,"Actually, even simpler: . ```; def downsample_matrix_table(mt: hl.MatrixTable, n_divisions: int, p_threshold: float) -> hl.Table:; mt = mt.choose_cols(list(range(10))). x = mt.locus.global_position(); y = -hl.log10(mt.Pvalue). downsampled = mt.annotate_cols(; binned=hl.agg.downsample(; x,; y,; label=hl.str(mt.Pvalue),; n_divisions=n_divisions; ); ; ); downsampled = downsampled.cols(). return downsampled. mt = hl.balding_nichols_model(3, 100, 1000); pmt = mt.annotate_rows(Pvalue = hl.rand_unif(0, 1)); downed = downsample_matrix_table(pmt, 4, .05); downed.show(); ```. I'm now somewhat convinced that the downsample aggregator is accessing cleared memory",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052:634,access,accessing,634,https://hail.is,https://github.com/hail-is/hail/issues/8240#issuecomment-594754052,2,['access'],['accessing']
Security,Added the verification function. Allows a single auth_request to handle both user and notebook authorization checks. . Obviously the MySQL connection handling will change. Relates to https://github.com/hail-is/hail/pull/5162/commits/3114234f51002ce6c477cca40a28c3ebb6ebe759,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-458849293:95,authoriz,authorization,95,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-458849293,1,['authoriz'],['authorization']
Security,"Adding a printout to the requirement, shows that the partitioner's key is not getting trimmed.; ```; java.lang.IllegalArgumentException: requirement failed: struct{locus: locus<GRCh38>} struct{locus: locus<GRCh38>, alleles: array<str>}; ```. ```diff; diff --git a/hail/src/main/scala/is/hail/rvd/RVD.scala b/hail/src/main/scala/is/hail/rvd/RVD.scala; index 88fdc84b3..dcf9a5773 100644; --- a/hail/src/main/scala/is/hail/rvd/RVD.scala; +++ b/hail/src/main/scala/is/hail/rvd/RVD.scala; @@ -43,7 +43,7 @@ class RVD(; self =>; require(crdd.getNumPartitions == partitioner.numPartitions); ; - require(typ.kType.virtualType isIsomorphicTo partitioner.kType); + require(typ.kType.virtualType isIsomorphicTo partitioner.kType, s""${typ.kType.virtualType} ${partitioner.kType}""); ; // Basic accessors; ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8027#issuecomment-581552954:781,access,accessors,781,https://hail.is,https://github.com/hail-is/hail/issues/8027#issuecomment-581552954,1,['access'],['accessors']
Security,"Adding bearer tokens would essentially add an element of expiration. our own issued tokens prove you are that particular instance, the bearer token proves you are the batch worker identity (at least that the batch worker identity requested an access token in the past X minutes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13071#issuecomment-1552039791:243,access,access,243,https://hail.is,https://github.com/hail-is/hail/pull/13071#issuecomment-1552039791,1,['access'],['access']
Security,Adding to list of PRs I'm going to dev deploy to validate since this is many major versions apart from what we have now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517#issuecomment-1061043455:49,validat,validate,49,https://hail.is,https://github.com/hail-is/hail/pull/11517#issuecomment-1061043455,1,['validat'],['validate']
Security,"After discussion with @danking, I redesigned this. The PR was failing due to timeouts since we weren't refreshing statuses in `wait` causing everything to loop forever. I split the API up into ""always hit the endpoint"", and ""if you use this function, hit the endpoint at most one time"". I think it's more explicit. We'll want to audit uses of `status()` to ensure that we're using the cached one if possible in several circumstances especially in `hailctl batch` itself, but that can be a future change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796:329,audit,audit,329,https://hail.is,https://github.com/hail-is/hail/pull/9510#issuecomment-703726796,1,['audit'],['audit']
Security,"Ah, yes, ex nihilio, should've taken a latin class. I added these to uncurated:; - [security] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:84,secur,security,84,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,20,"['certificate', 'secur']","['certificate', 'certificates', 'secure', 'security']"
Security,"Ah. This has nothing to do with notebook2. We have a wildcard DNS entry for *.hail.is so we don't have to modify DNS every time we add/remove a service. However, Let's Encrypt didn't support wildcard certificates when I wrote that code. So anything.hail.is will get a cert error. To fix this we either need to get a wildcard cert or fix the subdomains we use in DNS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416:168,Encrypt,Encrypt,168,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416,2,"['Encrypt', 'certificate']","['Encrypt', 'certificates']"
Security,"Almost by definition I'd suspect that adding a system administrator is a high security impact (that's not a judgement on you, just a statement about the security boundary getting wider). This is obviously fine in this case because we want you to be a system administrator, but we should let appsec know regardless. They'll also probably want to send you some standard trainings (and maybe background check forms?). I'll ping them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400614863:78,secur,security,78,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400614863,2,['secur'],['security']
Security,"Alright, rebased and made the requested change. Noticed one strange thing though. When I do ; `make install-deps`, the print out starts with:. ```; cat: /secrets//pypi-username: No such file or directory; cat: /secrets//pypi-password: No such file or directory; python3 -m pip install -U -r python/requirements.txt -r python/dev-requirements.txt; ```; Obviously that third line makes sense but I don't see why first 2 happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985:225,password,password,225,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985,1,['password'],['password']
Security,"Also - when we're ready, someone will need to authorize the final commit SHA in https://ci.hail.is/ for CI to run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14727#issuecomment-2419840886:46,authoriz,authorize,46,https://hail.is,https://github.com/hail-is/hail/pull/14727#issuecomment-2419840886,1,['authoriz'],['authorize']
Security,"Also for posterity, the reason we need to modify the audit from before was because we didn't completely dedup the resource id, so we cannot assume resource_id == deduped_resource_id in the new v3 table. This applies to all v3 tables and not just the bp_users one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13117#issuecomment-1563131585:53,audit,audit,53,https://hail.is,https://github.com/hail-is/hail/pull/13117#issuecomment-1563131585,1,['audit'],['audit']
Security,"Also, FYI - @cseed I had to change the permissions of the worker container to privileged and SYS_ADMIN. This doesn't apply to the user containers. But still we should double check the security implications of this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8960#issuecomment-644205887:184,secur,security,184,https://hail.is,https://github.com/hail-is/hail/pull/8960#issuecomment-644205887,1,['secur'],['security']
Security,"Also, I forgot to test that the query that does the audit doesn't lock any tables from writes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11990#issuecomment-1172526665:52,audit,audit,52,https://hail.is,https://github.com/hail-is/hail/pull/11990#issuecomment-1172526665,1,['audit'],['audit']
Security,"Also, I'm forging ahead for the rest of the day at least. I can't seem to figure out where the issue is for the validation errors that I'm seeing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839:112,validat,validation,112,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839,1,['validat'],['validation']
Security,"Also, to be more specific. The scope of work needed is 4 PRs. This is just 1 PR that exposes the REST API and the parser. There's still the list_batches query to implement as well as both UI search boxes with the help dropdown.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12748#issuecomment-1548471068:85,expose,exposes,85,https://hail.is,https://github.com/hail-is/hail/pull/12748#issuecomment-1548471068,1,['expose'],['exposes']
Security,"Although the test is still running now, I am pretty sure the following solution solved the problem. ```; #https://discuss.hail.is/t/i-get-a-negativearraysizeexception-when-loading-a-plink-file/899. export PYSPARK_SUBMIT_ARGS=""--driver-java-options '-XX:hashCode=0' --conf 'spark.executor.extraJavaOptions=-XX:hashCode=0' pyspark-shell"". ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1897770087:253,hash,hashCode,253,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1897770087,2,['hash'],['hashCode']
Security,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:364,password,password,364,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,2,['password'],['password']
Security,"An example of a script that currently fails on `main` but passes in this PR:. ```python; import hailtop.batch as hb. def test_python_function(*values):; # making this function smaller with 102 jobs submits successfully?; print(*values); h = hash(values); print(f'Hash is {h}'); # this return is important, otherwise it submits successfully; return h. if __name__ == '__main__':; b = hb.Batch('Scale size recursion test'); # 101 submits, 102 fails; for i in range(101):; j = b.new_python_job(f'Function call {i+1}'); j.call(test_python_function, i + 1). submitted = b.run(wait=False); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576#issuecomment-2161133045:241,hash,hash,241,https://hail.is,https://github.com/hail-is/hail/pull/14576#issuecomment-2161133045,2,"['Hash', 'hash']","['Hash', 'hash']"
Security,And why can't you just access `mt.globals`? (I guess that's the goal here?),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4027#issuecomment-408647913:23,access,access,23,https://hail.is,https://github.com/hail-is/hail/issues/4027#issuecomment-408647913,1,['access'],['access']
Security,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858:158,access,access,158,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858,2,['access'],['access']
Security,Are you saying that a future version of `SparkBackend.executeJSON()` will support the substitution that currently happens through e.g. `parse_table_ir()` without expecting a `java.util.HashMap` to map symbols to Java IR objects?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5340#issuecomment-463377623:185,Hash,HashMap,185,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463377623,1,['Hash'],['HashMap']
Security,As a separate follow up PR: we should inject the cloud location into the job's environment so that users' can make choices based on that information.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1269124272:38,inject,inject,38,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1269124272,1,['inject'],['inject']
Security,"Because in every application I know of, I want the binding not the top-level type. In `str` those coincide, but in the other applications, they don't. For example:. Example 1: From cseed/ordrdd2rb, I needed the type we're aggregating over to copy. I used your idiom, but my solution, it looks like:. ```; registerDependentAggregator(""take"", () => {; val t = TT.t; (n: Int) => new TakeAggregator(t, n); }, ...)(; aggregableHr(TTHr), int32Hr, arrayHr(TTHr)); ```. Yours would look like:. ```; registerDependentAggregator(""take"", (aggt: Type, argt: Type) => (n: Int) => new TakeAggregator(aggt.asInstanceOf[TAggregable].elementType, n), ...)(...); ```; Which do you prefer?. Example 2: this was motivated by the need to access the genome reference in operations on types like Variant, etc. Here's an example, my way:. ```; registerDependentMethod(""inXPar"", () => {; val gr = GR.gr; (x: Variant) => gr.inXPar(x); }, ...)(variantHr(GR), boolHr); ```. and your way:. ```; registerDependentMethod(""inXPar"", (vt: Type) => (x: Variant) => vt.asInstanceOf[Variant].gr.inXPar(v) }, ...)(variantHr(GR), boolHr); ```. This is what I mean by ""digging"". It gets worse, for example, with something like Variant unsplit: `unsplit(Array[Variant(GR)]): Variant(GR)` (assuming unsplit is implemented on the reference, which wouldn't necessarily be the case). I'll reiterate, the main point is that, in all the examples I'm aware of, we actually want the value of the binding directly and not the substituted type.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2226#issuecomment-334160156:717,access,access,717,https://hail.is,https://github.com/hail-is/hail/pull/2226#issuecomment-334160156,1,['access'],['access']
Security,Can we expose a HailContext.setEnv then? That seems like the right thing to do,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583464027:7,expose,expose,7,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583464027,1,['expose'],['expose']
Security,"Can we make it a job and/or batch level configuration? The user obviously can do whatever they like with their tokens. However, since most users don't need them, I prefer our default to be to not expose them. I think we just need a new method and attribute on `Job` and `Batch` that mirrors, say, `image` or `memory`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020:196,expose,expose,196,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020,1,['expose'],['expose']
Security,"Can you verify; 1. Batch has access to the hail-query bucket; 2. Our terraform correctly grants permissions for that? And if it currently doesnt, we should ping AUS to make sure theyre aware of this change",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138729334:29,access,access,29,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138729334,1,['access'],['access']
Security,Closing because ; 1. The change is purely cosmetic; 2. Breaking previously-run migration checksums feels like a bad idea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14669#issuecomment-2317996498:89,checksum,checksums,89,https://hail.is,https://github.com/hail-is/hail/pull/14669#issuecomment-2317996498,1,['checksum'],['checksums']
Security,"Closing in favor of https://github.com/hail-is/hail/pull/12386, which uses my fork since (I think?) CI only lets authorized users run pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11800#issuecomment-1294024957:113,authoriz,authorized,113,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1294024957,1,['authoriz'],['authorized']
Security,Closing until ExposedPorts goes in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7780#issuecomment-568618504:14,Expose,ExposedPorts,14,https://hail.is,https://github.com/hail-is/hail/pull/7780#issuecomment-568618504,1,['Expose'],['ExposedPorts']
Security,"Closing with recommended solution hashCode=0, long term plan: eliminate Spark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564#issuecomment-471050878:34,hash,hashCode,34,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471050878,1,['hash'],['hashCode']
Security,Confirmed that the hashCode solution works for Danfeng.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5564#issuecomment-471047536:19,hash,hashCode,19,https://hail.is,https://github.com/hail-is/hail/issues/5564#issuecomment-471047536,1,['hash'],['hashCode']
Security,Core.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:9023,access,access,9023,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['access'],['access']
Security,"Currently pruning dependencies, forking NextJS to remove poly fills for older browsers, and focusing on bundle size. Investigated using Inferno.js as a lighter alternative to React. Saves ~20-30KB bundle size, and is somewhat faster. However, main Inferno dev moved to React core team, and React is focusing on the optimizations present in Inferno for 2019 (DOM: move to native events where possible), as well as introducing optimizations not found in Inferno (compile time targets: initially inlining, future maybe web assembly binaries; move rendering work to separate thread / concurrent rendering). Furthermore, React ecosystem is orders of magnitude larger, so we can save a huge amount of dev time by avoiding Inferno (N modules * time to develop bespoke module avg), and have greater likelihood of LTS. Notably, I realized that most of my bundle size was coming from inefficient bundling of Material UI and due to Apollo's insanely large graphQL bundle. Removing these now. Lastly, React is actually very efficient. jQuery is ~31.1KB minified. React is 3KB, while React DOM is 33.8KB. In 2019 React DOM will shrink. In any case, given that React is both faster than jQuery, dramatically simplifies development, and introduces development structure, 4KB cost is imo worth it. Related issues:; https://github.com/zeit/next.js/issues/5923. Bundle (with header, authentication logic including jks-rsa verification of token, styles). Index.js is 336 B, _app is 2.89, and that is all that is needed for first page render. _app amortized over all other pages. Scorecard template w/fetch logic is 1.67KB. <img width=""341"" alt=""screen shot 2018-12-19 at 3 43 23 pm"" src=""https://user-images.githubusercontent.com/5543229/50247084-f3202200-03a4-11e9-8232-f1cd2a35958c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812:1365,authenticat,authentication,1365,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448652812,2,['authenticat'],['authentication']
Security,"Dan thanks for the comments, some great suggestions. I've addressed some, will get to the rest by Monday. I owe you at least one unit test. You can check the app out at app.hail.is (no SSL yet). Let me know if you have a problem logging in. Currently no one knows the workshop password but me (we can set this to whatever needed), but all team members, besides maybe Dan Goldstein should have access through the normal login. . Login will appear a bit slow because we've decided to not go the popup route, so there's an extra 2 apparent redirects. Also, safari causes some issues if ""Cross-site tracking"" protection is on. A satisfactory solution will be made in time, until then, either another browser, or disable that protection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036:277,password,password,277,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460040036,2,"['access', 'password']","['access', 'password']"
Security,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:65,XSS,XSS,65,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293,2,"['XSS', 'secur']","['XSS', 'security']"
Security,"Dataproc does have an option to tag all nodes (`--tags`), but we purposely intend to prevent SSH connections from the outside world to the worker nodes. If you need to access the worker machines, you can still connect to them from the master node (because, due to Spark/Dataproc, we must permit all TCP/IP connections between all Dataproc nodes). I realize this is annoying, but I have only needed to do this once in my three years of working on Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7978#issuecomment-578973470:168,access,access,168,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578973470,1,['access'],['access']
Security,Dataproc submission bash script: https://gist.github.com/mcovarr/06eaecad849e979d608adf43e2118f5a; Python script: https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/filter_VDS_and_shard_by_contig.py. A few things 0.2.123 introduced:; - new gradle (which has caused other dataproc issues); - SemanticHash. My guess is the combination of semantic hash and large JSON literals is pushing us past the 50GiB limit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13712#issuecomment-1757862008:384,hash,hash,384,https://hail.is,https://github.com/hail-is/hail/issues/13712#issuecomment-1757862008,1,['hash'],['hash']
Security,"Delay merging until https://github.com/broadinstitute/install-gcs-connector/pull/6 is merged. Without that PR, users will not have access to a version of the GCS Hadoop connector that does not use tons of memory in JVM 11.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1931024352:131,access,access,131,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1931024352,1,['access'],['access']
Security,"Done. {Matrix}Table._schema is now private. ttable/tmatrix are exposed, but not documented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4995#issuecomment-448336466:63,expose,exposed,63,https://hail.is,https://github.com/hail-is/hail/pull/4995#issuecomment-448336466,1,['expose'],['exposed']
Security,"Erm. There's an interface issue here. I need an output stream that can tell me the file position, but I don't know of any standard library interface that exposes that, so I just hardcoded the `FSDataOutputStream`. Is this going to be a problem? Are all `FS` implementations Hadoop file systems? cc: @akotlar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176:154,expose,exposes,154,https://hail.is,https://github.com/hail-is/hail/pull/6333#issuecomment-501732176,1,['expose'],['exposes']
Security,Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.Exec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1095,access,access,1095,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,2,['access'],['access']
Security,"Error message starting up Batch:. ```; raise ApiException(http_resp=r); kubernetes.client.rest.ApiException: (403); Reason: Forbidden; HTTP response headers: HTTPHeaderDict({'Audit-Id': 'fc886821-4fc7-4697-b8d2-a4bc656b45f6', 'Content-Type': 'application/json', 'X-Content-Type-Options': 'nosniff', 'Date': 'Thu, 25 Apr 2019 22:18:26 GMT', 'Content-Length': '252'}); HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \\""system:serviceaccount:batch-pods:default\\"" cannot watch pods in the namespace \\""test\\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}\n'; ```. Going to retest, but it seems like this was a one time thing...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273:175,Audit,Audit-Id,175,https://hail.is,https://github.com/hail-is/hail/pull/5956#issuecomment-487075273,1,['Audit'],['Audit-Id']
Security,FS$$anon$2.close(GoogleStorageFS.scala:306); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 0; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:7727,access,access,7727,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['access'],['access']
Security,"FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20145,Access,Access,20145,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['Access'],['Access']
Security,"First cut: https://github.com/broadinstitute/hail/commit/f5e93963844656449259ad893ec3ce7ddcef2f3c. Still needed: testing, implicit option manipulation, access to INFO field and QC results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/16#issuecomment-156226289:152,access,access,152,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-156226289,1,['access'],['access']
Security,Fixing with a change to dataproc image version requires a cloud tools upgrade form all our users. Perhaps we should start a new latest-hash file and push a cloud tools update that looks there and uses `1.2-deb9`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405:135,hash,hash,135,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405,1,['hash'],['hash']
Security,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:100,hash,hash-based,100,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,8,"['hash', 'validat']","['hash-based', 'validated', 'validation']"
Security,GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18413,Checksum,ChecksumFileSystem,18413,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,"Good questions:. > How does Terraform know to import the module gsa_k8s_secret? I assume it just scans the directory. In short, yes. Every module is defined by a `main.tf`, `variables.tf`, and `outputs.tf`, and it finds it through the `source` path in the module blocks. When you run `terraform init` in the `infra` directory, it scans `main.tf`, sees that there are module references to the local filesystem, and sets up a watch on that directory (you can still make changes to `gsa_k8s_secret` without re-running `init`). > What does ${module.ci_gsa_secret....} do? Does module refer to that terraform file?. The way references work in terraform is a little bizarre. There are basically three classes of value that we use right now in terraform: resources, modules and variables. You declare instances of them like so (note that I'm using class and instance colloquially):. ```; resource <resource_class_name> <resource_instance_name> {; ; }. # I like to think of modules as being essentially unnamed collections of resources; module <module_instance_name> {; source = '/path/to/module'; ; }. variable <variable_name> {; ; }; ```. and then reference them like so:. ```; <resource_class_name>.<resource_instance_name>; module.<module_instance_name>; var.<variable_name>; ```. So module.ci_gsa_secret.... is how you would access `output`s of the module instance called `ci_gsa_secret`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381:1325,access,access,1325,https://hail.is,https://github.com/hail-is/hail/pull/10785#issuecomment-900398381,1,['access'],['access']
Security,"Got it! You need to add the loop device. Now how to parse the correct thing from the xfs_info output is another story... ```; jigold@jg-file-cache:~$ xfs_info /mnt/test_xfs; meta-data=/dev/loop2 isize=512 agcount=4, agsize=65536 blks; = sectsz=512 attr=2, projid32bit=1; = crc=1 finobt=1 spinodes=0 rmapbt=0; = reflink=1; data = bsize=4096 blocks=262144, imaxpct=25; = sunit=0 swidth=0 blks; naming =version 2 bsize=4096 ascii-ci=0 ftype=1; log =internal bsize=4096 blocks=2560, version=2; = sectsz=512 sunit=0 blks, lazy-count=1; realtime =none extsz=4096 blocks=0, rtextents=0; ```. ```; jigold@jg-file-cache:~$ sudo docker run --rm --mount type=bind,source=/mnt/test_xfs,target=/host --cap-add SYS_ADMIN --security-opt apparmor:unconfined --device ""/dev/loop2:/dev/loop2:rwm"" test-xfs /bin/bash -c 'xfs_quota -x -c ""report -h"" /host'; Project quota on /host (/dev/loop2); Blocks; Project ID Used Soft Hard Warn/Grace; ---------- ---------------------------------; #0 4K 0 0 00 [------]; #200 0 0 0 00 [------]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004:709,secur,security-opt,709,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662462004,1,['secur'],['security-opt']
Security,Great. I don't think I have write access to merge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1423#issuecomment-281823930:34,access,access,34,https://hail.is,https://github.com/hail-is/hail/pull/1423#issuecomment-281823930,1,['access'],['access']
Security,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:469,hash,hash,469,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090,1,['hash'],['hash']
Security,"Great; I was debating whether to suggest that. I think we'll need sanitize the docker and k8s issues eventually, but for now, the more information the better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7484#issuecomment-551259023:66,sanitiz,sanitize,66,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551259023,1,['sanitiz'],['sanitize']
Security,Hail defaults to logging to `hail.log` in the working directory of the Spark process. Apparently the user running spark doesn't have permission to create files in its working directory. You might try ; ```python; hc = hail.HailContext(log='/tmp/hail.log'); ```. Or any other file to which you have write access.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337912555:304,access,access,304,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337912555,1,['access'],['access']
Security,"Hail doesn't have a conda package -- the bioconda package there was not uploaded by the Hail Team (could even be malware -- we don't know). It's certainly a very old version. If you install Hail with pip, you should pick up the latest version 0.2.100 and have access to hl.vds, which is somewhat recent functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111:260,access,access,260,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111,1,['access'],['access']
Security,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:31,integrity,integrity,31,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,1,['integrity'],['integrity']
Security,"Hey @tpoterba I tried, but am getting ; `remote: Permission to broadinstitute/hail.git denied to bw2.; fatal: unable to access 'https://github.com/broadinstitute/hail/': The requested URL returned error: 403`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248072411:120,access,access,120,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248072411,1,['access'],['access']
Security,"Hi @pettyalex, thank you for the detailed and thoughtful issue. Hopefully I can shed some light and address all your concerns. I think the assertion on Java 8 and 11 was an overly defensive precaution put in place some time ago, as hail uses some unsafe JVM APIs that have been deprecated for a while. But as you noted, the world goes on in Java 17 and I don't see a reason Hail shouldn't be compatible. Since most of our closest users use Hail on GCP Dataproc, we generally keep in lock-step with their platform which is unfortunately still on Java 11 so that is what we test against and officially support. Nevertheless, we should remove the restriction and add some light validation in CI against Java 17 and advertise it as unofficially supported until such a time that Dataproc moves to Java 17. Hopefully Spark 3.6 will force their hand. The release process for 0.2.129 is already underway but expect this to be resolved in 0.2.130. Thanks for your suggestions regarding bundling the JRE and the GC options, we'll definitely consider them. Regarding the `module-info.class` nonsense, my apologies. That just seems like a bug we should fix. I will create a separate tracking issue for that but I'm not yet sure where that will get prioritized. If it is more than an annoyance for you, please let us know. Regarding conda-forge, I don't think we currently have the bandwidth or demand (that we know of) to add more distribution systems. Again, this is something where hearing from the community is the best way to figure out how to direct our efforts. Hopefully this addresses your concerns. Please do follow up if I've missed anything or open more issues if you encounter new problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704:675,validat,validation,675,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704,1,['validat'],['validation']
Security,"Hi there @BioDCH, I reformatted your comment using [markdown code blocks](https://guides.github.com/features/mastering-markdown/#syntax). It looks like the unix user running `hail` does not have permission to edit `hail.log` file, this likely caused the other two errors. Please add `--log-file PATH` where `PATH` is a file path to which you have write access. For example:. ```; spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar ; --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf; ```. Assuming you have write access to `/user/hail/hail.log`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-250746848:353,access,access,353,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-250746848,4,['access'],['access']
Security,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229750270:862,hash,hash,862,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270,1,['hash'],['hash']
Security,"Hi, danking, @danking I tried two log file pathes ,all had access permission, but the error still appeared. 1HDFS file path /user/hail/hail.log have access permission; -rwxrwxrwx 3 hdfs supergroup 0 2016-10-08 10:54 /user/hail/hail.log; 2log filelocal PATH hava access permission; -rwxrwxrwx 1 root root 48523 Oct 8 11:42 hail.log. The error message was attached as follows ; [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/517467/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252404979:59,access,access,59,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252404979,3,['access'],['access']
Security,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:165,Access,Access,165,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145,1,['Access'],['Access']
Security,"Hmm. This means every dev deploy will generate a new root key. I'm worried about the derived keys and trust lists. After this runs, any service which was not dev deployed needs to know to reload the trust list and start using the new key. For example, if you dev deploy batch, then separately dev deploy query, the new query will get cert errors when talking to batch, I think. I will give some thought this week to the right long-term certificate strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199:436,certificate,certificate,436,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199,1,['certificate'],['certificate']
Security,"How are you running Spark? Are you running in local mode, or in cluster mode? In local mode, you won't have access to the HDFS file scheme.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-320923206:108,access,access,108,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-320923206,1,['access'],['access']
Security,"I addressed some comments. I still need to:; - expose hts_genotype_schema in python, and; - figure out what to rewrite instead of ""genotype"" in the VariantDataset docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699:47,expose,expose,47,https://hail.is,https://github.com/hail-is/hail/pull/2480#issuecomment-347072699,1,['expose'],['expose']
Security,"I agree users should have `storage.buckets.get` only to their own folder, and not `storage.buckets.get`. However, it looks like it is trying to create a bucket with the new folder name, and failing against that (non-existent) bucket:. > exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to untitled-folder. That's untitled-folder. Maybe the error is not permissions at all, but it is using the wrong base directory to create the folder?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482:495,access,access,495,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480380482,1,['access'],['access']
Security,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:530,authenticat,authentication,530,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595,1,['authenticat'],['authentication']
Security,"I am using a cluster with a PBS scheduler. Hail and my files are located in my home directory which is on a mounted NFS. The same NFS is mounted, and accessible, on the worker nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251762194:150,access,accessible,150,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251762194,1,['access'],['accessible']
Security,I authorized it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6432#issuecomment-507093890:2,authoriz,authorized,2,https://hail.is,https://github.com/hail-is/hail/pull/6432#issuecomment-507093890,1,['authoriz'],['authorized']
Security,"I can't `git pull` at the moment, but as of `c2508f35dc41` this is still an issue. Small example that you guys should have access to:. ```; mutation_ht = hl.import_table('gs://gnomad-resources/constraint/source/fordist_1KG_mutation_rate_table.txt',; delimiter=' ', impute=True); mutation_ht = mutation_ht.transmute(context=mutation_ht['from'], ref=mutation_ht['from'][1],; alt=mutation_ht.to[1]).key_by('context', 'ref', 'alt'); mu = mutation_ht.aggregate(hl.agg.group_by(; hl.struct(context=mutation_ht.context, ref=mutation_ht.ref, alt=mutation_ht.alt),; hl.agg.collect(mutation_ht.mu_snp))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4215#issuecomment-423534766:123,access,access,123,https://hail.is,https://github.com/hail-is/hail/issues/4215#issuecomment-423534766,1,['access'],['access']
Security,"I can't figure out why I'm getting an error in one test. But I also am not sure what to do with the `/batches` endpoint. I want the UI default to only show you your batches with the default query string 'user:jigold`. However, what should the REST endpoint be? All batches in all billing projects you have access for?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101:306,access,access,306,https://hail.is,https://github.com/hail-is/hail/pull/9954#issuecomment-770063101,1,['access'],['access']
Security,"I don't believe I have access to look at the test failures. If you let me know what failed, I'll do my best to fix it!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490:23,access,access,23,https://hail.is,https://github.com/hail-is/hail/pull/13129#issuecomment-1581212490,1,['access'],['access']
Security,"I don't think the last comment checks out. Access tokens aren't refreshable, right? So you'd have very short-lived access.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785490774:43,Access,Access,43,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785490774,2,"['Access', 'access']","['Access', 'access']"
Security,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:293,access,accessible,293,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366,4,['access'],['accessible']
Security,"I don't think we should merge this until we make dataproc jupyter notebooks have tokens. We set it to the empty string / no-token because we knew that you could only access the jupyter notebook if you had SSH credentials. I think this approach still left open CSRF attacks (random website tries to POST to localhost, so if people browse untrusted HTML/JS from their SSH tunneled web browsers that's not great). I think we should probably be setting tokens on the jupyter servers in general, but if you also intend to make the leader node's `connect_port` public, then we _definitely_ need to do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6173#issuecomment-496762764:166,access,access,166,https://hail.is,https://github.com/hail-is/hail/pull/6173#issuecomment-496762764,2,"['access', 'attack']","['access', 'attacks']"
Security,"I don't want to add a larger test to scala, since we're ripping it all out of scala soon. Currently this functionality isn't exposed to Python, and it should be tested when it's exposed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2884#issuecomment-365378540:125,expose,exposed,125,https://hail.is,https://github.com/hail-is/hail/pull/2884#issuecomment-365378540,2,['expose'],['exposed']
Security,I feel I may have complicated things slightly. It's probably not relevant that its running on a cluster. I'm essentially just using a node on the cluster as a workstation as I can easily access the VCFs from there. At the moment I'm trying out basic functionality of hail. I'm just using a single node and running a single instance of hail. All I have done so far is import a VCF and filter out some samples. Then when trying to annotate variants using a bed file I ran into the above issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251802552:187,access,access,187,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251802552,1,['access'],['access']
Security,I got a timeout!; ```; SocketTimeoutException: connect timed out. Java stack trace:; java.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token; 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:254); 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:406); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1471); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1630); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1612); 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:120,access,access,120,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,1,['access'],['access']
Security,"I got pretty fatigued from **hours** of iterating on `gradle doctest` and copy-paste after the ~25th iteration. I started sprinkling the NOTEST stuff in then. There are some places that seem unavoidable -- things that return dicts whose order may vary, for instance. I also added NOTEST directives sometimes when we print string representations of objects, because this was easier at the time and made the unit of work more manageable. I do want to fully audit the ones I've added, and have reasons for including each NOTEST. But I'd prefer to do that separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4808#issuecomment-440499564:455,audit,audit,455,https://hail.is,https://github.com/hail-is/hail/pull/4808#issuecomment-440499564,1,['audit'],['audit']
Security,"I guess I was afraid of somebody copying and pasting from the README. But if they're going to do that, they're not going to get right version anyway. You don't know the release hash until it goes in, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538:177,hash,hash,177,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538,1,['hash'],['hash']
Security,I guess it looks like you're still intending to use the proxy? I guess I'm just not sure what's going on and you're making changes (like the change to `c.NotebookApp.allow_remote_access`) that make me very uncomfortable security-wise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6173#issuecomment-496763057:220,secur,security-wise,220,https://hail.is,https://github.com/hail-is/hail/pull/6173#issuecomment-496763057,1,['secur'],['security-wise']
Security,"I had to rip out the SQLContext because its deprecated, but it does not appear to be used and was never exposed in our public python API. It's been replaced with SparkSession.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5951#issuecomment-486458681:104,expose,exposed,104,https://hail.is,https://github.com/hail-is/hail/pull/5951#issuecomment-486458681,1,['expose'],['exposed']
Security,I have a little reading list I still haven't gotten to yet about approximate median and the like:; - http://info.prelert.com/blog/q-digest-an-algorithm-for-computing-approximate-quantiles-on-a-collection-of-integers; - https://www.quora.com/Is-there-an-online-algorithm-to-calculate-the-median-of-a-stream-of-numbers-if-stream-elements-can-be-added-or-removed-at-any-point; - http://link.springer.com/chapter/10.1007/978-3-642-40273-9_7?no-access=true,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1083#issuecomment-260091875:440,access,access,440,https://hail.is,https://github.com/hail-is/hail/issues/1083#issuecomment-260091875,1,['access'],['access']
Security,"I know I said I thought this was a reasonable approach a while ago, but Ive been thinking hard about this change since last week, and I think I want us to explore a larger set of designs before committing to this strategy. The approach in this PR doubles down on the functional Code[T] structure, which is something were trying to move away from with CodeBuilder. I think if I could choose an interface for injecting line numbers from IR in emit it would look something like:. ```scala; class Emit[C](; val ctx: ExecuteContext,; val cb: EmitClassBuilder[C]) { emitSelf =>. val methods: mutable.Map[(String, Seq[Type], Seq[PType], PType), EmitMethodBuilder[C]] = mutable.Map(). private[ir] def emitVoid(cb: EmitCodeBuilder, ir: IR, mb: EmitMethodBuilder[C], region: StagedRegion, env: E, container: Option[AggContainer], loopEnv: Option[Env[LoopRef]]): Unit = {; cb.startLine(ir.lineNumber); ... implementaiton; cb.endLine(ir.lineNumber); ```. How could we make something like this work? Can we get away without every Code[T] knowing the source line? The JVM represents line numbers as an array of (line start bytecode index, line bytecode length) tuples, and I think it will be possible to produce this more easily. I think part of my concern is that Im not entirely sold by the need to have a whole stack of IR printouts and associated line numbers  right now, the option to get debug information by LIR line number or IR (fully lowered, compile-ready) seems plenty sufficient. Part of my pushback is that I'm hesitant to use Scala implicits pervasively without a careful cost/benefit consideration  theyve been a source of confusion and frustration in the past, and the intentional paucity of implicits in our current codebase reflects that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249:409,inject,injecting,409,https://hail.is,https://github.com/hail-is/hail/pull/9770#issuecomment-742027249,1,['inject'],['injecting']
Security,"I loaded gcc 4.9 and java 1.8 and now getting a new error while compiling.This is strange as earlier I dint face any issues.Is there some major changes that happened for code compilation. mkdir -p lib/linux-x86-64; g++ -fvisibility=hidden -rdynamic -shared -fPIC -ggdb -O3 -march=native -g -std=c++11 -Ilibsimdpp-2.0-rc2 -Wall -Werror ibs.cpp -o lib/linux-x86-64/libibs.so; :compileScala; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the pro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:481,access,access,481,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['access'],['access']
Security,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:972,password,password,972,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,1,['password'],['password']
Security,I made some changes. I had to rewrite the audits to not fill up the temp disk space and account for a bug in billing that was fixed for job private instances #10069. I'll test this afternoon after I figure out how to revert my first attempt.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379:42,audit,audits,42,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1201421379,1,['audit'],['audits']
Security,"I need access to the users' GCP SA key file, which I think is most naturally stored as a k8s secret.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5633#issuecomment-477341107:7,access,access,7,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-477341107,1,['access'],['access']
Security,"I pushed some addition changes: push requestedType into TableRead, expose (private) in Python for performance testing. On a chunk of gnomAD sites file, read count went from 19s (all fields) to 12s (keys + 1 int field). The Python changes should get removed once prune dead fields goes in. MatrixRead will require a bit more work (with the recent unification of matrix read/import IR nodes).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3667#issuecomment-392303741:67,expose,expose,67,https://hail.is,https://github.com/hail-is/hail/pull/3667#issuecomment-392303741,1,['expose'],['expose']
Security,"I realize I forgot to update the latest-hash lines, I'll fix that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372:40,hash,hash,40,https://hail.is,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372,1,['hash'],['hash']
Security,"I reverted the `gcp` changes entirely. Let's just get to work on having tracked infrastructure in GCP. We'll resolve the differences between `gcp` and `gcp-broad` (as much as possible) at some later date. In the short term, we need to get these security changes in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12882#issuecomment-1520905705:245,secur,security,245,https://hail.is,https://github.com/hail-is/hail/pull/12882#issuecomment-1520905705,1,['secur'],['security']
Security,"I saw this again today in a fairly simple and isolated test. I'm beginning to wonder if this is just a new form of transient error. We pick 22 random characters from a 62 character alphabet. Odds of collision are minuscule:; ```; In [2]: (1/62)**22; Out[2]: 3.693029961058969e-40; ```; I verified `SecureRandom` with no constructor uses a randomly chosen seed. There's three exceptions there (all the same one). The deepest one came during a write. The next two came during closes. The outermost exception is from the `using` cleaning up. I'm not sure where the middle exception comes from, I can't imagine who would try to `close` the stream other than `using`. Regardless, it appears that the upload fails in some unrecoverable way. We're writing 2GiB in 256 8MiB chunks in this test, so we have more chances for something to go wrong. Maybe we just have to retry the entire partition when this happens?. https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:298,Secur,SecureRandom,298,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Secur'],['SecureRandom']
Security,"I see, here's the bit that does suggest the prefix:. > Use a naming convention that distributes load evenly across key ranges; > Auto-scaling of an index range can be slowed when using sequential names, such as object keys based on a sequence of numbers or timestamp. This occurs because requests are constantly shifting to a new index range, making redistributing the load harder and less effective. > In order to maintain a high request rate, avoid using sequential names. Using completely random object names gives you the best load distribution. If you want to use sequential numbers or timestamps as part of your object names, introduce randomness to the object names by adding a hash value before the sequence number or timestamp.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680:685,hash,hash,685,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914242680,1,['hash'],['hash']
Security,"I see, so you receive an access denied when there are no artifacts. The build log has a long list of commands, starting with a git clone. It sounds like this isn't an issue then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5546#issuecomment-472479806:25,access,access,25,https://hail.is,https://github.com/hail-is/hail/issues/5546#issuecomment-472479806,1,['access'],['access']
Security,"I suppose this is where I started getting entangled with the domain issue. If the Australians run a workshop, what should happen if their users run `hailctl batch init`? Should they have to supply some additional argument so that _if_ they're not authenticated they get sent somewhere other than `hail.is`? Maybe that's ok, seems kind of awkward though. While it doesn't work this way today, I imagine that we should ultimately configure `hailctl auth login` to accept a domain. I feel like that would make the AUS scenario slightly less awkward, and the tool more consistent, though I admit it is conceding some of our own convenience.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567:247,authenticat,authenticated,247,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1663110567,1,['authenticat'],['authenticated']
Security,I tested the commands by creating a service account with 0 permissions and making sure that I could give it access and could read/write to the bucket and gcr.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8840#issuecomment-631733744:108,access,access,108,https://hail.is,https://github.com/hail-is/hail/pull/8840#issuecomment-631733744,1,['access'],['access']
Security,"I think it's useful if you want to write something like:. ```; hl.if_else(x < y, ...., hl.die(""Error: x must be less than y"")); ```. Otherwise you can't do input validation on expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584:162,validat,validation,162,https://hail.is,https://github.com/hail-is/hail/pull/8865#issuecomment-641495584,1,['validat'],['validation']
Security,"I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7466#issuecomment-551108566:55,access,access,55,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551108566,2,['access'],['access']
Security,"I think that's right, though we serialize other potentially private information. I think we ought to have a per-organization (Hail billing project?) cache, but also not very high priority. I'd be pretty chuffed to learn we're running important enough stuff that people are attempting timing attacks on our cache to learn what queries other people are executing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865:291,attack,attacks,291,https://hail.is,https://github.com/hail-is/hail/pull/10309#issuecomment-821250865,2,['attack'],['attacks']
Security,"I think the answer is that we can't treat the separate namespace as a security boundary. i.e. all internal traffic should be authenticated, encrypted, and authorized by the receiver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070:70,secur,security,70,https://hail.is,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070,4,"['authenticat', 'authoriz', 'encrypt', 'secur']","['authenticated', 'authorized', 'encrypted', 'security']"
Security,I think the docs you're referencing aren't the Compute Engine API docs: https://cloud.google.com/compute/docs/api-rate-limits. The audit logs also show 403s: https://console.cloud.google.com/logs/query;query=%22403%22%0A%22Rate%20Limit%20Exceeded%22;timeRange=2021-05-04T00:05:00.000Z%2F2021-05-04T01:06:00.000Z;pinnedLogId=2021-05-04T00:40:25.985091Z%2F-1xc71ve6z9k6;cursorTimestamp=2021-05-04T00:40:31.308803Z?project=hail-vdc&query=%0A,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-833010496:131,audit,audit,131,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-833010496,1,['audit'],['audit']
Security,I think the only additions I would need to use this in #2423 for summarize are `getNAlleles: Int` and `getAltAlleles: Array[AltAllele]` accessors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2428#issuecomment-344269060:136,access,accessors,136,https://hail.is,https://github.com/hail-is/hail/pull/2428#issuecomment-344269060,1,['access'],['accessors']
Security,"I think the only two things I'm stuck on are:; (a) Do we want users to pass a VEPConfig instead of a `config` dictionary (and add documentation)?; (b) What is the best way to expose the VEP command interface so a user can customize it to their setup? I wanted to do something like this, but I don't see how to do this with the bash script being called with an argument `/bin/bash -c ""...."" csq` or `/bin/bash -c ""..."" vep`. ```python3; vep_85_grch37_command = '''; #!/bin/bash. if [ $VEP_CONSEQUENCE -ne 0 ]; then; vcf_or_json=""--vcf""; else; vcf_or_json=""--json""; fi. export VEP_COMMAND=/vep/vep \; ${VEP_INPUT_FILE:+--input_file $VEP_INPUT_FILE} \; --format vcf \; ${vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir=${VEP_DATA_DIR} \; --plugin LoF,human_ancestor_fa:${VEP_DATA_DIR}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:${VEP_DATA_DIR}/loftee_data/phylocsf_gerp.sql,gerp_file:${VEP_DATA_DIR}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT. exec vep.py ""$@""; '''. supported_vep_configs = {; ('GRCh37', 'gcp', 'us-central1', 'hail.is'): VEPConfig(; 'hail-qob-vep-grch37-us-central1',; ['us-central1'],; HAIL_GENETICS_QOB_VEP_GRCH37_IMAGE,; '/vep_data/',; {},; VEPConfig.default_vep_json_typ,; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""vep""],; [""/bin/bash"", ""-c"", vep_85_grch37_command, ""csq_header""],; True,; 'gcp',; ),; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947:175,expose,expose,175,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1498124947,1,['expose'],['expose']
Security,"I think the takeaway is: notebook's ability to create pods makes it a security risk, so we gotta treat all this code with care.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018:70,secur,security,70,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479641018,1,['secur'],['security']
Security,I think this is dependent on the authorization PR going in so I can test it's working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053:33,authoriz,authorization,33,https://hail.is,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053,1,['authoriz'],['authorization']
Security,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:499,checksum,checksum,499,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346,2,['checksum'],"['checksum', 'checksums']"
Security,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:80,authoriz,authorization,80,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060,4,['authoriz'],['authorization']
Security,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:557,password,password,557,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,2,['password'],['password']
Security,"I used `RegionValueVariant` to clean up some of the code, and fixed a couple of bugs from #2451 in the process. I also replaced the `aggregatePartitions` method I wrote in e5f87c3 following @danking's comment, which was defined on `OrderedRDD2`, with `aggregateWithContext`, which is defined on a rich wrapper around `RDD`. I put it in the spark package to get access to private methods, making the implementation cleaner. It is now a simple modification of the implementation of `RDD.aggregate`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179:361,access,access,361,https://hail.is,https://github.com/hail-is/hail/pull/2423#issuecomment-347624179,2,['access'],['access']
Security,I will run lets encrypt now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9246#issuecomment-671463155:16,encrypt,encrypt,16,https://hail.is,https://github.com/hail-is/hail/pull/9246#issuecomment-671463155,1,['encrypt'],['encrypt']
Security,"I wonder if, using a consistent template, we could have CI do this, either by looking at the first comment, or by looking at the commit. Look for ""depends on: #hash"" string, much like GitHub does with ""closes #hash"" in commits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282:160,hash,hash,160,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282,2,['hash'],['hash']
Security,"I would also suggest the following as necessary for an upcoming release:. * #13728. Google's gcsfuse APT repository currently produces 502 Bad Gateway errors when accessed via http, which shows no sign of being resolved any time soon. I've commented on #13728 noting how this PR can work around the problem. At present (since early October), the `batch_worker_image` job always fails with 502 during a hail deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602:163,access,accessed,163,https://hail.is,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602,1,['access'],['accessed']
Security,"I'd argue that it's broadly useful, but rather the issue is that it's useful at a lower level of abstraction (e.g. how it's used in `ld_prune`, composed with `sparsify_row_intervals`). So I see why the `hl` namespace is may be too high level, but it's also strange to put in experimental a function that is used in non-experimental (as well as experimental) methods. One option is to underscore the method for use outside experimental, but expose through the experimental module. Another is to put it in a submodule, like genetics. I'd like to expose more high-level applications directly (e.g. an `ld_matrix` function that takes an optional `radius` and `coord_expr` and returns the sparse block matrix), and we can think about re-implementing in terms of scans once they come online (deriving the stops from the starts) and zipping the intervals and blocks without ever localizing should memory or performance be an issue.; I don't want to hide it entirely in the meantime as several groups want to make use of it already.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401468652:440,expose,expose,440,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401468652,2,['expose'],['expose']
Security,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:41,audit,audit,41,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997,1,['audit'],['audit']
Security,I'll PR a fix to the artifacts page that exposes the docs directly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5063#issuecomment-451556932:41,expose,exposes,41,https://hail.is,https://github.com/hail-is/hail/pull/5063#issuecomment-451556932,1,['expose'],['exposes']
Security,"I'll do a performance test, but there's still foreign key constraints on these rows. They're just redundant. We don't need a check on both `batches` and `attempts`. The rows in `attempts` wouldn't have been inserted without the check in `batches`. All of these proposed changes don't change anything about data integrity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811:311,integrity,integrity,311,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1163224811,1,['integrity'],['integrity']
Security,I'll prefix the thing with an underscore. I mostly want to expose this as an option so Chris can turn it back to 1 on the combiner,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5862#issuecomment-483878748:59,expose,expose,59,https://hail.is,https://github.com/hail-is/hail/pull/5862#issuecomment-483878748,1,['expose'],['expose']
Security,"I'll start reviewing functions based on ""High Speed Hashing for Integers and Strings"" so you can assign that PR to me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2288#issuecomment-336538490:52,Hash,Hashing,52,https://hail.is,https://github.com/hail-is/hail/pull/2288#issuecomment-336538490,1,['Hash'],['Hashing']
Security,I'm a little confused. The snippet in the PR body gives impression that the sphinx injection happens after inserting the headers/footers. Should the raw blocks be the other way around?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-813632797:83,inject,injection,83,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-813632797,1,['inject'],['injection']
Security,"I'm currently running this branch of CI on a pull request of itself on my own fork of hail, and it nearly passes all tests except for hailtop_batch_* because of requester pays permissions issues and monitoring, because I don't have a service account in my project with all the permissions for broad-ctsa. So unfortunately haven't fully validated that it will _not_ merge a passing PR, but this seemed good enough that we can push it through for azure (since both of these errors are gcp-dependent). If this goes through I can put in a follow-up PR that mirrors the infra resources that CI needs in azure (blob storage, acr permissions, etc.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539:336,validat,validated,336,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964437539,1,['validat'],['validated']
Security,"I'm going to close this PR because its so old. I've bumped the priority of certificates in my todo list. Sincere apologies @vladsaveliev , just too many things to do these days :|.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-1050025851:75,certificate,certificates,75,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-1050025851,1,['certificate'],['certificates']
Security,I'm not going to read 7000 lines of deletions. Fine with me if it passes! And fine to delete tests conditional on audit soon.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3990#issuecomment-407914682:114,audit,audit,114,https://hail.is,https://github.com/hail-is/hail/pull/3990#issuecomment-407914682,1,['audit'],['audit']
Security,"I'm not gonna do the BinaryIntHeap stuff. For graphs smaller than 2^13, the cost is dominated by non-heap-things. For fully connected graphs of 2^13 or larger, representing the full graph in memory (you'll note in my examples I cheated by never reifying the graph) is rather difficult because you've got 2^26 nodes. I can't reify a 2^13 fully connected graph on my laptop with 4GB of RAM. The edge array alone is gonna be about 2^26 / 2 * 8 positions long, which is like 130 MB already, then each pair is probably about 100 bytes, so like a gig, and all I did was create an array of edges. Then I'd have to manipulate that to get an array of all the vertex set, and a hash map from vertex to its integer. I don't see this being so great for fully connected graphs. Perhaps there's some investigation needed for less fully connected graphs. But in that case the quadratic behavior is less relevant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2148#issuecomment-326124085:668,hash,hash,668,https://hail.is,https://github.com/hail-is/hail/pull/2148#issuecomment-326124085,1,['hash'],['hash']
Security,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528:262,access,access,262,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528,8,['access'],"['access', 'access-token']"
Security,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:591,access,access,591,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141,7,"['access', 'authenticat', 'password']","['access', 'authentication', 'password']"
Security,I've authorized another test of this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-521290991:5,authoriz,authorized,5,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-521290991,1,['authoriz'],['authorized']
Security,I've authorized sha `600826710afb8dfef5fcbf19f440a43f263ec0ee` in CI so we should get a test run through soon.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2407899242:5,authoriz,authorized,5,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2407899242,1,['authoriz'],['authorized']
Security,"I've removed the Python `tempfile` approach in favor of adding `new_local_temp_file` to utils and a corresponding function to HailContext in Scala, which currently hardcodes `file:///temp` as the local temp directory. It may be more natural to have a localTmpDir on HailContext like we have tmpDir. ; I see there is a notion of local temp files on TempDir on the Scala side, but it doesn't seem to be used on the Python side. I also don't see if/where we wipe temp files on exit. In any case, I've tested that now it all works nicely on GCP, so ready for feedback/review. I think factoring through `tofile` and `fromfile` is useful for wider interoperability for the same reason that NumPy exposes them, but it's also good if you dont want to actually load the NumPy array into driver memory but just save it to read/copy later, or to load it multiple time without recomputing the BlockMatrix. And I've provided the simpler interface of `to_numpy` and `from_numpy` for the common case. I suspect that (de)serializing over the network and building the local matrix dominates local read/write, so that using a socket isn't going to do much better. I can profile more closely if/when we feel it's high priority to make this faster still.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433:690,expose,exposes,690,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-372165433,2,['expose'],['exposes']
Security,"I've repurposed the `TIterable` class so that ""Iterable"" means something you can iterate over (containers and streams) and ""Container"" means something with elements that you can access by index/out of order (arrays, sets, dicts). This should make typing IRs easier if we intend to have a `ToStream` IR to enforce non-instantiation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5610#issuecomment-473423764:178,access,access,178,https://hail.is,https://github.com/hail-is/hail/pull/5610#issuecomment-473423764,1,['access'],['access']
Security,"I've validated correctness for haploid calls manually, and confirmed that results for diploid calls don't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10251#issuecomment-818810396:5,validat,validated,5,https://hail.is,https://github.com/hail-is/hail/pull/10251#issuecomment-818810396,1,['validat'],['validated']
Security,I've validated our setup has those requirements and we're just hitting a FatalError from a commit a few weeks ago. https://github.com/hail-is/hail/blob/a0e8eb81e0f4d7ad446723e7cc04d4c6ac4ad066/hail/python/hail/context.py#L59-L67. If I revert this file we're able to pass in the existing SparkContext with the expected `hl.init(sc=sc)`. As general feedback it may be better to warn here than force exit. I may be wrong but I don't see a way around this for people using remote notebooks to talk to Spark via Livy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154:5,validat,validated,5,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154,2,['validat'],['validated']
Security,"IMHO, I'd just merge it if you're happy with everything else. Breaking the build results page only affects developers and it doesn't even really break anything since we can use `gsutil` to directly access the results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4551#issuecomment-431459832:198,access,access,198,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431459832,1,['access'],['access']
Security,"Ideally, I should be able to work directly from an `AltAlleleView`. But then I would have to copy all of the `isInsertion`, `isDeletion`, etc., methods from `AltAllele`, which feels wrong. As it's only a per-variant allocation, I think allocating `AltAllele` objects to get access to those methods is the right compromise currently, though I'm certainly open to alternatives.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2428#issuecomment-344369282:274,access,access,274,https://hail.is,https://github.com/hail-is/hail/pull/2428#issuecomment-344369282,1,['access'],['access']
Security,"If I'm understanding the example at the end correctly, it sounds like you really only need a hash of `(batch id, job id, attempt id)` (though hash functions and PRNGs are very closely related).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502#issuecomment-1693664506:93,hash,hash,93,https://hail.is,https://github.com/hail-is/hail/issues/13502#issuecomment-1693664506,2,['hash'],['hash']
Security,"If we go through route (2), this project can serve as a prototype C or C++ interface to Hail. This interface could take multiple forms. For example, we could actually re-build our memory representation implementations in C++ and compile SAIGE, at Hail-Query-compile-time (i.e. when we are compiling a *user's* query), to use whatever SType/PType that Hail has decided is the ideal. A simpler approach is to implement one canonical implementation of the Hail types in C++, fork & slightly modify SAIGE to accept these memory representations, compile SAIGE at Java compile time (i.e. in CI or when you run `make` on your laptop) against these mem reps, ship the compiled library with the Hail JAR, and expose it, via JNI, into the Hail Query language. This requires that the Query compiler can call a function which only supports arguments using one particular SType/PType.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816:700,expose,expose,700,https://hail.is,https://github.com/hail-is/hail/issues/13442#issuecomment-1679739816,2,['expose'],['expose']
Security,"In particular, the design of the Broad GCP Security Best Practices is to disable everything by default and then selectively enable. The master tag enables SSH access for the master node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7978#issuecomment-578973978:43,Secur,Security,43,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578973978,2,"['Secur', 'access']","['Security', 'access']"
Security,"In the worst case of a fully connected graph it's O(n) insertions, at an average cost of O(1), followed by O(n) `extractMax` which are O(log n) each. For each `extractMax` we have to decrease priority on every other node, so thats O(n^2) decreasePriority which is O(log n) each. So the dominating factor is O(n^2 log(n)). We should almost never see fully connected graphs. I suspect we'll usually have disconnected graphs of fully connected subgraphs. Then the runtime is O(m * n^2 log(n)) where `m` is the number of families and `n` is the average family size. `n` should be rather small, probably not higher than 20? probably smaller. `m` could be quite large, but we're linear in `m`. Some timings for fully connected graphs of size `n`:. | `n` | time (minutes) |; |-|-|; | 1024 | 0.0051 |; | 8192 | 0.13 |; | 32767 | 3.0 |; | 65535 | 12.5 |. According to the profiler, the vast majority of the time is spent interacting with the hash table. We could probably get the constants a lot lower by using an array instead of a hash table (since we have a perfect hash function: sample index). ---. ```; def fullyConnected(n: Int) {; val biggestFirst = new BinaryHeap[Int](). var i = 0; while (i <= n) {; biggestFirst.insert(i, n); i += 1; }. val g = (i: Int) => 0 to n. while (biggestFirst.maxPriority() > 0) {; val current = biggestFirst.extractMax(); val neighbors = g(current); neighbors.foreach { x =>; if (biggestFirst.contains(x)); biggestFirst.decreasePriority(x, _ - 1); }; }. val actual = biggestFirst.toArray; assert(actual.length === 1); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2148#issuecomment-326072568:933,hash,hash,933,https://hail.is,https://github.com/hail-is/hail/pull/2148#issuecomment-326072568,3,['hash'],['hash']
Security,"In-cluster I have the ability to create a pod, including the secret, which is slightly surprising to me. Does the ability create a pod give ability to mount any secret? Surely not. At the same time, my rbac for notebook clearly defines the only secret it can access:. ```; (base) alex:~/projects/hail/notebook2:$ k get role read-get-user-secret -o json; {; ""apiVersion"": ""rbac.authorization.k8s.io/v1"",; ""kind"": ""Role"",; ""rules"": [; {; ""apiGroups"": [; """"; ],; ""resourceNames"": [; ""get-users""; ],; ""resources"": [; ""secrets""; ],; ""verbs"": [; ""get""; ]; }; ]; }; ```. The other permissions are for service and pod resources. These pods are bound to the user's service account. I also don't appear to need to give that service account that is bound (SA ""B"") permission to read the mounted secret. This makes sense to me: the container should be able to access anything on its file system. The notebook leader defines what that is. cc @cseed, thought you may want to know. The following was from a manual in-cluster test:; <img width=""940"" alt=""Screenshot 2019-04-02 15 55 39"" src=""https://user-images.githubusercontent.com/5543229/55432272-78989e00-5560-11e9-960e-1362d277d759.png"">. Partial description of a recently created pod (sans status); ```sh; (base) alex:~/projects/hail/notebook2:$ k get pod notebook2-worker-d4snh -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b199",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:259,access,access,259,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,6,"['access', 'authoriz']","['access', 'authorization']"
Security,"Instead of creating a hail context, I've exposed the top level `init` and `stop` methods in hail2. . Env.hc() will call init if it's not been called yet, and print a message about initializing with default params.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2820#issuecomment-360908387:41,expose,exposed,41,https://hail.is,https://github.com/hail-is/hail/pull/2820#issuecomment-360908387,1,['expose'],['exposed']
Security,"Issue I ran into: I need to pass each child IR's ptype to the join point loop body. There isn't a very easy way to get one PType out of an IndexedSeq[PType] of ptypes. For instance, even though srvb.advance() runs inside the body of the JoinPoint loop, its staticIdx does not update (since the loop only iterates at runtime). I want to pass Code[IndexedSeq[PType]] but that isn't possible. Will work on tomorrow. . edit:. I think I need to do something like this to access individual ptypes (but within toEmitTriplet loop body): . ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692:466,access,access,466,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692,1,['access'],['access']
Security,"It seems that it does happen after the header is inserted but the footers are inserted after the Sphinx injection for the main block. So, will we need to create a separate CSS file for when the website runs through a second time it is pulling the most up-to-date styling changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-813640189:104,inject,injection,104,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-813640189,1,['inject'],['injection']
Security,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:61,access,access,61,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799,1,['access'],['access']
Security,"It would need BigQuery access to broad-ctsa, since that's where the data for the monitoring service lives. You can also look at the billing-monitor service account. I'm not sure much beyond that because I don't seem to be able to see broad-ctsa anymore in the console? And I only have IAM permissions in hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11053#issuecomment-964628523:23,access,access,23,https://hail.is,https://github.com/hail-is/hail/pull/11053#issuecomment-964628523,1,['access'],['access']
Security,"It's an edge case, largely in the case of a network partition where a worker node becomes unreachable. I believe that the batch driver will at some point declare the node dead and reschedule its jobs even if the machine is still running. This might not be hard to change given that the batch driver subscribes to the audit log and can use the vm api to verify a worker's state. I also think this barely ever happens except in the case of actual bugs on the worker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460#issuecomment-2427210749:317,audit,audit,317,https://hail.is,https://github.com/hail-is/hail/issues/14460#issuecomment-2427210749,1,['audit'],['audit']
Security,"Keeping in mind Cotton's queries last week, researched and found much lighter alternative to ExprsesJS for the server api. A few years ago, Express had low impact on node performance; it has become bloated. Found a light (~200 LOC) ""framework"" called Polka, that is small enough to maintain ourselves. It mainly adds light route-matching capabilities, to avoid repeating boilerplate when writing the Node server. Easy to follow. It's also the fastest ""framework"" available, outside of C/Go/Rust. Matches Falcon, and allows 1 language for server/web. (Also Node has a far larger ecosystem).; * https://github.com/the-benchmarker/web-frameworks ; * Polka also nearly compatible with Express's middleware api, so many existing packages are either directly usable, or with minor modifications. This was a desire of mine, since nearly everything server-y for node is really written for Express. Last commit removes all Express, adds a rewritten express-jwt for access token verification, and shows client credential exchange, backed by Redis cache, for <=4ms fetching of",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569:956,access,access,956,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-447583569,2,['access'],['access']
Security,Let's expose this. The peak memory usage on a region pool is between `allocationEchoThreshold / 2` and `allocationEchoThreshold`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10233#issuecomment-812071559:6,expose,expose,6,https://hail.is,https://github.com/hail-is/hail/pull/10233#issuecomment-812071559,1,['expose'],['expose']
Security,"Looking at the `/Packages` URL in the previous comment, 1.2.0 is now available (and 1.1.0 does not appear to be there). In our recent local hail update deployment, the `batch_worker_image` job failed repeatedly due to GoogleCloudPlatform/gcsfuse#1424. We worked around this as initially suggested on that issue with populationgenomics/hail@607408bee752dabca48d9a2732b14d32813ace9f, but later comments on the issue suggest that the better approach would be this PR with an additional change to access the apt repo via https:. ```diff; - echo ""deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; + echo ""deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502:494,access,access,494,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502,1,['access'],['access']
Security,"Made a more robust authentication library. One outstanding issue due to auth0js library, that we can solve by checking for and clearing wildcard auth0-prefixed cookies and startup, but this may have side-effects. Created an issue to track:; https://github.com/auth0/auth0.js/issues/897",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662:19,authenticat,authentication,19,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455907662,2,['authenticat'],['authentication']
Security,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:41,access,access,41,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049,1,['access'],['access']
Security,"My guess is the launched pod is the one accessing the secret, with its service account. You could check this by removing the user can read secret binding and try launching a pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479298393:40,access,accessing,40,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479298393,1,['access'],['accessing']
Security,"My team is pretty excited about hail being released with support for Spark 3.5. One thing I noticed is that it looks like the plan is to [restrict to Spark 3.5.0](https://github.com/hail-is/hail/pull/14158/files#diff-7e9fff5f09cc109665f7fe9baa107affaac24f5dc5a0aa8bc3769221a4c6c328R53) - would it be possible to allow some wiggle room for minor releases? Spark has been beginning to release upgrades much more often than in the past, so restricting to 3.5.0 will prevent access to bug fixes, feature enhancements, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582:471,access,access,471,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582,1,['access'],['access']
Security,"New new failure mode:. AccessDeniedException: 403 hail-ci-0-1@broad-ctsa.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test. Failing here in test-ci.py:. ```; deploy_artifact = run(['gsutil', 'cat', f'gs://hail-ci-test/{second_target_sha}'], stdout=subprocess.PIPE); deploy_artifact = deploy_artifact.stdout.decode('utf-8').strip(); assert f'commit {second_target_sha}' in deploy_artifact; ```. I don't know who's authorizing gcloud for hail-ci-0-1. Anyway, I gave hail-ci-0-1 permissions on hail-ci-test and am re-running.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114:23,Access,AccessDeniedException,23,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429196114,3,"['Access', 'access', 'authoriz']","['AccessDeniedException', 'access', 'authorizing']"
Security,"New test failure:. ```; + ./gradlew shadowJar archiveZip; Exception in thread ""main"" javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192); ```; Still not sure what to do about transient external dependency failures in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417:171,secur,security,171,https://hail.is,https://github.com/hail-is/hail/pull/4536#issuecomment-429360417,1,['secur'],['security']
Security,Next steps seem to be:; - [ ] try master SKAT in a debian docker image (i.e. is this a *nix issue?); - [ ] find a hash that SKAT succeeds in the cloud,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5565#issuecomment-472066037:114,hash,hash,114,https://hail.is,https://github.com/hail-is/hail/issues/5565#issuecomment-472066037,1,['hash'],['hash']
Security,"No, only step 1 up there has been completed. We currently accept both the old and new authentication tokens. We need to decide what kind of deprecation approach is appropriate here and then do it. Also as it currently stands copy-paste tokens require the old-style of authentication (I think because of an implementation detail that internally these tokens reference `session_id`s instead of users), so those either need to be migrated to be compatible with a user not having any `session_id`s in the database or removed entirely. A useful bit of context though, the only thing users need to do to get off the old tokens is `hailctl auth login` on a hail version with the changes in #13131.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1766798562:86,authenticat,authentication,86,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1766798562,2,['authenticat'],['authentication']
Security,"Not quite sure why this was re-marked WIP. Anyway, switching to AR meant users lost access to `gcr.io/hail-vdc/python-dill` anyway so this PR is no longer the source of a breaking change and now a fix so that users can use `hailgenetics/python-dill`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12230#issuecomment-1265436386:84,access,access,84,https://hail.is,https://github.com/hail-is/hail/pull/12230#issuecomment-1265436386,1,['access'],['access']
Security,Note to self that the audit should test for equality here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12761#issuecomment-1458597618:22,audit,audit,22,https://hail.is,https://github.com/hail-is/hail/pull/12761#issuecomment-1458597618,1,['audit'],['audit']
Security,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:523,validat,validation,523,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836,3,['validat'],"['validation', 'validator']"
Security,"OK, I think this is actually ready for a real review. Almost everything was spurious (I marked as such, so hopefully we won't have to do that on every PR). There were a few real things:; 1. use integrity checks for CDN javascript libraries; 2. don't let edits to the search textbox modify the URL arbitrarily; 3. don't let the target pages of anchor tags mutate the source page's DOM (wtf, how is this the default behavior???); 4. don't send the IntegrityError from mysql back to the users. I think this is basically safe because of how restrictive we are with which error is printed, but it's also not necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860:194,integrity,integrity,194,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1268879860,2,"['Integrity', 'integrity']","['IntegrityError', 'integrity']"
Security,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:425,secur,security,425,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,510,['secur'],['security']
Security,"OK, code is stable again, scale tests are working. Run with:. ```; ~/hail/notebook $ PYTHONPATH=../hail/python:../gear python3 scale-test.py 10 <workshop> <password>; ```. ```; successes: 10 / 10 = 1.0; mean time: 2.3504347085952757; max time: 3.135228157043457; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534603185:156,password,password,156,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534603185,1,['password'],['password']
Security,"OK, here's the most recent failure https://batch.hail.is/batches/8090848/jobs/21993. Don't be duped by my bad log message! There were zero transient errors. I added a log statement that increments the number of errors and prints that message after *every* error, even if it's not transient. . This time it was partition 20053 (we keep moving earlier?). I forgot to catch and rethrow the error with the toString of the input buffer, but I'm not sure there is much to learn from that anyway. FWIW, 20053 was successful in the two previous executions:; 1. https://batch.hail.is/batches/8069235/jobs/21993; 2. https://batch.hail.is/batches/8083195/jobs/21993. Interestingly the peak bytes are not consistent:; ```; 2023-10-24 19:59:47.756 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=58394624, peakBytesReadable=55.69 MiB, chunks requested=5513, cache hits=5501; 2023-10-24 19:59:47.759 : INFO: RegionPool: FREE: 55.7M allocated (7.7M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 9: pool-2-thread-1; ```; ```; 2023-11-08 19:42:40.000 : INFO: TaskReport: stage=0, partition=20053, attempt=0, peakBytes=61343744, peakBytesReadable=58.50 MiB, chunks requested=5513, cache hits=5501; 2023-11-08 19:42:40.000 : INFO: RegionPool: FREE: 58.5M allocated (10.5M blocks / 48.0M chunks), regions.size = 21, 0 current java objects, thread 10: pool-2-thread-2; ```. Whatever is causing this bug is rare. Approximately once every 31,000 partitions. The CDA IR is the same except for a couple iruid names and the order of the aggregators in the aggregator array is swapped (collect & take vs take & collect). AFAICT, the GCS Java library doesn't do any streaming verification of the hash. We could compute the CRC32c in a streaming manner and fail if/when we get to the end of the object, but this wouldn't work when we read intervals. I'm really mystified.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385:1715,hash,hash,1715,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834606385,2,['hash'],['hash']
Security,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:104,authenticat,authentication,104,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078,3,"['access', 'authenticat']","['access', 'authentication']"
Security,"OK, so the big insight is that ""InstanceConfig"" is really just ""ResourcesForAParticularInstance"" (well, and, sometimes, ""ResourcesOfARepresentativeInstance""). I trimmed the InstanceConfig down *significantly* removing the ""vm_config"". Now the InstanceConfig is cheap and easy to create and there's no circularity between vm_config and instance config. I pushed that through everywhere and then abstracted the common create_instance logic for pool and job-private into InstanceCollection. With both of those changes, I was able to modify the ResourceManager's API to expose methods for constructing instance configs. However, the instance config isn't critical to the operation of the ResourceManager. It's just an interface for communicating an instance's resources to the rest of the code base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279:566,expose,expose,566,https://hail.is,https://github.com/hail-is/hail/pull/10920#issuecomment-956500279,1,['expose'],['expose']
Security,"OK, so the current solution addresses the problem, but:; - if MathJax changes the vertical layout of the page, we might have scrolled too far or not far enough to have the anchor located just below the header; - if we navigate to a new URL using any means other than clicking on an anchor tag, our history will have the URL with the special hash",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544684545:341,hash,hash,341,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544684545,1,['hash'],['hash']
Security,"OK, so, this really feels like bad data. We just merged https://github.com/hail-is/hail/commit/98adcce1d07001995b0819fd6afe161bf34ba840 which fixed https://github.com/hail-is/hail/issues/13979 . Google Cloud Storage's Java library was very rarely returning just flat-out bad data. The frequency of occurrence on one particularly large pipeline appears to be 1/30000 tasks (0.003% or 3 in 100,000). The tasks were reading two files, the larger of which was 131MiB. The Java library reads in 8MiB chunks so that's at least 17 network requests per partition. That puts the frequency of this closer to 1 in 1,000,000 requests or 1 in 10TiB of data read. Before we had Zstandard, it seems that this data corruption either (a) was unnoticed (b) caused a rare decoding error or (c) caused segfaults. After we added Zstandard (0.2.119), decompression often failed due to corrupt data. It seems to me that Zstandard more aggressively verifies integrity than LZ4 does. OK, so, when was this bug introduced in Hail? As far as I can tell, this new code path was added in google-cloud-storage 2.17.0 almost one year ago: https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77 . We upgraded to 2.17.1 ( ) in Hail 0.2.109 https://github.com/hail-is/hail/commit/fec0cc2263c04c00e02cef5dda8ec46916717152 . All of the attempts above could have been plagued by this rare transient data corruption error. OK, action items:. - [ ] Ask Cal and Lindo to try their pipelines again with the next release of Hail 0.2.127.; - [x] Hail must introduce large-scale testing before releases. We, sadly, cannot assume our underlying storage libraries are reliable. https://github.com/hail-is/hail/issues/14082. Once the first action item is successfully completed, I will close this issue. For the second action item, I have created a separate ticket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114:934,integrity,integrity,934,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114,1,['integrity'],['integrity']
Security,"OK, so. - 401 unauthorized when you don't have a valid oauth2 token; - set env var in notebook indicating hail token location (we can't mount to user's home dir because we do not know which user name the image will run as); - rebased. @akotlar we seem to be down to one key difference of opinion:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? I'd argue that your API is made public through documentation and web links, not through your error code. The people who don't read those shouldn't have an easier time learning of them. agree: api is in GH, ergo public, so only point of contention is:. > The less information reveled the better: as you mentioned, do you want foreign agents who don't already know of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Point: its not just foreign agents but anyone who hits the API, including us making mistakes, ergo, I reformulate:. > ... do you want [someone] who [forgot about or is unaware] of your endpoint to learn that you serve it? ... The people who don't read those shouldn't have an easier time learning of them. Yes, because I know I will make mistakes (and users will make config mistakes) and I want an easily debuggable system. The risk is that an attacker may learn `/jobs` exists. If that knowledge substantially improves an attacker's ability to infiltrate batch, then we've made a severe error in securing batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146:1391,attack,attacker,1391,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483791146,6,"['attack', 'secur']","['attacker', 'securing']"
Security,"OK, the story is more complicated than I imagined. uniroot was added in post-0.1 devel and made available in the expression language. It hasn't been exposed in the Python interface, but I don't know why. It is straightforward now, but I don't think the IR story has been sorted out yet. I'm going to reopen until it is available in Python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826:149,expose,exposed,149,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388854826,1,['expose'],['exposed']
Security,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706:220,validat,validate,220,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706,2,['validat'],['validate']
Security,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:151,secur,secureOnly,151,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121,1,['secur'],['secureOnly']
Security,"Ok, figured out why Dan's image wasn't wholly working; the auth_request works, but then there are series of other requests, which lose the access_token. Will figure this out; Ideally once the initial authorization is made, requests for the nth css file don't require it. I've also enabled ssl for app.hail.is, and notebook-api.hail.is, so any issues due to crossing https to http (and vice versa boundaries), won't crop up (if they didn't exist in an existing implementation, since all existing hail services are behind ssl).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460094726:200,authoriz,authorization,200,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460094726,1,['authoriz'],['authorization']
Security,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:676,secur,security,676,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842,1,['secur'],['security']
Security,"Oof, good catch! The thing we're trying to avoid is `e^x` overflowing for large positive `x`. In double precision, the smallest `x` that overflows is 710. So to test that we handle overflow correctly, you can check `sigmoid(710) == 1.0` and `sigmoid(-710) == 0.0` (using approximate equality). Actually, after playing with this, if you just use the simple definition `sigmoid(x) = 1 / (1 + np.exp(-x))`, then `sigmoid(-710)` does overflow, but it returns the right answer since `np.exp(710)` returns `inf`, and `1 / inf == 0.0`. But `math.exp(710)` throws an exception. `hl.exp` seems to have the numpy behavior, so I think the simple version actually works. But we should add the above test. I think wrapping this in an exposed function is a good idea. I agree it should be called `expit`, both for consistency with scipy, and because as you say, `sigmoid` really just means an S shaped function. And if we do expose `expit`, we should probably expose its inverse `logit` too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244:721,expose,exposed,721,https://hail.is,https://github.com/hail-is/hail/pull/10606#issuecomment-866034244,6,['expose'],"['expose', 'exposed']"
Security,Other to-do items are to make sure the stack has tests for authorization for all new endpoints in `test_batch.py` in the corresponding PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1898818963:59,authoriz,authorization,59,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1898818963,1,['authoriz'],['authorization']
Security,"PkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""2612958c-ef4e-4678-8482-29726290ae20"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:9117,encrypt,encryptionSettings,9117,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['encrypt'],['encryptionSettings']
Security,"Possibly related: https://github.com/erdewit/nest_asyncio/issues/22#issuecomment-1300570745. If Ben W is using asyncio in *his* code, it seems likely we'll end up with unpatched tasks. If this is the problem, we should expose an async implementation of hailtop.batch that he can use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1833938619:219,expose,expose,219,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1833938619,1,['expose'],['expose']
Security,Problem with py4j conversion of Python dict to `java.util.HashMap` instead of `scala.collection.immutable.Map`. Tested locally.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4073#issuecomment-410049172:58,Hash,HashMap,58,https://hail.is,https://github.com/hail-is/hail/pull/4073#issuecomment-410049172,1,['Hash'],['HashMap']
Security,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:133,authoriz,authorize,133,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232,1,['authoriz'],['authorize']
Security,"Put **Notes** after example. Suggested rewrite:. ""This method registers new global annotations in the VDS. These annotations can then be accessed through expressions in downstream operations. The Hail data type must be provided and match the type of the Python object.""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085:137,access,accessed,137,https://hail.is,https://github.com/hail-is/hail/pull/1501#issuecomment-284859085,1,['access'],['accessed']
Security,"Regarding history and fake pages: Im confused as to why fake pages would be used, since upon refresh that fake page wouldnt correspond to a real page, but this shouldnt interfere. The behavior without this solution should be the same: the url is updated with a hash. If youve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Havent seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:264,hash,hash,264,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789,1,['hash'],['hash']
Security,"Regarding permissions. No problems except when creating folders. It appears to me that the folder is being created as separate bucket, rather than as an object in the bucket:. """"""; ...; exceptions.from_http_response(response) google.api_core.exceptions.Forbidden: 403 GET https://www.googleapis.com/storage/v1/b/untitled-folder?projection=noAcl: user-nrru16jaxrwmnzkv5f35xfibg@hail-vdc.iam.gserviceaccount.com does not have storage.buckets.get access to **untitled-folder**.; """""". There is an open issue describing this problem: https://github.com/src-d/jgscm/issues/13. I found an interesting ""solution"": first click on an already-created folder (an `.ipynb_checkpoints` folder is created when you create a python file, that works); this populates the path with /bucket_name/folder. Back up to ../ and create a new folder, voila. . <img width=""1226"" alt=""Screenshot 2019-04-04 22 50 30"" src=""https://user-images.githubusercontent.com/5543229/55601098-1640c880-572d-11e9-8afb-ac000040962d.png"">. So this seems like something that could be fixed in jgscm, or potentially by setting ""--notebook-dir"" in addition to ""GoogleStorageContentManager.default_path"". (""default_path"" is used, because apparently setting [""--notebook-dir"" to set the root directory to the chosen bucket doesn't work])(https://github.com/src-d/jgscm#usage).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480132253:444,access,access,444,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480132253,1,['access'],['access']
Security,"Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what https://github.com/hail-is/hail/pull/5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459893784:295,expose,expose,295,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459893784,1,['expose'],['expose']
Security,"Remarkable, thank you! Is this exposed in the most recent version of Hail , 0.2.128 ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1967660767:31,expose,exposed,31,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1967660767,1,['expose'],['exposed']
Security,"Right I understand, so Hail value refers to something Hail needs to access, with the modifier ""off-heap"". If a PType always places its items into one region, that doesn't seem problematic.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575731116:68,access,access,68,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731116,1,['access'],['access']
Security,"Right, but my concern is that relies on access to the original dataset. And who knows if columns got filtered or what now. I think something that preserves the state in the directory is the right thing to do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5208#issuecomment-457761043:40,access,access,40,https://hail.is,https://github.com/hail-is/hail/pull/5208#issuecomment-457761043,1,['access'],['access']
Security,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:29,Secur,Security,29,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731,3,['Secur'],"['Security', 'Security-Beginners-Guide']"
Security,"See the separate document in `team` about firewall-rules. In particular, the default network will block all connections.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7978#issuecomment-578967196:42,firewall,firewall-rules,42,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578967196,1,['firewall'],['firewall-rules']
Security,Snyk is failing because it suddenly realized that we have dependencies (#security  ). I will fix those issue separately.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1707206872:73,secur,security,73,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1707206872,1,['secur'],['security']
Security,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:529,expose,expose,529,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240,2,['expose'],['expose']
Security,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:573,threat,threat,573,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125,1,['threat'],['threat']
Security,"So then, loadLength would take instead of a Long, a RegionOwnedAddress, which would say be a (region, Int) tuple correct? Because I'm not exactly sure what Hail values we are referring to in this context, unless we mean ""a Scala value that points to some data that the Hail program needs to access"" (since currently we pass around memory addresses that are Scala primitives, and call methods on PTypes...so I thought the proposal was to give the PType management of regions, so that the caller would not need to think about this, which is closer to what I had envisioned).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333:291,access,access,291,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333,1,['access'],['access']
Security,"Some more comments on this array casting business:; - We could have made Array{Ref, Len} work on any container. That would fix most uses of CastToArray, but it would still leave out the cast that we genuinely want to convert a set/dict to an array. I don't think we ever do that, but a user could with hl.array.; - It is partially an accident that dict, set and array use the same representation. We could imagine others, e.g. a hashtable for set. In which case, the cast wouldn't work, the ArrayLen probably would, and ArrayRef might or might not.; - Array{Ref, Len} could apply to streams, too (they'll need custom codegen tho).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8171#issuecomment-592220580:429,hash,hashtable,429,https://hail.is,https://github.com/hail-is/hail/pull/8171#issuecomment-592220580,1,['hash'],['hashtable']
Security,"Somehow our CI server started using 8-char hashes in some places, and 7 in others. Should have this fixed later today!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2217#issuecomment-328882377:43,hash,hashes,43,https://hail.is,https://github.com/hail-is/hail/issues/2217#issuecomment-328882377,1,['hash'],['hashes']
Security,"Somewhat unrelatedly though, this seems like it would be a natural replacement for our copy-paste token (or identity delegation as the AUS folks do), as you could do the following:. 1. `hailctl auth print-access-token | pbcopy`; 2. On some notebook where you don't have access to your hail identity for some reason, do `hl.init_batch(token=)` or `hl.ServiceBackend(token=)`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785471404:205,access,access-token,205,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785471404,2,['access'],"['access', 'access-token']"
Security,"Sorry I missed your message! The code as written now is plainly wrong: we access a mutable map from two threads without synchronization. We need this change regardless of how it affects error messages. If the tests pass, I'm confident this is fine. Are there components of the system you don't think are well tested by our tests?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471:74,access,access,74,https://hail.is,https://github.com/hail-is/hail/pull/13546#issuecomment-1724475471,1,['access'],['access']
Security,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:277,expose,expose,277,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619,1,['expose'],['expose']
Security,Stored under notebook-secrets : data.authorized-users,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6265#issuecomment-499142882:37,authoriz,authorized-users,37,https://hail.is,https://github.com/hail-is/hail/pull/6265#issuecomment-499142882,1,['authoriz'],['authorized-users']
Security,"Thanks @vladsaveliev, I'll authorize this so we can run it through CI and try to get it through.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10382#issuecomment-831338774:27,authoriz,authorize,27,https://hail.is,https://github.com/hail-is/hail/pull/10382#issuecomment-831338774,1,['authoriz'],['authorize']
Security,"Thanks Tim!. It is easy to fix the Kryo serializers, but the function we need (require) is protected, so we need to use reflection or put something in the Kryo package to access it. Java serializers are trickier. Best I can think of is to have a thread-local pool of something like 8KB blocks to copy to/from.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2218#issuecomment-332209074:171,access,access,171,https://hail.is,https://github.com/hail-is/hail/pull/2218#issuecomment-332209074,1,['access'],['access']
Security,"Thanks for working on this!. Would it be possible to keep the logout button on every page but add a step where it takes the user to the Auth UI to make it work? Specifically, I'm thinking we could add logic to the `/user` route in `auth/auth/auth.py` such that if we pass in the query parameter `logout`, it calls the same code as the `/logout` endpoint, and then replace the `form` and `button` with something like:. ```html; <a href=""https://auth.hail.is/user?logout"">Log out</a>; ```. The tricky part of that might be getting the CSRF token, but since the `/user` page is only accessible by logged in users (because of the `authenticated_users_only` decorator), I *think* there should always be a CSRF token accessible via `request.cookies[""_csrf""]` (e.g. https://github.com/hail-is/hail/blob/main/web_common/web_common/web_common.py#L93).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2258267657:580,access,accessible,580,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2258267657,2,['access'],['accessible']
Security,"Thanks guys. Does using cloudtools get the latest build from master, or how can I access this to use?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5436#issuecomment-468746637:82,access,access,82,https://hail.is,https://github.com/hail-is/hail/pull/5436#issuecomment-468746637,1,['access'],['access']
Security,Thanks! That helped. There a couple of other issues that came up that required some tinkering. I list them below in case any one runs into them also. 1. curl failed when trying to download the ibsimdpp lib. The workaround was to download it with wget and move it to the Make Directory. ```; wget --no-check-certificate https://storage.googleapis.com/hail-common/libsimdpp-2.0-rc2.tar.gz; mv libsimdpp-2.0-rc2.tar.gz src/main/c; ```. 2. Needed to compile with newer version of gcc; ```; module load gcc/7.2.0; ./gradlew -Dspark.version=2.2.1 -Dpy4j.version=0.10.4 -Dbreeze.version=0.13.1 shadowJar archiveZip. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411:307,certificate,certificate,307,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375979411,1,['certificate'],['certificate']
Security,"Thanks, added with one tweak. Sadly I don't know how to convince your code analyser that using `randint` to make test cases in test code is not a security issue. Feel free to push to PR branches directly, or just to add things while merging. You folks are the Hail maintainers after all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2400960303:146,secur,security,146,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2400960303,1,['secur'],['security']
Security,"Thanks. @zaczap has a branch with a function exposed that uses it, so may want him in this to comment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1717#issuecomment-388877364:45,expose,exposed,45,https://hail.is,https://github.com/hail-is/hail/issues/1717#issuecomment-388877364,1,['expose'],['exposed']
Security,"The Google docs aren't clear about whether the hash needs to be a suffix or prefix: . > add the hash of the sequence number as part of the object name to make it non-sequential. I'm somewhat hesitant to make this change since it means our part outputs are no longer sorted lexicographically, and this property has been very useful in the past.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10836#issuecomment-914241609:47,hash,hash,47,https://hail.is,https://github.com/hail-is/hail/pull/10836#issuecomment-914241609,4,['hash'],['hash']
Security,"The Table ones are kinda broken because they get reordered because the `Struct` constructor hash-orders the fields. This means that `df.row.dtype != df.schema`, which is bad. We can fix this by moving to python 3",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2787#issuecomment-359798042:92,hash,hash-orders,92,https://hail.is,https://github.com/hail-is/hail/pull/2787#issuecomment-359798042,1,['hash'],['hash-orders']
Security,"The `*` means that route will be triggered for any request matching the specified URL for any method, be it GET or POST, etc. The reason I needed to make that change is that when envoy makes an authentication request to that endpoint, it uses the HTTP method of the original request. E.g. If I make a POST to https://internal.hail.is/dgoldste/batch/batches/create envoy will authenticate me with a POST request to auth:443/api/v1alpha/verify_dev_credentials. So I can't set that endpoint to be any one method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1282998946:194,authenticat,authentication,194,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1282998946,2,['authenticat'],"['authenticate', 'authentication']"
Security,"The current execution of ; ```; mt.group_rows_by(mt.gene); .aggregate(...); ```; will be emitted as a `MatrixMapRows` (to re-key) followed by a `MatrixAggregateRowsByKey`. This means that the dataset will be shuffled _in full_ to re-sort by gene, before doing the efficient collapsing in `MatrixAggregateRowsByKey`. This is really bad. We need to be doing map-side combines. The preferred execution would be one of two options:; 1. scan to compute the OrderedPartitioner for the new key. Aggregate to this partitioner.; 2. Aggregate to a HashPartitioner. Both of these things involve new map-side combiner architecture which we haven't built yet, but this is important.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3645#issuecomment-391497730:538,Hash,HashPartitioner,538,https://hail.is,https://github.com/hail-is/hail/issues/3645#issuecomment-391497730,1,['Hash'],['HashPartitioner']
Security,"The default is to put the modules in ${HOME}/hail_modules, which in many; environments; would indeed be an NFS directory. It only goes to /tmp/hail_modules if; ${HOME} is undefined. The thinking behind this is that there's a huge amount of re-use of code; for an individual; from one Hail analysis to the next, but probably much less overlap between; different users.; And while multi-user sharing ought to work, it raises potential issues; about file access; permissions which seemed like trouble without a clear benefit. On Fri, Aug 3, 2018 at 10:49 AM Patrick Schultz <notifications@github.com>; wrote:. > *@patrick-schultz* commented on this pull request.; > ------------------------------; >; > In src/main/c/NativeModule.cpp; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>:; >; > > +#include <string>; > +#include <vector>; > +; > +#if 0; > +#define D(fmt, ...) { \; > + char buf[1024]; \; > + sprintf(buf, fmt, ##__VA_ARGS__); \; > + fprintf(stderr, ""DEBUG: %s,%d: %s"", __FILE__, __LINE__, buf); \; > +}; > +#else; > +#define D(fmt, ...) { }; > +#endif; > +; > +namespace hail {; > +; > +namespace {; >; > The anonymous namespace can't be named, so no names introduced in an; > anonymous namespace can be referenced from outside the namespace. The; > exception is that on closing an anonymous namespace, it is automatically; > opened into the enclosing namespace. The typical use is to make things; > file-local.; >; > ; > You are receiving this because you modified the open/close state.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/3973#discussion_r207567962>, or mute; > the thread; > <https://github.com/notifications/unsubscribe-auth/AJzExuxprnaVF62eonAgCjSmqAERvBJiks5uNGLtgaJpZM4VbZpP>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410321049:452,access,access,452,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410321049,2,['access'],['access']
Security,"The failure here:; ```; E org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 8) (hostname-c5956f6f02 executor driver): java.io.EOFException: Invalid seek offset: position value (6) must be between 0 and 6 for 'gs://hail-services-requester-pays/hello'; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.validatePosition(GoogleCloudStorageReadChannel.java:665); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.position(GoogleCloudStorageReadChannel.java:546); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFSInputStream.seek(GoogleHadoopFSInputStream.java:178); E 	at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:65); ```. I hit this same error in Avro/GVS work recently -- I think the Google Hadoop API connector is wrong in that you cannot seek to the end of a file (N where N is the number of bytes in the file).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700:462,validat,validatePosition,462,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1245585700,1,['validat'],['validatePosition']
Security,The fix landed after the tagged 0.2.65 release. Can you share the git commit hash that's failing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311:77,hash,hash,77,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311,1,['hash'],['hash']
Security,The hashes are appended to the end of the filename. Shouldn't that be enough to ensure distinct names?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13502#issuecomment-1693813463:4,hash,hashes,4,https://hail.is,https://github.com/hail-is/hail/issues/13502#issuecomment-1693813463,1,['hash'],['hashes']
Security,"The interface needs some work, first, but this is probably a ~3 month timeline (the outstanding calls into java are for things like maximal_independent_set, the BlockMatrix linear algebra stuff, and a few utility functions). I'm also happy to take PRs now to change the java.util.HashMaps to java.util.Map",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896:280,Hash,HashMaps,280,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463827896,1,['Hash'],['HashMaps']
Security,The logs you're seeing are the cloud audit logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-833644335:37,audit,audit,37,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-833644335,1,['audit'],['audit']
Security,"The main point of this is to isolate the accessors in one place to make changing them easier, e.g. if we want to move the missing bits in structs to pack them more tightly, or make fields required, which is now almost trivial.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2093#issuecomment-321729305:41,access,accessors,41,https://hail.is,https://github.com/hail-is/hail/pull/2093#issuecomment-321729305,1,['access'],['accessors']
Security,The missing permission is `storage.buckets.get` though? It seems reasonable for a user to [be able to read metadata](https://cloud.google.com/storage/docs/access-control/iam-permissions) about their own bucket. I'd wager that jgscm was designed for use with the `roles/storage.legacyBucketWriter` role granted on their bucket. What role are we currently granting?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480372530:155,access,access-control,155,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480372530,1,['access'],['access-control']
Security,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:419,attack,attack,419,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474,1,['attack'],['attack']
Security,The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845:21,expose,expose,21,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674986845,1,['expose'],['expose']
Security,"The reason we use `java.util.HashMap` there is that the [py4j](https://www.py4j.org/) library will automatically convert Python dicts to `java.util.HashMap` objects. . I don't think you should call this method, though. We are in the process of completely separating the front end Python from the back end execution engine. Soon, all IR execution from Python will be done by calling [`Backend.execute`](https://github.com/hail-is/hail/blob/master/hail/python/hail/backend/backend.py). The `SparkBackend.executeJSON` should be the target for IR execution right now, I think. *Edit: s/LocalBackend/SparkBackend/*",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5340#issuecomment-463308529:29,Hash,HashMap,29,https://hail.is,https://github.com/hail-is/hail/issues/5340#issuecomment-463308529,2,['Hash'],['HashMap']
Security,The script in question is located at: gs://danking/1_Generate_Variant_Stats_NVXvC_v1.py . Ping me if you need access.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741:110,access,access,110,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791061741,1,['access'],['access']
Security,"The security issues with sharing CI pages is a bit frustrating for external committers like yourself. I've picked up this PR here: https://github.com/hail-is/hail/pull/10220 and I'll get it to tests passing. We'll keep working towards making the CI pages open to the public. Once that PR is done, I'll have you push the commits to this branch and get it merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-806046631:4,secur,security,4,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-806046631,1,['secur'],['security']
Security,"The test `testMatrixUnionRowsMemo` is still failing with this change. The error that I am seeing is:; ```; java.util.NoSuchElementException: key not found: RefEquality(MatrixMapRows(MatrixMapRows(MatrixLiteral(...),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8]))))),InsertFields(SelectFields(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),WrappedArray(rk, r2, r3)),List((the entries! [877f12a8827e18f61222c6c8c5fb04a8],GetField(Ref(va,struct{rk: int32, r2: struct{x: int32}, r3: array<struct{rr: int32}>, `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`: array<struct{e1: float64, e2: float64}>}),the entries! [877f12a8827e18f61222c6c8c5fb04a8])))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:32); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:47); 	at is.hail.expr.ir.PruneSuite$$anonfun$checkMemo$1.apply(PruneSuite.scala:46); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.PruneSuite.checkMemo(PruneSuite.scala:46); 	at is.hail.expr.ir.PruneSuite.testMatrixUnionRowsMemo(PruneSuite.scala:412); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952:1381,Hash,HashMap,1381,https://hail.is,https://github.com/hail-is/hail/pull/4891#issuecomment-444264952,2,['Hash'],['HashMap']
Security,"The test is fixed, we don't use encryption to talk to the internal gateway. I had to fix https.client_session to handle the gce case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9862#issuecomment-758202690:32,encrypt,encryption,32,https://hail.is,https://github.com/hail-is/hail/pull/9862#issuecomment-758202690,1,['encrypt'],['encryption']
Security,"The to-do list is roughly. - [ ] Get genome reference type in Variant, Interval, and Locus constructors in function registry; - [ ] Add default reference to HailContext ; - [ ] Add default reference to a bunch of RDD methods; - [ ] Expose genome reference in Python interface with import methods and as an input argument to TVariant(), etc.; - [ ] Make sure #2226 solves the problem of variant, locus, and interval methods having the correct function generated dependent on the genome reference; - [ ] Double check that if a user adds a new genome reference, it is visible on all workers.; - [ ] Add GenomeReference python class to documentation; - [ ] Convert GenomeReference -> ReferenceGenome (Jon's request); - [ ] Remove methods from Variant that do not take a GenomeReference as a parameter (Right now, everything is still hardcoded as GRCh37). I vaguely remember some debate on whether these functions should be removed from Variant completely and instead only called from GenomeReference.; - [ ] At some point, we may want to change Variant etc. so they store the contigIndex rather than the contig.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2208#issuecomment-332347245:232,Expose,Expose,232,https://hail.is,https://github.com/hail-is/hail/pull/2208#issuecomment-332347245,1,['Expose'],['Expose']
Security,"There is no requirement to support many OSes. We only have to support two platforms: recent Linux and OSX. They have variants but should be mutually compatible. Testing on Dataproc + recent version of Ubuntu or Debian (which we do in the CI) seems fine. OSX has flags for version support, we can just pick a version (10.10, say) and build for that and beyond. As I've said before, we control what we support and there is no need to make this a burden on ourselves. > If you build your own compiler + library, then you risk becoming incompatible with other libraries. Yes, we should ship with all our C++ dependencies. You convinced me of this? Then there is no issue. BLAS is a C library, so no issue there. Right now I think that just means the compiler and the standard library. > Probably not something I could do in the limited time available. Fair. I'm happy with partial progress in in the above direction, but this seems like a step backwards and something we will want to revert soon. I'm not inclined to go in this direction. Not having access to the standard library seems problematic. > Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1 on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). Did you test submitting jobs to the cluster itself? This can be quite a different environment than the tests. Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we submit cluster_sanity_check.py with and without C++ codegen enabled?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448:1046,access,access,1046,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424759448,1,['access'],['access']
Security,There were no other open holes from the Security Notes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/675#issuecomment-245383880:40,Secur,Security,40,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383880,1,['Secur'],['Security']
Security,"This also means all jobs need to be using a recent version of Hail so that they know how to use access tokens, right? Which version started supporting that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14059#issuecomment-1836781021:96,access,access,96,https://hail.is,https://github.com/hail-is/hail/pull/14059#issuecomment-1836781021,1,['access'],['access']
Security,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:676,checksum,checksum,676,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,1,['checksum'],['checksum']
Security,"This exposed a problem with typecheck - you can't check arguments that need to be the class you're defining, like `concordance` or `join`. . I marked these as `anytype` for now. I will ruminate on this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1826#issuecomment-301944755:5,expose,exposed,5,https://hail.is,https://github.com/hail-is/hail/pull/1826#issuecomment-301944755,1,['expose'],['exposed']
Security,This exposes a bug in the pruner in test_tdt that I'm still investigating.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5132#issuecomment-454277066:5,expose,exposes,5,https://hail.is,https://github.com/hail-is/hail/pull/5132#issuecomment-454277066,1,['expose'],['exposes']
Security,This fails currently. I want CI and eyes on this. Validation errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175:50,Validat,Validation,50,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175,1,['Validat'],['Validation']
Security,"This is all passing except check_batch is running in an image on python 3.6 and the worker is on python 3.7. The process initializer isn't exposed until 3.7. I think I need to change the base image, but am worried about breaking Hail and other dependencies. Otherwise, I can just pass the key file path and the project and create the credentials and gcs client each time a function is called rather than once per process. This will make it slower for small files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305:139,expose,exposed,139,https://hail.is,https://github.com/hail-is/hail/pull/8368#issuecomment-607588305,1,['expose'],['exposed']
Security,This is by design. The square bracket syntax is for joining two distinct matrix tables. To access a specific row and column do this:; ```; mt = mt.filter_rows(mt.row_key = 1); mt = mt.filter_cols(mt.col_key = 1); mt = mt.entry.collect()[0]; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9704#issuecomment-1145282437:91,access,access,91,https://hail.is,https://github.com/hail-is/hail/issues/9704#issuecomment-1145282437,1,['access'],['access']
Security,"This is caused by domain-by-domain CSRF tokens introduced in [#14180](https://github.com/hail-is/hail/issues/14180). An unfortunate side effect is that the tokens available on non-auth pages are no longer able to validate requests to the auth/logout API. Given the lack of apparent noise about this bug in our issues and zulip I suspect that this is not a common path for users, and that a fix along the lines of ""require add one button click to go to the User page first before logging out is acceptable"". On the other hand, the risk of a user clicking on the broken Logout button and believing themselves to be logged out when seeing a `401: Unauthorized` page (but actually still having logged-in state in their browser) raises this in my mind to a security bug rather than just a UX bug or an unfortunate user experience. Therefore my proposal is:; 1. To fix the bug as soon as possible; 2. Accept an additional redirect in a user flow which is rarely exercised; 3. To make the smallest number of potentially risky changes to the underlying security architecture; 4. Therefore: Remove the broken ""log out"" link in page headers and replace with a Log out button on the auth[...]/users page which is guaranteed to have the correct CSRF token in state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187:213,validat,validate,213,https://hail.is,https://github.com/hail-is/hail/issues/14635#issuecomment-2253086187,6,"['secur', 'validat']","['security', 'validate']"
Security,"This is due to FSs in `hailtop.fs` never getting closed. Unfortunately we exposed functions on a module, not a context manager. Options include; 1. adding a `hailtop.fs.close` method; 2. Using a new `RouterFS` on every `hailtop.fs` method; 3. Have users instantiate a `RouterFS` as a context manager and use that. Among these options I prefer 2 and 3. I think using a standalone function instead of properly allocating a context manager can be a convenience/performance tradeoff. 3 could use a bit of thought though as it adds more user-facing API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020:74,expose,exposed,74,https://hail.is,https://github.com/hail-is/hail/issues/14280#issuecomment-1936521020,1,['expose'],['exposed']
Security,"This is good. We should do the same thing with IntIterator, etc. (Maybe call it `SIterator` for specialized iterator?). A few remarks:. - Looking over the bytecode, making the members private[this] makes the bytecode much tighter since it doesn't generate accessor methods for b and size_. I'll make a quick PR for this. - Even tho ArrayBuilder is invariant, the specialized versions extend ArrayBuilder[Object] and implement all the generic, unspecialized methods. That worries me, but I don't know why it would ever get called. Maybe for backward compatibility to code compiled without specialization?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175:256,access,accessor,256,https://hail.is,https://github.com/hail-is/hail/pull/1549#issuecomment-287082175,1,['access'],['accessor']
Security,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments.  Options ;  --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified ;  shell. ;  [default: None] ;  --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified ;  shell, to copy it or customize the ;  installation. ;  [default: None] ;  --help Show this message and exit. ; ;  Commands ;  batch Manage batches running on the batch service managed by the Hail team. ;  config Manage Hail configuration. ;  curl Issue authenticated curl requests to Hail infrastructure. ;  version Print version information and exit. ; ; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS.  Arguments ;  * script PATH Path to the script [default: None] [required] ;  arguments [ARGUMENTS]... [default: None] ; ;  Options ;  --files PATH Files or directories to add to the working directory of the ;  job. ;  [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job ;  [default: (hailgenetics/hail)] ;  --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:909,authenticat,authenticated,909,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,1,['authenticat'],['authenticated']
Security,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:535,access,access,535,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['access'],['access']
Security,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:1647,access,access,1647,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731,2,['access'],['access']
Security,"This refers to setting up a fresh k8s cluster. In particular, when you're creating a k8s cluster from scratch, you need to create an access token to grant CI (or gateway, if gateway were to proxy requests to GitHub for CI) access to the PR ""merge"" endpoint. There are a couple other things like read access to our google buckets. These can all be generated by anyone with a sufficiently privileged GCP account and a broad login in the `hail` unix group (there are some credentials stored in cotton's home directory on the broad file system).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131:133,access,access,133,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449082131,3,['access'],['access']
Security,"This seems like the right approach. The other option is to expose the password in the config which we are trying to avoid. So ... even this is not idempotent. The current version of batch doesn't guarantee that children (or multiple retries of the same child) get the same or consistent set of files from their parents. This is because, while an attempt may have succeeded, there may be another attempt that is pending that succeeds and overwrites the outputs of the original successful attempt. I fixed this in my google batch backend by storing attempt outputs in per-attempt directory: /path/to/scratch/files/job_id/attempt_id/file. Then each child got the successful attempt (there could be multiple, but the driver selected one and only one) for each parent and that was used to localize the inputs. So this good enough and we should plan to add attempt consistency to Batch in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428:59,expose,expose,59,https://hail.is,https://github.com/hail-is/hail/pull/8833#issuecomment-631473428,2,"['expose', 'password']","['expose', 'password']"
Security,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:61,access,accessible,61,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056,2,['access'],['accessible']
Security,"This should be functioning (or very close) in GCP but is basically unimplemented in Azure. Azure needs a secret store like how we use Google Secret Manager that workers can access when they start up to load certificates. Azure Key Vault seems reasonable and can be created through terraform. The structure should mirror that in GCP, where we need a client that can upload the certs to Azure in `create_certs` and download them in the azure CloudWorkerAPI. I would leave this unimplemented in TerraAzure until an overall secrets story is established.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552:173,access,access,173,https://hail.is,https://github.com/hail-is/hail/pull/14581#issuecomment-2243030552,2,"['access', 'certificate']","['access', 'certificates']"
Security,"This should now be passing tests. I had to make one LocalBackend test conditional on docker, because we do not have docker in docker (DiD), and there were historically some security considerations that felt out of scope to fully grapple with in this PR (out of scope to create a DiD image and have test_hailtop_batch and test_batch_dcos use this DiD image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9219#issuecomment-670288861:173,secur,security,173,https://hail.is,https://github.com/hail-is/hail/pull/9219#issuecomment-670288861,1,['secur'],['security']
Security,This was for a different purpose - it was to expose the globals as a separate one row table. But I don't think that's necessary and am happy to close,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2970#issuecomment-408638988:45,expose,expose,45,https://hail.is,https://github.com/hail-is/hail/issues/2970#issuecomment-408638988,1,['expose'],['expose']
Security,"True. I thought at least for the copy-paste tokens that this would be intentional. Looks like you can get access tokens from GCP that last up to 12 hours, but that could be insufficient for large workloads. If we need something arbitrarily long-lived, our current implementation might be our best bet short of some better integration with OIDC.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525:106,access,access,106,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525,1,['access'],['access']
Security,Turns out you need a key or a password. Closing this for now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13653#issuecomment-1883522250:30,password,password,30,https://hail.is,https://github.com/hail-is/hail/pull/13653#issuecomment-1883522250,1,['password'],['password']
Security,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242074915:302,access,accessing,302,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915,1,['access'],['accessing']
Security,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:617,hash,hash,617,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,2,['hash'],['hash']
Security,Very reasonable! I think that makes a lot of sense and am glad we'll have the extra security.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736214789:84,secur,security,84,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736214789,1,['secur'],['security']
Security,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:41,inject,injecting,41,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048,1,['inject'],['injecting']
Security,"We are trying to setup hail `0.2.72` on spark `3.1.2` version. However, we are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:932,access,access,932,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,1,['access'],['access']
Security,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:126,secur,security,126,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362,1,['secur'],['security']
Security,We changed the name of this function from `import_keytable` to `import_table` with the changes for the 0.1 stable build yesterday. Is your Hail code out of date? Try running:; ```python; >>> hc.version; ```; If it says `devel-<git hash>` instead of `0.1-<git hash>` it's out of date.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-301725415:231,hash,hash,231,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725415,2,['hash'],['hash']
Security,"We don't really have the infrastructure to create arbitrary values in c++ right (without translating into potentially large trees of IR). Since they already exist as Scala annotations in the IR, I decided that it was easier to serialize them in scala and decode them in the function rather than try to create logic and encode them in the function itself (we can't create them at compile-time and pass around naked hail-values, since they're region-backed and the functions can get serialized and shipped to workers, which don't have access to those). I put the decoded literals on a SparkFunctionContext in the hopes that it will be easier to plug in another literal broadcaster if/when the `Literal` IR no longer holds a Scala annotation, but in the meantime this was a reasonably straightforward way to get them into the c++ Emit code. I think we may not need to serialize literals like this for the `cxx.Compile.apply` methods, since they should all execute on the master node, but we don't currently enforce that so I was serializing there just to be safe. The alternative is to enforce execution of these functions on the master node, which I don't see a huge problem with off the top of my head? @cseed do you have feelings about this? All the distributed computation is going to go through `CollectDistributedArray` and friends, which should create their own entrypoints.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874:533,access,access,533,https://hail.is,https://github.com/hail-is/hail/pull/5617#issuecomment-474522874,1,['access'],['access']
Security,"We now use an API token linked to hailgenetics instead of password auth. In order to do the org request, Daniel set up 2FA. There's no way to set up 2FA. There are recovery codes in the ""usual place"". No one besides Daniel can currently log into hailgenetics PyPI account as a result (other than using recovery codes).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767:58,password,password,58,https://hail.is,https://github.com/hail-is/hail/issues/13598#issuecomment-1808759767,1,['password'],['password']
Security,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:266,secur,security,266,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073,1,['secur'],['security']
Security,"We've recently updated from 0.2.126ish to 0.2.130ish, and encountered some teething issues with the new (to us) metadata server. Jobs using `gsutil` failed as their attempts to get credentials from the server resulted in 404. There seems to have been two problems:. 1. As shown (also via `curl`) in [batch 454410](https://batch.hail.populationgenomics.org.au/batches/454410/jobs/1), our `gsutil` queried for `http://169.254.169.254/computeMetadata/v1/instance/service-accounts` (without a final `/`) which resulted in a 404. I don't know if there's a more elegant way for the server to accept both, rather than just adding a route with and without. 2. With that fixed, [batch 454418](https://batch.hail.populationgenomics.org.au/batches/454418/jobs/1) shows a failure within `GetInstanceScopes()`. This is failing because the metadata server does not implement the `/scopes` endpoint. PR #14019 implemented only so much as is needed for `hail` and `gcloud` to get access tokens for hail GSAs so they can then make API calls to GCS or Hail Batch, but we seem to have needed a bit more. Not sure why you didn't encounter this yourselves: possibly sufficiently different versions of `gsutil` or the cloud SDK, or perhaps you are better at remembering to use `gcloud` rather than `gsutil` than we are!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331:965,access,access,965,https://hail.is,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331,1,['access'],['access']
Security,"What do you mean by validation? . For the 'annotation line' are you suggesting a general error-catching wrapper? I actually really like that, and I'll give it a go. > CNV work; > What I want to do with CNVs is something like ; > ; > ```; > val files: Array[String]; > sc.paralellize(files); > .map { f => readTable(f, config...) }; > .,map (convert to a hail better cnv representation); > ```; > ; > Can't do that if readTable gives you an RDD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233007740:20,validat,validation,20,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233007740,1,['validat'],['validation']
Security,"What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7162#issuecomment-536317934:152,validat,validation,152,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536317934,1,['validat'],['validation']
Security,"What if all commands were proxied through the gateway api? We already have an authentication layer, with support for 2FA (though in an alpha state, doesn't handle fancier things like merging multiple social accounts). So permissions can be granted on a per-user basis. Currently we can define any scopes for our own APIs, or use Github's available API's (or pass both along to CI). . It may still be useful to have CI grab and validate a gateway-api generated token containing the necessary scopes, although this would add overhead, and may not be as necessary if CI is only available through the gateway api. Another alternative is that it validates against Auth0, but I'd like to reduce/remove that overhead if securely possible.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425:78,authenticat,authentication,78,https://hail.is,https://github.com/hail-is/hail/issues/4556#issuecomment-449069425,4,"['authenticat', 'secur', 'validat']","['authentication', 'securely', 'validate', 'validates']"
Security,Would be great if we could hash this out. This PR / fix is blocking getting the PR in that uses the AsyncFS everywhere.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10771#issuecomment-904091995:27,hash,hash,27,https://hail.is,https://github.com/hail-is/hail/pull/10771#issuecomment-904091995,1,['hash'],['hash']
Security,"Would it make sense to expose the `a` parameter, to make it easier to move between the three examples you showed? The `mixture` parameter could just be floating point rather than boolean, treating the default `mixture=0` case specially.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3206#issuecomment-375313469:23,expose,expose,23,https://hail.is,https://github.com/hail-is/hail/pull/3206#issuecomment-375313469,1,['expose'],['expose']
Security,"XT Files or directories to add to the working directory of the job. [default: None] ;  --name TEXT The name of the batch. ;  --image-name TEXT Name of Docker image for the job (default: hailgenetics/hail) [default: None] ;  --output -o [text|yaml|json] [default: text] ;  --help Show this message and exit. ; . (base) dking@wm28c-761 hail % ; (base) dking@wm28c-761 hail % hailctl hdinsight submit --help ; ; Usage: hailctl hdinsight submit [OPTIONS] NAME STORAGE_ACCOUNT HTTP_PASSWORD ; SCRIPT [ARGUMENTS]... ; ; Submit a job to an HDInsight cluster configured for Hail. ; If you wish to pass option-like arguments you should use ""--"". For example: ; ; $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog ; ;  Arguments ;  * name TEXT [default: None] [required] ;  * storage_account TEXT Storage account in which the cluster's container exists. [default: None] [required] ;  * http_password TEXT Web password for the cluster [default: None] [required] ;  * script TEXT Path to script. [default: None] [required] ;  arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] ; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:2583,password,password,2583,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['password'],['password']
Security,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:110,access,access,110,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615,1,['access'],['access']
Security,"Yes - although would that ensure they're unique indices? I had using the variants in a feature vector in mind, which each index err...indexing the vector. (Could also work with samples.). When I did it outside of hail, I hashed the v.contig/v.start values and then used a String Indexer to get unique index values.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1785#issuecomment-300283835:221,hash,hashed,221,https://hail.is,https://github.com/hail-is/hail/issues/1785#issuecomment-300283835,1,['hash'],['hashed']
Security,"Yes I agree, this is something the user should not be exposed. . > On Sep 18, 2018, at 8:00 AM, Tim Poterba <notifications@github.com> wrote:; > ; > I think we need to hide partitioning and do more automatic resizing / partition combining.; > ; > ; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/445#issuecomment-422364795>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAiuXQNQQSrESG44JjWYXH62FVg0Fks5ucOB2gaJpZM4I995P>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/445#issuecomment-422365142:54,expose,exposed,54,https://hail.is,https://github.com/hail-is/hail/issues/445#issuecomment-422365142,1,['expose'],['exposed']
Security,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:213,encrypt,encrypt,213,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158,1,['encrypt'],['encrypt']
Security,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:269,hash,hash,269,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859,1,['hash'],['hash']
Security,You gave me a heart attack when I saw it was approved ;),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12193#issuecomment-1249710272:20,attack,attack,20,https://hail.is,https://github.com/hail-is/hail/pull/12193#issuecomment-1249710272,1,['attack'],['attack']
Security,You'll note that I in `query/Makefile` the JAR_LOCATION for `default` is *not* the normal location. I don't want developers accidentally uploading garbage JARs to publicly accessibly SHAs. We should probably be setting retention holds on the JARs uploaded by CI.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11645#issuecomment-1075853803:172,access,accessibly,172,https://hail.is,https://github.com/hail-is/hail/pull/11645#issuecomment-1075853803,1,['access'],['accessibly']
Security,"] enable mTLS for all services; - [security] disable TLS <1.3; - [security] comply with Mozilla's ""modern"" recommendations; - [batch][security] use a separate network for batch's callbacks. ### liveness probes. Ah, that's a good point. I'll rewrite to use curl and the client's own certificate and I'll make sure clients trust themselves. ### root cert. I don't think it is possible in aiohttp to both verify a certificate has a valid chain from a root cert and, separately, exists in a list of trusted certificates. The effect would be that every client would trust every server because every server certificate is signed by the same root certificate. I think using a root cert is quite secure (a big improvement over our current situation!). However, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little har",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1007,certificate,certificate,1007,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,8,['certificate'],"['certificate', 'certificates']"
Security,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14532,secur,securityContext,14532,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['secur'],['securityContext']
Security,"` all behave as expected. ```; import hail as hl. giab_gs_path = 'gs://xxxxxxxx'; giab_ds = hl.import_vcf(giab_gs_path, reference_genome='GRCh38'); giab_sm = hl.SplitMulti(giab_ds); giab_ds = giab_sm.result(); giab_ds.describe(); gaib_ds.count(); gaib_ds._force_count_rows(); # all good. sent_gs_path = 'gs://xxxxxxxxxxx'; sent_ds = hl.import_vcf(sent_gs_path, reference_genome='GRCh38'); sent_sm = hl.SplitMulti(sent_ds); sent_ds = sent_sm.result(); sent_ds.describe(); sent_ds.count(); sent_ds._force_count_rows(); # all good. # then this gives the stack trace below. giab_22_ds = hl.filter_intervals(giab_ds, [hl.parse_locus_interval('chr22', reference_genome='GRCh38')]); ```. Stack trace:. ```; FatalError: ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 20 times, most recent failure: Lost task 0.19 in stage 19.0 (TID 312, gilson-validation-test-2-w-4.c.perfect-atrium-179917.internal, executor 2): java.lang.ClassCastException: is.hail.codegen.generated.C29 cannot be cast to is.hail.asm4s.AsmFunction5; 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1641); 	at is.hail.expr.TableMapRows$$anonfun$execute$5.apply(Relational.scala:1637); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$mapPartitions$1.apply(ContextRDD.scala:151); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$19$$anonfun$apply$20.apply(ContextRDD.scala:280); 	at scala.collection.It",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:1200,validat,validation-test-,1200,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['validat'],['validation-test-']
Security,`HailContext()` was an 0.1 phenomenon. At some point (when we've fully moved things over to using IRs probably) we can totally remove it from Python. It's definitely not meant to be user-exposed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4987#issuecomment-448322113:187,expose,exposed,187,https://hail.is,https://github.com/hail-is/hail/pull/4987#issuecomment-448322113,1,['expose'],['exposed']
Security,"`[:, :]` is used to access the globals of one thing in another context",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4027#issuecomment-408647882:20,access,access,20,https://hail.is,https://github.com/hail-is/hail/issues/4027#issuecomment-408647882,1,['access'],['access']
Security,"``; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for tempor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:4760,access,access,4760,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security,"`ci-test` is failing, and I don't have access to see why",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10058#issuecomment-781728668:39,access,access,39,https://hail.is,https://github.com/hail-is/hail/pull/10058#issuecomment-781728668,1,['access'],['access']
Security,"`hl.foo.bar.baz` looks at the __init__files, which work differently from imports. I think you could do `from hail.experimental.import_gtf import _load_gencode_gtf`. One of the tasks for 0.3 is a refactor that prevents everything from being imported and exposed as `hl.blah.blah.blah`, which leads to super long import times.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221:253,expose,exposed,253,https://hail.is,https://github.com/hail-is/hail/pull/8264#issuecomment-597047221,1,['expose'],['exposed']
Security,a.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.ex,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18467,Checksum,ChecksumFileSystem,18467,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,able.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201422,Hash,HashSet,201422,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"able.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205069,Hash,HashSet,205069,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"ah, annoying. It's hard to figure out when stuff is actually going to be executed in make. We can fix this by moving those definitions inside the target, I think:; ```; pypi-deploy: check-pypi wheel; 	 TWINE_USERNAME=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-username) \ ; 	 TWINE_PASSWORD=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-password) \; 	 twine upload build/deploy/dist/*; ```. mind making that change in a separate branch?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676:336,password,password,336,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676,1,['password'],['password']
Security,aha it gets a push for every task end! Running even a 5000-partition short job is a denial of service attack against the extension,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532900121:102,attack,attack,102,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532900121,1,['attack'],['attack']
Security,ail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gservicea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:2162,access,access,2162,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security,"ally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. up",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2042,certificate,certificate,2042,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['certificate'],['certificate']
Security,"arams={'access_token': access_token},; + ); + if userinfo['aud'] != oauth2_client_audience and userinfo['aud'] != userinfo['sub']:; + return None; +; + email = userinfo['email']; + if email.endswith('iam.gserviceaccount.com'):; + return userinfo['sub']; + # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email.; + return email; + except httpx.ClientResponseError as e:; + if e.status in (400, 401):; + return None; + raise; +; +; +class AadJwk(TypedDict):; + kid: str; + x5c: List[str]; +; ; class AzureFlow(Flow):; + _aad_keys: Optional[List[AadJwk]] = None; +; def __init__(self, credentials_file: str):; with open(credentials_file, encoding='utf-8') as f:; data = json.loads(f.read()); ; tenant_id = data['tenant']; authority = f'https://login.microsoftonline.com/{tenant_id}'; - client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); -; - self._client = client; + self._client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); self._tenant_id = tenant_id; ; def initiate_flow(self, redirect_uri: str) -> dict:; @@ -107,10 +170,31 @@ class AzureFlow(Flow):; ; return FlowResult(token['id_token_claims']['oid'], token['id_token_claims']['preferred_username'], token); ; -; -def get_flow_client(credentials_file: str) -> Flow:; - cloud = get_global_config()['cloud']; - if cloud == 'azure':; - return AzureFlow(credentials_file); - assert cloud == 'gcp'; - return GoogleFlow(credentials_file); + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + tenant_id = oauth2_client['tenant']; + authority = f'https://login.microsoftonline.com/{tenant_id}'; + app = msal.PublicClientApplication(oauth2_client['appId'], authority=authority); + credentials = app.acquire_token_interactive([oauth2_client['userOauthScope']]); + return {**oauth2_client, 'refreshToken': credentials['refresh_token']}; +; + @staticmethod; + async def get_identity_uid_fro",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:4943,Confidential,ConfidentialClientApplication,4943,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,2,"['Confidential', 'password']","['ConfidentialClientApplication', 'password']"
Security,ark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203568,Hash,HashSet,203568,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dki,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:9081,secur,security,9081,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200345,Hash,HashMap,200345,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,"ataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412), restarting docker on a failed worker works. Docker starts fine. However, I missed a subtlety: we must restart *after* installation but *before* we try to pull our VEP docker image. I also added a sleep in hopes that gives various things a chance to die off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13621,certificate,certificates,13621,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['certificate'],['certificates']
Security,"cating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3096,certificate,certificates,3096,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,2,['certificate'],['certificates']
Security,chElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200232,Hash,HashMap,200232,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,cloud/aiogoogle/client/bigquery_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/billing_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/compute_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/container_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/iam_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/logging_client.py'; adding 'hailtop/aiocloud/aiogoogle/client/storage_client.py'; adding 'hailtop/aiocloud/common/__init__.py'; adding 'hailtop/aiocloud/common/base_client.py'; adding 'hailtop/aiocloud/common/credentials.py'; adding 'hailtop/aiocloud/common/session.py'; adding 'hailtop/aiogoogle/__init__.py'; adding 'hailtop/aiotools/__init__.py'; adding 'hailtop/aiotools/aio_contextlib.py'; adding 'hailtop/aiotools/copy.py'; adding 'hailtop/aiotools/delete.py'; adding 'hailtop/aiotools/diff.py'; adding 'hailtop/aiotools/local_fs.py'; adding 'hailtop/aiotools/router_fs.py'; adding 'hailtop/aiotools/tasks.py'; adding 'hailtop/aiotools/utils.py'; adding 'hailtop/aiotools/validators.py'; adding 'hailtop/aiotools/weighted_semaphore.py'; adding 'hailtop/aiotools/fs/__init__.py'; adding 'hailtop/aiotools/fs/copier.py'; adding 'hailtop/aiotools/fs/exceptions.py'; adding 'hailtop/aiotools/fs/fs.py'; adding 'hailtop/aiotools/fs/stream.py'; adding 'hailtop/auth/__init__.py'; adding 'hailtop/auth/auth.py'; adding 'hailtop/auth/flow.py'; adding 'hailtop/auth/sql_config.py'; adding 'hailtop/auth/tokens.py'; adding 'hailtop/batch/__init__.py'; adding 'hailtop/batch/backend.py'; adding 'hailtop/batch/batch.py'; adding 'hailtop/batch/batch_pool_executor.py'; adding 'hailtop/batch/conftest.py'; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:25127,validat,validators,25127,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['validat'],['validators']
Security,com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.Ne,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8639,secur,security,8639,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,"cool, I'll proably rewrite the tests using that. the option for 1 already exists in `LoadMatrix.apply`; I just haven't exposed it to HailContext because there's a lot of stuff there already and I couldn't figure out what the name of the flag should be (currently, it's hasRowKeyLabel but I don't feel like that's super descriptive) but I could do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622:119,expose,exposed,119,https://hail.is,https://github.com/hail-is/hail/pull/2440#issuecomment-345032622,1,['expose'],['exposed']
Security,d.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18333,Checksum,ChecksumFileSystem,18333,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,d.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18493,Checksum,ChecksumFileSystem,18493,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,"d_config import get_global_config; +from hailtop import httpx; +from hailtop.utils import retry_transient_errors; +; +log = logging.getLogger('auth'); ; ; class FlowResult:; - def __init__(self, login_id: str, email: str, token: dict):; + def __init__(self, login_id: str, email: str, refresh_token: str):; self.login_id = login_id; self.email = email; - self.token = token; + self.refresh_token = refresh_token; ; ; class Flow(abc.ABC):; @@ -35,9 +44,24 @@ class Flow(abc.ABC):; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; raise NotImplementedError; ; + @staticmethod; + @abc.abstractmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; + raise NotImplementedError; +; + @staticmethod; + @abc.abstractmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + """"""; + Validate a user-provided access token. If the token is valid, return the identity; + to which it belongs. If it is not valid, return None.; + """"""; + raise NotImplementedError; +; ; class GoogleFlow(Flow):; - scopes: ClassVar[List[str]] = [; + scopes = [; 'https://www.googleapis.com/auth/userinfo.profile',; 'https://www.googleapis.com/auth/userinfo.email',; 'openid',; @@ -48,7 +72,7 @@ class GoogleFlow(Flow):; ; def initiate_flow(self, redirect_uri: str) -> dict:; flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(; - self._credentials_file, scopes=self.scopes, state=None; + self._credentials_file, scopes=GoogleFlow.scopes, state=None; ); flow.redirect_uri = redirect_uri; authorization_url, state = flow.authorization_url(access_type='offline', include_granted_scopes='true'); @@ -61,7 +85,7 @@ class GoogleFlow(Flow):; ; def receive_callback(self, request: aiohttp.web.Request, flow_dict: dict) -> FlowResult:; flow = google_auth_oauthlib.flow.Flow.from_c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:1831,Validat,Validate,1831,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,2,"['Validat', 'access']","['Validate', 'access']"
Security,"docs/reference/generated/kubernetes-api/v1.13/#podsecuritycontext-v1-core) which lets us restrict linux permissions, the linux user id, group id, SELinux policies, and permitted sysctl resources. I suspect for CI purposes non-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1101,access,access,1101,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573,1,['access'],['access']
Security,"eJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [; {; ""caching"": ""None"",; ""createOption"": ""Empty"",; ""deleteOption"": ""Delete"",; ""detachOption"": null,; ""diskIopsReadWrite"": null,; ""diskMBpsReadWrite"": null,; ""diskSizeGb"": 375,; ""image"": null,; ""lun"": 2,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""toBeDetached"": false,; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; ],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:3562,secur,securityProfile,3562,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['secur'],['securityProfile']
Security,eTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203076,Hash,HashMap,203076,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashMap']
Security,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2301,Audit,Audit-Id,2301,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851,1,['Audit'],['Audit-Id']
Security,"endentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGSche",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:205672,Hash,HashSet,205672,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1571,access,access,1571,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,4,['access'],"['access', 'accessing']"
Security,er(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199429,Hash,HashMap,199429,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashMap']
Security,er.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAG,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200492,Hash,HashMap,200492,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,erCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8898,secur,security,8898,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,"es into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - until the whole project got canned]. ... and in the time I was there, the Endeca/Oracle stuff wasn't distributed, which could be another; place where the generate-LLVM-IR needs some kind of extra glue for distributing compiled code,; whereas the conventional DLL's are trivial to ship around. Not claiming that part of it is difficult,; just that it didn't happen at Oracle until long after I left. In contrast, at PhysicsSpeed it was fairly smooth to implement nice abstractions (dense-join-table,; hash-join-table, tuple-with-order) as template classes which could be tested and debugged; in a standalone environment, and then have simpler codegen using those abstractions. At least,; that's a good way to get a lot of functionality with modest effort - and it doesn't preclude migrating; towards a more complex codegen later. It's nice to be able to have templates as low-runtime-cost; abstractions, but you don't have to use them if you don't want to. In short, most of the usual arguments about the benefits of HLL's apply. Plus the larger pool of; potential hires w/ C++ experience compared to the pool of people w/ lower-level/LLVM/IR/compiler-; internals expertise, and the possibility of very occasionally picking up useful fragments of open-source code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:3003,hash,hash-join-table,3003,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['hash'],['hash-join-table']
Security,executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10711,Checksum,ChecksumFileSystem,10711,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,ey not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.schedul,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200265,Hash,HashMap,200265,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,fun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndInde,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200555,Hash,HashMap,200555,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,ge$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); E 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$1(LocalBackend.scala:109); E 	at is.hail.backend.local.LocalBackend.executeToEncoded(LocalBackend.scala:208); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeEncode$2(LocalBackend.scala:237); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:76); E 	at is.hail.utils.package$.using(package.scala:657); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:62); E 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$1(LocalBackend.scala:109); E 	at is.hail.backend.local.LocalBackend.$anonfun$executeEncode$1(LocalBackend.scala:236); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.backend.local.LocalBackend.executeEncode(LocalBackend.scala:234); E 	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:750); E ; E ; E ; E Hail version: 0.2.124-b31f1dcea5d2; E Error summary: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:15288,access,access,15288,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['access'],['access']
Security,geFS$$anon$2.close(GoogleStorageFS.scala:326); 		at java.io.FilterOutputStream.close(FilterOutputStream.java:159); 		... 27 more; 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:7916,access,access,7916,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['access'],['access']
Security,"gies are in order, I totally lead you astray by mentioning the makefile. The Makefile *is* the source of hail version truth, but invoking the makefile inside an image build step feels wrong to me. Each step creates a layer which inflates the image sizes. Hail's images are already too big!. I took your commits and added one of my own that snags the version from the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:1086,access,access,1086,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,2,['access'],['access']
Security,"git hash looks like a standard Hail install, nevermind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312:4,hash,hash,4,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312,1,['hash'],['hash']
Security,"google died:; ```; E Log:	{'main': ""ERROR: (gcloud.auth.activate-service-account) There was a problem refreshing your current auth tokens: {u'status': u'UNAVAILABLE', u'message': u'The service is currently unavailable.', u'code': 503}\nPlease run:\n\n $ gcloud auth login\n\nto obtain new credentials, or if you have already logged in with a\ndifferent account:\n\n $ gcloud config set account ACCOUNT\n\nto select an already authenticated account to use.\n""}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019:426,authenticat,authenticated,426,https://hail.is,https://github.com/hail-is/hail/pull/5934#issuecomment-486735019,1,['authenticat'],['authenticated']
Security,"haha, you are an Awk Queen! This looks great to me, I'm glad we were able to nail down the security.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023:91,secur,security,91,https://hail.is,https://github.com/hail-is/hail/pull/9076#issuecomment-662526023,1,['secur'],['security']
Security,hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; adding 'hailtop/hailctl/dataproc/submit.py'; adding 'hailtop/hailctl/dataproc/utils.py'; adding 'hailtop/hailctl/dev/__init__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB);  53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB);  97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collect,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:28779,validat,validate,28779,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['validat'],['validate']
Security,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:106,validat,validation,106,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434,1,['validat'],['validation']
Security,"hat hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1139,access,access,1139,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['access'],['access']
Security,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3255,certificate,certificate,3255,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,10,['certificate'],"['certificate', 'certificates']"
Security,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3426,access,access,3426,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['access'],['access']
Security,"hmm, need to expose this in sphinx",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1645#issuecomment-294021662:13,expose,expose,13,https://hail.is,https://github.com/hail-is/hail/pull/1645#issuecomment-294021662,1,['expose'],['expose']
Security,https://ci.hail.is/batches/7404773/jobs/145; ```; starting test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt...; Exception:; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:1893,access,access,1893,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['access'],['access']
Security,"https://developers.google.com/identity/protocols/oauth2/web-server#exchange-authorization-code. So the google oauth flow example shows this when getting the token:. ```python3; flow.redirect_uri = flask.url_for('oauth2callback', _external=True); ```. Let me see if I can incorporate this behavior instead of what I did.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11147#issuecomment-994998700:76,authoriz,authorization-code,76,https://hail.is,https://github.com/hail-is/hail/pull/11147#issuecomment-994998700,1,['authoriz'],['authorization-code']
Security,ider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_j,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8735,secur,security,8735,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,"ject for temporary files generated by Hail? [y/n]: y; What is the name of the new bucket (Example: hail-batch-jigold-yrxul): hail-batch-jigold-yrxul; Which google project should hail-batch-jigold-yrxul be created in? This project will incur costs for storing your Hail generated data. (Example: hail-jigold): hail-jigold; Which region does your data reside in? (Example: us-central1): us-central1; Do you want to set a lifecycle policy (automatically delete files after a time period) on the bucket hail-batch-jigold-yrxul? [y/n]: y; After how many days should files be automatically deleted from bucket hail-batch-jigold-yrxul? (30): 15; Created bucket hail-batch-jigold-yrxul in project hail-jigold.; Updated bucket hail-batch-jigold-yrxul in project hail-jigold with lifecycle rule set to 15 days and labels {'bucket': 'hail-batch-jigold-yrxul', 'owner': 'jigold', 'data_type': 'temporary'}.; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-yrxul in project hail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:1123,access,access,1123,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security,ld-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-yrxul in project hail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-ba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:2025,access,access,2025,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security,led$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203090,Hash,HashMap,203090,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashMap']
Security,"looks like the differences between `--unsafe-fixes` and my manual edits based on the feedback the linter gave were:; * `assert <boolean value> == True` becomes `assert <boolean value> is True`, not `assert <boolean value>`; * `if <value> == foo or <value> == bar` becomes `if <value> in (foo, bar)`, not `if <value> in {foo, bar}`; * `<unused variable> = foo` becomes `foo` instead of being deleted outright. those seem fine, though i think the manual version of the latter two is better in the cases where i had added it, as i only used sets for hashable types and deleted things that didn't have side effects, afaik",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355:547,hash,hashable,547,https://hail.is,https://github.com/hail-is/hail/pull/14128#issuecomment-1883396355,2,['hash'],['hashable']
Security,mand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18387,Checksum,ChecksumFileSystem,18387,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,"n incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))])org.apache.spark.rdd.RDD[(is.hail.variant.Variant, (is.hail.annotations.Annotation, Iterable[is.hail.variant.Genotype]))]; and method coalesce in class RDD of type (numPartitions: Int, shuffle: Boolean)(implicit ord: Ordering[(is.hail.variant.Variant, (",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1776,access,access,1776,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['access'],['access']
Security,"n load it. Depending on ; precisely how Spark manages things (which I wouldn't want to depend on too much anyway). In fact it's essential that all the workers share the same DLL file, because otherwise they'd be; trying to load multiple DLL's defining the same symbols. That aspect of it could be handled by; putting the files into a per-process directory and using in-memory (std::mutex) synchronization.; But y'know, given that we have to write the DLL's out, it just seemed natural to let them persist; (and until debugging it on MacOS, I thought I could manage it with nothing but atomic file-create; and atomic-rename, but that didn't quite pan out). As for writing LLVM IR, it can definitely be done, because that's what Endeca/Oracle did. But there; was such a huge learning curve that only 3 people ever did it successfully (I wasn't one of them),; and debugging seemed very unpleasant and slow. [It was also a masterful achievement in ; job-security-through-obscurity, because no-one in management was going to mess with the; two people who wrote it - until the whole project got canned]. ... and in the time I was there, the Endeca/Oracle stuff wasn't distributed, which could be another; place where the generate-LLVM-IR needs some kind of extra glue for distributing compiled code,; whereas the conventional DLL's are trivial to ship around. Not claiming that part of it is difficult,; just that it didn't happen at Oracle until long after I left. In contrast, at PhysicsSpeed it was fairly smooth to implement nice abstractions (dense-join-table,; hash-join-table, tuple-with-order) as template classes which could be tested and debugged; in a standalone environment, and then have simpler codegen using those abstractions. At least,; that's a good way to get a lot of functionality with modest effort - and it doesn't preclude migrating; towards a more complex codegen later. It's nice to be able to have templates as low-runtime-cost; abstractions, but you don't have to use them if yo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385:2392,secur,security-through-obscurity,2392,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412742385,2,['secur'],['security-through-obscurity']
Security,n.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutabl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200421,Hash,HashTable,200421,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashTable']
Security,"n: Job aborted due to stage failure: Task 12 in stage 103.0 failed 4 times, most recent failure: Lost task 12.3 in stage 103.0 (TID 644994, scc-q21.scc.bu.edu, executor 2): java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:10551,Checksum,ChecksumFileSystem,10551,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['Checksum'],['ChecksumFileSystem']
Security,"nNull(v);; requireNonNull(iparam);; requireNonNull(ipntr);; requireNonNull(workd);; requireNonNull(info);; checkIndex(offsetresid + n - 1, resid.length);; checkIndex(offsetv + n * ncv - 1, v.length); // HERE; checkIndex(offsetiparam + 11 - 1, iparam.length);; checkIndex(offsetipntr + 11 - 1, ipntr.length);; checkIndex(offsetworkd + 3 * n - 1, workd.length);; checkIndex(offsetworkl + lworkl - 1, workl.length);; dsaupdK(ido, bmat, n, which, nev, tol, resid, offsetresid, ncv, v, offsetv, ldv, iparam, offsetiparam, ipntr, offsetipntr, workd, offsetworkd, workl, offsetworkl, lworkl, info);; }; ```; where `v.length = 177860`, `n = 8893`, and `ncv = 20`. `checkIndex` is; ```; private void checkIndex(int index, int length) {; if (index < 0 || index >= length) {; throw new IndexOutOfBoundsException(String.format(""Index %s out of bounds for length %s"", index, length));; }; }; ```; which a) shouldn't throw in this case, b) throws an `IndexOutOfBoundsException`, not an `ArrayIndexOutOfBoundsException`, and c) isn't the root of the stacktrace; ```; 	at org.netlib.blas.Dcopy.dcopy(blas.f); 	at org.netlib.arpack.Dsaitr.dsaitr(arpack.f); 	at org.netlib.arpack.Dsaup2.dsaup2(arpack.f); 	at org.netlib.arpack.Dsaupd.dsaupd(arpack.f); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupdK(F2jARPACK.java:189); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:560); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at dev.ludovic.netlib.arpack.AbstractARPACK.dsaupd(AbstractARPACK.java:536); 	at dev.ludovic.netlib.arpack.F2jARPACK.dsaupd(F2jARPACK.java:30); 	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:106); ```; So it looks like somehow bad indices are making it through all these checks and a bad array access is hapenning deep in blas?. So I'm at a complete loss. If anybody else could take a stab at it with a fresh perspective, I'd be happy to discuss, but I'm low on bandwidth to keep digging myself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313:3960,access,access,3960,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1760360313,2,['access'],['access']
Security,"n[T] -> (U -> Boolean) -> Gen[Unit]`; - A `Gen[Unit]` is a bit artificial because the test framework halts execution (presumably with an exception) when a counter-example is found. I instead prefer that `Prop.forAll` has type: `Gen[T] -> (U -> Boolean) -> Gen[Boolean]`; - Now `Prop.forAll` has the same type as `Gen.flatMap[Boolean]`. It seems the difference between `forAll` and `flatMap` is that `forAll` conceptually preforms a product operation while `flatMap` performs a sampling. However, I think they are, in reality, the same operation: sampling. The implementation for `GenProp3` looks like:. ``` scala; for (i <- 0 until p.count) {; val v1 = g1(p); val v2 = g2(p); val v3 = g3(p); val r = f(v1, v2, v3); if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; }; ```. Which could be re-written as:. ``` scala; for (i <- 0 until p.count) {; (for (v1 <- g1; v2 <- g2; v3 <- g3) {; if (!r) {; println(s""! ${prefix}Falsified after $i passed tests.""); println(s""> ARG_0: $v1""); println(s""> ARG_1: $v2""); println(s""> ARG_2: $v3""); assert(r); }; })(p); }; ```. The primary difference between `flatMap` and `forAll` seems to be in error reporting. We can fix this by noting `Gen[T]` is currently a Reader monad on `Parameters`. If we add a ""forAll stack"" to `Parameters` we could implement `forAll` as:. ``` scala; def forAll[T,U](gt: Gen[T], gu: T -> Gen[U]): Gen[U] =; for (t <- gt; u <- local(pushQuantified(t), gu(t)) yield u. def pushQuantified(x: Any)(Parameters p): Paramters =; new Parameters(p.rng, p.size, p.count, (x :: p.quanitifed)); ```. We complete the Reader monad transformation by adding the `local` operation to `class Gen[T]`. ``` scala; // in class Gen; def local(modify: Parameters -> Parameters, gu: Gen[U]): Gen[U] =; Gen { p => gu(modify(p)) }; ```. Finally, the `check` method can access this stack of quantified variables to provide a useful error message. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/400#issuecomment-238901220:2112,access,access,2112,https://hail.is,https://github.com/hail-is/hail/issues/400#issuecomment-238901220,1,['access'],['access']
Security,nagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199921,Hash,HashSet,199921,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):. > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode. Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission. See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957). [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539:10,Authoriz,Authorization,10,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640539,5,"['Authoriz', 'access', 'authoriz']","['Authorization', 'access', 'access-authn-authz', 'authorization']"
Security,"no, it doesn't exist now -- I think we want to expose aggregators on rows/cols of block matrices though. How do we do conditional matrix multiply?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6097#issuecomment-492762915:47,expose,expose,47,https://hail.is,https://github.com/hail-is/hail/issues/6097#issuecomment-492762915,1,['expose'],['expose']
Security,"no. That fixed the intrusion of this bug into the Expression.take/show/collect stuff, but it's still a problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3492#issuecomment-386354483:19,intrusion,intrusion,19,https://hail.is,https://github.com/hail-is/hail/issues/3492#issuecomment-386354483,1,['intrusion'],['intrusion']
Security,"not passing tests:; ```; + java -cp 'hail-test.jar:/spark-2.4.0-bin-hadoop2.7/jars/*' org.testng.TestNG -listener is.hail.LogTestListener testng-build.xml; [[ClassInfoMap]] Unable to open class is.hail.stats.LeveneHaldane - unable to resolve class reference is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; Exception in thread ""main"" java.lang.NoClassDefFoundError: is/hail/relocated/org/apache/commons/math3/distribution/AbstractIntegerDistribution; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:756); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:612,secur,security,612,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,6,"['Access', 'Secur', 'access', 'secur']","['AccessController', 'SecureClassLoader', 'access', 'security']"
Security,"o pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfoInternal(GoogleCloudStorageFileSystemImpl.java:861); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystemImpl.getFileInfo(GoogleCloudStorageFileSystemImpl.java:833); E 	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.getFileStatus(GoogleHadoopFileSystem.java:724); E 	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115); E 	at org.apache.hadoop.fs.Globber.doGlob(Globber.java:349); E 	at org.apache.hadoop.fs.Globber.glob(Globber.java:202); E 	at org.apache.hadoop.fs.FileSystem.globStat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:1567,access,accessing,1567,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,1,['access'],['accessing']
Security,"ocal/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:86: in execute; raise e.maybe_user_error(ir) from None; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); /usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro552', <py4j.java_gateway.GatewayClient object at 0x7f01e1182160>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py'>; s = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; tpl = JavaObject id=o553; deepest = 'RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; full = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E 	at is.hail.annotations.Memory.checkAddress(Memory.java:226); E 	at is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:2767,access,access,2767,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,1,['access'],['access']
Security,"oh wow, it doesn't even merge main before running the security checks. ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12269#issuecomment-1271793472:54,secur,security,54,https://hail.is,https://github.com/hail-is/hail/pull/12269#issuecomment-1271793472,1,['secur'],['security']
Security,olExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Shell output: main : command provided 1; main : run as user is farrell; main : requested yarn user is farrell. Container exited with a non-zero exit code 1. 2019-01-22 13:12:06 BlockManagerMaster: INFO: Removal of executor 12 requested; 2019-01-22 13:12:06 BlockManagerMasterEndpoint: INFO: Trying to remove executor 12 from BlockManagerMaster.; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Asked to remove non-existent executor 12; 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:199443,Hash,HashMap,199443,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashMap']
Security,ommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:238); at java.lang.Thread.run(Thread.java:745). java.io.FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files); at java.io.FileOutputStream.open0(Native Method); at java.io.FileOutputStream.open(FileOutputStream.java:270); at java.io.FileOutputStream.<init>(FileOutputStream.java:213); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:222); at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209); at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296); at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328); at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:402); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461); at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:778); at is.hail.io.fs.HadoopFS.createNoCompression(HadoopFS.scala:60); at is.hail.io.fs.FS$class.create(FS.scala:151); at is.hail.io.fs.HadoopFS.create(HadoopFS.scala:56); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:18284,Checksum,ChecksumFileSystem,18284,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,2,['Checksum'],"['ChecksumFSOutputSummer', 'ChecksumFileSystem']"
Security,"on-root user is fine. k8s [already limits](https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/) pods to the ""safe"" sysctl resources. Each container in a pod as a [`SecurityContext`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.13/#securitycontext-v1-core). By default containers have `privileged` set to false, meaning they don't have root-like abilities on the host system. @cseed I don't see anyway for a pod to modify the mounted volumes, *particularly* if has no privileges to access the k8s api. If the volumes it is already given have no credentials that give access to k8s, I don't see how it could possibly ask k8s to mount a volume for it. ---. Containers within a single pod share a network. I have not found anyway to restrict the network access of individual containers. In particular, the [V1Container.md doc file](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V1Container.md) states "" Any port which is listening on the default '0.0.0.0' address inside a container will be accessible from the network."" In the case of an initContainer, I don't think this is an issue since the container must terminate before the regular containers start. In the case of a sidecar container waiting for its peer to die so it can copy out results, we must be very careful to not expose any comprisable service on any port. ---. ""ExitContainers"" / ""anti-initContainers"". I think we can pull this off with a second container that polls the k8s API to check if its peer container has died or not. I need to investigate further how much we can limit this API access. Ideally you would only be able to get information about the pod you're in. [How to access the pod's service account credentials within a container](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/). We'll want the pod's service account to have minimal privileges and we'll add another volume just for the sidecar with credentials to access the API.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573:1364,access,accessible,1364,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456988573,6,"['access', 'expose']","['access', 'access-authn-authz', 'accessible', 'expose']"
Security,on;JJ)V; L0; LDC 0; LSTORE 6; LDC 0; LSTORE 8; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 8; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // we fail here; ISTORE 10; ILOAD 10; ISTORE 11; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I // or here; ISTORE 12; ALOAD 0; ALOAD 0; ALOAD 1; ILOAD 11; INVOKEVIRTUAL __C44CompiledWithAggs.__m82str (Lis/hail/annotations/Region;I)J; LDC 9999; ILOAD 12; ISUB; INVOKEVIRTUAL __C44CompiledWithAggs.__m67take_by_seqop (JI)V; RETURN; L1; LOCALVARIABLE get_tup_elem_o J L0 L1 6; LOCALVARIABLE get_tup_elem_o J L0 L1 8; LOCALVARIABLE invoke I L0 L1 10; LOCALVARIABLE local11 I L0 L1 11; LOCALVARIABLE invoke I L0 L1 12; MAXSTACK = 5; MAXLOCALS = 13; ```. ```; /// PREVIOUS VERSION; // access flags 0x1; public __m85wrapped(Lis/hail/annotations/Region;JJ)V; L0; GOTO L1; L1; FRAME SAME; LLOAD 4; LSTORE 6; GOTO L2; L2; FRAME APPEND [J]; LDC 0; ISTORE 8; GOTO L3; L3; FRAME APPEND [I]; ALOAD 0; ILOAD 8; PUTFIELD __C46CompiledWithAggs.__f78vm : Z; GOTO L4; L4; FRAME SAME; LLOAD 4; LSTORE 9; GOTO L5; L5; FRAME APPEND [J]; LDC 0; ISTORE 11; GOTO L6; L6; FRAME APPEND [I]; ALOAD 0; ILOAD 11; PUTFIELD __C46CompiledWithAggs.__f77km : Z; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f78vm : Z; IFNE L7; GOTO L8; L8; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 6; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$.loadInt (J)I; ISTORE 12; ILOAD 12; ISTORE 13; ALOAD 0; ALOAD 1; ILOAD 13; INVOKEVIRTUAL __C46CompiledWithAggs.__m86str (Lis/hail/annotations/Region;I)J; LSTORE 14; GOTO L9; L9; FRAME APPEND [T T J]; ALOAD 0; GETFIELD __C46CompiledWithAggs.__f77km : Z; IFNE L10; GOTO L11; L11; FRAME SAME; GETSTATIC is/hail/annotations/Region$.MODULE$ : Lis/hail/annotations/Region$;; LLOAD 9; LDC 0; LADD; INVOKEVIRTUAL is/hail/annotations/Region$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408:1093,access,access,1093,https://hail.is,https://github.com/hail-is/hail/pull/8671#issuecomment-621635408,1,['access'],['access']
Security,onda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2424,Secur,SecurityManager,2424,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,8,"['Secur', 'authenticat']","['SecurityManager', 'authentication']"
Security,"one more change required to error message in AST:. ```; s""""""Tried to access index [$i] on array ${ JsonMethods.compact(localT.toJSON(a)) } of length ${ a.length }; | Hint: All arrays in Hail are zero-indexed (`array[0]' is the first element); | Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/658#issuecomment-242136648:69,access,access,69,https://hail.is,https://github.com/hail-is/hail/pull/658#issuecomment-242136648,2,['access'],"['access', 'accessing']"
Security,"or temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:4897,access,access,4897,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['access'],['access']
Security,pply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$fa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200539,Hash,HashMap,200539,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['Hash'],['HashMap']
Security,"r [192.168.0.9:2181]; 2016-08-24 16:42:27,070:8532(0x7fef76bfd700):ZOO_INFO@check_events@1750: session establishment complete on server [192.168.0.9:2181], sessionId=0x255cb9ea22102ec, negotiated timeout=10000; I0824 16:42:27.070502 8726 group.cpp:331] Group process (group(1)@192.168.0.15:12239) connected to ZooKeeper; I0824 16:42:27.070544 8726 group.cpp:805] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0); I0824 16:42:27.070554 8726 group.cpp:403] Trying to create path '/mesos' in ZooKeeper; I0824 16:42:27.071593 8705 detector.cpp:156] Detected a new leader: (id='66'); I0824 16:42:27.071702 8746 group.cpp:674] Trying to get '/mesos/json.info_0000000066' in ZooKeeper; I0824 16:42:27.072525 8706 detector.cpp:481] A new leading master (UPID=master@192.168.0.9:5050) is detected; I0824 16:42:27.072593 8736 sched.cpp:262] New master detected at master@192.168.0.9:5050; I0824 16:42:27.072742 8736 sched.cpp:272] No credentials provided. Attempting to register without authentication; I0824 16:42:27.074246 8746 sched.cpp:641] Framework registered with 0233fcf9-88ce-407f-8ed5-b015adf9b59c-1932; hail: info: running: importvcf file:///mnt/lustre/schoi/projects/TOPMed/BROAD/Chr22/TopMed_8k.853.vcf.bgz; [Stage 0:=====================================================> (53 + 3) / 56]hail: info: Ordering unsorted dataset with network shuffle; hail: info: running: splitmulti; hail: info: running: annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; hail: info: running: count; [Stage 3:> (0 + 56) / 56]hail: count: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 50 in stage 3.0 failed 4 times, most recent failure: Lost task 50.3 in stage 3.0 (TID 296, nid00004.urika.com): java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AS",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:2698,authenticat,authentication,2698,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['authenticat'],['authentication']
Security,"r, there is a [GCS connector](https://www.hdfgroup.org/solutions/cloud-amazon-s3-storage-hdf5-connector/). It's not an object store, but there's also support for [Hadoop HDFS](https://www.hdfgroup.org/solutions/hadoop-hdfs-hdf5-connector/). There's also [the Virtual Object Layer](https://docs.hdfgroup.org/hdf5/develop/_h5_v_l__u_g.html) which appears to be a file system abstraction that would permit storing HDF5 ""files"" in multiple objects which plays well with cloud object store scaling. We should prioritize an importer because no one has asked for HDF5 export nor is it clear that the HDF5 client libraries make it easy to write a single HDF5 ""file"" from a cluster of cores separated by a network. An importer would look something like `MatrixVCFReader`. It will need to use an HDF5 Java client library. An HDF5 client API is described [here](https://docs.hdfgroup.org/hdf5/develop/_h_d_f5_l_i_b.html) but they don't link to any JARs or maven repositories. This [support thread from 2022](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/14) appears to ultimately conclude that [netcdf-java](https://forum.hdfgroup.org/t/how-to-get-started-wih-hdf5-java/10346/24) supports reading HDF5 files. Including netcdf-java in a gradle or maven project is described [here](https://docs.unidata.ucar.edu/netcdf-java/current/userguide/using_netcdf_java_artifacts.html). It is not entirely clear how to use netcdf-java to access objects in Google Cloud Storage or Azure Blob Storage. There's an [open issue to support S3](https://github.com/Unidata/netcdf-java/issues/111). ---. OK, so, this is roughly what I'd do:. Driver side:; 1. Get the schema, cook up a corresponding Hail type.; 2. Choose a partitioning of the index space. Worker side:; 1. Read the same slice of each field/column based on the partition information.; 2. Construct a Hail SType/PType. See `GVCFPartitionReader` for an example. That class is misnamed, it's just a VCF partition reader, its not specific to GVCFs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14311#issuecomment-1955112694:1824,access,access,1824,https://hail.is,https://github.com/hail-is/hail/issues/14311#issuecomment-1955112694,2,['access'],['access']
Security,r.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203584,Hash,HashSet,203584,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,rCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SN,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8815,secur,security,8815,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,report/accumulators have been removed: https://github.com/hail-is/hail/pull/2024. No longer relevant. .... although better input integrity checking and error reporting would still be nice.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-319497230:129,integrity,integrity,129,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-319497230,1,['integrity'],['integrity']
Security,"return __original_func(*args_, **kwargs_); /usr/local/lib/python3.9/dist-packages/hail/table.py:1285: in aggregate; return Env.backend().execute(hl.ir.MakeTuple([agg_ir]))[0]; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:86: in execute; raise e.maybe_user_error(ir) from None; /usr/local/lib/python3.9/dist-packages/hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); /usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro552', <py4j.java_gateway.GatewayClient object at 0x7f01e1182160>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/usr/local/lib/python3.9/dist-packages/pyspark/__init__.py'>; s = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; tpl = JavaObject id=o553; deepest = 'RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000'; full = 'java.lang.RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000\n\tat is.h...ava:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: RuntimeException: invalid memory access: 120001305f726574/00000004: not in 7f15e9ffb1f8/00010000; E ; E Java stack trace:; E",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198:2514,access,access,2514,https://hail.is,https://github.com/hail-is/hail/pull/13814#issuecomment-1771905198,2,['access'],['access']
Security,romise.scala:33) ~[scala-library-2.12.15.jar:?]; 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8416,secur,security,8416,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,"rs or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to differentiate between k8 outages and lack of user requests. I also want to be able to quickly present a potential investor aggregate data, for notebook and all other manner of hail services: for notebook case: # notebooks created, length a notebook was used, how many notebooks were shared with others (I think this could be a useful feature, even if ""sharing"" meant taking a user-opt-in text dump that could be used to re-create a notebook, rather than connecting to that user's notebook), how many times a user re-visited a notebook, what our fullfillment rate was, what our error rate was, what the conversion rate is ( users that visited and created / users that visited our service). Cons against using a separate data store: non-atomic operations at the boundary between sql and k8. This is isn't a problem if we choose one master view (our db), and frankly seems unavoidable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:2423,access,accessed,2423,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['access'],['accessed']
Security,s.package$.using(package.scala:664); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); E 	at is.hail.utils.package$.using(package.scala:664); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); E 	at is.hail.backend.spark.SparkBackend.$anonfun$withExecuteContext$2(SparkBackend.scala:407); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); E 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:393); E 	at is.hail.backend.Backend.$anonfun$matrixTableType$1(Backend.scala:185); E 	at is.hail.backend.Backend.jsonToBytes(Backend.scala:175); E 	at is.hail.backend.Backend.matrixTableType(Backend.scala:185); E 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:104); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:848); E 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:817); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:201); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:560); E 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run(ServerImpl.java:526); E 	at java.base/java.lang.Thread.run(Thread.java:829); E; E; E; E Hail version: 0.2.127-e81ad92151ab; E Error summary: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:5968,Checksum,ChecksumException,5968,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,2,['Checksum'],"['Checksum', 'ChecksumException']"
Security,"sReadWrite"": null,; ""diskMBpsReadWrite"": null,; ""diskSizeGb"": 375,; ""image"": null,; ""lun"": 2,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-data"",; ""toBeDetached"": false,; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; ],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""4eaaa3a5-69ce-4eb0-ba8c-266bd66c821e"",; ""zones"": null; },; {; ""additionalCapabilities"": null,; ""applicationProfile"": null,; ""availabilitySet"": null,; ""billingProfile"": {; ""maxPrice"": -1.0; },; ""capacityReservation"": null,; ""diagnosticsProfile"": null,; ""evictionPolicy"": ""Delete"",",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:4774,encrypt,encryptionSettings,4774,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['encrypt'],['encryptionSettings']
Security,"s_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resource IDs on both tables. I recommend looking at the main function towards the bottom of the script and then working your way through it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:3088,audit,audit,3088,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,4,['audit'],['audit']
Security,scala-library-2.12.15.jar:?]; 	... 3 more; Caused by: javax.crypto.AEADBadTagException: Tag mismatch!; 	at com.sun.crypto.provider.GaloisCounterMode.decryptFinal(GaloisCounterMode.java:620) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.finalNoPadding(CipherCore.java:1116) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.fillOutputBuffer(CipherCore.java:1053) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.CipherCore.doFinal(CipherCore.java:941) ~[sunjce_provider.jar:1.8.0_392]; 	at com.sun.crypto.provider.AESCipher.engineDoFinal(AESCipher.java:491) ~[sunjce_provider.jar:1.8.0_392]; 	at javax.crypto.CipherSpi.bufferCrypt(CipherSpi.java:779) ~[?:1.8.0_392]; 	at javax.crypto.CipherSpi.engineDoFinal(CipherSpi.java:730) ~[?:1.8.0_392]; 	at javax.crypto.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:8532,secur,security,8532,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['secur'],['security']
Security,"serinfo = await retry_transient_errors(; + session.get_read_json,; + 'https://www.googleapis.com/oauth2/v3/tokeninfo',; + params={'access_token': access_token},; + ); + if userinfo['aud'] != oauth2_client_audience and userinfo['aud'] != userinfo['sub']:; + return None; +; + email = userinfo['email']; + if email.endswith('iam.gserviceaccount.com'):; + return userinfo['sub']; + # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email.; + return email; + except httpx.ClientResponseError as e:; + if e.status in (400, 401):; + return None; + raise; +; +; +class AadJwk(TypedDict):; + kid: str; + x5c: List[str]; +; ; class AzureFlow(Flow):; + _aad_keys: Optional[List[AadJwk]] = None; +; def __init__(self, credentials_file: str):; with open(credentials_file, encoding='utf-8') as f:; data = json.loads(f.read()); ; tenant_id = data['tenant']; authority = f'https://login.microsoftonline.com/{tenant_id}'; - client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); -; - self._client = client; + self._client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); self._tenant_id = tenant_id; ; def initiate_flow(self, redirect_uri: str) -> dict:; @@ -107,10 +170,31 @@ class AzureFlow(Flow):; ; return FlowResult(token['id_token_claims']['oid'], token['id_token_claims']['preferred_username'], token); ; -; -def get_flow_client(credentials_file: str) -> Flow:; - cloud = get_global_config()['cloud']; - if cloud == 'azure':; - return AzureFlow(credentials_file); - assert cloud == 'gcp'; - return GoogleFlow(credentials_file); + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + tenant_id = oauth2_client['tenant']; + authority = f'https://login.microsoftonline.com/{tenant_id}'; + app = msal.PublicClientApplication(oauth2_client['appId'], authority=authority); + credentials = app.acquire_token_interactive([oauth2_client['userOauthScope']]); + re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:4818,Confidential,ConfidentialClientApplication,4818,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,2,"['Confidential', 'password']","['ConfidentialClientApplication', 'password']"
Security,"sh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"":",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:8557,secur,securityProfile,8557,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['secur'],['securityProfile']
Security,"sh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:12905,secur,securityProfile,12905,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['secur'],['securityProfile']
Security,"spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2652,HASH,HASH,2652,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,3,"['HASH', 'hash']","['HASH', 'hash']"
Security,"such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDelaySeconds: 5; periodSeconds: 5; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:1450,authoriz,authorization,1450,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805,2,['authoriz'],['authorization']
Security,"tProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52); 2019-01-22 13:12:06 AbstractConnector: INFO: Stopped Spark@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:12:06 SparkUI: INFO: Stopped Spark web UI at http://10.48.225.55:4040; 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:206833,Hash,HashSet,206833,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Hash'],['HashSet']
Security,"tebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-name; secret:; defaultMode: 420; secretName: gsa-key-j7gwm; - name: user-kmpnh-token-hbdd4; secret:; defaultMode: 420; secretName: user-kmpnh-token-hbdd4. hostIP: 10.128.0.32; phase: Running; podIP: 10.32.19.165; qosClass: Burstable; startTime: ""2019-04-02T19:50:21Z""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2963,secur,securityContext,2963,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['secur'],['securityContext']
Security,though we need to expose it in python,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7173#issuecomment-570318532:18,expose,expose,18,https://hail.is,https://github.com/hail-is/hail/issues/7173#issuecomment-570318532,1,['expose'],['expose']
Security,"to answer (1), I think the right thing is for us to implement our own hashable immutable data structures (and use frozenset for sets, for instance) for results of Hail computations. I think we have yet to nail down whether this would be a breaking interface change, forcing us to wait until 0.3. To answer (2), you *may* be able to do `hl.stop(); hl.init()` to reset the session, but not sure this will work in every case. The driver should really only die for OOM and faults, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090:70,hash,hashable,70,https://hail.is,https://github.com/hail-is/hail/issues/8076#issuecomment-584795090,1,['hash'],['hashable']
Security,"type SparkSession in package org.apache.spark.sql,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/driver/package.scala:25: overloaded method value save with alternatives:; (javaRDD: org.apache.spark.api.java.JavaRDD[org.bson.Document])Unit <and>; (dataFrameWriter: org.apache.spark.sql.DataFrameWriter[_])Unit <and>; [D](dataset: org.apache.spark.sql.Dataset[D])Unit <and>; [D](rdd: org.apache.spark.rdd.RDD[D])(implicit evidence$5: scala.reflect.ClassTag[D])Unit; cannot be applied to (org.apache.spark.sql.DataFrameWriter); MongoSpark.save(kt.toDF(sqlContext); ^; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/sparkextras/OrderedRDD.scala:382: class PartitionCoalescer in package rdd cannot be accessed in package org.apache.spark.rdd; override def coalesce(maxPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty); ^; missing or invalid dependency detected while loading class file 'package.class'.; Could not access type DataFrame in package org.apache.spark.sql.package,; because it (or its dependencies) are missing. Check your build definition for; missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); A full rebuild may help if 'package.class' was compiled against an incompatible version of org.apache.spark.sql.package.; /gpfs/home/tpathare/haillatest/hail/src/main/scala/is/hail/variant/VariantSampleMatrix.scala:1143: ambiguous reference to overloaded definition,; both method coalesce in class OrderedRDD of type (maxPartitions: Int, shuffle: Boolean, partitionCoalescer: Option[<error>])(implicit ord: Ordering[(is.hail.variant.Variant, (is.hail.annotations.Ann",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831:1507,access,accessed,1507,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277494831,1,['access'],['accessed']
Security,"uct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_377,GetField(Ref(__iruid_377,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:42); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:617); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:266); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:92); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:17); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:22); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); 	at is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:9234,Hash,HashMap,9234,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Hash'],['HashMap']
Security,"uct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:91); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:16); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:21); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); 	at is.hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8954,Hash,HashMap,8954,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Hash'],['HashMap']
Security,un(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:5682,access,access,5682,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['access'],['access']
Security,"us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-jigold-test-multi-regional.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-jigold-test-multi-regional; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:6239,access,access,6239,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,2,['access'],['access']
Security,"uter-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1167,encrypt,encrypted,1167,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['encrypt'],['encrypted']
Security,"vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_377,GetField(Ref(__iruid_377,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:42); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:617); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:266); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:92); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:17); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:22); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:9220,Hash,HashMap,9220,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['Hash'],['HashMap']
Security,"vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Requiredness.run(Requiredness.scala:91); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:16); 	at is.hail.expr.ir.Requiredness$.apply(Requiredness.scala:21); 	at is.hail.expr.ir.lowering.LowerArrayAggsToRunAggsPass$.transform(LoweringPass.scala:95); ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8940,Hash,HashMap,8940,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,1,['Hash'],['HashMap']
Security,we should also add logic to cloudtools that checks if it's looking at an out-of-date hash,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417437115:85,hash,hash,85,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417437115,1,['hash'],['hash']
Security,"what about `array<set<dict<str, str>>>` < `array<set<dict<str, str>>>`? I agree that Locus comparisons should be specially supported, but I think that the change to expose comparisons on **any** types was wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6458#issuecomment-505219406:165,expose,expose,165,https://hail.is,https://github.com/hail-is/hail/issues/6458#issuecomment-505219406,1,['expose'],['expose']
Security,"xWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEventsProfile"": null,; ""securityProfile"": null,; ""storageProfile"": {; ""dataDisks"": [],; ""imageReference"": {; ""exactVersion"": ""0.0.12"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/galleries/dgoldste_batch/images/batch-worker/versions/0.0.12"",; ""offer"": null,; ""publisher"": null,; ""resourceGroup"": ""dgoldste"",; ""sharedGalleryImageId"": null,; ""sku"": null,; ""version"": null; },; ""osDisk"": {; ""caching"": ""ReadOnly"",; ""createOption"": ""FromImage"",; ""deleteOption"": ""Delete"",; ""diffDiskSettings"": null,; ""diskSizeGb"": 30,; ""encryptionSettings"": null,; ""image"": null,; ""managedDisk"": {; ""diskEncryptionSet"": null,; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Compute/disks/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun-os"",; ""resourceGroup"": ""dgoldste"",; ""storageAccountType"": ""Standard_LRS""; },; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun-os"",; ""osType"": ""Linux"",; ""vhd"": null,; ""writeAcceleratorEnabled"": null; }; },; ""tags"": {; ""batch-worker"": ""1"",; ""namespace"": ""pr-11144-default-nbthv8fduvd6""; },; ""type"": ""Microsoft.Compute/virtualMachines"",; ""userData"": null,; ""virtualMachineScaleSet"": null,; ""vmId"": ""13ddde82-50c6-4b61-af8b-c651eba9a674"",; ""zones"": null; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:13465,encrypt,encryptionSettings,13465,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,1,['encrypt'],['encryptionSettings']
Security,"yeah, still deciding if that should even be exposed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2701#issuecomment-357081710:44,expose,exposed,44,https://hail.is,https://github.com/hail-is/hail/pull/2701#issuecomment-357081710,1,['expose'],['exposed']
Security,"yes, exact same problem. The VDSs are:; `gs://future-variant-calling/future-pipeline/future.vds`; and; `gs://future-variant-calling/old-pipeline/past.vds`. You should have access in case you want to try things.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-358477102:172,access,access,172,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-358477102,1,['access'],['access']
Security,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:1769,expose,exposed,1769,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328,1,['expose'],['exposed']
Security,you'll notice that BroadcastValue requires the type but doesn't do anything with it -- this is to make it trivial to extend it to expose something like `broadcastRegionValue`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3130#issuecomment-372654665:130,expose,expose,130,https://hail.is,https://github.com/hail-is/hail/pull/3130#issuecomment-372654665,1,['expose'],['expose']
Security,"~Note to self, there's a backwards compatibility issue with the validator.~ [FIXED]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12729#issuecomment-1443933050:64,validat,validator,64,https://hail.is,https://github.com/hail-is/hail/pull/12729#issuecomment-1443933050,1,['validat'],['validator']
Security,". (base) dking@wm28c-761 hail % ; (base) dking@wm28c-761 hail % hailctl hdinsight submit --help ; ; Usage: hailctl hdinsight submit [OPTIONS] NAME STORAGE_ACCOUNT HTTP_PASSWORD ; SCRIPT [ARGUMENTS]... ; ; Submit a job to an HDInsight cluster configured for Hail. ; If you wish to pass option-like arguments you should use ""--"". For example: ; ; $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog ; ;  Arguments ;  * name TEXT [default: None] [required] ;  * storage_account TEXT Storage account in which the cluster's container exists. [default: None] [required] ;  * http_password TEXT Web password for the cluster [default: None] [required] ;  * script TEXT Path to script. [default: None] [required] ;  arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] ; ;  Options ;  --help Show this message and exit. ; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:3181,password,password,3181,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,1,['password'],['password']
Testability,	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:63); E 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:342); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$1(SparkBackend.scala:487); E 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); E 	at is.hail.backend.spark.SparkBackend.executeEncode(SparkBackend.scala:486); E 	at jdk.internal.reflect.GeneratedMethodAccessor72.invoke(Unknown Source); E 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.base/java.lang.reflect.Method.invoke(Method.java:566); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182); E 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106); E 	at java.base/java.lang.Thread.run(Thread.java:834); E ; E ; E ; E Hail version: 0.2.109-c163bcb21073; E Error summary: AssertionError: assertion failed. hail/backend/py4j_backend.py:35: FatalError. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:6562,Assert,AssertionError,6562,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,2,"['Assert', 'assert']","['AssertionError', 'assertion']"
Testability, 	at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source) ~[?:?]; 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_382]; 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:119) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_382]; 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_382]; 	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_382]; Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.15.jar:?]; 	at is.hail.io.StreamBlockInputBuffer.readBlock(InputBuffers.scala:552) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.ensure(InputBuffers.scala:384) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.BlockingInputBuffer.readInt(InputBuffers.scala:409) ~[gs:__hail-query-ger0g_jars_be9d88a80695b04a2a9eb5826361e0897d94c042.jar.jar:0.0.1-SNAPSHOT]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m20INPLACE_DECODE_r_int32_TO_r_int32(Unknown Source) ~[?:?]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m19INPLACE_DECODE_r_struct_of_r_int32ANDr_int32ANDr_binaryANDr_int64ANDr_int64ANDr_boolEND_TO_r_struct_of_r_int32ANDr_int32ANDr_stringANDr_int64ANDr_int64ANDr_boolEND(Unknown Source) ~[?:?]; 	at __C16collect_distributed_array_table_coerce_sortedness.__m18DECODE_r_struct_of_r_s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553:3416,assert,assert,3416,https://hail.is,https://github.com/hail-is/hail/issues/13356#issuecomment-1719508553,2,['assert'],['assert']
Testability," ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6004,test,test,6004,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability," ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-te",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2877,test,test,2877,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability," 'duration': 46}}, 'container_status': {'started_at': 1682966179253, 'finished_at': 1682966179340, 'state': 'finished', 'exit_code': 0, 'out_of_memory': False}}}, 'timing': {'setup_io': {'start_time': 1682966178998, 'finish_time': 1682966178999, 'duration': 1}, 'configuring xfsquota': {'start_time': 1682966178999, 'finish_time': 1682966179035, 'duration': 36}, 'populating secrets': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'adding cloudfuse support': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'post-job finally block': {'start_time': 1682966179578}}}, 'spec': {'always_copy_output': False, 'job_id': 1, 'process': {'command': ['true'], 'image': 'haildev.azurecr.io/ubuntu:20.04', 'mount_docker_socket': False, 'type': 'docker'}, 'resources': {'req_cpu': '0.25', 'req_memory': '50Mi', 'req_storage': '0Gi', 'cores_mcpu': 250, 'memory_bytes': 1073741824, 'storage_gib': 0, 'preemptible': True}, 'secrets': [{'namespace': 'pr-12955-default-rrlcxki12v8r', 'name': 'test-gsa-key', 'mount_path': '/gsa-key', 'mount_in_copy': True}], 'env': [{'name': 'AZURE_APPLICATION_CREDENTIALS', 'value': '/gsa-key/key.json'}]}}, {'status': {'id': 74, 'user': 'test', 'billing_project': 'test', 'token': 'tiQog0l-k0Xp2TkavWlV37Xy9FWcC1530EtCMRfg8XQ', 'state': 'success', 'complete': True, 'closed': True, 'n_jobs': 1, 'n_completed': 1, 'n_succeeded': 1, 'n_failed': 0, 'n_cancelled': 0, 'time_created': '2023-05-01T18:36:18Z', 'time_closed': None, 'time_completed': '2023-05-01T18:36:19Z', 'duration': 'a second', 'msec_mcpu': 0, 'cost': 1.1510333711392028e-06, 'attributes': {'name': 'test_pool_highcpu_instance_cheapest'}}, 'jobs': [{'log': {'main': ''}, 'status': {'batch_id': 74, 'job_id': 1, 'name': None, 'user': 'test', 'billing_project': 'test', 'state': 'Success', 'exit_code': 0, 'duration': 609, 'cost': 1.1510333711392028e-06, 'msec_mcpu': 0, 'status': {'version': 5, 'worker': 'batch-worker-pr-12955-default-rrlcxki12v8r-standard-0e2wl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228:3068,test,test-gsa-key,3068,https://hail.is,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228,1,['test'],['test-gsa-key']
Testability, +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1513,test,test,1513,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability," --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5750,test,test,5750,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability," 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get G",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1720,test,test,1720,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['test'],['test']
Testability," 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:3724,log,log,3724,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,3,"['Log', 'log']","['Logging', 'log']"
Testability, 3: __C1203Tuple3.<init>\n2022-11-15 20:30:18.247 root: INFO: instruction count: 12: __C1203Tuple3.<init>\n2022-11-15 20:30:18.262 root: INFO: executing D-Array [table_aggregate_singlestage] with 50 tasks\n2022-11-15 20:30:18.262 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: nPartitions 50\n2022-11-15 20:30:18.262 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: writing f and contexts\n2022-11-15 20:30:18.263 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: writing contexts\n2022-11-15 20:30:18.264 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff\n2022-11-15 20:30:18.264 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:23615,test,test-,23615,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability," 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data is quantitative.; Writing linear model association results to; /U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1092,log,log,1092,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,1,['log'],['log']
Testability," ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3437,log,login,3437,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['log'],['login']
Testability," = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1293,Assert,AssertionError,1293,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,"['Assert', 'assert']","['AssertionError', 'assertion']"
Testability," >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux PartitionIterators.cpp -MG -M -MF build/PartitionIterators.d -MT build/PartitionIterators.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:5316,test,test,5316,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['test'],['test']
Testability," Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://clo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:2849,log,logregpvalwaldlrt,2849,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409,1,['log'],['logregpvalwaldlrt']
Testability," Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E + Call(alleles=[0, 0], phased=True),; E Call(alleles=[0, 0], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 1], phased=True),; E - Call(alleles=[1, 1], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[0, 0], phased=True),; E - Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 0], phased=True),; E Call(alleles=[0, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E + Call(alleles=[1, 1], phased=True),; E ]. test/hail/methods/test_statgen.py:1642: AssertionError; ------------------------------ Captured log call -------------------------------; WARNING backend.service_backend:java.py:186 To ensure reproducible randomness across Hail sessions, you must set the ""global_seed"" parameter in hl.init(), in addition to the local seed in each random function.; INFO backend.service_backend:java.py:190 balding_nichols_model: generating genotypes for 1 populations, 5 samples, and 5 variants...; INFO batch_client.aioclient:aioclient.py:741 created batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859; INFO batch_client.aioclient:aioclient.py:758 updated batch 859. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495:4071,test,test,4071,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397330495,3,"['Assert', 'log', 'test']","['AssertionError', 'log', 'test']"
Testability, Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; l,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4564,test,testng,4564,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability, Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4371,test,testng,4371,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability," RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes need to see libhail.so. The header files are also in the jar, and have to be; unpacked in a convoluted way, and that could probably be simplified if/when we change; the approach to packaging. Once this goes in, I can follow it with a PR which adds the NativePackDecoder in RowStore.scala,; controlled by whether environment variable ""HAIL_ENABLE_CPP_CODEGEN"" is defined; (so defaulting to using the JVM bytecode CompiledPackDecoder).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:1782,test,tested,1782,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,6,"['log', 'test']","['logging', 'test', 'tested']"
Testability," SHLVL=1; HOME=/home/hadoop; LOGNAME=hadoop; QTLIB=/usr/lib64/qt-3.3/lib; SSH_CONNECTION=103.37.196.84 60539 192.168.124.160 22; LESSOPEN=||/usr/bin/lesspipe.sh %s; XDG_RUNTIME_DIR=/run/user/995; _=/usr/bin/env; ```; </p>; </details> . ```sh ; /usr/bin/which: no scala in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/); ```. ## Debug mode. ```sh; $ set -x; ++ printf '\033]0;%s@%s:%s\007' hadoop ip-192-168-124-160 '~'; $ spark-shell; + spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings scala.reflect.internal.settings.MutableSettings$.SettingsOps(scala.reflect.internal.settings.MutableSettings)'; at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$1(ILoop.scala:914); at scala.tools.nsc.interpreter.ILoop.mkReader$1(ILoop.scala:920); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$4(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$3(ILoop.scala:926); at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:926); at org.apache.spark.repl.SparkILoop.$anonfun$process$1(SparkILoop.scala:138); at scala.Option.fold(Option.scala:251); at org.apache.spark.repl.SparkILoop.newReader$1(SparkILoop.scala:138); at org.apache.spark.repl.SparkILoop.preLoop$1(SparkILoop.scala:142); at org.apache.spark.repl.SparkILoop.$an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:9438,log,log,9438,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['log'],['log']
Testability, ```; mysql> select * from aggregated_billing_project_user_resources_v3 where resource_id = 6 limit 100;; +----------------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:633,test,test,633,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,32,['test'],['test']
Testability," a 35K cohort. The VCF format of chr1 is 2.4T.; > ; > Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" ([samtools/hts-specs#434](https://github.com/samtools/hts-specs/pull/434)) and ""reference blocks"" ([samtools/hts-specs#435](https://github.com/samtools/hts-specs/pull/435)). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF.; > ; > What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. Looks like VDS is a better solution than HailMatrix. However, we got the joint call result as vcf alreay. Can VDS Combiner read joint call VCF and then save it as VDS format? I cannot find any example to transfer VCF to VDS. Thanks. > ; > > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11.; > ; > Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344:1435,test,tested,1435,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344,1,['test'],['tested']
Testability," amino acid for the highest-impact effect resulting from the current variant (in HGVS style)"">; ##INFO=<ID=SNPEFF_CODON_CHANGE,Number=1,Type=String,Description=""Old/New codon for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_EFFECT,Number=1,Type=String,Description=""The highest-impact effect resulting from the current variant (or one of the highest-impact effects, if there is a tie)"">; ##INFO=<ID=SNPEFF_EXON_ID,Number=1,Type=String,Description=""Exon ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_FUNCTIONAL_CLASS,Number=1,Type=String,Description=""Functional class of the highest-impact effect resulting from the current variant: [NONE, SILENT, MISSENSE, NONSENSE]"">; ##INFO=<ID=SNPEFF_GENE_BIOTYPE,Number=1,Type=String,Description=""Gene biotype for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_GENE_NAME,Number=1,Type=String,Description=""Gene name for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_IMPACT,Number=1,Type=String,Description=""Impact of the highest-impact effect resulting from the current variant [MODIFIER, LOW, MODERATE, HIGH]"">; ##INFO=<ID=SNPEFF_TRANSCRIPT_ID,Number=1,Type=String,Description=""Transcript ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=STR,Number=0,Type=Flag,Description=""Variant is a short tandem repeat"">; ##INFO=<ID=VQSLOD,Number=1,Type=Float,Description=""Log odds ratio of being a true variant versus being false under the trained gaussian mixture model"">; ##INFO=<ID=culprit,Number=1,Type=String,Description=""The annotation which was the worst performing in the Gaussian mixture model, likely the reason why the variant was filtered out"">; ##INFO=<ID=set,Number=1,Type=String,Description=""Source VCF for the merged record in CombineVariants"">; ##OriginalSnpEffCmd=""SnpEff eff -v -onlyCoding true -c /seq/references/Homo_sapiens_assembly19/v1/snpEff/Homo_sapiens_assembly19.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:10565,Log,Log,10565,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['Log'],['Log']
Testability," and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large ba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3678,test,test,3678,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['test'],['test']
Testability," authority); -; - self._client = client; + self._client = msal.ConfidentialClientApplication(data['appId'], data['password'], authority); self._tenant_id = tenant_id; ; def initiate_flow(self, redirect_uri: str) -> dict:; @@ -107,10 +170,31 @@ class AzureFlow(Flow):; ; return FlowResult(token['id_token_claims']['oid'], token['id_token_claims']['preferred_username'], token); ; -; -def get_flow_client(credentials_file: str) -> Flow:; - cloud = get_global_config()['cloud']; - if cloud == 'azure':; - return AzureFlow(credentials_file); - assert cloud == 'gcp'; - return GoogleFlow(credentials_file); + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + tenant_id = oauth2_client['tenant']; + authority = f'https://login.microsoftonline.com/{tenant_id}'; + app = msal.PublicClientApplication(oauth2_client['appId'], authority=authority); + credentials = app.acquire_token_interactive([oauth2_client['userOauthScope']]); + return {**oauth2_client, 'refreshToken': credentials['refresh_token']}; +; + @staticmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + audience = oauth2_client['appIdentifierUri']; +; + try:; + kid = jwt.get_unverified_header(access_token)['kid']; +; + if AzureFlow._aad_keys is None:; + resp = await session.get_read_json('https://login.microsoftonline.com/common/discovery/keys'); + AzureFlow._aad_keys = resp['keys']; +; + jwk = [key for key in AzureFlow._aad_keys if key['kid'] == kid][0]; + der_cert = base64.b64decode(jwk['x5c'][0]); + cert = x509.load_der_x509_certificate(der_cert, default_backend()); + pem_key = cert.public_key().public_bytes(encoding=serialization.Encoding.PEM, format=serialization.PublicFormat.SubjectPublicKeyInfo).decode(); +; + decoded = jwt.decode(access_token, pem_key, algorithms=['RS256'], audience=audience); + return decoded['oid']; + except jwt.InvalidTokenError:; + return None; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:6289,log,login,6289,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['log'],['login']
Testability," below. Python 3.6, (should) only appear in the linting image for python 3.6. The `hail-ubuntu` image explicitly installs Python 3.7. I'm happy to drop linting for Python 3.6 from build.yaml if compilers team is OK with that (ask Tim). We are already using Ubuntu 20.04 for our services and tests. See below for details, but `hail-ubuntu` is based on 20.04. We explicitly install JDK 8 in the `base` image. ---. This PR doesn't change the hail-ubuntu image which is the basis for nearly all our images. `DOCKER_ROOT_IMAGE` seems to have been introduced [here](https://github.com/hail-is/hail/pull/9660). That's my bad for not flagging in the review that this isn't actually a ""root image"". `DOCKER_ROOT_IMAGE` is just some Linux image with the standard utils. It was introduced as a distinct global concept when Docker Hub began enforcing rate limits (so we needed all our test images to live in GCR). If you look at the occurrences of `DOCKER_ROOT_IMAGE` or `docker_root_image` you'll find it's almost exclusively used in the tests except for one occurrence in `build-batch-worker-image-startup-gcp.sh`. We could just remove that line. That line is an attempt to keep a relatively recent version of the ubuntu image cached on the worker VM so that we can save some time when pulling the worker Docker image. In practice, the ubuntu image is extraordinarily tiny and quickly becomes out of date (because we rarely rebuild the VM). hail-ubuntu uses a timestamped ubuntu 20.04 tag: `ubuntu:focal-20201106`. I did this because we kept getting screwed by new ubuntu images getting released which were incompatible with us. We would only find out later when we changed the hail-ubuntu Dockerfile and triggered a refetch of the latest image at the `ubuntu:18.04` tag which included the breaking changes. You're correct that this is technical debt of ours. DOCKER_ROOT_IMAGE should really be SOME_LINUX_IMAGE and we shouldn't bother trying to fetch it in the worker image startup script. You're fighting the ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805:1165,test,tests,1165,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805,1,['test'],['tests']
Testability," chr1	159205821	.	CT	C	.	.	.; chr1	173803162	.	G	T	.	.	.; chr1	179813831	.	G	A	.	.	.; chr1	179917551	.	T	C	.	.	.; chr1	180935962	.	G	C	.	.	.; chr1	180941229	.	G	A	.	.	.; chr1	186893053	.	C	A	.	.	.; chr1	201363319	.	G	A	.	.	.; chr1	223749094	.	A	G	.	.	.; chr1	224294328	.	G	A	.	.	.; chr1	235809337	.	G	A	.	.	.; chr1	241592073	.	G	T	.	.	.; chr2	9376947	.	G	A	.	.	.; chr2	11618532	.	C	T	.	.	.; ```. We can see the characteristic super high memory use.; <img width=""570"" alt=""Screenshot 2023-11-28 at 16 35 26"" src=""https://github.com/hail-is/hail/assets/106194/e5dfa586-5c77-479b-8050-9b0b7d2fe319"">. ---. If we use the same header, but just one variant, it succeeds, but notice that the RAM use grows rapidly. https://batch.hail.is/batches/8089064/jobs/3; ```; chr1	241592073	.	G	T	.	.	.; ```; <img width=""577"" alt=""Screenshot 2023-11-28 at 16 37 39"" src=""https://github.com/hail-is/hail/assets/106194/90c5ab45-9ca4-43e0-9a97-bf6032863f32"">. ---. If we use the same header with this variant from our (successful) test VCF, the RAM use grows much more slowly. https://batch.hail.is/batches/8089322/jobs/3; ```; chr1	10122	.	A	C	128.00	AC0	AC=0;AN=90064;AF=0.00000e+00;lcr;variant_type=snv;n_alt_alleles=1;ReadPosRankSum=-6.69000e-01;MQRankSum=-1.58200e+00;RAW_MQ=1.86842e+05;DP=119;MQ_DP=137;VarDP=119;MQ=3.69298e+01;QD=1.07563e+00;FS=2.34212e+00;SB=281,217,5,6;InbreedingCoeff=-1.46711e-05;AS_VQSLOD=-6.44890e+00;NEGATIVE_TRAIN_SITE;culprit=AS_QD;SOR=6.72000e-01;AC_asj_female=0;AN_asj_female=1182;AF_asj_female=0.00000e+00;nhomalt_asj_female=0;AC_eas_female=0;AN_eas_female=878;AF_eas_female=0.00000e+00;nhomalt_eas_female=0;AC_afr_male=0;AN_afr_male=11652;AF_afr_male=0.00000e+00;nhomalt_afr_male=0;AC_female=0;AN_female=45840;AF_female=0.00000e+00;nhomalt_female=0;AC_fin_male=0;AN_fin_male=5562;AF_fin_male=0.00000e+00;nhomalt_fin_male=0;AC_oth_female=0;AN_oth_female=702;AF_oth_female=0.00000e+00;nhomalt_oth_female=0;AC_ami=0;AN_ami=488;AF_ami=0.00000e+00;nhomalt_ami=0;AC_oth=0;AN_oth=1340;AF_ot",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344:1974,test,test,1974,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830846344,2,['test'],['test']
Testability," class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2747,log,logging,2747,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,3,['log'],"['log', 'logging']"
Testability," deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1727,test,test,1727,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability," deleted.""); /Users/dking/projects/hail/datasets/extract/extract_CADD.py:26: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:1061: return self.domain is not None and (self.domain == 'gcr.io' or self.domain.endswith('docker.pkg.dev')); /Users/dking/projects/hail/hail/python/hailtop/aiocloud/aiogoogle/client/container_client.py:6: super().__init__(f'https://gcr.io/v2/{project}', **kwargs); /Users/dking/projects/hail/datasets/extract/extract_dbSNP.py:22: j.image(""gcr.io/broad-ctsa/datasets:050521"")",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:1376,assert,assert,1376,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['assert'],['assert']
Testability," directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of linux, we can reevaluate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:1137,log,log,1137,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,['log'],['log']
Testability, gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 2]hail: info: count:; nSamples 1; nVariants 398; nCalled 195; callRate 48.995%; hail: info: timing:; gen: 1.126s; count: 638.722ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 6]hail: info: count:; nSamples 147; nVariants 3; nCalled 229; callRate 51.927%; hail: info: timing:; gen: 1.624s; count: 1.126s; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 6]hail: info: count:; nSamples 5; nVariants 2; nCalled 5; callRate 50.000%; hail: info: timing:; gen: 1.062s; count: 614.361ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 9]hail: info: count:; nSamples 6; nVariants 27; nCalled 84; callRate 51.852%; hail: info: timing:; gen: 1.280s; count: 990.637ms. ```. This one sampled the rectangle 900x1. The 0 count for variants is caused by `buildableOf` and friends returning an empty sequence with probability `1/(fuel+1)`: 50% in this case. The 900 fuel units happened to yield 820 partitions which each shared the 900 fuel. Many of the sample ids were single digits in this run (due to low fuel). ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 10]hail: info: count:; nSamples 820; nVariants 0; nCalled 0; callRate NA; hail: info: timing:; gen: 952.424ms; count: 569.784ms; ```. ```; hail: info: running: gen; hail: info: running: count --genotypes; [Stage 0:> (0 + 0) / 5]hail: info: count:; nSamples 28; nVariants 22; nCalled 328; callRate 53.247%; hail: info: timing:; gen: 1.169s; count: 1.355s; ```. And here's the above pairs plotted with samples on the x and variants on the y. Second one is log-log.; ![figure_2](https://cloud.githubusercontent.com/assets/106194/17625714/05957f52-6078-11e6-98cb-7f41c7970b83.png); ![figure_1](https://cloud.githubusercontent.com/assets/106194/17625713/05928482-6078-11e6-9154-a364bb739e16.png),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/572#issuecomment-239462375:3273,log,log-log,3273,https://hail.is,https://github.com/hail-is/hail/pull/572#issuecomment-239462375,1,['log'],['log-log']
Testability," happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 35",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:1146,log,log,1146,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,2,['log'],['log']
Testability," issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=4.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:1143,assert,assertEqual,1143,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406,1,['assert'],['assertEqual']
Testability," looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1042,test,test-tiny-limit,1042,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['test'],['test-tiny-limit']
Testability," much as ~12 GB/s of aggregate bandwidth. We seem to have room for improvement, but this seems good enough for now. # On AWS, GCS -> S3. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 34 seconds | 154.5 MB/s |; | 1 | 42.9 GB | 4 minutes | 161.6 MB/s |; | 200 | 5.4 GB | 35 seconds | 151.1 MB/s |; | 40000 | 5.4 GB | 4 minutes | 22.0 MB/s |. # On GCP, S3 -> GCS. | Files | Bytes | Time | Rate |; | ----- | ----- | ---- | ---- |; | 1 | 5.4 GB | 17 seconds | 304.2 MB/s |; | 1 | 42.9 GB | 3 minutes | 235.5 MB/s |; | 200 | 5.4 GB | 20 seconds | 267.8 MB/s |; | 40000 | 5.4 GB | 6 minutes | 13.3 MB/s |. # machine parsable form; ```; [{'config': 'one',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/one',; 'times': [34.76],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/one'},; {'config': 'some',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/some',; 'times': [35.527],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/some'},; {'config': 'many',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/many',; 'times': [244.154],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 'gs://1-day/tmp/test-copy/dking-benchmark/huge',; 'times': [265.719],; 'to': 's3://hail-test-dy5rg/tmp/target/dking-benchmark/huge'},; {'config': 'one',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/one',; 'times': [17.65],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/one'},; {'config': 'some',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/some',; 'times': [20.048],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/some'},; {'config': 'many',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/many',; 'times': [402.267],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/many'},; {'config': 'huge',; 'from': 's3://hail-test-dy5rg/tmp/test-copy/dking-benchmark/huge',; 'times': [182.355],; 'to': 'gs://1-day/tmp/test-copy/target/dking-benchmark/huge'}]. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697:1361,test,test-,1361,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-897651697,26,"['benchmark', 'test']","['benchmark', 'test-', 'test-copy']"
Testability," o.s.j.s.ServletContextHandler@57318cba{/environment/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5c9c4006{/executors,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/ge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:6975,log,log,6975,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['log'],['log']
Testability," on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_H",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1037,test,tests,1037,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721,1,['test'],['tests']
Testability, org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at is.hail.io.fs.FSSuite.largeDirectoryOperations(FSSuite.scala:445); at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:430); at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:136); at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:658); at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:219); at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50); at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:923); at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:192); at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146); at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128); at java.util.ArrayList.forEach(ArrayList.java:1259); at org.testng.TestRunner.privateRun(TestRunner.java:808); at org.testng.TestRunner.run(TestRunner.java:603); at org.testng.SuiteRunner.runTest(SuiteRunner.java:429); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:383); at org.testng.SuiteRunner.run(SuiteRunner.java:326); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1249); at org.testng.TestNG.runSuitesLocally(TestNG.java:1169); at org.testng.TestNG.runSuites(TestNG.java:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547:1490,Test,TestInvoker,1490,https://hail.is,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547,1,['Test'],['TestInvoker']
Testability," pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main': ""2022-11-15 20:30:18.004 Tokens: INFO: tokens found for namespaces {default}\n2022-11-15 20:30:18.004 tls: INFO: ssl config file found at /batch/2bbb233e4e3c4a96bbffb515019daac9/secrets/ssl-config/ssl-config.json\n2022-11-15 20:30:18.006 GoogleStorageFS$: INFO: Initializing google storage client from service account key\n2022-11-15 20:30:18.114 root: INFO: RegionPool: initialized for thread 8: pool-1-thread-1\n2022-11-15 20:30:18.114 ServiceBackend$: INFO: executing: cEPZ5IV9gUtSnCiAiHXOPs None\n2022-11-15 20:30:18.127 root: INFO: optimize optimize: darrayLowerer, initial IR: before: IR size 17: \n(Let __rng_state\n (RNGStateLiteral (0 0 0 0))\n (MakeTuple (0)\n (TableAggregate\n (TableMapRows\n (TableOrderBy (Aidx) (TableRange 100000000 50))\n (InsertFields\n (SelectFields () (SelectFields (idx) (Ref row)))\n None\n (idx (GetField idx (Ref row)))))\n (MakeStruct\n (idx\n (ApplyAggOp Colle",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:64808,test,test,64808,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['test'],['test']
Testability, scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:276); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:40); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:39); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:339); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:314); 	at is.hail.expr.ir.IRSuite.testStreamLenUnconsumedInnerStream(IRSuite.scala:1800); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:3109,assert,assertEvalsTo,3109,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,2,"['Test', 'assert']","['TestUtils', 'assertEvalsTo']"
Testability," simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipped) by replacing the dark mode icon svg with a reference to the material-design-icons font.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:1539,log,logged,1539,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,4,['log'],['logged']
Testability," spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough  in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1706,log,log,1706,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,2,['log'],"['log', 'logging']"
Testability," src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Upcalls.cpp -MG -M -MF build/Upcalls.d -MT build/Upcalls.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux testutils/unit-tests.cpp -MG -M -MF build/testutils/unit-tests.d -MT build/testutils/unit-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux test.cpp -MG -M -MF build/test.d -MT build/test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Region_test.cpp -MG -M -MF build/Region_test.d -MT build/Region_test.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Region.cpp -MG -M -MF build/Region.d -MT build/Region.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I..",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:5023,test,testutils,5023,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['test'],"['tests', 'testutils']"
Testability," struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 3520, 51 row lock(s), undo log entries 30; MySQL thread id 1941930, OS thread handle 140298159240960, query id 1869168731 10.32.3.8 dgoldste update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll, token, n_running_cancellable_jobs, running_can; cellable_cores_mcpu); VALUES (OLD.batch_id, OLD.inst_coll, rand_token, -1, -OLD.cores_mcpu); ON DUPLICATE KEY UPDATE; n_running_cancellable_jobs = n_running_cancellable_jobs - 1,; running_cancellable_cores_mcpu = running_cancellable_cores_mcpu - OLD.cores_mcpu; *** (2) HOLDS THE LOCK(S):; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409370 lock_mode X locks rec but not gap; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:2128,log,log,2128,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,2,['log'],['log']
Testability, test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2242,test,test,2242,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2277,test,test,2277,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2311,test,test,2311,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | test | 6 | 100 | 44608 |; | test | test | 6 | 101 | 120512 |; | test | test | 6 | 102 | 233472 |; | test | test | 6 | 103 | 219584 |; | test | test | 6 | 105 | 399872 |; | test | test | 6 | 106 | 617600 |; | test | test | 6 | 107 | 114880 |; | test | test | 6 | 108 | 47616 |; | test | test | 6 | 109 | 98304 |; | test | test | 6 | 110 | 73472 |; | test | test | 6 | 111 | 44928 |; | test | test | 6 | 112 | 37696 |; +----------------------------------+------+-------------+-------+------------+; 100 rows in set (0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:3208,test,test,3208,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,51,['test'],['test']
Testability, test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 | test | test | 6 | 116 | 49216 |; | 2023-07-13 | test | test | 6 | 117 | 24640 |; | 2023-07-13 | test | test | 6 | 118 | 254080 |; | 2023-07-13 | test | test | 6 | 119 | 37056 |; | 2023-07-13 | test | test | 6 | 120 | 110784 |; | 2023-07-13 | test | test | 6 | 122 | 29888 |; | 2023-07-13 | test | test | 6 | 124 | 39616 |; | 2023-07-13 | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:11079,test,test,11079,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2035,test,test,2035,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1792,test,test,1792,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2235,test,test,2235,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2270,test,test,2270,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2304,test,test,2304,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8993,test,test,8993,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 | test | test | 6 | 116 | 49216 |; | 2023-07-13 | test | test | 6 | 117 | 24640 |; | 2023-07-13 | test | test | 6 | 118 | 254080 |; | 2023-07-13 | test | test | 6 | 119 | 37056 |; | 2023-07-13 | test | test | 6 | 120 | 110784 |; | 2023-07-13 | test | test | 6 | 122 | 29888 |; | 2023-07-13 | test | test | 6 | 124 | 39616 |; | 2023-07,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:11072,test,test,11072,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2028,test,test,2028,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability," that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvironments/hail/python`. `echo $HAIL_HOME; /scratch/PI/dpwall/computeEnvironments/hail`. Thank you, and if you have any ideas why the above tests are failing I would love to hear it. Thanks again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:2762,test,tests,2762,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721,1,['test'],['tests']
Testability," the HTTP status code returned by cluster deletion. FWIW, the repository appears to be deleted now (https://github.com/hail-ci-test/ci-test-p4a9fxo7), and, AFAIK, no other part of our system deletes these repositories. [1]:; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://ap",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1637,test,test,1637,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2073,log,log,2073,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,3,['log'],"['log', 'logger', 'logging']"
Testability," vds = hl.vds.read_vds(vds_file); ...: mt = hl.vds.to_dense_mt(vds); ...: t = gnomad.utils.sparse_mt.default_compute_info(mt); ...: t = t.annotate(info=t.info.drop('AS_SB_TABLE')); ...: t = t.annotate(info = t.info.drop(; ...: 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; ...: )); ...: t = t.drop('AS_lowqual'); ...: ; ...: hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```; worker failure:; ```; 2023-09-27 16:43:10.389 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 2: /batch/83e7aee9e9244f6884b8a84ea81b4c7a; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 3: /batch/83e7aee9e9244f6884b8a84ea81b4c7a/log; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 4: gs://hail-test-ezlis/dking/jars/ch4g3zvqceyo/09526a168d57dac1a26f8caa4ab49593931ed2ef.jar; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 5: worker; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 7: 7028; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 8: 9060; 2023-09-27 16:43:10.390 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-27 16:43:10.393 Worker$: INFO: is.hail.backend.service.Worker 09526a168d57dac1a26f8caa4ab49593931ed2ef; 2023-09-27 16:43:10.394 Worker$: INFO: running job 7028/9060 at root gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 WorkerTimer$: INFO: readInputs took 384.743327 ms.; 2023-09-27 16:43:10.779 : INFO: RegionPool: initial",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:2497,test,test-ezlis,2497,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-ezlis']
Testability, where resource_id = 6 limit 100;; +----------------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1341,test,test,1341,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability," will issue warnings when used, because it doesnt distinguish between dependencies that impact the public API of a Java library project and those that dont. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our test code references these third-party classes:; ```; import breeze.linalg.DenseMatrix; import breeze.linalg._; import breeze.linalg.{*, diag, DenseMatrix => BDM, DenseVector => BDV}; import breeze.linalg.{DenseMatrix => BDM, _}; import breeze.linalg.{DenseMatrix => BDM}; import breeze.linalg.{DenseMatrix, DenseVector, eigSym, svd}; import breeze.linalg.{DenseMatrix, DenseVector}; import breeze.linalg.{DenseMatrix, Matrix, Vector}; import breeze.linalg.{Vector => BVector}; import htsjdk.samtools.reference.ReferenceSequenceFileFactory; import htsjdk.samtools.util.BlockCompressedFilePointerUtil; import htsjdk.tribble.readers.{TabixReader => HtsjdkTabixReader}; import org.apache.avro.SchemaBuilder; import org.apache.avro.file.DataFileWriter; import org.apache.avro.generic.{GenericDatumWriter, GenericRecord, GenericRecordBuilder}; import org.apache.commons.io.IOUtils; import org.apache.commons.math3.distribution.ChiSquaredDistribution; import or",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:1916,test,testImplementation,1916,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,4,['test'],"['test', 'testImplementation']"
Testability, | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2692,test,test,2692,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | test | 6 | 100 | 44608 |; | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2726,test,test,2726,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | test | 6 | 100 | 44608 |; | test | test | 6 | 101 | 120512 |; | test | test | 6 | 102 | 233472 |; | test | test | 6 | 103 | 219584 |; | test | test | 6 | 105 | 399872 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2864,test,test,2864,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1656,test,test,1656,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; +--------------+----------------------------------+------+-------------+-------+------------+; 4 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; | 2023-07-13 | test | test | 6 | 59 | 51584 |; | 2023-07-13 | test | test | 6 | 62 | 55616 |; | 2023-07-13 | test | test | 6 | 138 | 700160 |; | 2023-07-13 | test | test | 6 | 183 | 54976 |; +--------------+----------------------------------+------+-------------+-------+------------+; 8 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 202,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:13737,test,test,13737,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,12,['test'],['test']
Testability, | test | 6 | 120 | 110784 |; | 2023-07-13 | test | test | 6 | 122 | 29888 |; | 2023-07-13 | test | test | 6 | 124 | 39616 |; | 2023-07-13 | test | test | 6 | 126 | 31232 |; | 2023-07-13 | test | test | 6 | 127 | 61696 |; | 2023-07-13 | test | test | 6 | 129 | 60096 |; | 2023-07-13 | test | test | 6 | 131 | 167040 |; | 2023-07-13 | test | test | 6 | 132 | 312768 |; | 2023-07-13 | test | test | 6 | 134 | 53312 |; | 2023-07-13 | test | test | 6 | 136 | 51840 |; | 2023-07-13 | test | test | 6 | 137 | 68224 |; +--------------+----------------------------------+------+-------------+-------+------------+; 100 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; +--------------+----------------------------------+------+-------------+-------+------------+; 4 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; | 202,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:12996,test,test,12996,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,4,['test'],['test']
Testability, | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9326,test,test,9326,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9373,test,test,9373,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9421,test,test,9421,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9468,test,test,9468,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9515,test,test,9515,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9562,test,test,9562,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9657,test,test,9657,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9704,test,test,9704,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2685,test,test,2685,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9988,test,test,9988,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | test | 6 | 100 | 44608 |; | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2719,test,test,2719,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10035,test,test,10035,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10082,test,test,10082,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10129,test,test,10129,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | test | 6 | 76 | 121536 |; | test | test | 6 | 77 | 132736 |; | test | test | 6 | 78 | 42432 |; | test | test | 6 | 79 | 563648 |; | test | test | 6 | 80 | 943616 |; | test | test | 6 | 81 | 34048 |; | test | test | 6 | 82 | 23424 |; | test | test | 6 | 83 | 37952 |; | test | test | 6 | 84 | 428352 |; | test | test | 6 | 86 | 40576 |; | test | test | 6 | 88 | 141312 |; | test | test | 6 | 89 | 203584 |; | test | test | 6 | 90 | 465536 |; | test | test | 6 | 91 | 47808 |; | test | test | 6 | 92 | 35456 |; | test | test | 6 | 93 | 768192 |; | test | test | 6 | 94 | 172416 |; | test | test | 6 | 95 | 48384 |; | test | test | 6 | 96 | 146816 |; | test | test | 6 | 97 | 69568 |; | test | test | 6 | 99 | 203520 |; | test | test | 6 | 100 | 44608 |; | test | test | 6 | 101 | 120512 |; | test | test | 6 | 102 | 233472 |; | test | test | 6 | 103 | 219584 |; | test | test | 6 | 105 | 399872 |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:2857,test,test,2857,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10699,test,test,10699,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 | test |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10746,test,test,10746,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 | test | test | 6 | 116 | 49216 |; | 2023-07-13 | test | test | 6 | 117 | 24640 |; | 2023-07-13 | test | test | 6 | 118 | 254080 |; | 2023-07-13 | test | test | 6 | 119 | 37056 |; | 2023-07-13 | test | test | 6 | 120 | 110784 |; | 2023-07-13 | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10983,test,test,10983,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1966,test,test,1966,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9319,test,test,9319,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9366,test,test,9366,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9414,test,test,9414,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9461,test,test,9461,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9508,test,test,9508,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9555,test,test,9555,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9650,test,test,9650,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9697,test,test,9697,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9981,test,test,9981,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10028,test,test,10028,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10075,test,test,10075,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10122,test,test,10122,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10692,test,test,10692,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 |,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10739,test,test,10739,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | test | test | 6 | 114 | 54400 |; | 2023-07-13 | test | test | 6 | 115 | 106240 |; | 2023-07-13 | test | test | 6 | 116 | 49216 |; | 2023-07-13 | test | test | 6 | 117 | 24640 |; | 2023-07-13 | test | test | 6 | 118 | 254080 |; | 2023-07-13 | test | test | 6 | 119 | 37056 |; | 2023-07-13 | test | test | 6 | 120 | 110784 |; | 2023-07,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10976,test,test,10976,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | test | test | 6 | 74 | 28352 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1959,test,test,1959,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8709,test,test,8709,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1717,test,test,1717,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability, |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1855,test,test,1855,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,![Screen Shot 2020-05-18 at 5 07 59 PM](https://user-images.githubusercontent.com/106194/82259993-7d6a0e00-992a-11ea-8c3c-2e675fa61c42.png); ![Screen Shot 2020-05-18 at 5 07 53 PM](https://user-images.githubusercontent.com/106194/82259994-7d6a0e00-992a-11ea-9eee-7a03e83d12db.png). I tested it by making these changes in my browser on the live CI website.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8818#issuecomment-630437136:284,test,tested,284,https://hail.is,https://github.com/hail-is/hail/pull/8818#issuecomment-630437136,1,['test'],['tested']
Testability,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:6294,log,log,6294,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,6,['log'],['log']
Testability,"""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000263119.5:c.2558G>A"",""hgvsp"":""ENSP00000263119.5:p.(Arg853Gln)"",""isCanonical"":null,""polyPhenScore"":0.625,""polyPhenPrediction"":""possibly damaging"",""proteinId"":""ENSP00000263119.5"",""proteinPos"":""853"",""siftScore"":0.01,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000405822.2"",""bioType"":""protein_coding"",""aminoAcids"":""R/Q"",""cDnaPos"":""2502"",""codons"":""cGg/cAg"",""cdsPos"":""2408"",""exons"":""17/36"",""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000405822.2:c.2408G>A"",""hgvsp"":""ENSP00000384694.2:p.(Arg803Gln)"",""isCanonical"":null,""polyPhenScore"":0.94,""polyPhenPrediction"":""probably damaging"",""proteinId"":""ENSP00000384694.2"",""proteinPos"":""803"",""siftScore"":0.02,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000398319.2"",""bioType"":""protein_coding"",""aminoAcids"":""R/Q"",""cDnaPos"":""2943"",""codons"":""cGg/cAg"",""cdsPos"":""2558"",""exons"":""18/37"",""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""missense_variant""],""hgvsc"":""ENST00000398319.2:c.2558G>A"",""hgvsp"":""ENSP00000381364.2:p.(Arg853Gln)"",""isCanonical"":true,""polyPhenScore"":0.625,""polyPhenPrediction"":""possibly damaging"",""proteinId"":""ENSP00000381364.2"",""proteinPos"":""853"",""siftScore"":0.01,""siftPrediction"":""deleterious""},{""transcript"":""ENST00000484593.1"",""bioType"":""retained_intron"",""aminoAcids"":null,""cDnaPos"":null,""codons"":null,""cdsPos"":null,""exons"":null,""introns"":null,""geneId"":""ENSG00000099991"",""hgnc"":""CABIN1"",""consequence"":[""downstream_gene_variant""],""hgvsc"":null,""hgvsp"":null,""isCanonical"":null,""polyPhenScore"":null,""polyPhenPrediction"":null,""proteinId"":null,""proteinPos"":null,""siftScore"":null,""siftPrediction"":null}]},""genes"":null}]}}; ```. I've added the experimental warning until there is some form of automated testing in place (perhaps validating that consistency is maintained against a fixed output file that's been otherwise reviewed by Nirvana). Note that this method is modeled on VEP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:6421,test,testing,6421,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['test'],['testing']
Testability,"""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4170,test,test,4170,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2024,test,test,2024,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability,"""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1938,test,test,1938,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability,"""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_377,GetField(Ref(__iruid_377,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:42); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:617); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:266); 	at is.hail.expr.ir.Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:9026,test,test,9026,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['test'],['test']
Testability,"""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collection.mutable.HashMap.apply(HashMap.scala:65); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:38); 	at is.hail.expr.ir.Memo.lookup(RefEquality.scala:37); 	at is.hail.expr.ir.Memo.apply(RefEquality.scala:40); 	at is.hail.expr.ir.Requiredness.lookup(Requiredness.scala:41); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:616); 	at is.hail.expr.ir.Requiredness$$anonfun$analyzeIR$16.apply(Requiredness.scala:615); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at is.hail.expr.ir.Requiredness.analyzeIR(Requiredness.scala:615); 	at is.hail.expr.ir.Requiredness.analyze(Requiredness.scala:265); 	at is.hail.expr.ir.Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8746,test,test,8746,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['test'],['test']
Testability,"""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4079,test,test,4079,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1344,log,logLkhd,1344,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538,7,['log'],"['logDelta', 'logLkhd']"
Testability,"## delta.tsv has two columns; # logDelta given by global.lmmreg.fit.logDeltaGrid; # logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1094,log,log,1094,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538,4,['log'],"['log', 'logLkhd']"
Testability,"### Experiment 1. ```scala; private def boundary(node: IR): IR = {; var streamified = streamify(node). if (streamified.typ.isInstanceOf[TStream] && node.typ.isInstanceOf[TArray]) {; streamified = ToArray(streamified); }. if (streamified.typ.isInstanceOf[TArray] && node.typ.isInstanceOf[TStream]); streamified = ToStream(streamified). println(s""\n\nbefore: \n${node} of typ: ${node.typ}\nafter: ${streamified} of typ: ${streamified.typ}\n\n""); assert(streamified.typ == node.typ); streamified; }. private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TArray]) {; ToStream(node); } else {; node; }; }; }; }. // in streamify; case _ =>; val newChildren = node.children.map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:444,assert,assert,444,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,2,"['assert', 'test']","['assert', 'testDictContains']"
Testability,"##INFO=<ID=MLEAF,Number=A,Type=Float,Description=""Maximum likelihood expectation (MLE) for the allele frequency (not necessarily the same as the AF), for each ALT allele, in the same order as listed"">; ##INFO=<ID=MQ,Number=1,Type=Float,Description=""RMS Mapping Quality"">; ##INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ##INFO=<ID=MQRankSum,Number=1,Type=Float,Description=""Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities"">; ##INFO=<ID=OriginalContig,Number=1,Type=String,Description=""The name of the source contig/chromosome prior to liftover."">; ##INFO=<ID=OriginalStart,Number=1,Type=String,Description=""The position of the variant on the source contig prior to liftover."">; ##INFO=<ID=QD,Number=1,Type=Float,Description=""Variant Confidence/Quality by Depth"">; ##INFO=<ID=RPA,Number=.,Type=Integer,Description=""Number of times tandem repeat unit is repeated, for each allele (including reference)"">; ##INFO=<ID=RU,Number=1,Type=String,Description=""Tandem repeat unit (bases)"">; ##INFO=<ID=ReadPosRankSum,Number=1,Type=Float,Description=""Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias"">; ##INFO=<ID=SNPEFF_AMINO_ACID_CHANGE,Number=1,Type=String,Description=""Old/New amino acid for the highest-impact effect resulting from the current variant (in HGVS style)"">; ##INFO=<ID=SNPEFF_CODON_CHANGE,Number=1,Type=String,Description=""Old/New codon for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_EFFECT,Number=1,Type=String,Description=""The highest-impact effect resulting from the current variant (or one of the highest-impact effects, if there is a tie)"">; ##INFO=<ID=SNPEFF_EXON_ID,Number=1,Type=String,Description=""Exon ID for the highest-impact effect resulting from the current variant"">; ##INFO=<ID=SNPEFF_FUNCTIONAL_CLASS,Number=1,Type=String,Description=""Functional class of the highest-impact effect resulting from the current variant: [NONE, SILENT, MISSENSE, NONSENSE]"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:8964,test,test,8964,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['test'],['test']
Testability,#13131 is stacked on top of this. Tests passing in that PR means these are the correct IDs that can be used to identify identities in GCP IAM and Azure AD.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13207#issuecomment-1608238964:34,Test,Tests,34,https://hail.is,https://github.com/hail-is/hail/pull/13207#issuecomment-1608238964,1,['Test'],['Tests']
Testability,"#13839 is I think the right way to implement this. It passes `test_block_matrix_entries`, which is the only test I could find that exercises this path. Others that should, like `test_to_table`, can't yet be fully lowered. Maybe try rebasing on my branch and see if you still see the indexing errors?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13807#issuecomment-1766713503:108,test,test,108,https://hail.is,https://github.com/hail-is/hail/pull/13807#issuecomment-1766713503,1,['test'],['test']
Testability,#5872 fixes the problem and tests more robustly,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287:28,test,tests,28,https://hail.is,https://github.com/hail-is/hail/pull/5871#issuecomment-482348287,1,['test'],['tests']
Testability,"#9309 up to solve these issues. If I can get all the tests to pass, I'll feel pretty good about stacking this on top of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676650099:53,test,tests,53,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676650099,1,['test'],['tests']
Testability,"'). regression = (hc.import_vcf(test_resources + '/regressionLinear.vcf'); .split_multi(); .annotate_samples_table(cov, root='sa.cov'); .annotate_samples_table(phen1, root='sa.pheno.Pheno'); .annotate_samples_table(phen2, root='sa.pheno.isCase')).with_id('regression'). vds_assoc = (regression; .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""1"", 1, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno.PhenoLMM = (1 + 0.1 * sa.cov.Cov1 * sa.cov.Cov2) * sa.culprit').with_id('vds_assoc')). vds_kinship = vds_assoc.filter_variants_expr('v.start < 4'); km = vds_kinship.rrm(False, False).with_id('km'); vds_assoc = vds_assoc.lmmreg(km, 'sa.pheno.PhenoLMM', ['sa.cov.Cov1', 'sa.cov.Cov2']); vds_assoc.export_variants('/tmp/lmmreg3.tsv', 'Variant = v, va.lmmreg.*'); ```. History output:; ```; # 2017-08-01T20:23:38.202686; # version: devel-37d32d3. hc = (HailContext(min_block_size=6)). regression = (hc; .import_vcf('src/test/resources/regressionLinear.vcf'); .split_multi(); .annotate_samples_table(hc; .import_table('src/test/resources/regressionLinear.cov', types={'Cov1': TDouble(), 'Cov2': TDouble()}); .key_by('Sample'), root='sa.cov'); .annotate_samples_table(hc; .import_table('src/test/resources/regressionLinear.pheno', types={'Pheno': TDouble()}, missing='0'); .key_by('Sample'), root='sa.pheno.Pheno'); .annotate_samples_table(hc; .import_table('src/test/resources/regressionLogisticBoolean.pheno', types={'isCase': TBoolean()}, missing='0'); .key_by('Sample'), root='sa.pheno.isCase')). vds_assoc = (regression; .annotate_samples_expr('sa.culprit = gs.filter(g => v == Variant(""1"", 1, ""C"", ""T"")).map(g => g.gt).collect()[0]'); .annotate_samples_expr('sa.pheno.PhenoLMM = (1 + 0.1 * sa.cov.Cov1 * sa.cov.Cov2) * sa.culprit')). km = (vds_assoc; .filter_variants_expr('v.start < 4'); .rrm()). (vds_assoc; .lmmreg(km, 'sa.pheno.PhenoLMM', covariates=['sa.cov.Cov1', 'sa.cov.Cov2']); .export_variants('/tmp/lmmreg3.tsv', 'Variant = v, va.lmmreg.*')); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2060#issuecomment-319533736:1587,test,test,1587,https://hail.is,https://github.com/hail-is/hail/pull/2060#issuecomment-319533736,3,['test'],['test']
Testability,"': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'adding cloudfuse support': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'post-job finally block': {'start_time': 1682966179578}}}, 'spec': {'always_copy_output': False, 'job_id': 1, 'process': {'command': ['true'], 'image': 'haildev.azurecr.io/ubuntu:20.04', 'mount_docker_socket': False, 'type': 'docker'}, 'resources': {'req_cpu': '0.25', 'req_memory': '50Mi', 'req_storage': '0Gi', 'cores_mcpu': 250, 'memory_bytes': 1073741824, 'storage_gib': 0, 'preemptible': True}, 'secrets': [{'namespace': 'pr-12955-default-rrlcxki12v8r', 'name': 'test-gsa-key', 'mount_path': '/gsa-key', 'mount_in_copy': True}], 'env': [{'name': 'AZURE_APPLICATION_CREDENTIALS', 'value': '/gsa-key/key.json'}]}}, {'status': {'id': 74, 'user': 'test', 'billing_project': 'test', 'token': 'tiQog0l-k0Xp2TkavWlV37Xy9FWcC1530EtCMRfg8XQ', 'state': 'success', 'complete': True, 'closed': True, 'n_jobs': 1, 'n_completed': 1, 'n_succeeded': 1, 'n_failed': 0, 'n_cancelled': 0, 'time_created': '2023-05-01T18:36:18Z', 'time_closed': None, 'time_completed': '2023-05-01T18:36:19Z', 'duration': 'a second', 'msec_mcpu': 0, 'cost': 1.1510333711392028e-06, 'attributes': {'name': 'test_pool_highcpu_instance_cheapest'}}, 'jobs': [{'log': {'main': ''}, 'status': {'batch_id': 74, 'job_id': 1, 'name': None, 'user': 'test', 'billing_project': 'test', 'state': 'Success', 'exit_code': 0, 'duration': 609, 'cost': 1.1510333711392028e-06, 'msec_mcpu': 0, 'status': {'version': 5, 'worker': 'batch-worker-pr-12955-default-rrlcxki12v8r-standard-0e2wl', 'batch_id': 74, 'job_id': 1, 'attempt_id': 'gAaTm8', 'user': 'test', 'state': 'succeeded', 'format_version': 7, 'resources': [{'name': 'az/vm/Standard_D8ds_v4/spot/eastus/1682899200000', 'quantity': 32}, {'name': 'az/disk/E4_LRS/eastus/1546300800000', 'quantity': 1024}, {'name': 'az/ip-fee/1024/2021-12-01', 'quantity': 32}, {'name': 'az/service-fee/2021-12-01', 'quantity': 250",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228:3249,test,test,3249,https://hail.is,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228,2,['test'],['test']
Testability,"'; ...: vds_file = 'gs://neale-bge/bge-wave-1.vds'; ...: out = 'gs://danking/foo.vcf.bgz'; ...: ; ...: vds = hl.vds.read_vds(vds_file); ...: mt = hl.vds.to_dense_mt(vds); ...: t = gnomad.utils.sparse_mt.default_compute_info(mt); ...: t = t.annotate(info=t.info.drop('AS_SB_TABLE')); ...: t = t.annotate(info = t.info.drop(; ...: 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; ...: )); ...: t = t.drop('AS_lowqual'); ...: ; ...: hl.methods.export_vcf(dataset = t, output = out, tabix = True); ```; worker failure:; ```; 2023-09-27 16:43:10.389 JVMEntryway: INFO: is.hail.JVMEntryway received arguments:; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 0: /hail-jars/gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar; 2023-09-27 16:43:10.389 JVMEntryway: INFO: 1: is.hail.backend.service.Main; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 2: /batch/83e7aee9e9244f6884b8a84ea81b4c7a; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 3: /batch/83e7aee9e9244f6884b8a84ea81b4c7a/log; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 4: gs://hail-test-ezlis/dking/jars/ch4g3zvqceyo/09526a168d57dac1a26f8caa4ab49593931ed2ef.jar; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 5: worker; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 6: gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g=; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 7: 7028; 2023-09-27 16:43:10.390 JVMEntryway: INFO: 8: 9060; 2023-09-27 16:43:10.390 JVMEntryway: INFO: Yielding control to the QoB Job.; 2023-09-27 16:43:10.393 Worker$: INFO: is.hail.backend.service.Worker 09526a168d57dac1a26f8caa4ab49593931ed2ef; 2023-09-27 16:43:10.394 Worker$: INFO: running job 7028/9060 at root gs://1-day/parallelizeAndComputeWithIndex/al3OJfYZMMNoi9F2jvcAf0jBirVTayRVqro03dnIa1g= with scratch directory '/batch/83e7aee9e9244f6884b8a84ea81b4c7a'; 2023-09-27 16:43:10.398 GoogleStorageFS$: INFO: Initializing google storage client from service account key; 2023-09-27 16:43:10.779 Wo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:2436,log,log,2436,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['log'],['log']
Testability,"'region': 'eastus', 'start_time': 1682966178997, 'end_time': 1682966179606, 'container_statuses': {'main': {'name': 'batch-74-job-1-main', 'state': 'succeeded', 'timing': {'pulling': {'start_time': 1682966179035, 'finish_time': 1682966179224, 'duration': 189}, 'setting up overlay': {'start_time': 1682966179224, 'finish_time': 1682966179253, 'duration': 29}, 'setting up network': {'start_time': 1682966179253, 'finish_time': 1682966179253, 'duration': 0}, 'running': {'start_time': 1682966179253, 'finish_time': 1682966179340, 'duration': 87}, 'uploading_log': {'start_time': 1682966179340, 'finish_time': 1682966179361, 'duration': 21}, 'uploading_resource_usage': {'start_time': 1682966179361, 'finish_time': 1682966179407, 'duration': 46}}, 'container_status': {'started_at': 1682966179253, 'finished_at': 1682966179340, 'state': 'finished', 'exit_code': 0, 'out_of_memory': False}}}, 'timing': {'setup_io': {'start_time': 1682966178998, 'finish_time': 1682966178999, 'duration': 1}, 'configuring xfsquota': {'start_time': 1682966178999, 'finish_time': 1682966179035, 'duration': 36}, 'populating secrets': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'adding cloudfuse support': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'post-job finally block': {'start_time': 1682966179578}}}, 'spec': {'always_copy_output': False, 'job_id': 1, 'process': {'command': ['true'], 'image': 'haildev.azurecr.io/ubuntu:20.04', 'mount_docker_socket': False, 'type': 'docker'}, 'resources': {'req_cpu': '0.25', 'req_memory': '50Mi', 'req_storage': '0Gi', 'cores_mcpu': 250, 'memory_bytes': 1073741824, 'storage_gib': 0, 'preemptible': True}, 'secrets': [{'namespace': 'pr-12955-default-rrlcxki12v8r', 'name': 'test-gsa-key', 'mount_path': '/gsa-key', 'mount_in_copy': True}], 'env': [{'name': 'AZURE_APPLICATION_CREDENTIALS', 'value': '/gsa-key/key.json'}]}}}]}); E assert 'highcpu' in 'batch-worker-pr-12955-default-rrlcxki12v8r-standard-0e2wl'. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228:6182,test,test-gsa-key,6182,https://hail.is,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228,2,"['assert', 'test']","['assert', 'test-gsa-key']"
Testability,(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:3620,test,testng,3620,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability,(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:3813,test,testng,3813,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability,(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4134,test,testng,4134,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability,(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4327,test,testng,4327,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:108,log,logs,108,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287,5,['log'],"['log', 'logging', 'logs']"
Testability,(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.goog,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4945,Test,TestNG,4945,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestNG']
Testability,(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.go,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4752,Test,TestNG,4752,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestNG']
Testability,"(on restart, CI doesn't know that feature branches have been previously tested against master, so it tries to get at least one status finished, for developer feedback purposes)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373:72,test,tested,72,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026373,2,['test'],['tested']
Testability,") 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper.; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); Fi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6210,LOG,LOGGING,6210,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['LOG'],['LOGGING']
Testability,")))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseStruct{key:+EBaseStruct{idx:+EInt32},offset:+EInt64,annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{first_idx:Int64,keys:Array[Struct{key:Struct{idx:Int32},offset:Int64,annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_377,GetField(Ref(__iruid_377,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:8886,test,test,8886,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['test'],['test']
Testability,")))), JObject(List((start,JObject(List((idx,JInt(80))))), (end,JObject(List((idx,JInt(90))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(90))))), (end,JObject(List((idx,JInt(100))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))))),IndexSpec2(../index,{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{first_idx:+EInt64,keys:+EArray[+EBaseStruct{key:+EBaseStruct{idx:+EInt32},offset:+EInt64,annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{first_idx:Int64,keys:Array[Struct{key:Struct{idx:Int32},offset:Int64,annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},{""name"":""TypedCodecSpec"",""_eType"":""EBaseStruct{children:+EArray[+EBaseStruct{index_file_offset:+EInt64,first_idx:+EInt64,first_key:+EBaseStruct{idx:+EInt32},first_record_offset:+EInt64,first_annotation:+EBaseStruct{}}]}"",""_vType"":""Struct{children:Array[Struct{index_file_offset:Int64,first_idx:Int64,first_key:Struct{idx:Int32},first_record_offset:Int64,first_annotation:Struct{}}]}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},struct{idx: int32},struct{},None),Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_473,GetField(Ref(__iruid_473,struct{filePath: str, partitionCounts: int64}),partitionCounts))),TableSpecWriter(gs://danking/workshop-test/1kg.mt,Table{global:Struct{},key:[idx],row:Struct{idx:Int32}},rows,globals,references,true))))),RelationalWriter(gs://danking/workshop-test/1kg.mt,true,Some((references,Set()))))); 	at scala.collection.MapLike$class.default(MapLike.scala:228); 	at scala.collection.AbstractMap.default(Map.scala:59); 	at scala.collect",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:8606,test,test,8606,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['test'],['test']
Testability,"); 	.logreg('wald', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.wald'); 	.logreg('lrt', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.lrt'); 	.logreg('score', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.score'); 	.logreg('firth', 'sa.pheno', ['sa.cov1', 'sa.cov2'], root='va.firth'); 	.export_variants('/Users/jbloom/data/logreg.tsv', 'Variant = v, va.qc.*, linBeta = va.linreg.beta, waldBeta = va.wald.wald.beta, lrtBeta = va.lrt.lrt.beta, firthBeta = va.firth.firth.beta, linPval = va.linreg.pval, waldPval = va.wald.wald.pval, lrtPval = va.lrt.lrt.pval, firthPval = va.firth.firth.pval, scorePval = va.score.score.pval, waldIter = va.wald.fit.nIter, lrtIter = va.lrt.fit.nIter, firthIter = va.firth.fit.nIter')); ```. Beta for all variants, note that Firth resolves quasi-separation issues and regresses toward the zero:; ![logregbetalrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867286/9e219bc2-f153-11e6-896a-c49e55593312.png). Pvals for all variants, note that Firth is more conservative:; ![logregpvallrtfirth](https://cloud.githubusercontent.com/assets/3201642/22867294/a86b012c-f153-11e6-8155-23004f9127fe.png). Beta for variants with at least 20 hets, basically the same:; ![logregbeta20hets](https://cloud.githubusercontent.com/assets/3201642/22859644/23feabb6-f0b1-11e6-88d5-a04af188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://clo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:1713,log,logregpvallrtfirth,1713,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409,1,['log'],['logregpvallrtfirth']
Testability,); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:74); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:369); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:363); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:362); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:351); 	at org.testng.internal.ClassHelper.forName(ClassHelper.java:94); 	at org.testng.xml.XmlClass.loadClass(XmlClass.java:78); 	at org.testng.xml.XmlClass.getSupportClass(XmlClass.java:89); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:25); 	at org.testng.internal.ClassInfoMap.<init>(ClassInfoMap.java:18); 	at org.testng.TestRunner.initMethods(TestRunner.java:408); 	at org.testng.TestRunner.init(TestRunner.java:235); 	at org.testng.TestRunner.init(TestRunner.java:205); 	at org.testng.TestRunner.<init>(TestRunner.java:153); 	at org.testng.SuiteRunner$DefaultTestRunnerFactory.newTestRunner(SuiteRunner.java:536); 	at org.testng.SuiteRunner.init(SuiteRunner.java:159); 	at org.testng.SuiteRunner.<init>(SuiteRunner.java:113); 	at org.testng.TestNG.createSuiteRunner(TestNG.java:1299); 	at org.testng.TestNG.createSuiteRunners(TestNG.java:1286); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1140); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); Caused by: java.lang.ClassNotFoundException: is.hail.relocated.org.apache.commons.math3.distribution.AbstractIntegerDistribution; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:382); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:418); 	at sun.misc.Launcher$AppClassLoader.loadC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460:1601,Test,TestRunner,1601,https://hail.is,https://github.com/hail-is/hail/pull/8700#issuecomment-624324460,1,['Test'],['TestRunner']
Testability,"*/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=htt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2262,log,log,2262,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,['log'],['log']
Testability,++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp; g++ -o build/Decoder.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Decoder.d -MT build/Decoder.o -c Decoder.cpp; g++ -o build/Encoder.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Encoder.d -MT build/Encoder.o -c Encoder.cpp; g++ -o build/Logging.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Logging.d -MT build/Logging.o -c Logging.cpp; g++ -o build/NativeCodeSuite.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeCodeSuite.d -MT build/NativeCodeSuite.o -c NativeCodeSuite.cp; p; g++ -o build/NativeLongFunc.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeLongFunc.d -MT build/NativeLongFunc.o -c NativeLongFunc.cpp; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -DALL_HEADER_CKSUM=2474410629UL -c NativeModule.cpp -o build/NativeModule.o; g++ -o build/NativePtr.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:11739,Log,Logging,11739,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Log'],['Logging']
Testability,+----------------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1376,test,test,1376,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,+1-4. Net -3 lines for a 1/2 reduction in test time is pretty good ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12716#issuecomment-1435318302:42,test,test,42,https://hail.is,https://github.com/hail-is/hail/pull/12716#issuecomment-1435318302,1,['test'],['test']
Testability,+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8466,test,test,8466,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,", we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBufferSpec""}}}}; ```; Error for run 1.; ```; Caused by: com.github.luben.zstd.ZstdException: Corrupted block detected; 	at com.github.luben.zstd.ZstdDecompressCtx.decompressByteArray(ZstdDecompressCtx.java:157) ~[zstd-jni-1.5.2-1.jar:1.5.2-1]; 	at is.hail.io.ZstdInputBlockBuffer.readBlock(InputBuffers.scala:65",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:1424,Log,Log,1424,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['Log'],['Log']
Testability,"- Backported some changes needed to unpack C++ headers from a jar (which doesn't happen; in the Scala test environment, but is essential for running w/ Spark). - Made various changes in response to review comments. I still have a chunk of work to do on the src/main/c/Makefile and building libraries; which can run on both C++ ABI v2 (gcc-5.x and later) and C++ ABI v1 (gcc-4.x, in particular; gcc-4.8.3 as on CI machines).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-409695090:102,test,test,102,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-409695090,1,['test'],['test']
Testability,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/83#issuecomment-160692682:389,test,testSingletonVariants,389,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682,2,['test'],"['testFilterSamples', 'testSingletonVariants']"
Testability,- [ ] Nuke lz4 decompression logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1140#issuecomment-280764177:29,log,logic,29,https://hail.is,https://github.com/hail-is/hail/issues/1140#issuecomment-280764177,1,['log'],['logic']
Testability,"- `render` is already handled by `head_str()` being defined; - just added this in a commit; - i dont see any tests in this suite for any of the other XToTableApply IR's, do you think having the PCRelateSuite is enough?; - i'm looking into this -- i'm not sure how to obtain a ""partitionCounts"" from a blockmatrix; - hm. perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable""?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6689#issuecomment-513932854:110,test,tests,110,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513932854,1,['test'],['tests']
Testability,"- separated `BlockMatrix.from_entry_expr` (a classmethod that uses a temp file) and `BlockMatrix.write_from_entry_expr` (a staticmethod that takes a path); - added buffers specifically for handling Doubles, removed StreamingRawBlockingBuffer, etc; - moved RichDenseMatrixDouble tests from BlockMatrixSuite to its own suite, now also testing condition of the file being shorten than expected",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-373213036:278,test,tests,278,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-373213036,2,['test'],"['testing', 'tests']"
Testability,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:269,log,log,269,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674,3,['log'],"['log', 'logs']"
Testability,-+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1410,test,test,1410,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1417,test,test,1417,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1452,test,test,1452,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,--+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1547,test,test,1547,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,--+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1554,test,test,1554,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,---+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1588,test,test,1588,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-----+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8419,test,test,8419,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-----+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8608,test,test,8608,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-----+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8426,test,test,8426,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8615,test,test,8615,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,--------+; 8 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; | 2023-07-13 | test | test | 6 | 11 | 171840 |; | 2023-07-13 | test | test | 6 | 59 | 51584 |; | 2023-07-13 | test | test | 6 | 62 | 55616 |; | 2023-07-13 | test | test | 6 | 138 | 700160 |; | 2023-07-13 | test | test | 6 | 183 | 54976 |; +--------------+----------------------------------+------+-------------+-------+------------+; 9 rows in set (0.00 sec). mysql> select * from aggregated_billing_project_user_resources_by_date_v3 where resource_id = 6 limit 100;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4160033472 |; | 2023-07-13 | test | test | 6 | 11 | 171840 |; | 2023-07-13 | test | test | 6 | 59 | 51584 |; | 2023-07-13 | test | test | 6 | 62 | 55616 |; | 2023-07-13 | test | test | 6 | 138 | 700160 |; | 2023-07-13 | test | test | 6 | 183 | 54976 |; +--------------+----------------------------------+------+-------------+-------+------------+; 9 rows in set (0.01 sec); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:15649,test,test,15649,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,14,['test'],['test']
Testability,--------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1445,test,test,1445,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,----------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1581,test,test,1581,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-----------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8378,test,test,8378,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8567,test,test,8567,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,----------------------------+------+-------------+-------+------------+; | billing_project | user | resource_id | token | usage |; +----------------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1383,test,test,1383,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-----------------------------+------+-------------+-------+------------+; | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1520,test,test,1520,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,--------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8560,test,test,8560,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"----------------------------------------------------------------------------------------------------------------------------+; | batch_id | job_group_id | resources |; +----------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | 0 | {""service-fee/1"": 39973000, ""ip-fee/preemptible/1024/1"": 2558272, ""ip-fee/nonpreemptible/1024/1"": 3449280, ""disk/pd-ssd/us-central1/1707831761013"": 51165440, ""gcp-support-logs-specs-and-firewall-fees/1"": 39973000, ""memory/n1-preemptible/us-central1/1707831761013"": 153496320, ""compute/n1-preemptible/us-central1/1707831761013"": 39973000, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 206956800, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 53895000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 959352000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1293480000} |; +----------+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:5454,log,logs-specs-and-firewall-fees,5454,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['log'],['logs-specs-and-firewall-fees']
Testability,"---------------------------------------------------------------------------------------------------------------------------------+; | batch_id | ancestor_id | resources |; +----------+-------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | 0 | {""service-fee/1"": 14452500, ""ip-fee/preemptible/1024/1"": 924960, ""ip-fee/nonpreemptible/1024/1"": 4239936, ""disk/pd-ssd/us-central1/1707831761013"": 18499200, ""gcp-support-logs-specs-and-firewall-fees/1"": 14452500, ""memory/n1-preemptible/us-central1/1707831761013"": 55497600, ""compute/n1-preemptible/us-central1/1707831761013"": 14452500, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 254396160, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 66249000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 346860000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1589976000} |; +----------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:2553,log,logs-specs-and-firewall-fees,2553,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['log'],['logs-specs-and-firewall-fees']
Testability,"---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | batch_id | resources |; +----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+; | 1 | {""service-fee/1"": 427000, ""ip-fee/preemptible/1024/1"": 27328, ""ip-fee/nonpreemptible/1024/1"": 4239936, ""disk/pd-ssd/us-central1/1707831761013"": 546560, ""gcp-support-logs-specs-and-firewall-fees/1"": 427000, ""memory/n1-preemptible/us-central1/1707831761013"": 1639680, ""compute/n1-preemptible/us-central1/1707831761013"": 427000, ""memory/n1-nonpreemptible/us-central1/1707831761013"": 254396160, ""compute/n1-nonpreemptible/us-central1/1707831761013"": 66249000, ""disk/local-ssd/preemptible/us-central1/1707831761013"": 10248000, ""disk/local-ssd/nonpreemptible/us-central1/1707831761013"": 1589976000} |; +----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575:8282,log,logs-specs-and-firewall-fees,8282,https://hail.is,https://github.com/hail-is/hail/pull/14282#issuecomment-1941610575,1,['log'],['logs-specs-and-firewall-fees']
Testability,"--------------------------; ImportError Traceback (most recent call last); <ipython-input-2-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/representation/__init__.py in <module>(); ----> 1 from hail.representation.variant import Variant, Locus, AltAllele; 2 from hail.representation.interval import Interval; 3 from hail.representation.genotype import Genotype, Call; 4 from hail.representation.annotations import Struct; 5 from hail.representation.pedigree import Trio, Pedigree. /opt/Software/hail/python/hail/representation/variant.py in <module>(); 1 from hail.java import scala_object, Env, handle_py4j; ----> 2 from hail.typecheck import *; 3 ; 4 class Variant(object):; 5 """""". /opt/Software/hail/python/hail/typecheck/__init__.py in <module>(); ----> 1 from check import *; 2 ; 3 __all__ = ['typecheck',; 4 'typecheck_method',; 5 'none',. ImportError: No module named 'check'. In [3]: hc = HailContext(sc); ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-3-2e980fcce31d> in <module>(); ----> 1 hc = HailContext(sc). NameError: name 'HailContext' is not defined. In [4]: vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'); ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-4-7c2f3eb5060d> in <module>(); ----> 1 vds = hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf'). NameError: name 'hc' is not defined. In [5]: ; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160:4064,test,test,4064,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321420160,2,['test'],['test']
Testability,"--depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; ```; * Edit `build.gradle` and add `exclude group: 'org.scala-lang', module: 'scala-reflect'`; * Build Hail; ```sh; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```; * Symlink hail-all-spark.jar into /opt ( At the EMR creation step (before hail installation) I edit the `spark-defaults` properties in order to link `hail-all-spark.jar`... This config was needed & works successfuly for an old version of Hail (0.2.60)... can be revisit if not appropriate for recent version; ```sh; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * start pyspark; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a954",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:1312,log,logger,1312,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['log'],['logger']
Testability,-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8892,test,test,8892,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8939,test,test,8939,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9127,test,test,9127,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10263,test,test,10263,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10311,test,test,10311,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10358,test,test,10358,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10406,test,test,10406,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10453,test,test,10453,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/ibs.d -MT build/ibs.o -c ibs.cpp; g++ -o build/Decoder.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Decoder.d -MT build/Decoder.o -c Decoder.cpp; g++ -o build/Encoder.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Encoder.d -MT build/Encoder.o -c Encoder.cpp; g++ -o build/Logging.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Logging.d -MT build/Logging.o -c Logging.cpp; g++ -o build/NativeCodeSuite.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeCodeSuite.d -MT build/NativeCodeSuite.o -c NativeCodeSuite.cp; p; g++ -o build/NativeLongFunc.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeLongFunc.d -MT build/NativeLongFunc.o -c NativeLongFunc.cpp; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -DALL_HEADER_CKSUM=2474410629UL -c NativeModule.cpp -o build/NativeModule.o; g++ -o build/NativePtr.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-stri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:11752,Log,Logging,11752,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Log'],['Logging']
Testability,"-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1128,log,logConf,1128,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['log'],['logConf']
Testability,-46ff-9191-93b8c4589083/1890.suffix; at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at org.scalatest.testng.TestNGSuite.newAssertionFailedException(TestNGSuite.scala:67); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at is.hail.io.fs.FSSuite.largeDirectoryOperations(FSSuite.scala:445); at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:430); at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:136); at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:658); at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:219); at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50); at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:923); at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:192); at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146); at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128); at java.util.ArrayList.forEach(ArrayList.java:1259); at org.testng.TestRunner.privateRun(TestRunner.java:808); at org.testng.TestRunner.run(TestRunner.java:603); at org.testng.SuiteRunner.runTest(SuiteRunner.java:429); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:383); at org.testng.SuiteRunner.run(SuiteRunner.java:326); at org.test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547:1219,test,testng,1219,https://hail.is,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547,1,['test'],['testng']
Testability,-LMY%3D%2Fresult.10 response 200\n2022-11-15 20:31:30.522 ServiceBackend$: INFO: result 10 complete - 8157265 bytes\n2022-11-15 20:31:30.522 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.11\n2022-11-15 20:31:30.691 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.11 response 200\n2022-11-15 20:31:30.712 ServiceBackend$: INFO: result 11 complete - 8157265 bytes\n2022-11-15 20:31:30.712 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12\n2022-11-15 20:31:30.968 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:42056,test,test-,42056,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.10\n2022-11-15 20:31:30.499 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.10 response 200\n2022-11-15 20:31:30.522 ServiceBackend$: INFO: result 10 complete - 8157265 bytes\n2022-11-15 20:31:30.522 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.11\n2022-11-15 20:31:30.691 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.11 response 200\n2022-11-15 20:31:30.712 ServiceBackend$: INFO: result 11 complete - 8157265 bytes\n2022-11-15 20:31:30.712 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12\n2022-11-15 20:31:30.968 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:41840,test,test-,41840,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.11 response 200\n2022-11-15 20:31:30.712 ServiceBackend$: INFO: result 11 complete - 8157265 bytes\n2022-11-15 20:31:30.712 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12\n2022-11-15 20:31:30.968 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:42584,test,test-,42584,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.11\n2022-11-15 20:31:30.691 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.11 response 200\n2022-11-15 20:31:30.712 ServiceBackend$: INFO: result 11 complete - 8157265 bytes\n2022-11-15 20:31:30.712 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12\n2022-11-15 20:31:30.968 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:42368,test,test-,42368,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:43112,test,test-,43112,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.12\n2022-11-15 20:31:30.968 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.12 response 200\n2022-11-15 20:31:30.990 ServiceBackend$: INFO: result 12 complete - 8157265 bytes\n2022-11-15 20:31:30.990 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:42896,test,test-,42896,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:43640,test,test-,43640,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.13\n2022-11-15 20:31:31.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.13 response 200\n2022-11-15 20:31:31.213 ServiceBackend$: INFO: result 13 complete - 8157265 bytes\n2022-11-15 20:31:31.213 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:43424,test,test-,43424,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:44168,test,test-,44168,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.14\n2022-11-15 20:31:31.518 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.14 response 200\n2022-11-15 20:31:31.545 ServiceBackend$: INFO: result 14 complete - 8157265 bytes\n2022-11-15 20:31:31.545 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:43952,test,test-,43952,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:44696,test,test-,44696,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.15\n2022-11-15 20:31:31.742 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.15 response 200\n2022-11-15 20:31:31.760 ServiceBackend$: INFO: result 15 complete - 8157265 bytes\n2022-11-15 20:31:31.761 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:44480,test,test-,44480,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:45224,test,test-,45224,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.16\n2022-11-15 20:31:31.925 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.16 response 200\n2022-11-15 20:31:31.949 ServiceBackend$: INFO: result 16 complete - 8157265 bytes\n2022-11-15 20:31:31.950 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:45008,test,test-,45008,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:45752,test,test-,45752,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.17\n2022-11-15 20:31:32.168 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.17 response 200\n2022-11-15 20:31:34.006 ServiceBackend$: INFO: result 17 complete - 8157265 bytes\n2022-11-15 20:31:34.007 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:45536,test,test-,45536,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:46280,test,test-,46280,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.18\n2022-11-15 20:31:34.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.18 response 200\n2022-11-15 20:31:34.236 ServiceBackend$: INFO: result 18 complete - 8157265 bytes\n2022-11-15 20:31:34.236 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:46064,test,test-,46064,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:46808,test,test-,46808,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.19\n2022-11-15 20:31:34.397 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.19 response 200\n2022-11-15 20:31:34.417 ServiceBackend$: INFO: result 19 complete - 8157265 bytes\n2022-11-15 20:31:34.417 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:46592,test,test-,46592,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:47336,test,test-,47336,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.20\n2022-11-15 20:31:34.653 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.20 response 200\n2022-11-15 20:31:34.672 ServiceBackend$: INFO: result 20 complete - 8157265 bytes\n2022-11-15 20:31:34.672 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:47120,test,test-,47120,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:47864,test,test-,47864,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.21\n2022-11-15 20:31:34.852 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.21 response 200\n2022-11-15 20:31:34.872 ServiceBackend$: INFO: result 21 complete - 8157265 bytes\n2022-11-15 20:31:34.873 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:47648,test,test-,47648,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:48392,test,test-,48392,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.22\n2022-11-15 20:31:35.103 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.22 response 200\n2022-11-15 20:31:35.122 ServiceBackend$: INFO: result 22 complete - 8157265 bytes\n2022-11-15 20:31:35.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:48176,test,test-,48176,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:48920,test,test-,48920,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.23\n2022-11-15 20:31:35.354 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.23 response 200\n2022-11-15 20:31:35.373 ServiceBackend$: INFO: result 23 complete - 8157265 bytes\n2022-11-15 20:31:35.374 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:48704,test,test-,48704,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:49448,test,test-,49448,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.24\n2022-11-15 20:31:35.570 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.24 response 200\n2022-11-15 20:31:35.601 ServiceBackend$: INFO: result 24 complete - 8157265 bytes\n2022-11-15 20:31:35.601 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:49232,test,test-,49232,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:49976,test,test-,49976,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.25\n2022-11-15 20:31:35.762 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.25 response 200\n2022-11-15 20:31:35.782 ServiceBackend$: INFO: result 25 complete - 8157265 bytes\n2022-11-15 20:31:35.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:49760,test,test-,49760,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:50504,test,test-,50504,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.26\n2022-11-15 20:31:35.961 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.26 response 200\n2022-11-15 20:31:36.046 ServiceBackend$: INFO: result 26 complete - 8157265 bytes\n2022-11-15 20:31:36.046 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:50288,test,test-,50288,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:51032,test,test-,51032,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.27\n2022-11-15 20:31:36.217 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.27 response 200\n2022-11-15 20:31:36.238 ServiceBackend$: INFO: result 27 complete - 8157265 bytes\n2022-11-15 20:31:36.238 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:50816,test,test-,50816,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:51560,test,test-,51560,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.28\n2022-11-15 20:31:36.449 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.28 response 200\n2022-11-15 20:31:36.469 ServiceBackend$: INFO: result 28 complete - 8157265 bytes\n2022-11-15 20:31:36.469 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:51344,test,test-,51344,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:52088,test,test-,52088,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.29\n2022-11-15 20:31:36.695 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.29 response 200\n2022-11-15 20:31:36.732 ServiceBackend$: INFO: result 29 complete - 8157265 bytes\n2022-11-15 20:31:36.732 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:51872,test,test-,51872,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:52616,test,test-,52616,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.30\n2022-11-15 20:31:36.902 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.30 response 200\n2022-11-15 20:31:36.930 ServiceBackend$: INFO: result 30 complete - 8157265 bytes\n2022-11-15 20:31:36.931 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:52400,test,test-,52400,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:53144,test,test-,53144,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.31\n2022-11-15 20:31:37.122 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.31 response 200\n2022-11-15 20:31:37.146 ServiceBackend$: INFO: result 31 complete - 8157265 bytes\n2022-11-15 20:31:37.146 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:52928,test,test-,52928,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:53672,test,test-,53672,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.32\n2022-11-15 20:31:37.413 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.32 response 200\n2022-11-15 20:31:37.434 ServiceBackend$: INFO: result 32 complete - 8157265 bytes\n2022-11-15 20:31:37.434 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:53456,test,test-,53456,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:54200,test,test-,54200,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.33\n2022-11-15 20:31:37.610 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.33 response 200\n2022-11-15 20:31:37.637 ServiceBackend$: INFO: result 33 complete - 8157265 bytes\n2022-11-15 20:31:37.637 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:53984,test,test-,53984,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:54728,test,test-,54728,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.34\n2022-11-15 20:31:37.848 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.34 response 200\n2022-11-15 20:31:37.870 ServiceBackend$: INFO: result 34 complete - 8157265 bytes\n2022-11-15 20:31:37.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:54512,test,test-,54512,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:55256,test,test-,55256,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.35\n2022-11-15 20:31:38.077 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.35 response 200\n2022-11-15 20:31:38.170 ServiceBackend$: INFO: result 35 complete - 8157265 bytes\n2022-11-15 20:31:38.170 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:55040,test,test-,55040,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:55784,test,test-,55784,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.36\n2022-11-15 20:31:38.305 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.36 response 200\n2022-11-15 20:31:38.359 ServiceBackend$: INFO: result 36 complete - 8157265 bytes\n2022-11-15 20:31:38.359 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:55568,test,test-,55568,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:56312,test,test-,56312,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.37\n2022-11-15 20:31:38.531 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.37 response 200\n2022-11-15 20:31:38.559 ServiceBackend$: INFO: result 37 complete - 8157265 bytes\n2022-11-15 20:31:38.559 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:56096,test,test-,56096,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:56840,test,test-,56840,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.38\n2022-11-15 20:31:38.829 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.38 response 200\n2022-11-15 20:31:38.856 ServiceBackend$: INFO: result 38 complete - 8157265 bytes\n2022-11-15 20:31:38.856 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:56624,test,test-,56624,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:57368,test,test-,57368,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.39\n2022-11-15 20:31:38.999 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.39 response 200\n2022-11-15 20:31:39.031 ServiceBackend$: INFO: result 39 complete - 8157265 bytes\n2022-11-15 20:31:39.031 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:57152,test,test-,57152,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:57896,test,test-,57896,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.40\n2022-11-15 20:31:39.200 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.40 response 200\n2022-11-15 20:31:39.251 ServiceBackend$: INFO: result 40 complete - 8157265 bytes\n2022-11-15 20:31:39.252 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:57680,test,test-,57680,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:58424,test,test-,58424,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.41\n2022-11-15 20:31:39.439 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.41 response 200\n2022-11-15 20:31:39.460 ServiceBackend$: INFO: result 41 complete - 8157265 bytes\n2022-11-15 20:31:39.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:58208,test,test-,58208,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:58952,test,test-,58952,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.42\n2022-11-15 20:31:39.669 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.42 response 200\n2022-11-15 20:31:39.693 ServiceBackend$: INFO: result 42 complete - 8157265 bytes\n2022-11-15 20:31:39.693 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:58736,test,test-,58736,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:59480,test,test-,59480,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.43\n2022-11-15 20:31:39.938 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.43 response 200\n2022-11-15 20:31:39.965 ServiceBackend$: INFO: result 43 complete - 8157265 bytes\n2022-11-15 20:31:39.966 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:59264,test,test-,59264,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:60008,test,test-,60008,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.44\n2022-11-15 20:31:40.151 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.44 response 200\n2022-11-15 20:31:40.186 ServiceBackend$: INFO: result 44 complete - 8157265 bytes\n2022-11-15 20:31:40.186 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:59792,test,test-,59792,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:60536,test,test-,60536,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.45\n2022-11-15 20:31:40.377 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.45 response 200\n2022-11-15 20:31:40.461 ServiceBackend$: INFO: result 45 complete - 8157265 bytes\n2022-11-15 20:31:40.461 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:60320,test,test-,60320,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,"-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 curr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:61064,test,test-,61064,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-LMY%3D%2Fresult.46\n2022-11-15 20:31:40.643 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.46 response 200\n2022-11-15 20:31:40.671 ServiceBackend$: INFO: result 46 complete - 8157265 bytes\n2022-11-15 20:31:40.671 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:60848,test,test-,60848,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,"-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), reg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:61592,test,test-,61592,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,"-LMY%3D%2Fresult.47\n2022-11-15 20:31:40.870 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.47 response 200\n2022-11-15 20:31:40.893 ServiceBackend$: INFO: result 47 complete - 8157265 bytes\n2022-11-15 20:31:40.893 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48\n2022-11-15 20:31:41.050 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.48 response 200\n2022-11-15 20:31:41.071 ServiceBackend$: INFO: result 48 complete - 8157265 bytes\n2022-11-15 20:31:41.071 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49\n2022-11-15 20:31:41.231 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.49 response 200\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: result 49 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:61376,test,test-,61376,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,-Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; g++ -o build/Upcalls.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/Upcalls.d -MT build/Upcalls.o -c Upcalls.cpp; g++ -o build/FS.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/FS.d -MT build/FS.o -c FS.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:14904,Log,Logging,14904,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['Log'],['Logging']
Testability,"-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2322,test,test,2322,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5340,test,test,5340,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1634,test,test-ci,1634,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['test'],['test-ci']
Testability,". I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed? Should I increase the service queue size?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:1578,log,logs,1578,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['log'],['logs']
Testability,". Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1030,log,login,1030,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,2,['log'],"['login', 'logout']"
Testability,".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8179,test,test-,8179,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['test'],['test-']
Testability,"...................done.; ERROR: Create cluster failed!; ERROR: gcloud crashed (AttributeError): 'Operation' object has no attribute 'details'. If you would like to report this issue, please run the following command:; gcloud feedback. To check gcloud for common problems, please run the following command:; gcloud info --run-diagnostics; gcloud command:; gcloud beta dataproc clusters create \; ci-test-6boype3d \; --image-version=1.2-deb9 \; --metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:8776,log,logging,8776,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['log'],['logging']
Testability,"../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Region.d -MT build/Region.o -c Region.cpp; c++ -o build/Hadoop.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin -MD -MF build/Hadoop.d -MT build/Hadoop.o -c Hadoop.cpp; c++ -fvisibility=default -dynamiclib -Wl,-undefined,dynamic_lookup -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/include/darwin build/davies.o build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/NativeCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/Hadoop.o -o lib/darwin/libhail.dylib; > Building 33% > :compileScala. :compileScala; :processResources; :classes; :compileTestJava UP-TO-DATE; :compileTestScala; :processTestResources UP-TO-DATE; :testClasses; :test; Running test: Test method testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2)))))))(is.hail.expr.ir.ForwardLetsSuite). Gradle suite > Gradle test > is.hail.expr.ir.ForwardLetsSuite.testForwardingOps[0](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeStruct(WrappedArray((a,I32(1)), (b,ApplyBinaryPrimOp(Add(),Ref(x,int32),I32(2))))))) PASSED; Running test: Test method testForwardingOps[1](Let(x,ApplyBinaryPrimOp(Add(),In(0,int32),In(0,int32)),MakeTuple(WrappedAr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925:2082,Log,Logging,2082,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492893925,1,['Log'],['Logging']
Testability,".0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unfiltered.vcf) discordance=(RodBinding name= source=UNBOUND) concordance=(RodBinding name= source=UNBOUND) out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sample_name=[] sample_expressions=null sample_file=null exclude_sample_name=[] exclude_sample_file=[] select_expressions=[] excludeNonVariants=false excludeFiltered=false regenotype=false restrictAllelesTo=ALL keepOriginalAC=false mendelianViolation=false mendelianViolationQualThreshold=0.0 select_random_fraction=0.0 remove_fraction_genotypes=0.0 selectTypeToInclude=[INDEL] keepIDs=null fullyDecode=false forceGenotypesDecode=false justRead=false maxIndelSize=2147483647 ALLOW_NONOVERLAPPING_COMMAND_LINE_SAMPLES=false filter_mismatching_base_and_quals=false""; ##UnifiedGenotyper=""analysis_type=UnifiedGenotyper input_file=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.bam.list] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/scatter/temp_0001_of_1200/scattere",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:12883,stub,stubs,12883,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['stub'],['stubs']
Testability,".0.0.0', 5000); await site.start(); ; try:; token = secrets.token_urlsafe(32); b = create_batch(; client, callback=url_for('/test'), attributes={'foo': 'bar', 'name': 'test_callback'}, token=token; ); head = b.create_job('alpine:3.8', command=['echo', 'head']); b.create_job('alpine:3.8', command=['echo', 'tail'], parents=[head]); b.submit(); await asyncio.wait_for(callback_event.wait(), 5 * 60); callback_body = callback_bodies[0]; ; # verify required fields present; callback_body.pop('cost'); callback_body.pop('msec_mcpu'); callback_body.pop('time_created'); callback_body.pop('time_closed'); callback_body.pop('time_completed'); callback_body.pop('duration'); callback_body.pop('duration_ms'); callback_body.pop('cost_breakdown'); > assert callback_body == {; 'id': b.id,; 'user': 'test',; 'billing_project': 'test',; 'token': token,; 'state': 'success',; 'complete': True,; 'closed': True,; 'n_jobs': 2,; 'n_completed': 2,; 'n_succeeded': 2,; 'n_failed': 0,; 'n_cancelled': 0,; 'attributes': {'foo': 'bar', 'name': 'test_callback'},; }, callback_body; E AssertionError: {'attributes': {'client_job': '8051758-182', 'foo': 'bar', 'name': 'test_callback'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}; E assert {'id': 260, 'user': 'test', 'billing_project': 'test', 'token': 'dL_z32z_mbXzpd2hcI3aVy3rySdAOjxPQoqAdERnyzg', 'state': 'success', 'complete': True, 'closed': True, 'n_jobs': 2, 'n_completed': 2, 'n_succeeded': 2, 'n_failed': 0, 'n_cancelled': 0, 'attributes': {'name': 'test_callback', 'foo': 'bar', 'client_job': '8051758-182'}} == {'id': 260, 'user': 'test', 'billing_project': 'test', 'token': 'dL_z32z_mbXzpd2hcI3aVy3rySdAOjxPQoqAdERnyzg', 'state': 'success', 'complete': True, 'closed': True, 'n_jobs': 2, 'n_completed': 2, 'n_succeeded': 2, 'n_failed': 0, 'n_cancelled': 0, 'attributes': {'foo': 'bar', 'name': 'test_callback'}}; E Common items:; E {'billing_project': 'test',; E 'closed': True,; E 'complete': True,; E 'id': 260,; E 'n_cancelled': 0,; E",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427:1668,test,test,1668,https://hail.is,https://github.com/hail-is/hail/pull/13739#issuecomment-1739224427,4,"['Assert', 'test']","['AssertionError', 'test']"
Testability,.847 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:31:27.880 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:31:27.880 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: reading results\n2022-11-15 20:31:27.881 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0\n2022-11-15 20:31:28.080 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0 response 200\n2022-11-15 20:31:28.104 ServiceBackend$: INFO: result 0 complete - 8157265 bytes\n2022-11-15 20:31:28.104 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1\n2022-11-15 20:31:28.293 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1 response 200\n2022-11-15 20:31:28.313 ServiceBackend$: INFO: result 1 complete - 8157265 bytes\n2022-11-15 20:31:28.313 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.2\n2022-11-15 20:31:28.522 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.2 response 200\n2022-11-15 20:31:28.587 ServiceBackend$: INFO: result 2 complete - 8157265 bytes\n2022-11-15 20:31:28.587 Requester: INFO: request GET http://memory.hail/api/v1alpha/object,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:36059,test,test-,36059,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,".; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches runnin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1581,test,test-tiny-limit,1581,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['test'],['test-tiny-limit']
Testability,".; ldd (GNU libc) 2.12; Copyright (C) 2010 Free Software Foundation, Inc.; This is free software; see the source for copying conditions. There is NO; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.; Written by Roland McGrath and Ulrich Drepper. ```. This is an error we are getting when submitted to the hadoop cluster. This runs fine locally. ```; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.4-d602a3d7472d; LOGGING: writing to /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-0019-0.2.4-d602a3d7472d.log; 2019-01-22 00:19:54 Hail: INFO: Number of BGEN files parsed: 1; 2019-01-22 00:19:54 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:6319,log,log,6319,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['log'],['log']
Testability,".com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,parti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4465,test,test,4465,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:3777,test,testng,3777,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability,.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:3970,test,testng,3970,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability,".git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""network_count"": 0,; ""subscribers_count"": 0; }; ```. [2]:; ```; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6952,test,test,6952,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability,.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.TestUtils$.eval(TestUtils.scala:256); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:276); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:40); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:39); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:339); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:314); 	at is.hail.expr.ir.IRSuite.testStreamLenUnconsumedInnerStream(IRSuite.scala:1800); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:2284,assert,assertEvalsTo,2284,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,2,"['Test', 'assert']","['TestUtils', 'assertEvalsTo']"
Testability,.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1637,test,test,1637,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,1,['test'],['test']
Testability,.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushD,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1732,test,test,1732,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,1,['test'],['test']
Testability,.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:3264,test,testSeekMoreThanMaxInt,3264,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testSeekMoreThanMaxInt']
Testability,.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:3457,test,testSeekMoreThanMaxInt,3457,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testSeekMoreThanMaxInt']
Testability,.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://st,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4039,Test,TestMethodWorker,4039,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestMethodWorker']
Testability,.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://st,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4232,Test,TestMethodWorker,4232,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestMethodWorker']
Testability,".init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.executor.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:3314,test,test,3314,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['test'],['test']
Testability,.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4816,test,testng,4816,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability,.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4623,test,testng,4623,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability,.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2015,test,testng,2015,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['test'],['testng']
Testability,.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.reloc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4926,Test,TestNG,4926,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestNG']
Testability,.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.rel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4733,Test,TestNG,4733,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestNG']
Testability,.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2201,test,testng,2201,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['test'],['testng']
Testability,".js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used in that package. This can be used with Kubernetes. `kubectl create secret generic secretesfile --from-file=prod/env.txt`. The .env for the web app, where localhost would be replaced by our sub.domain. If you get it running, you may notice there isn't a way to log out... I ripped out all of the UI stuff after speaking with Cotton, and began writing a minimal interface. Just clear the cookie if you need to log out. ```; AUTH0_CLIENT_ID=TD78k23CcdM4pMWoYZwYwKJbQPBj06jY; AUTH0_DOMAIN=hail.auth0.com; AUTH0_SCOPE='opened profile repo read:users read:user_idp_tokens'; AUTH0_AUDIENCE='hail'; AUTH0_REDIRECT_URI='https://localhost/auth0callback'; SCORECARD_URL='https://scorecard.localhost/json'; SCORECARD_USER_URL='https://scorecard.localhost/json/users'; GRAPHQL_URL='https://localhost/api/graphql'; ```. The .env for the gateway; ```; AUTH0_WEB_KEY_SET_URL=https://hail.auth0.com/.well-known/jwks.json; AUTH0_AUDIENCE=hail; AUTH0_DOMAIN=https://hail.auth0.com/. AUTH0_MANAGEMENT_API_CLIENT=eqDY6HWOKd6MzC8kWCFaZUAoZgNUHypA; AUTH0_MANAGEMENT_API_SECRET=<I'll give this>; AUTH0_MANAGEMENT_API_TOKEN_URL=https://hail.auth0.com/oauth/token; AUTH0_MANAGEMENT_API_URL=https://hail.auth0.com/api/v2/users; AUTH0_MANAGEMENT_API_AUDIENCE=https://hail.auth0.com/api/v2/; ```. Organization of the web app is simple. There is a pages directory. Routes match the folder structure. Pages that don't need t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:2410,log,log,2410,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,2,['log'],['log']
Testability,".map(child => boundary(child.asInstanceOf[IR])); val x = if ((node.children, newChildren).zipped.forall(_ eq _)); node; else; node.copy(newChildren). if(x.typ.isInstanceOf[TArray]); ToStream(x); else; x; ```. IRSuite.testDictContains:. Before Lower: ; MakeTuple(ArrayBuffer((0,ApplyIR(contains,ArrayBuffer(GetTupleElement(In(0,PCTuple[0:PCDict[PInt32,PCString]]),0), NA(int32)))))); ...; before: ; ToArray(Ref(__iruid_56,dict<int32, str>)) of typ: array<struct{key: int32, value: str}>; after: Ref(__iruid_56,dict<int32, str>) of typ: dict<int32, str>. java.lang.AssertionError: assertion failed. 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.ir.LowerArrayToStream$.is$hail$expr$ir$LowerArrayToStream$$boundary(LowerArrayToStream.scala:19). #### Fix:; ```scala; private def toStream(node: IR): IR = {; node match {; case _: ToStream => node; case _ => {; if(node.typ.isInstanceOf[TIterable]) {; ToStream(node); } else {; node; }; }; }; }; ```. New issues (many more failures in IRSuite, and surely plenty more in other tests):. First of these:; Before Lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum()))), ArrayFor(ArrayMap(ArrayRange(I32(0),I32(4),I32(1)),__iruid_304,Cast(Ref(__iruid_304,int32),int64)),__iruid_303,Begin(ArrayBuffer(SeqOp(0,ArrayBuffer(Ref(__iruid_303,int64)),AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None),Sum())))))),ResultOp(0,WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),WrappedArray(AggStateSignature(Map(Sum() -> AggSignature(Sum(),ArrayBuffer(),ArrayBuffer(int64))),Sum(),None))),GetTupleElement(Ref(__iruid_302,tuple(int64)),0))))). After lower: ; MakeTuple(ArrayBuffer((0,Let(__iruid_302,RunAgg(Begin(ArrayBuffer(Begin(ArrayBuffer(InitOp(0,ArrayBuffer(),AggStateSignature",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113:1764,test,tests,1764,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-586602113,1,['test'],['tests']
Testability,.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFil,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1827,test,test,1827,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,1,['test'],['test']
Testability,.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVaria,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2020,test,test,2020,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,1,['test'],['test']
Testability,.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:177) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	... 12 more; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14817,test,test-,14817,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,5,['test'],['test-']
Testability,.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/connect.py'; adding 'hailtop/hailctl/dataproc/deploy_metadata.py'; adding 'hailtop/hailctl/dataproc/diagnose.py'; adding 'hailtop/hailctl/dataproc/gcloud.py'; adding 'hailtop/hailctl/dataproc/modify.py'; adding 'hailtop/hailctl/dataproc/start.py'; addi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:27011,log,login,27011,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['log'],['login']
Testability,.relocated.com.google.cloud.storage.JsonResumableSessionQueryTask.call(JsonResumableSessionQueryTask.java:100) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.query(JsonResumableSession.java:57) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:73) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWritableByteChannel.internalWrite(ApiaryUnbufferedWritableByteChanne,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:9907,test,test-,9907,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:308) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:177) ~[gs:__hail-test-ezl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14259,test,test-,14259,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.r,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1789,Test,TestRunner,1789,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['Test'],['TestRunner']
Testability,.suffix; at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at org.scalatest.testng.TestNGSuite.newAssertionFailedException(TestNGSuite.scala:67); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at is.hail.io.fs.FSSuite.largeDirectoryOperations(FSSuite.scala:445); at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:430); at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:136); at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:658); at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:219); at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50); at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:923); at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:192); at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146); at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128); at java.util.ArrayList.forEach(ArrayList.java:1259); at org.testng.TestRunner.privateRun(TestRunner.java:808); at org.testng.TestRunner.run(TestRunner.java:603); at org.testng.SuiteRunner.runTest(SuiteRunner.java:429); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:383); at org.testng.SuiteRunner.run(SuiteRunner.java:326); at org.testng.SuiteRunnerWorker.runSui,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547:1244,Test,TestInvoker,1244,https://hail.is,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547,1,['Test'],['TestInvoker']
Testability,.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigBGzip PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSameAsOrigNoCompression PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportVcfSuite.testSorted PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.evalTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.filterTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.FilterSuite.treeTransformerTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.GQByDPBinSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeStreamSuite.testGenotypeStream PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.GenotypeSuite.testGenotype PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.HWESuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.IntervalListSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:1360,test,test,1360,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,1,['test'],['test']
Testability,".test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.LEB128Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.exactTestsTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.meanTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.modeTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.pmfTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.stats.LeveneHaldaneSuite.varianceTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.LinearRegressionSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MendelErrorsSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.MultiArray2Suite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.PedigreeSuite.test PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.SampleQCSuite.testStoreAfterFilter PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.vcf.SplitSuite.SplitTest PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testD_$eq$eq PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.utils.UtilsSuite.testFlushDouble PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testFilterSamples PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.vsm.VSMSuite.testSame PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.variant.VariantSuite.testVariant PASSED. Gradle suite > Gradle test > org.broadinstitute.hail.methods.gqDpStatsSuite.test PASSED; :check. BUILD SUCCESSFUL; ```. **TIMING**. import, write chr22; - Current master, best of 3: 1m48.5s; - map-any, best of 3: 1m43.1s. read, filtervariants --keep -c 'va.info.MQ>20' filtergenotypes --keep -c 'g.gq > 20' count; - master, best of 3: 2m55s; - map-any, best of 3: 35.3s. read, count takes 9s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/158#issuecomment-173700450:2440,test,testStoreAfterFilter,2440,https://hail.is,https://github.com/hail-is/hail/pull/158#issuecomment-173700450,13,['test'],"['test', 'testFilterSamples', 'testFlushDouble', 'testSame', 'testStoreAfterFilter', 'testVariant']"
Testability,.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:240); at org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2426,test,testng,2426,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['test'],['testng']
Testability,".zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""configBucket"": ""dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us"",; ""gceClusterConfig"": {; ""zoneUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f"",; ""networkUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/global/networks/default"",; ""serviceAccountScopes"": [; ""https://www.googleapis.com/auth/bigquery"",; ""https://www.googleapis.com/auth/bigtable.admin.table"",; ""https://www.googleapis.com/auth/bigtable.data"",; ""https://www.googleapis.com/auth/cloud.useraccounts.readonly"",; ""https://www.googleapis.com/auth/devstorage.full_control"",; ""https://www.googleapis.com/auth/devstorage.read_write"",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:3847,log,logging,3847,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,['log'],['logging']
Testability,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notific",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4363,test,test,4363,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"//api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:4664,test,test,4664,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"/_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2917,LOG,LOGGING,2917,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['LOG'],['LOGGING']
Testability,"/batch/worker/worker.py:705: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/datasets/extract/extract_CADD.py:26: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:1061: return self.domain is not None and (self.domain == 'gcr.io' or self.domain.endswith('docker.pkg.dev')); /Users/dking/projects/hail/hail/python/hailtop/aiocloud/aiogoogle/client/container_client.py:6: s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:1232,test,test,1232,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['test'],['test']
Testability,"/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1676,test,test-monitoring,1676,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['test'],['test-monitoring']
Testability,"/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_iss",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5244,test,test,5244,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7926,log,logConf,7926,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,['log'],['logConf']
Testability,"/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3521,log,login,3521,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['log'],['login']
Testability,"/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:1061: return self.domain is not None and (self.domain == 'gcr.io' or self.domain.endswith('docker.pkg.dev')); /Users/dking/projects/hail/hail/python/hailtop/aiocloud/aiogoogle/client/container_client.py:6: super().__init__(f'https://gcr.io/v2/{project}', **kwargs); /Users/dking/projects/hail/datasets/extract/extract_dbSNP.py:22: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/infra/gcp/main.tf:55: ""gcr.io/${var.gcp_project}""; /Users/dking/projects/hail/infra/gcp/main.tf:375: disp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:1504,assert,assert,1504,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['assert'],['assert']
Testability,"0 indelDebug=false ignoreSNPAlleles=false allReadsSP=false ignoreLaneInfo=false reference_sample_calls=(RodBinding name= source=UNBOUND) reference_sample_name=null sample_ploidy=2 min_quality_score=1 max_quality_score=40 site_quality_prior=20 min_power_threshold_for_calling=0.95 min_reference_depth=100 exclude_filtered_reference_sites=false heterozygosity=0.0010 genotyping_mode=DISCOVERY output_mode=EMIT_VARIANTS_ONLY standard_min_confidence_threshold_for_calling=30.0 standard_min_confidence_threshold_for_emitting=30.0 alleles=(RodBinding name= source=UNBOUND) max_alternate_alleles=6 p_nonref_model=EXACT_INDEPENDENT contamination_fraction_to_filter=0.0 contamination_percentage_per_sample_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.v1.contamination_levels.txt logRemovedReadsFromContaminationFiltering=null exactcallslog=null dbsnp=(RodBinding name=dbsnp source=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.dbsnp.vcf) comp=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub debug_file=null metrics_file=null annotation=[] excludeAnnotation=[] filter_mismatching_base_and_quals=false""; ##VariantAnnotator=""analysis_type=VariantAnnotator input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE down",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:16343,stub,stubs,16343,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['stub'],['stubs']
Testability,0 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1820,test,test,1820,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1325,log,log,1325,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,6,['log'],['log']
Testability,07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8986,test,test,8986,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1305,log,log,1305,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815,1,['log'],['log']
Testability,"1 is main, 2 is this branch; ```; $ python -m benchmark_hail compare 2023-05-16-0.2.116-a2b070f715d8-zstd-main-cmp.json 2023-05-16-0.2.116-78a43d968e7b-zstd.json; Failed benchmarks in run 1:; pc_relate; king; Failed benchmarks in run 2:; king; Benchmark Name Ratio Time 1 Time 2 Mem Ratio Mem 1 (MB) Mem 2 (MB); -------------- ----- ------ ------ --------- ---------- ----------; table_foreign_key_join_same_cardinality 302.7% 13.710 41.502 100.0% 3 3; table_foreign_key_join_left_higher_cardinality 280.9% 13.990 39.294 100.0% 3 3; table_key_by_shuffle 250.0% 5.910 14.778 100.0% 2 2; shuffle_order_by_10m_int 235.5% 61.407 144.602 100.0% 2 2; shuffle_key_by_aggregate_bad_locality 195.9% 67.872 132.991 100.0% 2 2; table_take 155.3% 1.018 1.581 100.0% 1 1; shuffle_key_rows_by_4096_byte_rows 151.9% 10.150 15.412 100.0% 2 2; read_force_count_p1000 142.0% 3.998 5.678 100.0% 1 1; read_force_count_p10 140.8% 1.803 2.539 100.0% 1 1; join_p100_p100 138.4% 3.702 5.123 100.0% 1 1; join_p100_p10 134.2% 13.166 17.671 100.0% 1 1; test_inner_join_region_memory 129.9% 2.362 3.069 100.0% 382 382; large_range_matrix_table_sum 129.1% 206.853 266.961 100.0% 17 17; table_annotate_many_flat 125.9% 1.134 1.428 100.0% 1 1; test_map_filter_region_memory 121.7% 2.176 2.647 100.0% 382 382; import_vcf_count_rows 121.5% 7.532 9.150 100.0% 1 1; join_p10_p100 121.1% 14.040 17.000 100.0% 1 1; shuffle_key_rows_by_mt 119.1% 33.339 39.722 100.0% 3 3; table_aggregate_downsample_worst_case 118.3% 21.323 25.229 100.0% 1 1; variant_and_sample_qc_nested_with_filters_2 118.2% 25.267 29.873 100.0% 1 1; union_p1000_p1000 118.0% 8.160 9.625 100.0% 1 1; read_force_count_p100 117.9% 2.336 2.754 100.0% 1 1; mt_group_by_memory_usage 117.5% 25.597 30.069 100.0% 136 136; matrix_table_filter_entries_unfilter 116.9% 10.308 12.052 100.0% 1 1; table_group_by_aggregate_sorted 116.2% 3.138 3.645 100.0% 2 2; matrix_table_aggregate_entries 114.8% 8.030 9.218 100.0% 1 1; union_p100_p100 114.5% 5.197 5.950 100.0% 1 1; split_multi 1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1550388849:170,benchmark,benchmarks,170,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1550388849,3,"['Benchmark', 'benchmark']","['Benchmark', 'benchmarks']"
Testability,1-py2.py3-none-any.whl (15.6 MB); Collecting portalocker==2.7.0; Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB); Collecting protobuf==3.20.2; Using cached protobuf-3.20.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB); Collecting py4j==0.10.9.5; Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB); Collecting pyasn1==0.5.0; Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB); Collecting pyasn1-modules==0.3.0; Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB); Collecting pycares==4.3.0; Using cached pycares-4.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB); Collecting pycparser==2.21; Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB); Collecting pygments==2.16.1; Using cached Pygments-2.16.1-py3-none-any.whl (1.2 MB); Collecting pyjwt[crypto]==2.8.0; Using cached PyJWT-2.8.0-py3-none-any.whl (22 kB); Collecting python-dateutil==2.8.2; Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB); Collecting python-json-logger==2.0.7; Using cached python_json_logger-2.0.7-py3-none-any.whl (8.1 kB); Collecting pytz==2023.3.post1; Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB); Collecting pyyaml==6.0.1; Using cached PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB); Collecting regex==2023.8.8; Using cached regex-2023.8.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB); Collecting requests==2.31.0; Using cached requests-2.31.0-py3-none-any.whl (62 kB); Collecting requests-oauthlib==1.3.1; Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB); Collecting rich==12.6.0; Using cached rich-12.6.0-py3-none-any.whl (237 kB); Collecting rsa==4.9; Using cached rsa-4.9-py3-none-any.whl (34 kB); Collecting s3transfer==0.6.2; Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB); Collecting scipy==1.11.2; Using cached scipy-1.11.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.5 MB); Collecting six==1.16.0; Using cac,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:38901,log,logger,38901,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['log'],['logger']
Testability,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:2223,test,test,2223,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,4,"['log', 'test']","['log', 'test']"
Testability,"1. I need to add some tests.; 2. I flipped a coin and you came up @daniel-goldstein (as opposed to Ed), feel free to reassign if you feel uncomfortable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14560#issuecomment-2125707106:22,test,tests,22,https://hail.is,https://github.com/hail-is/hail/pull/14560#issuecomment-2125707106,1,['test'],['tests']
Testability,1. Won't we have conflicts with the test databases all starting their job ids from `1...`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5866#issuecomment-485464925:36,test,test,36,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485464925,1,['test'],['test']
Testability,"1. instance_id should go away once persistence goes in. Then it should be just be job_id/task_name/job.log. 2. Deleting a job should delete the log, otherwise it is should stay. I think it's fine for now. We're going to want to do some serious cleanup once everything is in a manageable state.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5866#issuecomment-485461284:103,log,log,103,https://hail.is,https://github.com/hail-is/hail/pull/5866#issuecomment-485461284,2,['log'],['log']
Testability,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:5657,test,test,5657,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,4,['test'],['test']
Testability,103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:297); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:279); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:297); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.Suit,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:3181,test,testSeekMoreThanMaxInt,3181,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testSeekMoreThanMaxInt']
Testability,103); 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 	at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 	at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 	at java.io.DataOutputStream.flush(DataOutputStream.java:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.Suit,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:3374,test,testSeekMoreThanMaxInt,3374,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testSeekMoreThanMaxInt']
Testability,13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8845,test,test,8845,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9175,test,test,9175,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9223,test,test,9223,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9744,test,test,9744,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9791,test,test,9791,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9838,test,test,9838,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10216,test,test,10216,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10501,test,test,10501,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3transfer-0.6.2 scipy-1.11.2 six-1.16.0 sortedcontainers-2.4.0 tabulate-0.9.0 tenacity-8.2.3 tornado-6.3.3 typer-0.9.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wrapt-1.15.0 xyzservices; -2023.7.0 yarl-1.9.2. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; python3 -m pip uninstall -y hail; WARNING: Skipping hail as it is not installed.; python3 -m pip install build/deploy/dist/hail-0.2.124-py3-none-any.whl --no-deps; Defaulting to user installation because normal site-packages is not writeable; Processing ./build/deploy/dist/hail-0.2.124-py3-none-any.whl; Installing collected packages: hail; Successfully installed hail-0.2.124. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; hailctl config set query/backend",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:44038,log,logger-,44038,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['log'],['logger-']
Testability,1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8702,test,test,8702,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"188c276.png). Pvals for variants with at least 20 hets, basically the same:; ![logregpval20hets](https://cloud.githubusercontent.com/assets/3201642/22859658/c6d27e12-f0b1-11e6-814f-b4a75dd54162.png). Comparison of iterations until convergence, note that LRT is bimodal due to quasi-separation, whereas Firth is not. When well-posed, Firth takes more iterations to converge as expected:; ![logregiter](https://cloud.githubusercontent.com/assets/3201642/22859638/df6c31ee-f0b0-11e6-9443-1a00bb2e9848.png). LRT iterations:; ![logreglrtiter](https://cloud.githubusercontent.com/assets/3201642/22859676/816119b4-f0b2-11e6-8401-2b6600d6f443.png). Firth iterations:; ![logregfirthiter](https://cloud.githubusercontent.com/assets/3201642/22859677/883c4a6a-f0b2-11e6-9aad-b79e613f2ba7.png). For the record, Wald is mis-calibrated for small counts:; ![logregpvalwaldlrt](https://cloud.githubusercontent.com/assets/3201642/22859691/f629f5fe-f0b2-11e6-98fe-b96b3ef497ec.png). Score is more conservative than LRT:; ![logregpvalscorelrt](https://cloud.githubusercontent.com/assets/3201642/22859708/92750f7a-f0b3-11e6-93af-3219eb9e025f.png). Firth is more conservative than score:; ![logregpvalscorefirth](https://cloud.githubusercontent.com/assets/3201642/22859693/fed4555a-f0b2-11e6-9636-2a8075b0a04a.png). And linear betas are super conservative:; ![logregbetafirthlin](https://cloud.githubusercontent.com/assets/3201642/22867304/c63d2b76-f153-11e6-87b3-445c58796695.png). But linear pvals are okay:; ![logregpvalfirthlin](https://cloud.githubusercontent.com/assets/3201642/22867309/dc0f0d70-f153-11e6-840d-308dc0570a6a.png). And essentially identical to score test:; ![logregpvalscorelin](https://cloud.githubusercontent.com/assets/3201642/22867310/e475f730-f153-11e6-9cba-acec78a12964.png). Here's a QQ-plot:; ![logreg qqplot](https://cloud.githubusercontent.com/assets/3201642/23096398/c6c3db0e-f5e9-11e6-97e8-4a565bcc9cb8.png). Here's a QQ-plot restricted to variants with > 20 hets:; ![logreg qqplot 20het](h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409:3011,log,logregpvalscorelrt,3011,https://hail.is,https://github.com/hail-is/hail/pull/1375#issuecomment-279196409,1,['log'],['logregpvalscorelrt']
Testability,1cd9g3etP2sS4f13bN6iJPU_sbnRnyRE91VPtjUpuYLPqmOq13; 	| ; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:185) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:117) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionFailureScenario.toStorageException(JsonResumableSessionFailureScenario.java:98) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSessionQueryTask.call(JsonResumableSessionQueryTask.java:100) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.query(JsonResumableSession.java:57) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:73) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76) ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:9057,test,test-,9057,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,2.1.0 test was removed from CI (and as a merge dependency),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2639#issuecomment-356088106:6,test,test,6,https://hail.is,https://github.com/hail-is/hail/pull/2639#issuecomment-356088106,1,['test'],['test']
Testability,"2079a85bce (41 minutes):. I could try to make the tests even more fine-grained and split up even more long-running tests. Seems like some of the bottlenecks I'm hitting now are:; 1. Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency.; 2. The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down). I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. <img width=""2032"" alt=""Screen Shot 2023-05-22 at 12 30 47"" src=""https://github.com/hail-is/hail/assets/106194/aaa3fbb7-176d-4487-b65e-586c235e2089"">; <img width=""541"" alt=""Screen Shot 2023-05-22 at 12 31 23"" src=""https://github.com/hail-is/hail/assets/106194/016f1089-d08d-4555-ae86-c01353f39c78"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015:50,test,tests,50,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015,2,['test'],['tests']
Testability,"22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: BlockManagerMasterEndpoint up; 2019-01-22 13:11:21 DiskBlockManager: INFO: Created local directory at /tmp/blockmgr-8d910f25-2ae8-439c-8577-377758342d28; 2019-01-22 13:11:21 MemoryStore: INFO: MemoryStore started with capacity 2.5 GB; 2019-01-22 13:11:22 SparkEnv: INFO: Registering OutputCommitCoordinator; 2019-01-22 13:11:22 log: INFO: Logging initialized @11836ms; 2019-01-22 13:11:22 Server: INFO: jetty-9.3.z-SNAPSHOT; 2019-01-22 13:11:22 Server: INFO: Started @12028ms; 2019-01-22 13:11:22 AbstractConnector: INFO: Started ServerConnector@1433e9ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 2019-01-22 13:11:22 Utils: INFO: Successfully started service 'SparkUI' on port 4040.; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1fc6c1cc{/jobs,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@75771d8a{/jobs/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@56931c6{/jobs/job,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@7d4d6f14{/jobs/job/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@23f9d06d{/stages,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextH",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:10516,log,log,10516,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,3,"['Log', 'log']","['Logging', 'log']"
Testability,"24, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3530, in any; collection = arg_check(args[1], 'any', 'collection', collection_type); File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 586, in arg_check; raise TypeError(""{fname}: parameter '{argname}': ""; TypeError: any: parameter 'collection': expected expression of type set<any> or array<any>, found list: [['10', 123, 'G', 'C'], ['10', 456, 'T', 'A']]; ```; So, hail doesn't support heterogeneous arrays. Converting to a homogeneous array:. ```python; variants = [(""10"", 123, [""G"", ""C""]), (""10"", 456, [""T"", ""A""])]. expr = hl.any(; lambda x:; (mt.locus.contig == hl.literal(x[0])) & \; (mt.locus.position == hl.literal(int(x[1]))) & \; (mt.alleles == hl.literal(x[2])),; variants; ). hl.eval(expr). ```; Leads to the following error (which looks like the bug!):; ```; Traceback (most recent call last):; File ""test.py"", line 10, in <module>; expr = hl.any(lambda x:; File ""/home/edmund/.local/src/hail/hail/python/hail/expr/functions.py"", line 3531, in any; return collection.any(f); File ""<decorator-gen-510>"", line 2, in any; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 68, in any; return hl.array(self).fold(lambda accum, elt: accum | f(elt), False); File ""<decorator-gen-518>"", line 2, in fold; File ""/home/edmund/.local/src/hail/hail/python/hail/typecheck/check.py"", line 577, in wrapper; return __original_func(*args_, **kwargs_); File ""/home/edmund/.local/src/hail/hail/python/hail/expr/expressions/typed_expressions.py"", line 221, in fold; return collection._to_stream().fold(lambda x, y: f(x, y), zero); File ""<decorator-gen-650>"", line 2, in fold; File ""/h",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982:5218,test,test,5218,https://hail.is,https://github.com/hail-is/hail/issues/13046#issuecomment-1624278982,1,['test'],['test']
Testability,"280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {""name"":""BlockingBufferSpec"",""blockSize"":65536,""child"":; {""name"":""ZstdBlockBufferSpec"",""blockSize"":65536,""child"":; {""name"":""StreamBlockBuff",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:1077,log,logic,1077,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,2,['log'],['logic']
Testability,2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:347); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:341); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more. test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt FAILURE; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:9105,test,testSeekMoreThanMaxInt,9105,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,4,['test'],"['test', 'testSeekMoreThanMaxInt']"
Testability,"2})), (writeCtx,Ref(__iruid_373,str)))),AssertSameLength),Literal(struct{},[]),__iruid_370,__iruid_371,WritePartition(Let(__iruid_374,GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_374,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_374,struct{start: int32, end: int32}),end),I32(1),false),__iruid_375,MakeStruct(ArrayBuffer((idx,Ref(__iruid_375,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_370,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_290)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray()",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:4318,test,test,4318,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['test'],['test']
Testability,"2})), (writeCtx,Ref(__iruid_469,str)))),AssertSameLength),Literal(struct{},[]),__iruid_466,__iruid_467,WritePartition(Let(__iruid_470,GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),oldCtx),StreamMap(StreamRange(GetField(Ref(__iruid_470,struct{start: int32, end: int32}),start),GetField(Ref(__iruid_470,struct{start: int32, end: int32}),end),I32(1),false),__iruid_471,MakeStruct(ArrayBuffer((idx,Ref(__iruid_471,int32)))))),Apply(concat,WrappedArray(),ArrayBuffer(GetField(Ref(__iruid_466,struct{oldCtx: struct{start: int32, end: int32}, writeCtx: str}),writeCtx), UUID4(__iruid_386)),str),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/rows/parts/,Some((gs://danking/workshop-test/1kg.mt/index/,+PCStruct{idx:+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray()",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:4038,test,test,4038,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['test'],['test']
Testability,"3 install -U \; hail \; git+https://github.com/broadinstitute/gnomad_methods.git \; git+https://github.com/broadinstitute/gnomad_qc.git. git clone git@github.com:broadinstitute/gnomad-readviz.git; ```. I applied this patch:; ```diff; diff --git a/step1__select_samples.py b/step1__select_samples.py; index c159207..9ba1812 100644; --- a/step1__select_samples.py; +++ b/step1__select_samples.py; @@ -38,14 +38,7 @@ def hemi_expr(mt):; ; def main(args):; ; - hl.init(log=""/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); - meta_ht = hl.import_table(args.sample_metadata_tsv, force_bgz=True); - meta_ht = meta_ht.key_by(""s""); - meta_ht = meta_ht.filter(hl.is_defined(meta_ht.cram_path) & hl.is_defined(meta_ht.crai_path), keep=True); - meta_ht = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants with at least one non-ref GT""); mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref())); ; - #logger.info(f""Saving checkpoint""); - #mt = mt.checkpoint(os.path.join(args.temp_bucket, ""readviz_select_samples_checkpoint1.vds""),; - # overwrite=Tru",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:1198,log,log,1198,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,1,['log'],['log']
Testability,3 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9271,test,test,9271,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9602,test,test,9602,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9886,test,test,9886,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9933,test,test,9933,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10169,test,test,10169,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10549,test,test,10549,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10596,test,test,10596,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | test | 6 | 66 | 90752 |; | 2023-07-13 | test | test | 6 | 67 | 68928 |; | 2023-07-13 | test | test | 6 | 68 | 91712 |; | 2023-07-13 | test | test | 6 | 69 | 93632 |; | 2023-07-13 | test | test | 6 | 70 | 118784 |; | 2023-07-13 | test | test | 6 | 72 | 76544 |; | 2023-07-13 | test | test | 6 | 73 | 145472 |; | 2023-07-13 | test | test | 6 | 74 | 47040 |; | 2023-07-13 | test | test | 6 | 77 | 226944 |; | 2023-07-13 | test | test | 6 | 79 | 157568 |; | 2023-07-13 | test | test | 6 | 80 | 37120 |; | 2023-07-13 | test | test | 6 | 81 | 106496 |; | 2023-07-13 | test | test | 6 | 82 | 494464 |; | 2023-07-13 | test | test | 6 | 83 | 79424 |; | 2023-07-13 | test | test | 6 | 85 | 148608 |; | 2023-07-13 | test | test | 6 | 86 | 52288 |; | 2023-07-13 | test | test | 6 | 87 | 181696 |; | 2023-07-13 | test | test | 6 | 89 | 39232 |; | 2023-07-13 | test | test | 6 | 90 | 90624 |; | 2023-07-13 | test | test | 6 | 92 | 133504 |; | 2023-07-13 | test | test | 6 | 93 | 156672 |; | 2023-07-13 | test | test | 6 | 95 | 190528 |; | 2023-07-13 | test | test | 6 | 96 | 38016 |; | 2023-07-13 | test | test | 6 | 98 | 99392 |; | 2023-07-13 | test | test | 6 | 99 | 109568 |; | 2023-07-13 | test | test | 6 | 100 | 92928 |; | 2023-07-13 | test | test | 6 | 101 | 75712 |; | 2023-07-13 | test | test | 6 | 105 | 50048 |; | 2023-07-13 | test | test | 6 | 107 | 41472 |; | 2023-07-13 | test | test | 6 | 109 | 30208 |; | 2023-07-13 | test | test | 6 | 110 | 72064 |; | 2023-07-13 | test | test | 6 | 111 | 36672 |; | 2023-07-13 | test | test | 6 | 113 | 32832 |; | 2023-07-13 | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:10644,test,test,10644,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,3); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2208,Test,TestNG,2208,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['Test'],['TestNG']
Testability,"3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16768,log,log,16768,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['log'],['log']
Testability,31ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.close(GoogleStorageFS.scala:308) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14091,test,test-,14091,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,3931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.query(JsonResumableSession.java:57) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.lambda$put$0(JsonResumableSession.java:73) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.Retrying.lambda$run$0(Retrying.java:102) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.Retrying.run(Retrying.java:99) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.JsonResumableSession.put(JsonResumableSession.java:68) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWritableByteChannel.internalWrite(ApiaryUnbufferedWritableByteChannel.java:114) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedWr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:10096,test,test-,10096,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,3ab-abb4-d67a0978ee46/791.suffix; 	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); 	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); 	at org.scalatest.testng.TestNGSuite.newAssertionFailedException(TestNGSuite.scala:67); 	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); 	at is.hail.io.fs.FSSuite.largeDirectoryOperations(FSSuite.scala:413); 	at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:398); 	at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827#issuecomment-1769490277:1269,test,testng,1269,https://hail.is,https://github.com/hail-is/hail/issues/13827#issuecomment-1769490277,1,['test'],['testng']
Testability,40); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4082,Test,TestRunner,4082,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestRunner']
Testability,40); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4275,Test,TestRunner,4275,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestRunner']
Testability,"42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.jar,ZIP=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac1f1d53288171a54/hail.zip \; --properties=spark:spark.driver.memory=3g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,hdfs:dfs.replication=1,dataproc:dataproc.logging.stackdriver.enable=false,dataproc:dataproc.monitoring.stackdriver.enable=false \; --initialization-actions=gs://dataproc-initialization-actions/conda/bootstrap-conda.sh,gs://hail-common/cloudtools/init_notebook1.py,gs://hail-common/vep/vep/vep85-loftee-1.0-GRCh37-init-docker.sh \; --master-machine-type=n1-standard-1 \; --master-boot-disk-size=40GB \; --num-master-local-ssds=0 \; --num-preemptible-workers=0 \; --num-worker-local-ssds=0 \; --num-workers=2 \; --preemptible-worker-boot-disk-size=40GB \; --worker-boot-disk-size=40 \; --worker-machine-type=n1-standard-1 \; --zone=us-central1-b \; --initialization-action-timeout=20m \; --bucket=hail-ci-0-1-dataproc-staging-bucket \; --max-idle=10m; Starting cluster 'ci-test-6boype3d'...; Traceback (most recent call last):; File ""/home/hail/.conda/envs/hail/bin/cluster"", line 10, in <module>; sys.exit(main()); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/__main__.py"", line 85, in main; start.main(args); File ""/home/hail/.conda/envs/hail/lib/python3.6/site-packages/cloudtools/start.py"", line 210, in main; check_call(cmd); File ""/home/hail/.conda/envs/hail/lib/python3.6/subprocess.py"", line 291, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'beta', 'dataproc', 'clusters', 'create', 'ci-test-6boype3d', '--image-version=1.2-deb9', '--metadata=MINICONDA_VERSION=4.4.10,JAR=gs://hail-ci-0-1/temp/25aa42b2d6d442615931b2eb65c5f8e012de52a0/96d6daa14989dd0cff08b30ac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518:9506,test,test-,9506,https://hail.is,https://github.com/hail-is/hail/issues/4530#issuecomment-475782518,2,['test'],['test-']
Testability,43_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1683,test,test,1683,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,445); at is.hail.io.fs.FSSuite.largeDirectoryOperations$(FSSuite.scala:430); at is.hail.io.fs.GoogleStorageFSSuite.largeDirectoryOperations(GoogleStorageFSSuite.scala:10); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.testng.internal.invokers.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:136); at org.testng.internal.invokers.TestInvoker.invokeMethod(TestInvoker.java:658); at org.testng.internal.invokers.TestInvoker.invokeTestMethod(TestInvoker.java:219); at org.testng.internal.invokers.MethodRunner.runInSequence(MethodRunner.java:50); at org.testng.internal.invokers.TestInvoker$MethodInvocationAgent.invoke(TestInvoker.java:923); at org.testng.internal.invokers.TestInvoker.invokeTestMethods(TestInvoker.java:192); at org.testng.internal.invokers.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:146); at org.testng.internal.invokers.TestMethodWorker.run(TestMethodWorker.java:128); at java.util.ArrayList.forEach(ArrayList.java:1259); at org.testng.TestRunner.privateRun(TestRunner.java:808); at org.testng.TestRunner.run(TestRunner.java:603); at org.testng.SuiteRunner.runTest(SuiteRunner.java:429); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:423); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:383); at org.testng.SuiteRunner.run(SuiteRunner.java:326); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:95); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1249); at org.testng.TestNG.runSuitesLocally(TestNG.java:1169); at org.testng.TestNG.runSuites(TestNG.java:1092); at org.testng.TestNG.run(TestNG.java:1060); at org.testng.TestNG.privateMain(TestNG.java:1403); at org.testng.TestNG.main(TestNG.java:13,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547:1646,test,testng,1646,https://hail.is,https://github.com/hail-is/hail/issues/13827#issuecomment-1957725547,1,['test'],['testng']
Testability,"4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/ha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3234,test,test,3234,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allow_rebase_merge"": true,; ""organization"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:6074,test,test,6074,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8751,test,test,8751,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,6); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:276); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:40); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:39); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:339); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:314); 	at is.hail.expr.ir.IRSuite.testStreamLenUnconsumedInnerStream(IRSuite.scala:1800); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:3039,Test,TestUtils,3039,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,1,['Test'],['TestUtils']
Testability,6); at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158); at org.apache.spark.SparkContext.clean(SparkContext.scala:1622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAcces,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1617,Test,TestMethodWorker,1617,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['Test'],['TestMethodWorker']
Testability,"6.4; 950 | amazon-ebs: Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB); 951 | amazon-ebs:  249.9/249.9 kB 45.1 MB/s eta 0:00:00; 952 | amazon-ebs: Requirement already satisfied: pandas<1.5.0,>=1.3.0 in /usr/local/lib64/python3.7/site-packages (1.3.5); 953 | amazon-ebs: Collecting parsimonious<0.9; 954 | amazon-ebs: Downloading parsimonious-0.8.1.tar.gz (45 kB); 955 | amazon-ebs:  45.1/45.1 kB 10.2 MB/s eta 0:00:00; 956 | amazon-ebs: Preparing metadata (setup.py): started; 957 | amazon-ebs: Preparing metadata (setup.py): finished with status 'done'; 958 | amazon-ebs: Collecting plotly<5.11,>=5.5.0; 959 | amazon-ebs: Downloading plotly-5.10.0-py2.py3-none-any.whl (15.2 MB); 960 | amazon-ebs:  15.2/15.2 MB 57.8 MB/s eta 0:00:00; 961 | amazon-ebs: Collecting PyJWT; 962 | amazon-ebs: Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB); 963 | amazon-ebs: Collecting python-json-logger==2.0.2; 964 | amazon-ebs: Downloading python_json_logger-2.0.2-py3-none-any.whl (7.4 kB); 965 | amazon-ebs: Collecting requests==2.25.1; 966 | amazon-ebs: Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB); 967 | amazon-ebs:  61.2/61.2 kB 15.6 MB/s eta 0:00:00; 968 | amazon-ebs: Requirement already satisfied: scipy<1.8,>1.2 in /usr/local/lib64/python3.7/site-packages (1.7.3); 969 | amazon-ebs: Requirement already satisfied: sortedcontainers==2.4.0 in /usr/local/lib/python3.7/site-packages (2.4.0); 970 | amazon-ebs: Collecting tabulate==0.8.9; 971 | amazon-ebs: Downloading tabulate-0.8.9-py3-none-any.whl (25 kB); 972 | amazon-ebs: Requirement already satisfied: tqdm==4.* in /usr/local/lib/python3.7/site-packages (4.64.1); 973 | amazon-ebs: Collecting uvloop==0.16.0; 974 | amazon-ebs: Downloading uvloop-0.16.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.8 MB); 975 | amazon-ebs: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:6795,log,logger,6795,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['log'],['logger']
Testability,60 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1785,test,test,1785,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"609, 'cost': 1.1510333711392028e-06, 'msec_mcpu': 0, 'status': {'version': 5, 'worker': 'batch-worker-pr-12955-default-rrlcxki12v8r-standard-0e2wl', 'batch_id': 74, 'job_id': 1, 'attempt_id': 'gAaTm8', 'user': 'test', 'state': 'succeeded', 'format_version': 7, 'resources': [{'name': 'az/vm/Standard_D8ds_v4/spot/eastus/1682899200000', 'quantity': 32}, {'name': 'az/disk/E4_LRS/eastus/1546300800000', 'quantity': 1024}, {'name': 'az/ip-fee/1024/2021-12-01', 'quantity': 32}, {'name': 'az/service-fee/2021-12-01', 'quantity': 250}], 'region': 'eastus', 'start_time': 1682966178997, 'end_time': 1682966179606, 'container_statuses': {'main': {'name': 'batch-74-job-1-main', 'state': 'succeeded', 'timing': {'pulling': {'start_time': 1682966179035, 'finish_time': 1682966179224, 'duration': 189}, 'setting up overlay': {'start_time': 1682966179224, 'finish_time': 1682966179253, 'duration': 29}, 'setting up network': {'start_time': 1682966179253, 'finish_time': 1682966179253, 'duration': 0}, 'running': {'start_time': 1682966179253, 'finish_time': 1682966179340, 'duration': 87}, 'uploading_log': {'start_time': 1682966179340, 'finish_time': 1682966179361, 'duration': 21}, 'uploading_resource_usage': {'start_time': 1682966179361, 'finish_time': 1682966179407, 'duration': 46}}, 'container_status': {'started_at': 1682966179253, 'finished_at': 1682966179340, 'state': 'finished', 'exit_code': 0, 'out_of_memory': False}}}, 'timing': {'setup_io': {'start_time': 1682966178998, 'finish_time': 1682966178999, 'duration': 1}, 'configuring xfsquota': {'start_time': 1682966178999, 'finish_time': 1682966179035, 'duration': 36}, 'populating secrets': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'adding cloudfuse support': {'start_time': 1682966179035, 'finish_time': 1682966179035, 'duration': 0}, 'post-job finally block': {'start_time': 1682966179578}}}, 'spec': {'always_copy_output': False, 'job_id': 1, 'process': {'command': ['true'], 'image': 'haildev.azurecr.io/ubun",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228:987,test,test,987,https://hail.is,https://github.com/hail-is/hail/pull/12955#issuecomment-1530133228,2,['test'],['test']
Testability,61z7543_FUitX | test | 6 | 0 | 1817536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1615,test,test,1615,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,622); at org.apache.spark.rdd.RDD.map(RDD.scala:286); at org.broadinstitute.hail.methods.MendelErrors.writeMendel(MendelErrors.scala:143); at org.broadinstitute.hail.methods.MendelErrorsSuite.test(MendelErrorsSuite.scala:50); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:1753,Test,TestRunner,1753,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['Test'],['TestRunner']
Testability,7-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | tes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8798,test,test,8798,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,7-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9033,test,test,9033,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,7-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | test | 6 | 45 | 100800 |; | 2023-07-13 | test | test | 6 | 48 | 86592 |; | 2023-07-13 | test | test | 6 | 49 | 94336 |; | 2023-07-13 | test | test | 6 | 50 | 39744 |; | 2023-07-13 | test | test | 6 | 51 | 73344 |; | 2023-07-13 | test | test | 6 | 52 | 357248 |; | 2023-07-13 | test | test | 6 | 54 | 55872 |; | 2023-07-13 | test | test | 6 | 56 | 76864 |; | 2023-07-13 | test | test | 6 | 57 | 94016 |; | 2023-07-13 | test | test | 6 | 60 | 93504 |; | 2023-07-13 | test | test | 6 | 61 | 149952 |; | 2023-07-13 | test | test | 6 | 62 | 48064 |; | 2023-07-13 | test | test | 6 | 63 | 250560 |; | 2023-07-13 | test | test | 6 | 64 | 64704 |; | 2023-07-13 | test | test | 6 | 65 | 51968 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:9080,test,test,9080,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; Proxy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1485,log,log,1485,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['log'],['log']
Testability,7536 |; | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | ci | ci | 6 | 0 | 79640784 |; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1649,test,test,1649,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,"8 is reconciled, to protect against k8 operational errors. Obviously that means our record of state is a point of failure, but db failure modes and uptime solutions are well defined across cloud/db provider vendors (we can minimize lock-in as well). The idea seemed to be that we don't particularly care how k8 works, or how much state it persists; the contract is with our users, and we should satisfy . * K8 does not store all state indefinitely. It's more like a rotating log: https://stackoverflow.com/questions/40636021/how-to-list-kubernetes-recently-deleted-pods . For users, and for investors, we want to have a permanent record of all user interactions. * Some kinds of data may be awkward to store and query within pod labels. For instance, how much user state do we want to store in labels? How do we store operation graphs / history?; ; * aggregation operations across users or resources; * a given, or all users' history: so the user can manage, see, so we can track (some, gross) metrics for billing; * various sorting operations (by date/time, etc); * full log of state for a given set of related resources (I think k8 stores last 5 events, this is probably configurable) ; ability to retry in a user-controlled way, even if pod is deleted from etcd. * Operations across N k8 resources seems like it may take up to N queries (i.e k8s.list_namespaced_service, k8s.list_namespaced_pod). There may be more efficient ways of handling this (there either is, or should be a way of querying one selector across all resources). . * Performance, even for very basic queries. An initial assumption, may prove to be incorrect, but I think we will find a fast db to be much faster than K8, even if we could directly query etcd. This is based on my experience with Amazon data stores, and general impression/experience with google cs products.; * Open issue on this: even directly accessed, etcd is slow, doesn't index selectors https://github.com/kubernetes/kubernetes/issues/4817. * We may want to",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290:1612,log,log,1612,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459054290,1,['log'],['log']
Testability,"8, 2015 21:2_. Hail runtime for linreg with 10 PCs on profile.vds: 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filters and QC.; Phenotype data ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:1055,Log,Logging,1055,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,1,['Log'],['Logging']
Testability,80 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: reading results\n2022-11-15 20:31:27.881 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0\n2022-11-15 20:31:28.080 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0 response 200\n2022-11-15 20:31:28.104 ServiceBackend$: INFO: result 0 complete - 8157265 bytes\n2022-11-15 20:31:28.104 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1\n2022-11-15 20:31:28.293 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1 response 200\n2022-11-15 20:31:28.313 ServiceBackend$: INFO: result 1 complete - 8157265 bytes\n2022-11-15 20:31:28.313 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.2\n2022-11-15 20:31:28.522 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.2 response 200\n2022-11-15 20:31:28.587 ServiceBackend$: INFO: result 2 complete - 8157265 bytes\n2022-11-15 20:31:28.587 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.3\n2022-11-15 20:31:28.782 Requester: INFO: request GET http://memory.hail/api/v1alpha/object,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:36274,test,test-,36274,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWrite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4892,Test,TestNG,4892,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestNG']
Testability,85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength: 16777216; localOffset: 1325400064; remoteOffset: 1342177280; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWri,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4699,Test,TestNG,4699,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestNG']
Testability,9526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.UnbufferedWritableByteChannelSession$UnbufferedWritableByteChannel.writeAndClose(UnbufferedWritableByteChannelSession.java:40) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.DefaultBufferedWritableByteChannel.close(DefaultBufferedWritableByteChannel.java:166) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$SynchronizedBufferedWritableByteChannel.close(StorageByteChannels.java:119) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.StorageException.wrapIOException(StorageException.java:179) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.BaseStorageWriteChannel.close(BaseStorageWriteChannel.java:84) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$2(GoogleStorageFS.scala:310) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:280) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$close$1(GoogleStorageFS.scala:310) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:12222,test,test-,12222,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['test'],['test-']
Testability,"9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5654,test,test,5654,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5444,test,test,5444,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,": ; 2.49.0; Measurement time: ; 2023-06-05 03:25:16 PM ; Running on GCE: ; True; GCE Instance:; 	; Bucket location: ; US-CENTRAL1; Bucket storage class: ; REGIONAL; Google Server: ; ; Google Server IP Addresses: ; 142.250.128.128; 142.251.6.128; 108.177.112.128; 74.125.124.128; 172.217.212.128; 172.217.214.128; 172.253.119.128; 108.177.111.128; 142.250.1.128; 108.177.121.128; 142.250.103.128; 108.177.120.128; 142.250.159.128; 142.251.120.128; 142.251.161.128; 74.125.126.128; Google Server Hostnames: ; ib-in-f128.1e100.net; ic-in-f128.1e100.net; jo-in-f128.1e100.net; jp-in-f128.1e100.net; jq-in-f128.1e100.net; jr-in-f128.1e100.net; jt-in-f128.1e100.net; jv-in-f128.1e100.net; jw-in-f128.1e100.net; jx-in-f128.1e100.net; jy-in-f128.1e100.net; jz-in-f128.1e100.net; ie-in-f128.1e100.net; if-in-f128.1e100.net; ig-in-f128.1e100.net; ik-in-f128.1e100.net; Google DNS thinks your IP is: ; ; CPU Count: ; 16; CPU Load Average: ; [32.39, 33.2, 19.0]; Total Memory: ; 57.5 GiB; Free Memory: ; 38.41 GiB; TCP segment counts not available because ""netstat"" was not found during test runs; Disk Counter Deltas:; disk reads writes rbytes wbytes rtime wtime ; loop0 0 0 0 0 0 0 ; loop1 0 0 0 0 0 0 ; loop3 0 0 0 0 0 0 ; loop4 0 0 0 0 0 0 ; loop5 0 0 0 0 0 0 ; nvme0n1 4385 4694 581857280 1743810560 6453 527129 ; sda1 0 544 0 3731456 0 429 ; sda14 0 0 0 0 0 0 ; sda15 0 0 0 0 0 0 ; TCP /proc values:; tcp_timestamps = 1; tcp_sack = 1; tcp_window_scaling = 1; Boto HTTPS Enabled: ; True; Requests routed through proxy: ; False; Latency of the DNS lookup for Google Storage server (ms): ; 1.5; Latencies connecting to Google Storage server IPs (ms):; 74.125.126.128 = 1.1. ------------------------------------------------------------------------------; In-Process HTTP Statistics ; ------------------------------------------------------------------------------; Total HTTP requests made: 149; HTTP 5xx errors: 0; HTTP connections broken: 0; Availability: 100%. Output file written to '/tmp/output.json'.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597:3738,test,test,3738,https://hail.is,https://github.com/hail-is/hail/issues/12923#issuecomment-1577071597,1,['test'],['test']
Testability,: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:31:15.929 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:31:18.037 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:31:22.827 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:31:22.846 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:31:27.847 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:31:27.880 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:31:27.880 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: reading results\n2022-11-15 20:31:27.881 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0\n2022-11-15 20:31:28.080 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.0 response 200\n2022-11-15 20:31:28.104 ServiceBackend$: INFO: result 0 complete - 8157265 bytes\n2022-11-15 20:31:28.104 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1\n2022-11-15 20:31:28.293 Requester: INFO: request GET http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fresult.1 response 200\n2022-11-15 20:31:28.313 ServiceBackend$: INFO: result 1 complete - 8157265 bytes\n2022-11-15 20:31:28.313 Requester: INFO: request GET http://memory.hail/api/v1alpha/object,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:35534,test,test-,35534,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3060,test,test,3060,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## 2I installed the atlas-devel , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:2266,test,test,2266,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,3,"['Test', 'log', 'test']","['Test', 'log', 'test']"
Testability,": null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.connector.TCPConnector object at 0x10cfe21d0>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch; usage: hailctl batch [-h] {billing,list,get,cancel,delete,log,job,wait} ... Manage batches running on the batch service managed by the Hail team. positional arguments:; {billing,list,get,cancel,delete,log,job,wait}; billing List billing; list List batches; get Get a particular batch's info; cancel Cancel a batch; delete Delete a batch; log Get log for a job; job Get the status and specifica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1853,test,test-tiny-limit,1853,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['test'],['test-tiny-limit']
Testability,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_369,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_376,GetField(Ref(__iruid_376,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601:5697,test,test,5697,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-772011601,1,['test'],['test']
Testability,":+PInt32})),None)),Some(TableStageDependency(WrappedArray()))),Begin(ArrayBuffer(WriteMetadata(MakeArray(ArrayBuffer(GetField(WritePartition(MakeStream(ArrayBuffer(Literal(struct{},[])),stream<struct{}>,false),Str(""part-0""),PartitionNativeWriter({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},gs://danking/workshop-test/1kg.mt/globals/parts/,None,None)),filePath)),array<str>),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/globals,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{}"",""_vType"":""Struct{}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(),JArray(List(JObject(List((start,JObject(List())), (end,JObject(List())), (includeStart,JBool(true)), (includeEnd,JBool(true)))))),null,Map()))), WriteMetadata(ToArray(StreamMap(ToStream(Ref(__iruid_465,array<struct{filePath: str, partitionCounts: int64}>),false),__iruid_472,GetField(Ref(__iruid_472,struct{filePath: str, partitionCounts: int64}),filePath))),RVDSpecWriter(gs://danking/workshop-test/1kg.mt/rows,RVDSpecMaker({""name"":""TypedCodecSpec"",""_eType"":""+EBaseStruct{idx:+EInt32}"",""_vType"":""Struct{idx:Int32}"",""_bufferSpec"":{""name"":""LEB128BufferSpec"",""child"":{""name"":""BlockingBufferSpec"",""blockSize"":32768,""child"":{""name"":""LZ4HCBlockBufferSpec"",""blockSize"":32768,""child"":{""name"":""StreamBlockBufferSpec""}}}}},WrappedArray(idx),JArray(List(JObject(List((start,JObject(List((idx,JInt(0))))), (end,JObject(List((idx,JInt(10))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List((start,JObject(List((idx,JInt(10))))), (end,JObject(List((idx,JInt(20))))), (includeStart,JBool(true)), (includeEnd,JBool(false)))), JObject(List(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693:5417,test,test,5417,https://hail.is,https://github.com/hail-is/hail/issues/9856#issuecomment-756194693,2,['test'],['test']
Testability,"://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:2246,test,test,2246,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4503,test,testng,4503,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['test'],['testng']
Testability,:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-6BO4gZ18Lheigp3ir9RSOh&uploadType=resumable&upload_id=ADPycduiXx2Jtiy_0Ll131_pPeEYKnnA23Hlk28_9TFESUMaubA9OqLK_n8Td5rPhTXnlpssGo796Q4bJxUeblhmSaYcCSWAMg2k; chunkOffset: 16777216; chunkLength:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:4310,test,testng,4310,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,2,['test'],['testng']
Testability,:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:323); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:321); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients upl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756:3966,Test,TestMethodWorker,3966,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1544209756,1,['Test'],['TestMethodWorker']
Testability,:123); 	at java.io.FilterOutputStream.close(FilterOutputStream.java:158); 	at is.hail.utils.package$.using(package.scala:640); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt(FSSuite.scala:341); 	at is.hail.fs.FSSuite.testSeekMoreThanMaxInt$(FSSuite.scala:339); 	at is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt(GoogleStorageFSSuite.scala:12); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at org.testng.TestRunner.privateRun(TestRunner.java:767); 	at org.testng.TestRunner.run(TestRunner.java:617); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); 	at org.testng.SuiteRunner.run(SuiteRunner.java:254); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); 	at org.testng.TestNG.run(TestNG.java:1057); 	at org.testng.TestNG.privateMain(TestNG.java:1364); 	at org.testng.TestNG.main(TestNG.java:1333); 	Suppressed: is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients upl,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:4159,Test,TestMethodWorker,4159,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['Test'],['TestMethodWorker']
Testability,:124); at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1534,log,logWarning,1534,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,2,['log'],['logWarning']
Testability,:18.262 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: nPartitions 50\n2022-11-15 20:30:18.262 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: writing f and contexts\n2022-11-15 20:30:18.263 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: writing contexts\n2022-11-15 20:30:18.264 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff\n2022-11-15 20:30:18.264 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.802 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.852 Requester:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:23831,test,test-,23831,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['test'],['test-']
Testability,":26: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:1061: return self.domain is not None and (self.domain == 'gcr.io' or self.domain.endswith('docker.pkg.dev')); /Users/dking/projects/hail/hail/python/hailtop/aiocloud/aiogoogle/client/container_client.py:6: super().__init__(f'https://gcr.io/v2/{project}', **kwargs); /Users/dking/projects/hail/datasets/extract/extract_dbSNP.py:22: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/infra/gcp/main.tf:55: ""gcr.io/${var.gcp_pr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:1466,test,test,1466,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['test'],['test']
Testability,; 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:78); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext.scala:65); 	at is.hail.backend.local.LocalBackend.$anonfun$withExecuteContext$2(LocalBackend.scala:144); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:55); 	at is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:62); 	at is.hail.backend.local.LocalBackend.withExecuteContext(LocalBackend.scala:130); 	at is.hail.backend.local.LocalBackend.execute(LocalBackend.scala:308); 	at is.hail.backend.BackendHttpHandler.handle(BackendServer.scala:88); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:82); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:80); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:692); 	at jdk.httpserver/com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:77); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:664); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$DefaultExecutor.execute(ServerImpl.java:159); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.handle(ServerImpl.java:442); 	at jdk.httpserver/sun.net.httpserver.ServerImpl$Dispatcher.run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:3390,log,logTime,3390,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,4,['log'],['logTime']
Testability,"; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/events"",; ""assignees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/assignees{/user}"",; ""branches_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/branches{/branch}"",; ""tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/tags"",; ""blobs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/blobs{/sha}"",; ""git_tags_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/tags{/sha}"",; ""git_refs_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/refs{/sha}"",; ""trees_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/trees{/sha}"",; ""statuses_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/statuses/{sha}"",; ""languages_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/languages"",; ""stargazers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/stargazers"",; ""contributors_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contributors"",; ""subscribers_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscribers"",; ""subscription_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/subscription"",; ""commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/commits{/sha}"",; ""git_commits_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/git/commits{/sha}"",; ""comments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/comments{/number}"",; ""issue_comment_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/comments{/number}"",; ""contents_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/contents/{+path}"",; ""compare_url"": ""https://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:3609,test,test,3609,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,2,['test'],"['test', 'test-']"
Testability,"; ```; {; ""id"": 152339517,; ""node_id"": ""MDEwOlJlcG9zaXRvcnkxNTIzMzk1MTc="",; ""name"": ""ci-test-p4a9fxo7"",; ""full_name"": ""hail-ci-test/ci-test-p4a9fxo7"",; ""private"": false,; ""owner"": {; ""login"": ""hail-ci-test"",; ""id"": 43152710,; ""node_id"": ""MDEyOk9yZ2FuaXphdGlvbjQzMTUyNzEw"",; ""avatar_url"": ""https://avatars1.githubusercontent.com/u/43152710?v=4"",; ""gravatar_id"": """",; ""url"": ""https://api.github.com/users/hail-ci-test"",; ""html_url"": ""https://github.com/hail-ci-test"",; ""followers_url"": ""https://api.github.com/users/hail-ci-test/followers"",; ""following_url"": ""https://api.github.com/users/hail-ci-test/following{/other_user}"",; ""gists_url"": ""https://api.github.com/users/hail-ci-test/gists{/gist_id}"",; ""starred_url"": ""https://api.github.com/users/hail-ci-test/starred{/owner}{/repo}"",; ""subscriptions_url"": ""https://api.github.com/users/hail-ci-test/subscriptions"",; ""organizations_url"": ""https://api.github.com/users/hail-ci-test/orgs"",; ""repos_url"": ""https://api.github.com/users/hail-ci-test/repos"",; ""events_url"": ""https://api.github.com/users/hail-ci-test/events{/privacy}"",; ""received_events_url"": ""https://api.github.com/users/hail-ci-test/received_events"",; ""type"": ""Organization"",; ""site_admin"": false; },; ""html_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""description"": null,; ""fork"": false,; ""url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7"",; ""forks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/forks"",; ""keys_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/keys{/key_id}"",; ""collaborators_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/collaborators{/collaborator}"",; ""teams_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/teams"",; ""hooks_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/hooks"",; ""issue_events_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues/events{/number}"",; ""events_url"": ""https://api.github.com/repos/hail-ci-test/ci",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:1872,test,test,1872,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['test'],['test']
Testability,"; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-04-02T19:50:21Z""; generateName: notebook2-worker-; labels:; app: notebook2-worker; hail.is/notebook2-instance: f4dc8213468f4799a3c7f94cb6969309; jupyter_token: 484b71e2c12d42c79b169b1991602d45; name: a_notebook; user_id: e7e7b9c420f0b0ff503ab6711355f27748522a8a37d9d22b2c8e0af4; uuid: 84873cf540014e128cce18f5481fb682; name: notebook2-worker-d4snh; namespace: default; resourceVersion: ""41241284""; selfLink: /api/v1/namespaces/default/pods/notebook2-worker-d4snh; uid: 8cb3c1c2-5580-11e9-bcd4-42010a8000c9; spec:; containers:; - command:; - jupyter; - notebook; - --NotebookApp.token=484b71e2c12d42c79b169b1991602d45; - --NotebookApp.base_url=/instance/84873cf540014e128cce18f5481fb682/; - --ip; - 0.0.0.0; - --no-browser; image: gcr.io/hail-vdc/hail-jupyter:2c2281012d0b2171837e99fe50c8656395c7adafd93b3821af6c0a605ffaea1e; imagePullPolicy: IfNotPresent; name: default; ports:; - containerPort: 8888; protocol: TCP; readinessProbe:; failureThreshold: 3; httpGet:; path: /instance/84873cf540014e128cce18f5481fb682/login; port: 8888; scheme: HTTP; periodSeconds: 5; successThreshold: 1; timeoutSeconds: 1; resources:; requests:; cpu: ""1.601""; memory: 1.601G; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key-secret-name; name: gsa-key-secret-name; readOnly: true; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: user-kmpnh-token-hbdd4; readOnly: true; dnsPolicy: ClusterFirst; nodeName: gke-vdc-non-preemptible-pool-0106a51b-l48l; restartPolicy: Always; schedulerName: default-scheduler; securityContext: {}; serviceAccount: user-kmpnh; serviceAccountName: user-kmpnh; terminationGracePeriodSeconds: 30; tolerations:; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key-secret-nam",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611:2406,log,login,2406,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479174611,2,['log'],['login']
Testability,; at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:696); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:882); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1189); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:767); at org.testng.TestRunner.run(TestRunner.java:617); at org.testng.SuiteRunner.runTest(SuiteRunner.java:348); at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:343); at org.testng.SuiteRunner.privateRun(SuiteRunner.java:305); at org.testng.SuiteRunner.run(SuiteRunner.java:254); at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224); at org.testng.TestNG.runSuitesLocally(TestNG.java:1149); at org.testng.TestNG.run(TestNG.java:1057); at org.testng.IDEARemoteTestNG.run(IDEARemoteTestNG.java:72); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:122); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144); Caused by: java.lang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassM,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/68#issuecomment-155304880:2263,test,testng,2263,https://hail.is,https://github.com/hail-is/hail/pull/68#issuecomment-155304880,1,['test'],['testng']
Testability,"; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1517,log,log,1517,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,1,['log'],['log']
Testability,; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux FS.cpp -MG -M -MF build/FS.d -MT build/FS.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Encoder.cpp -MG -M -MF build/Encoder.d -MT build/Encoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux Decoder.cpp -MG -M -MF build/Decoder.d -MT build/Decoder.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux cache-tests.cpp -MG -M -MF build/cache-tests.d -MT build/cache-tests.o; g++ -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux ApproximateQuantiles_test.cpp -MG -M -MF build/ApproximateQuantiles_test.d -MT build/ApproximateQuantiles_te; st.o; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; make[1]: Entering directory `/mnt/tmp/hail/hail/src/main/c'; g++ -o build/NativeBoot.o -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux -MD -MF build/NativeBoot.d -MT build/NativeBoot.o -c NativeBoot.cpp; g++ -fvisibility=default -rdynamic -shared -march=sandybridge -O3 -std=c++14 -Ilibsimdpp-2.1 -Wall -Wextra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/NativeBoot.o -o lib/linux-x86-,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:9551,test,tests,9551,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['test'],['tests']
Testability,"; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:161: in eval_timed; return Env.backend().execute(MakeTuple([ir]), timed=True)[0]; hail/backend/py4j_backend.py:82: in execute; raise e.maybe_user_error(ir) from None; hail/backend/py4j_backend.py:76: in execute; result_tuple = self._jbackend.executeEncode(jir, stream_codec, timed); ../../.venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321: in __call__; return_value = get_return_value(; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . args = ('xro549', <py4j.clientserver.JavaClient object at 0x7fd0d58f6fb0>, 'o1', 'executeEncode'); kwargs = {}; pyspark = <module 'pyspark' from '/home/edmund/.local/src/hail/.venv/lib/python3.10/site-packages/pyspark/__init__.py'>; s = 'java.lang.AssertionError: assertion failed', tpl = JavaObject id=o550; deepest = 'AssertionError: assertion failed'; full = 'java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:208)\n\tat is.hail.expr.ir.BlockMa...lientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\n\n'; error_id = -1. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full, error_id = tpl._1(), tpl._2(), tpl._3(); > raise fatal_error_from_java_error_triplet(deepest, full, error_id) from None; E hail.utils.java.FatalError: AssertionError: assertion failed; E ; E Java stack trace:; E java.lang.AssertionError: assertion failed; E 	at scala.Predef$.assert(Predef.scala:208); E 	at is.hail.expr.ir.BlockMatrixMap.execute(BlockMatrixIR.scala:269); E 	at is.hail.expr.ir.BlockMatrixMap2.execute(BlockMatrixIR.scala:393); E 	at is.hail.expr.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:2031,assert,assert,2031,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['assert'],['assert']
Testability,"; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10cfe2278>; Unclosed connector; connections: ['[(<aiohttp.client_proto.ResponseHandler object at 0x10d062c78>, 2.214990943)]']; connector: <aiohttp.conne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:1156,test,test-zero-limit,1156,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['test'],['test-zero-limit']
Testability,; | test | test | 6 | 0 | 4063028160 |; | test | test | 6 | 1 | 189760 |; | test | test | 6 | 3 | 607168 |; | test | test | 6 | 4 | 749952 |; | test | test | 6 | 5 | 46912 |; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1751,test,test,1751,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,; | test | test | 6 | 6 | 158336 |; | test | test | 6 | 7 | 70336 |; | test | test | 6 | 8 | 167680 |; | test | test | 6 | 9 | 523136 |; | test | test | 6 | 10 | 40640 |; | test | test | 6 | 11 | 616448 |; | test | test | 6 | 12 | 497024 |; | test | test | 6 | 14 | 87680 |; | test | test | 6 | 15 | 111104 |; | test | test | 6 | 16 | 120128 |; | test | test | 6 | 17 | 28736 |; | test | test | 6 | 19 | 42240 |; | test | test | 6 | 20 | 271232 |; | test | test | 6 | 21 | 88320 |; | test | test | 6 | 22 | 149760 |; | test | test | 6 | 23 | 47232 |; | test | test | 6 | 24 | 45888 |; | test | test | 6 | 25 | 41664 |; | test | test | 6 | 27 | 56704 |; | test | test | 6 | 28 | 36864 |; | test | test | 6 | 30 | 57792 |; | test | test | 6 | 31 | 62848 |; | test | test | 6 | 32 | 40320 |; | test | test | 6 | 33 | 61888 |; | test | test | 6 | 34 | 43520 |; | test | test | 6 | 35 | 219328 |; | test | test | 6 | 36 | 141760 |; | test | test | 6 | 38 | 157632 |; | test | test | 6 | 40 | 72064 |; | test | test | 6 | 41 | 317888 |; | test | test | 6 | 42 | 83648 |; | test | test | 6 | 43 | 123200 |; | test | test | 6 | 44 | 196032 |; | test | test | 6 | 45 | 62848 |; | test | test | 6 | 47 | 232768 |; | test | test | 6 | 48 | 937472 |; | test | test | 6 | 49 | 78272 |; | test | test | 6 | 50 | 88320 |; | test | test | 6 | 51 | 128320 |; | test | test | 6 | 52 | 36160 |; | test | test | 6 | 53 | 1268800 |; | test | test | 6 | 54 | 37568 |; | test | test | 6 | 55 | 53568 |; | test | test | 6 | 56 | 147520 |; | test | test | 6 | 57 | 102784 |; | test | test | 6 | 58 | 95360 |; | test | test | 6 | 60 | 156480 |; | test | test | 6 | 61 | 54976 |; | test | test | 6 | 62 | 123072 |; | test | test | 6 | 63 | 88128 |; | test | test | 6 | 64 | 202112 |; | test | test | 6 | 65 | 54848 |; | test | test | 6 | 67 | 715456 |; | test | test | 6 | 69 | 51264 |; | test | test | 6 | 70 | 127296 |; | test | test | 6 | 71 | 95040 |; | test | test | 6 | 72 | 72320 |; | test | test | 6 | 73 | 81856 |; | te,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:1924,test,test,1924,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,";5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:; MAIL=/var/spool/mail/hadoop; PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/; AWS_DEFAULT_REGION=ap-southeast-1; PWD=/home/hadoop; JAVA_HOME=/etc/alternatives/jre; LANG=en_US.UTF-8; HISTCONTROL=ignoredups; SHLVL=1; HOME=/home/hadoop; LOGNAME=hadoop; QTLIB=/usr/lib64/qt-3.3/lib; SSH_CONNECTION=103.37.196.84 57805 192.168.96.172 22; LESSOPEN=||/usr/bin/lesspipe.sh %s; XDG_RUNTIME_DIR=/run/user/995; _=/usr/bin/env; ```; </p>; </details> . ```sh ; /usr/bin/which: no scala in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/); ```. ## With Hail (after hail build). ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.21; Branch HEAD; Compiled by user liangchi on 2023-02-11T02:24:04Z; Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6; Url https://github.com/apache/spark; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; /usr/bin/spark-submit; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-oper",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:4176,log,logger,4176,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['log'],['logger']
Testability,;; +--------------+----------------------------------+------+-------------+-------+------------+; | billing_date | billing_project | user | resource_id | token | usage |; +--------------+----------------------------------+------+-------------+-------+------------+; | 2023-07-13 | __testproject_iizhz61z7543_FUitX | test | 6 | 0 | 1817536 |; | 2023-07-13 | __testproject_iizhz61z7543_uvxWn | test | 6 | 0 | 11331136 |; | 2023-07-13 | ci | ci | 6 | 0 | 79640784 |; | 2023-07-13 | test | test | 6 | 0 | 4142226688 |; | 2023-07-13 | test | test | 6 | 1 | 108608 |; | 2023-07-13 | test | test | 6 | 2 | 80576 |; | 2023-07-13 | test | test | 6 | 5 | 35648 |; | 2023-07-13 | test | test | 6 | 7 | 578240 |; | 2023-07-13 | test | test | 6 | 9 | 33024 |; | 2023-07-13 | test | test | 6 | 10 | 33472 |; | 2023-07-13 | test | test | 6 | 11 | 110464 |; | 2023-07-13 | test | test | 6 | 14 | 47744 |; | 2023-07-13 | test | test | 6 | 15 | 45440 |; | 2023-07-13 | test | test | 6 | 16 | 79616 |; | 2023-07-13 | test | test | 6 | 17 | 604928 |; | 2023-07-13 | test | test | 6 | 18 | 78016 |; | 2023-07-13 | test | test | 6 | 19 | 87040 |; | 2023-07-13 | test | test | 6 | 20 | 89792 |; | 2023-07-13 | test | test | 6 | 21 | 188736 |; | 2023-07-13 | test | test | 6 | 22 | 75648 |; | 2023-07-13 | test | test | 6 | 24 | 96128 |; | 2023-07-13 | test | test | 6 | 25 | 2143680 |; | 2023-07-13 | test | test | 6 | 26 | 33728 |; | 2023-07-13 | test | test | 6 | 27 | 70400 |; | 2023-07-13 | test | test | 6 | 28 | 93312 |; | 2023-07-13 | test | test | 6 | 31 | 53696 |; | 2023-07-13 | test | test | 6 | 32 | 70976 |; | 2023-07-13 | test | test | 6 | 33 | 57280 |; | 2023-07-13 | test | test | 6 | 34 | 99712 |; | 2023-07-13 | test | test | 6 | 35 | 87360 |; | 2023-07-13 | test | test | 6 | 36 | 144384 |; | 2023-07-13 | test | test | 6 | 37 | 703616 |; | 2023-07-13 | test | test | 6 | 39 | 344832 |; | 2023-07-13 | test | test | 6 | 42 | 129152 |; | 2023-07-13 | test | test | 6 | 44 | 56000 |; | 2023-07-13 | test | t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785:8371,test,test,8371,https://hail.is,https://github.com/hail-is/hail/pull/13213#issuecomment-1634765785,1,['test'],['test']
Testability,=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:; MAIL=/var/spool/mail/hadoop; PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/; AWS_DEFAULT_REGION=ap-southeast-1; PWD=/home/hadoop; JAVA_HOME=/etc/alternatives/jre; LANG=en_US.UTF-8; HISTCONTROL=ignoredups; SHLVL=1; HOME=/home/hadoop; LOGNAME=hadoop; QTLIB=/usr/lib64/qt-3.3/lib; SSH_CONNECTION=103.37.196.84 57805 192.168.96.172 22; LESSOPEN=||/usr/bin/lesspipe.sh %s; XDG_RUNTIME_DIR=/run/user/995; _=/usr/bin/env; ```; </p>; </details> . ```sh ; /usr/bin/which: no scala in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/); ```. ## With Hail (after hail build). ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:3568,LOG,LOGNAME,3568,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['LOG'],['LOGNAME']
Testability,"=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:; MAIL=/var/spool/mail/hadoop; PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/; AWS_DEFAULT_REGION=ap-southeast-1; PWD=/home/hadoop; JAVA_HOME=/etc/alternatives/jre; LANG=en_US.UTF-8; HISTCONTROL=ignoredups; SHLVL=1; HOME=/home/hadoop; LOGNAME=hadoop; QTLIB=/usr/lib64/qt-3.3/lib; SSH_CONNECTION=103.37.196.84 60539 192.168.124.160 22; LESSOPEN=||/usr/bin/lesspipe.sh %s; XDG_RUNTIME_DIR=/run/user/995; _=/usr/bin/env; ```; </p>; </details> . ```sh ; /usr/bin/which: no scala in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/); ```. ## Debug mode. ```sh; $ set -x; ++ printf '\033]0;%s@%s:%s\007' hadoop ip-192-168-124-160 '~'; $ spark-shell; + spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:8468,LOG,LOGNAME,8468,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['LOG'],['LOGNAME']
Testability,"=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:; MAIL=/var/spool/mail/hadoop; PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/; AWS_DEFAULT_REGION=ap-southeast-1; PWD=/home/hadoop; JAVA_HOME=/etc/alternatives/jre; LANG=en_US.UTF-8; HISTCONTROL=ignoredups; SHLVL=1; HOME=/home/hadoop; LOGNAME=hadoop; QTLIB=/usr/lib64/qt-3.3/lib; SSH_CONNECTION=103.37.196.84 60539 192.168.124.160 22; LESSOPEN=||/usr/bin/lesspipe.sh %s; XDG_RUNTIME_DIR=/run/user/995; _=/usr/bin/env; ```; </p>; </details> . ```sh ; /usr/bin/which: no scala in (/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/aws/puppet/bin/); ```. ## Debug mode. ```sh; $ set -x; ++ printf '\033]0;%s@%s:%s\007' hadoop ip-192-168-124-160 '~'; $ spark-shell; + spark-shell; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings scala.reflect.internal.settings.MutableSettings$.SettingsOps(scala.reflect.internal.settings.MutableSettings)'; at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$1(ILoop.scala:914); at scala.tools.nsc.interpreter.ILoop.mkReader$1(ILoop.scala:920); at scala.tools.nsc.interpreter.ILoop.$anonfun$chooseReader$4(ILoop.scala:926); at scala.tools.nsc.inte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:9016,log,logger,9016,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['log'],['logger']
Testability,"=======================; platform darwin -- Python 3.9.17, pytest-7.4.0, pluggy-1.3.0; rootdir: /Users/dgoldste/hail/hail/python/test; configfile: pytest.ini; plugins: anyio-4.0.0, xdist-2.5.0, instafail-0.5.0, timeout-2.1.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, forked-1.6.0; asyncio: mode=auto; collected 8218 items / 8133 deselected / 85 selected. <Package hail>; <Module test_context.py>; <Function test_init_hail_context_twice>; <Function test_top_level_functions_are_do_not_error>; <Function test_tmpdir_runs>; <Module test_randomness.py>; <Function test_table_explode>; <Package backend>; <Module test_service_backend.py>; <Function test_tiny_driver_has_tiny_memory>; <Function test_big_driver_has_big_memory>; <Function test_tiny_worker_has_tiny_memory>; <Function test_big_worker_has_big_memory>; <Function test_regions>; <Package expr>; <Module test_expr.py>; <UnitTestCase Tests>; <TestCaseFunction test_aggregators>; <TestCaseFunction test_densify_table>; <TestCaseFunction test_scan>; <Package genetics>; <Module test_reference_genome.py>; <Function test_reference_genome>; <Function test_reference_genome_sequence>; <Function test_reference_genome_liftover>; <Function test_read_custom_reference_genome>; <Package matrixtable>; <Module test_grouped_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunction test_joins_work_correctly>; <Module test_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunction test_collect_cols_by_key>; <TestCaseFunction test_naive_coalesce>; <TestCaseFunction test_range_count>; <Package methods>; <Module test_family_methods.py>; <UnitTestCase Tests>; <TestCaseFunction test_trio_matrix_1>; <Module test_impex.py>; <UnitTestCase VCFTests>; <TestCaseFunction test_glob>; <TestCaseFunction test_import_gvcfs>; <Module test_qc.py>; <UnitTestCase Tests>; <TestCaseFunction test_sample_qc>; <TestCaseFunction test_variant_qc>; <Module test_skat.py>; <Function test_logistic_skat_phenotypes_are_binary>; <Function tes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851:1568,Test,Tests,1568,https://hail.is,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851,4,['Test'],"['TestCaseFunction', 'Tests']"
Testability,"==============================================; platform darwin -- Python 3.6.8, pytest-3.8.0, py-1.7.0, pluggy-0.8.1; rootdir: /Users/alex/projects/hail/hail/python, inifile:; plugins: xdist-1.22.2, metadata-1.8.0, html-1.19.0, forked-1.0.2; collected 591 items . test/hail/test_context.py E. ============================================================================================== ERRORS ==============================================================================================; _______________________________________________________________________ ERROR at setup of Tests.test_init_hail_context_twice _______________________________________________________________________. def startTestHailContext():; global _initialized; if not _initialized:; url = os.environ.get('HAIL_TEST_SERVICE_BACKEND_URL'); if url:; hl.init(master='local[2]', min_block_size=0, quiet=True, _backend=hl.backend.ServiceBackend(url)); else:; > hl.init(master='local[2]', min_block_size=0, quiet=True). test/hail/helpers.py:18: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ; hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:264: in init; _optimizer_iterations,_backend); hail/typecheck/check.py:561: in wrapper; return __original_func(*args_, **kwargs_); hail/context.py:99: in __init__; min_block_size, branching_factor, tmp_dir, optimizer_iterations); /miniconda3/envs/hail/lib/python3.6/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . answer = 'xspy4j.Py4JException: Method apply([null, class java.lang.String, class scala.Some, class java.lang",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928:1631,test,test,1631,https://hail.is,https://github.com/hail-is/hail/pull/5878#issuecomment-484651928,1,['test'],['test']
Testability,"=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ERROR: dlopen(""/data05/hadoop/yarn/local/usercache/farrell/appcache/application_1542127286896_0174/container_e2435_1542127286896_0174_01_000011/tmp/libhail6914320507038502759.so""): /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.18' not found (required by /data05/hadoop/yarn/local/user",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:2092,log,log,2092,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['log'],['log']
Testability,"> #5482 :P. Haha man we're on the same wavelength today: https://github.com/akotlar/hail/commit/c0c3751d9de9008b1ec4d0281afa77c5dbd7d186; (g++ is a neater solution, I was going to test that next)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468434781:180,test,test,180,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434781,1,['test'],['test']
Testability,"> #9309 up to solve these issues. If I can get all the tests to pass, I'll feel pretty good about stacking this on top of that. Yes, agreed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676650821:55,test,tests,55,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676650821,1,['test'],['tests']
Testability,"> (If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). looks great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248:110,log,logs,110,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248,1,['log'],['logs']
Testability,> . Does this not work? `((dev)|(pr)|(test))-.*`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12232#issuecomment-1260030697:38,test,test,38,https://hail.is,https://github.com/hail-is/hail/pull/12232#issuecomment-1260030697,1,['test'],['test']
Testability,"> 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis. Can you point me to the benchmarks? The only head-to-head one I found was this:. https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto. where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082:120,benchmark,benchmarks,120,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461195082,1,['benchmark'],['benchmarks']
Testability,"> > 1. aiohttp is an option, but appears to be generally considered slow on a per-response basis; > ; > Can you point me to the benchmarks? The only head-to-head one I found was this:; > ; > https://github.com/samuelcolvin/aiohttp-vs-sanic-vs-japronto; > ; > where sanic is faster for smaller examples but aiohttp performs MUCH better (3x latency, 10x throughput) when interacting with a db. Sanic was chosen because it's a near drop-in for Flask, and is the most popular afaik library built around asyncio. It has 2x as many stars as aiohttp, slightly more forks. The original motivation for considering aiohttp: ; https://magic.io/blog/uvloop-blazing-fast-python-networking. In short, one of the creators of asyncio discusses uvloop performance relative to other libraries. They key is:. ""However, the performance bottleneck in aiohttp turned out to be its HTTP parser, which is so slow, that it matters very little how fast the underlying I/O library is."". <img width=""1001"" alt=""screen shot 2019-02-06 at 7 29 00 pm"" src=""https://user-images.githubusercontent.com/5543229/52382977-77a62d00-2a45-11e9-8c04-b8142586eb5c.png"">. <img width=""936"" alt=""screen shot 2019-02-06 at 7 29 19 pm"" src=""https://user-images.githubusercontent.com/5543229/52382985-812f9500-2a45-11e9-9155-97c00ef9784b.png"">. As an aside I've spent some time reading about this over the last ~month, and besides relatively consistent messaging about the messiness of Python's ecosystem, performance and user experience are deeply important to me, so when I read things like:. ""I dont think performance matter. I think asgi does not matter in 2018 in general. Usability and complexity matters. Python is not very good choice for high performance system in any case...For me high performance python is a fantasy, but i dont do aiohttp/python anymore. In the end it is up to @asvetlov"". from one of the creators of aiohttp, I'm not encouraged about the long-term health of the project. https://github.com/aio-libs/aiohttp/issues/29",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:128,benchmark,benchmarks,128,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,2,['benchmark'],['benchmarks']
Testability,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:316,test,tested,316,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311,4,['test'],"['test', 'tested', 'testing']"
Testability,"> @akotlar : @danking and I discussed this in person and we can add the test once the service account infrastructure is in place in Batch. Until then, this approach will not work. got it, so ready to approve?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5762#issuecomment-479632064:72,test,test,72,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479632064,1,['test'],['test']
Testability,"> @akotlar Ok, so I finally managed to remove the internal requests. The asyncio learning curve was higher than I expected.; > ; > The final product is a lot tighter than I expected. I eliminated all the run_forever and run_once stuff, all the threading, and I was able to move the log back into server.py.; > ; > It involves three new things:; > ; > * [kill the whole loop if anything goes wrong](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R899), which works wonderfully with k8s' automatic pod restarting; > * [use a concurrent thread pool](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R904) for any legacy blocking operations; > * [a blocking-to-async convertor](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R823) and a [blocking iterator to async iterator](https://github.com/hail-is/hail/pull/5844/files#diff-14c16d042ba8b8608b60b3fcd1029869R828) both of which stick blocking operations on a separate thread pool.; > ; > Legacy blocking operations might end up queueing behind one another in the ""blocking pool"", but the rest of the application continues without interruption on the main event loop.; > ; > I took the chance to reorder the k8s refresh and the k8s watch functions to be closer together, but that made the diff worse :/.; > ; > Probably demands another review on Monday.; > ; > cc: @cseed, possibly some asyncio engineering best practices in this. Awesome work. I think this will make batch much more performant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679:282,log,log,282,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482761679,2,['log'],['log']
Testability,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:370,test,test,370,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526,1,['test'],['test']
Testability,"> @tpoterba the changes you proposed cause tests to fail (OrderingSuite.testBinarySearchOnDict) . Also, I'm confused by the proposal, because arrayRep has no fundamental type, and while we could add one, the fundamental types in PSet and PDict were constructed differently in master (PSet's elementType is not necessarily a PStruct in master). > edit: Ah, I think I misunderstood, you wanted override val fundamentalType: PArray = arrayRep.fundamentalType I think. I could place a lazy fundamentalType on arrayRep that used whatever elementType was defined, and completely remove fundamentalType from PSet and PDict. Actually, you can set the fundamentalType on PCanonicalArrayBackedCollection. That seems best.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7687#issuecomment-565585194:43,test,tests,43,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-565585194,2,['test'],"['testBinarySearchOnDict', 'tests']"
Testability,> Activating the service account in the test locally screwed up my local permissions. Need to fix before merging. sounds good,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5762#issuecomment-479579007:40,test,test,40,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479579007,1,['test'],['test']
Testability,"> Also all the tests need to be fixed. Yep, working on it. Serialization failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599:15,test,tests,15,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-491423599,1,['test'],['tests']
Testability,"> Are there tests for the existence of these files? (I can see how that would be counterproductive if tests fail because the files move, but on the other hand, it's information you may want to know - maybe an ""optional"" test?). Not currently, that may be something worth looking into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9693#issuecomment-724956223:12,test,tests,12,https://hail.is,https://github.com/hail-is/hail/pull/9693#issuecomment-724956223,3,['test'],"['test', 'tests']"
Testability,> But if we move later to hosting logs in user-provided buckets instead of our own bucket there's no reason why they shouldn't be able to write large logs if they want to. Good point. I'll think about this more once I've done a bit more research.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570847814:34,log,logs,34,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570847814,2,['log'],['logs']
Testability,"> Calls to Region's constructors should only be in tests or by the RVDContext. Also, I think Region() is still called in the WriteableRegionValue constructor.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3301#issuecomment-379274927:51,test,tests,51,https://hail.is,https://github.com/hail-is/hail/pull/3301#issuecomment-379274927,1,['test'],['tests']
Testability,"> Can you lock down this behaviour with a test?. As I said in the description, I tried hard to write a test, but I couldn't manage to find a way to exercise this with a targeted test. And this bug is currently blocking a user, so I want to get it released asap. I can try again, but I suspect it would end up being a day or two of extra work, and would likely still end up brittle and unsatisfactory. I'd rather spend my effort getting back into the line of work that would eventually make it much easier to target specific compiler code paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541:42,test,test,42,https://hail.is,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541,3,['test'],['test']
Testability,> Can you make the same change to benchmark/Makefile?. Sure. I plan to remove wheel generation from benchmark/Makefile as we don't use it anymore.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13957#issuecomment-1792680812:34,benchmark,benchmark,34,https://hail.is,https://github.com/hail-is/hail/pull/13957#issuecomment-1792680812,2,['benchmark'],['benchmark']
Testability,"> Can you share the long term vision for this page? Will notebook become a generic `app.hail.is` with many tabs? Changing the index page to a page with no content seems strictly less helpful to the user. They must make an extra click after they type `notebook.hail.is` into their URL bar to get to a login form. If I was the user, I'd want to get dumped right into the log-in form (but maybe I'm a weird user?). Notebook will host the batch stuff. I have no comments about vision longer term than that, it's undefined as far as I know (meaning, I don't know). We can get rid of the home page as well. I ran the home page idea by @cseed but maybe I misunderstood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5452#issuecomment-468711409:300,log,login,300,https://hail.is,https://github.com/hail-is/hail/pull/5452#issuecomment-468711409,2,['log'],"['log-in', 'login']"
Testability,> Cotton has a vcf he generated for testing BGZipCodec: see BGZipCodecSuite. This has a lot of the edge cases. I'll tabix it and see what happens.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9304#issuecomment-676510464:36,test,testing,36,https://hail.is,https://github.com/hail-is/hail/pull/9304#issuecomment-676510464,1,['test'],['testing']
Testability,"> Crash on trying to create a notebook. I looked into this. The notebook launch failed because your account in my namespace isn't complete, it can only login, but doesn't have the other secrets necessary to launch a notebook (e.g. gsa-key).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532273328:152,log,login,152,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532273328,1,['log'],['login']
Testability,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:356,log,logged,356,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223,2,['log'],"['logged', 'login']"
Testability,"> Did you ssh to CI, start a python session, start a batch client, and delete it in that manner?. Yes. And the CI status (https://ci.hail.is/watched_branches/0/pr/6561) continued to error out with 500s (caused by batch lookup 404s in the logs) even after a heal loop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134:238,log,logs,238,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134,1,['log'],['logs']
Testability,"> Even this may be expensive. This would all be staged. The generated code would just directly allocate into the provided region, no overhead. But I agree we should benchmark first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102:165,benchmark,benchmark,165,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102,1,['benchmark'],['benchmark']
Testability,> Example output in logs:; > ; > ```; > INFO: timing is.hail.backend.BackendHttpHandler#handle x$3 total 1m42.0s self 20.824ms children 1m42.0s %children 99.98%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.expr.ir.IRParser.parse_value_ir total 179.902ms self 157.170ms children 22.733ms %children 12.64%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.expr.ir.IRParser.parse_value_ir/is.hail.expr.ir.TypeCheck.apply total 22.733ms self 22.733ms children 0.000ms %children 0.00%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute total 1m41.8s self 3.404ms children 1m41.8s %children 100.00%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.TypeCheck.apply total 0.211ms self 0.211ms children 0.000ms %children 0.00%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.analyses.SemanticHash.apply total 31.924ms self 6.413ms children 25.511ms %children 79.91%; > is.hail.backend.BackendHttpHandler#handle x$3/is.hail.backend.spark.SparkBackend#execute/is.hail.expr.ir.analyses.SemanticHash.apply/is.hail.expr.ir.NormalizeNames.apply total 25.511ms self 25.511ms children 0.000ms %children 0.00%; > ```. I'm going to change from `sourcecode.Enclosing` to something a little more compact in a subsequent change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14731#issuecomment-2417815038:20,log,logs,20,https://hail.is,https://github.com/hail-is/hail/pull/14731#issuecomment-2417815038,1,['log'],['logs']
Testability,"> FYI @tpoterba rather than just testing them independently, it might be worthwhile to have write/read interop tests between the various backends. Spark to local is partially tested by the pre-existing (matrix)tables tests, but not the other way. Agreed, this is a good idea.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595:33,test,testing,33,https://hail.is,https://github.com/hail-is/hail/pull/9596#issuecomment-707898595,4,['test'],"['tested', 'testing', 'tests']"
Testability,> Failing tests for real. it makes me sad we have to clarify :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8156#issuecomment-594140056:10,test,tests,10,https://hail.is,https://github.com/hail-is/hail/pull/8156#issuecomment-594140056,1,['test'],['tests']
Testability,"> First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. Nope. But I didn't run many tests with setup/cleanup containers actually doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176:230,test,tests,230,https://hail.is,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176,1,['test'],['tests']
Testability,"> Having the Auth service ping the Batch API with it to verify the token is valid. I believe the way our CRSF is implemented, we don't actually ever ""validate"" the tokens, we only check that the token in the formdata matches the token in the cookie. > Or perhaps we could just make this UI a single page application instead of a bunch of pages on different subdomains that resemble one. . This would be wonderful! Sort of similar-but-better to my thought of hosting the ""top menu bar"" as a separate iframe that always comes from auth. For the same reason (in particular, the apparently lack of regular usage of the logout button), that kind of change is probably larger than the scope of getting this bug fixed... but would cool to look into some day!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369:615,log,logout,615,https://hail.is,https://github.com/hail-is/hail/pull/14639#issuecomment-2269645369,1,['log'],['logout']
Testability,"> Hmm, flake is failing on this; > ; > ```; > + flake8 batch; > batch/server/__init__.py:1:1: F401 '.server.serve' imported but unused; > batch/server/__init__.py:1:1: F401 '.server.run_once' imported but unused; > ```; > But that's an incorrect bug. I wonder if the CI is loading the wrong python version. I'll check. Strange. .serve is unmodified from master; run_once is only used in a test, could that be an issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130:389,test,test,389,https://hail.is,https://github.com/hail-is/hail/pull/5065#issuecomment-451274130,1,['test'],['test']
Testability,> Hmm. Is the TODO suggesting that we assert the state field is READY in the response?. I handled the other states and removed the TODO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1971996799:38,assert,assert,38,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1971996799,1,['assert'],['assert']
Testability,"> How did you verify only one is created?. I printed in the ReferenceGenome constructor. I guess it could have been serialized, but the issues I was seeing were all on the master, and I don't see how it could have been. I haven't succeeded in making an isolated test case that motivates this change. @chrisvittal has a complicated joint calling pipeline that fails. However, we define equality on ReferenceGenome in terms of value quality, and I think unify should use the same notion of equality so this look like a bug on the face of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872:262,test,test,262,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872,1,['test'],['test']
Testability,"> I believe you're referencing Chrome Bug 675308 which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. My point was that there are browser inconsistencies. > The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. . I also can't explain it, and haven't found a Safari bug that would suggest why this is the case, and yet it is clear from that screenshot that there is a difference. > It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. My current mental model is that we're just hitting a resolution issue. Internally threejs I believe sets the webgl buffer to pixelRatio * width/height. With resolution 1, resolution seems too low, and this causes fuzzy lines. I don't know why it doesn't work better. To test this hypothesis, I've tried setting pixelRatio to 1 manually on a hidpi display, and it gave a similar fuzzy/thick result, which of course doesn't make any sense if linewidth actually did what it seems it should, and so I agree that not relying on linewidth would be nicer, but would also be additional work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792:899,test,test,899,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646227792,2,['test'],['test']
Testability,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1481,log,logging,1481,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367,2,['log'],"['log', 'logging']"
Testability,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:397,test,test,397,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757,1,['test'],['test']
Testability,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:202,log,logging,202,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077,7,"['log', 'test']","['log', 'logging', 'test']"
Testability,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:312,test,tests,312,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944,5,['test'],"['test', 'tests']"
Testability,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:465,test,testing,465,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332,6,['test'],"['test', 'tested', 'testing', 'tests']"
Testability,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:34,test,testPython,34,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503,7,"['Test', 'test']","['Tests', 'test', 'testPython', 'tested', 'testing', 'tests']"
Testability,"> I kinda prefer a fresh file with freshly written tests? I feel like it's a bit hard to get a total view of what is and is not tests when we're using annotations. I agree a single file feels nice, but I'm a little hesitant to copy paste tests. Unless you think I should move where these tests are? That also feels weird. Does the following pytest invocation make you feel better about markers?. ```; (hail) dgoldste@wmce3-cb7 hail % pytest --collect-only -m qobtest hail/python/test; ============================================================================== test session starts ===============================================================================; platform darwin -- Python 3.9.17, pytest-7.4.0, pluggy-1.3.0; rootdir: /Users/dgoldste/hail/hail/python/test; configfile: pytest.ini; plugins: anyio-4.0.0, xdist-2.5.0, instafail-0.5.0, timeout-2.1.0, devtools-0.12.2, asyncio-0.21.1, timestamper-0.0.9, metadata-3.0.0, html-1.22.1, forked-1.6.0; asyncio: mode=auto; collected 8218 items / 8133 deselected / 85 selected. <Package hail>; <Module test_context.py>; <Function test_init_hail_context_twice>; <Function test_top_level_functions_are_do_not_error>; <Function test_tmpdir_runs>; <Module test_randomness.py>; <Function test_table_explode>; <Package backend>; <Module test_service_backend.py>; <Function test_tiny_driver_has_tiny_memory>; <Function test_big_driver_has_big_memory>; <Function test_tiny_worker_has_tiny_memory>; <Function test_big_worker_has_big_memory>; <Function test_regions>; <Package expr>; <Module test_expr.py>; <UnitTestCase Tests>; <TestCaseFunction test_aggregators>; <TestCaseFunction test_densify_table>; <TestCaseFunction test_scan>; <Package genetics>; <Module test_reference_genome.py>; <Function test_reference_genome>; <Function test_reference_genome_sequence>; <Function test_reference_genome_liftover>; <Function test_read_custom_reference_genome>; <Package matrixtable>; <Module test_grouped_matrix_table.py>; <UnitTestCase Tests>; <TestCaseFunct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851:51,test,tests,51,https://hail.is,https://github.com/hail-is/hail/pull/13620#issuecomment-1720268851,7,['test'],"['test', 'tests']"
Testability,"> I made changes to TBaseStruct, etc. so that types is now a val. I timed it locally for profile225 with import_vcf().count() and compared it with master. Ah, yeah, I totally misread that and thought it was timing read/count. We do need to test that -- this change involves making `types` a virtual function call rather than an array reference. I assume the JIT will make it fast, but we need to be sure since that's used in critical places.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2922#issuecomment-368346661:240,test,test,240,https://hail.is,https://github.com/hail-is/hail/pull/2922#issuecomment-368346661,1,['test'],['test']
Testability,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:302,test,tested,302,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541,4,['test'],"['test', 'tested', 'testing']"
Testability,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:4,test,tested,4,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890,2,['test'],"['test', 'tested']"
Testability,"> I tested this with a pile of hacks to deploy this into an anonymous namespace in vdc. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of vdc/ and gateway/ to be more modular. Oh, I read that as you weren't ready to merge this. It sounds like you aren't ready to PR the testing stuff?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995:4,test,tested,4,https://hail.is,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995,2,['test'],"['tested', 'testing']"
Testability,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:154,assert,assert,154,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940,8,['assert'],"['assert', 'assertion']"
Testability,"> I tried making a plot with only shape, not color. It should behave the same as color (in the discrete case), only using shapes instead of colors, but right now the legend uses ""trace 0"" etc. instead of the data values. I was able to get the category names showing properly in the legend with just shapes by adding a `shape_legend -> name` mapping to `GeomPoint.trace_args`, but this caused the shape names to override the color names when using both; I added logic to `Geom._add_aesthetics_to_trace_args` to concatenate the names with one legend entry per pair of color and shape, which causes the shapes to stop working and a separate color to be assigned to each legend entry. ```python; import hail as hl; from hail.ggplot import ggplot, aes, geom_point; ht = hl.utils.range_table(10); ht = ht.annotate(squared=ht.idx ** 2); ht = ht.annotate(even=hl.if_else(ht.idx % 2 == 0, ""yes"", ""no"")); ht = ht.annotate(threeven=hl.if_else(ht.idx % 3 == 0, ""good"", ""bad"")); fig = (; ggplot(ht); + aes(x=ht.idx, y=ht.squared, color=ht.even, shape=ht.threeven); + geom_point(); ); fig.show(); ```. ![newplot(1)](https://user-images.githubusercontent.com/84595986/191850065-fa9cf15a-44b5-48cc-ad95-47af67d76ec1.png). I'm having trouble tracking down why this is happening, so I'll ask for some help with that tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12207#issuecomment-1255544008:461,log,logic,461,https://hail.is,https://github.com/hail-is/hail/pull/12207#issuecomment-1255544008,1,['log'],['logic']
Testability,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:172,test,testing,172,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446,2,['test'],"['test', 'testing']"
Testability,"> I'm confused even more. I thought test_instance was a database within db-gh0um the same as all of the other databases within db-gh0um. Is this a separate cloud sql instance?. Correct. No. test_instance is a database that conceptually an instance, in that it is used to satisfy ""create database"" requests in test CI, but just gives out test_instance as the ""created"" database without any isolation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562890518:309,test,test,309,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562890518,1,['test'],['test']
Testability,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:758,test,tested,758,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,3,"['log', 'test']","['login', 'tested']"
Testability,"> I'm going to start testing, but I think the only thing that wasn't clear to me was how to resolve these sorts of comments. Do I try and fix them now or in a separate PR with an issue to make sure it gets noted? [#14170 (comment)](https://github.com/hail-is/hail/pull/14170#discussion_r1473442106). Since this is merely propagating existing bad behavior, let's change it separately so as to keep the conceptual overhead of this change as small as possible. Please make an issue and paste the link into each comment so that we can later track how we resolved each comment. . > Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclinatio",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021:21,test,testing,21,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021,4,['test'],"['testing', 'tests']"
Testability,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:315,log,logo-cropped,315,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546,1,['log'],['logo-cropped']
Testability,"> I'm not sure what you mean by this. Also, were we just not type checking MatrixIR, etc. before? Was there a reason for this? I thought @patrick-schultz was working on something to do with type checking. Currently, typechecking assertions happen in the constructors of relational IRs. This will consolidate typechecking nicely. The expression problem strikes again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5443#issuecomment-467677829:229,assert,assertions,229,https://hail.is,https://github.com/hail-is/hail/pull/5443#issuecomment-467677829,1,['assert'],['assertions']
Testability,"> I'm ready to merge this. Woohoo!. I'm in a state of deep depression about the replicability of benchmarks right now. They seem (subjectively) even more noisy from run to run than things were six months ago. This could be 1-2% slower overall, or identical to main -- I can't tell, since benchmarks of main vary by nearly 5% from day to day. Either way, I'm good to merge this even with the worst-case 2% degradation. Progress toward stack struct is worth it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9868#issuecomment-763075785:97,benchmark,benchmarks,97,https://hail.is,https://github.com/hail-is/hail/pull/9868#issuecomment-763075785,2,['benchmark'],['benchmarks']
Testability,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:373,test,test,373,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091,1,['test'],['test']
Testability,"> If this fix makes UK Biobank import too slow. Looks good, that's my only concern. With the persist, I think it should be OK, but can we benchmark to verify? A before/after `bgen_import().count()` on a small but non-trivial example should suffice. We'll be running again soon rather than later, esp. since we just got approved for all the phenotypes (!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239#issuecomment-334154778:138,benchmark,benchmark,138,https://hail.is,https://github.com/hail-is/hail/pull/2239#issuecomment-334154778,1,['benchmark'],['benchmark']
Testability,"> If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to QoBAppender). We might want to do this if we get rid of GSA keys. We can't have any more jars that presume the existence of some key file. It would also be a good time to fully delete the `memory` service, even though old jars should be able to tolerate that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692:162,log,log,162,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692,1,['log'],['log']
Testability,"> In particular, the matrixtable available in doc examples as `ds` lives at `hail/hail/python/hail/docs/data/example.mt`. Thanks, this is exactly what I was looking for. I am having trouble testing my new example locally though. When I run `make -C hail doctest-query`, the tests fail with a checksum error. I tried running `make -C hail clean` and retrying, but I still get the same error. ```; E hail.utils.java.FatalError: ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E; E Java stack trace:; E org.apache.hadoop.fs.ChecksumException: Checksum error: file:/Users/willtyler/Desktop/hail/hail/python/hail/docs/data/example.8bits.bgen.idx2/metadata.json.gz at 0 exp: 982431825 got: -2031629660; E 	at org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347); E 	at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303); E 	at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252); E 	at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at is.hail.io.fs.HadoopFS$$anon$2.read(HadoopFS.scala:58); E 	at java.base/java.io.DataInputStream.read(DataInputStream.java:149); E 	at org.apache.commons.compress.utils.CountingInputStream.read(CountingInputStream.java:56); E 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252); E 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:185); E 	at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:168); E 	at is.hail.io.fs.GZipCompressionCodec$.makeInputStream(FS.scala:125); E 	at is.hail.io.fs.FS.open(FS.scala:563); E 	at is.hail.io.fs.FS.open$(FS.scala:560); E 	",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001:190,test,testing,190,https://hail.is,https://github.com/hail-is/hail/pull/14255#issuecomment-1933346001,2,['test'],"['testing', 'tests']"
Testability,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:15,test,test,15,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285,2,"['log', 'test']","['log', 'test']"
Testability,"> Interesting. When I was helping Sophie earlier, I was able to see them for workers in her namespace??? https://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%0AlogName:%22syslog%22%0Alabels.%22compute.googleapis.com%2Fresource_name%22:%22batch-worker-parsa-job-private-39d9d%22;summaryFields=:false:32:beginning;cursorTimestamp=2023-08-27T13:45:49Z;duration=P2D?project=hail-vdc. Are those logs from after the worker container started?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13509#issuecomment-1696127157:138,log,logs,138,https://hail.is,https://github.com/hail-is/hail/pull/13509#issuecomment-1696127157,2,['log'],['logs']
Testability,"> It looks like this deletes notebook. Please do NOT delete notebook. Right now we have no tested alternative and @tpoterba and I need to use it for a workshop in two weeks. I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. What do you mean by tested? I thought we established that the version of notebook I made worked... it worked for the Feb 5th demo, it worked for Jackie's demo?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464203521:91,test,tested,91,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464203521,2,['test'],['tested']
Testability,"> It's a little concerning that this is necessary. Are there typecheck cases that don't make assertions about some of their children?. This check would have to go in a LOT of places otherwise. Reading down from the top: CastRename, NA, IsNA, Coalesce, AggLet, TailLoop, MakeArray, ...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9974#issuecomment-772079087:93,assert,assertions,93,https://hail.is,https://github.com/hail-is/hail/pull/9974#issuecomment-772079087,1,['assert'],['assertions']
Testability,> It's calling into the db once per job? That's bad. Why does it even need to do that?. That's an easy fix. It's just asserting the job wasn't seen before. Remnants of debugging probably. We can also have a callback that starts all of the jobs not in the create_batch call.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6454#issuecomment-505178184:118,assert,asserting,118,https://hail.is,https://github.com/hail-is/hail/pull/6454#issuecomment-505178184,1,['assert'],['asserting']
Testability,"> It's definitely a bit unclear to met that we got everything and I'm not convinced the test checks everything. I don't think I'd feel much better unless we were randomly restarting the context mid tests. We actually do that -- this test_context runs in the same python process as the other tests. I also think we don't really guarantee you can stop and restart, but this at least makes it work with our tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941:88,test,test,88,https://hail.is,https://github.com/hail-is/hail/pull/5872#issuecomment-484602941,4,['test'],"['test', 'tests']"
Testability,"> Just so I understand correctly (and sorry if this is obvious), the current job logs interface is still the same. But if you want a container's logs, then you'll get bytes which the user will have to decode themselves. How does that affect the file download button in the UI and the hailctl batch logs functionality you have? Will you see text or a random byte string?. Good question. For the download button, the file you download is still a normal text file and I've confirmed that I can download and view a log file as I would expect. For the `hailctl batch logs` functionality, I added logic to the CLI in this PR where I download the bytes of the log and if I can decode it as UTF-8 I do, so it prints exactly as before, and if I can't I just print the bytes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373:81,log,logs,81,https://hail.is,https://github.com/hail-is/hail/pull/12666#issuecomment-1426373373,7,['log'],"['log', 'logic', 'logs']"
Testability,"> Just starting to explore this PR. I compared this in-progress CI run to 7126798 (from #12737). The time to service backend starting is ~7 minutes. In the other PR, its ~8 minutes. I suppose that's because this PR isn't hitting any caches, right?; > ; > Hmm, it also seems like the critical path to the service backend test is through `build_hail_jar_and_wheel_only`. I wonder if we double the cores, would the time halve? On my laptop a fresh build is like 3m.; > . Ya I really only focus here on docker image steps, not on the overall critical path. But I like the idea of adding more cores I'll try that. The docker images are hitting cache though since every PR caches from its own previous runs in addition to main. The reason some images still take ~1 minute is because they introduce a new layer (normally the hail_version has changed since the SHA has changed) and they spend most of the time localizing the layer that has the dependencies installed (why I want lazy pulling). A retry of this PR should build the images very quickly, though I haven't tried that recently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223:320,test,test,320,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223,1,['test'],['test']
Testability,> Looks like a couple of the reenabled tests are failing now... these seem to be just flaky tests :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14721#issuecomment-2407784886:39,test,tests,39,https://hail.is,https://github.com/hail-is/hail/pull/14721#issuecomment-2407784886,2,['test'],['tests']
Testability,"> Looks like we need to set the severity correctly in the worker logs. I'm also seeing a lot of this; WARNING: Published ports are discarded when using host network mode; Also looks like we incorrectly log a ContainerTimeoutError as an error log even though that's a user error: https://cloudlogging.app.goo.gl/TUGWNxnFiBiEdsDo9. For what it's worth, this was showing up as an info log because this is a docker log message not from our code, so it's not going through our logging filters. The reason this showed up in the Google Logging query was because the query included this line; ```; severity=ERROR OR WARNING; ```; which means ""logs whose severity is ERROR or whose log entry contains ""WARNING"""", it is *not* equivalent to `severity=ERROR OR severity=WARNING` which does not show that log entry. Either way, #14252 gets rid of that log message entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702:65,log,logs,65,https://hail.is,https://github.com/hail-is/hail/issues/14240#issuecomment-1930672702,11,"['Log', 'log']","['Logging', 'log', 'logging', 'logs']"
Testability,"> Maybe the doubling of the block size is having a negative effect? Anyway, let's benchmark on something more realistic than the Hail tests and assess. More likely it's replacing the LZ4SizeBased buffer spec with zstd and not using a size based zstd buffer spec. The 0-byte compress time for calling lz4 was really high, so we branch on whether the number of bytes to compress is over a threshold before calling compression methods. We should do the same for zstd.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1537470938:82,benchmark,benchmark,82,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1537470938,2,"['benchmark', 'test']","['benchmark', 'tests']"
Testability,"> Maybe we should also use this opportunity to add hailctl to scorecard / remove cloudtools?. yeah, you could replace cloudtools with hailctl dataproc as well. Especially since you need to bump to fix a random test ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6449#issuecomment-505070267:210,test,test,210,https://hail.is,https://github.com/hail-is/hail/pull/6449#issuecomment-505070267,1,['test'],['test']
