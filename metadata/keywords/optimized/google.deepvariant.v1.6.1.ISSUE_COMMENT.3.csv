quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn.; W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn.; I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized.; I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op.; I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op.; I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA...; I0911 02:28:54.011811 139937686464256 sav",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:8977,optimiz,optimizations,8977,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['optimiz'],['optimizations']
Performance,"790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:73113,cache,cache,73113,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"8, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112340,cache,cache,112340,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-28:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449:2994,queue,queues,2994,,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449,1,['queue'],['queues']
Performance,"85321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1; > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB; > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter.; > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions; > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5; > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database...; > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete!; > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up.; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads; > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles; > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:12539,Load,Loaded,12539,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['Load'],['Loaded']
Performance,"8; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa; bogomips : 4000.35; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:6028,cache,cache,6028,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['cache'],['cache']
Performance,"92 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; > random_seed=random_seed)); > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; > paul@gubuntu:~/deepvariant/bazel-bin$; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/52#issuecomment-371293506>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWD1QdO0gBW0VvSmC7tBatJdKNAzBhQlks5tcFLhgaJpZM4SejU_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:3231,cache,cache,3231,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,4,['cache'],['cache']
Performance,"9a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72727,cache,cache,72727,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,": Already exists ; 6e3dea686609: Already exists ; dc216b407a52: Already exists ; c6710cf0efec: Already exists ; 6a519085af15: Already exists ; fd35c1634889: Already exists ; 0e4b2b2ad2db: Already exists ; 87e7c72faeb5: Already exists ; 690adc142e08: Already exists ; abfd217d5088: Already exists ; 30b033b0505f: Already exists ; 853ad599972a: Already exists ; f20c79af8049: Already exists ; 26703b5b7abd: Already exists ; f3b33765da79: Already exists ; c382f9fc227e: Already exists ; 3a233bad0db5: Already exists ; f6ac10e59ad4: Already exists ; 9e1d2d199a37: Already exists ; b50b4a1202e8: Already exists ; 1286a89300c9: Already exists ; 11f1b6d48e7b: Already exists ; 09154ad67b50: Already exists ; aad195d6c4df: Already exists ; f8376ea6a177: Already exists ; b44e5a321822: Already exists ; e81a72561181: Already exists ; e96d0f626428: Already exists ; bea852b4c2f5: Already exists ; 13d620954600: Already exists ; 123b4e4b7a6e: Already exists ; Digest: sha256:6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e; Status: Downloaded newer image for google/deepvariant:1.5.0; 2023-08-22 01:54:42.917386: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0822 01:56:08.344358 140205078546240 run_deepvariant.py:364] Re-using the directory for intermediate results in /tmp/tmp26397nkc. ***** Intermediate results will be written to /tmp/tmp26397nkc in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/bs_filled.fasta"" --reads ""/input/aln_sort.bam"" --examples ""/tmp/tmp26397nkc/make_examples.tfrecord@16.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmp26397nkc/gvcf.tfrecord@16.gz"" --task {}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/700#issuecomment-1687461144:2727,optimiz,optimized,2727,,https://github.com/google/deepvariant/issues/700#issuecomment-1687461144,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:2019,perform,performed,2019,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040,1,['perform'],['performed']
Performance,":25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5378,optimiz,optimization,5378,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['optimiz'],['optimization']
Performance,":39.923217 140366075229952 genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0911 02:28:40.247040 140366075229952 make_examples.py:587] 6 candidates (6 examples) [0.33s elapsed]; I0911 02:28:41.745102 140366075229952 make_examples.py:587] Found 78 candidate variants; I0911 02:28:41.745359 140366075229952 make_examples.py:587] Created 86 examples. real 0m5.273s; user 0m5.898s; sys 0m3.792s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:5663,optimiz,optimized,5663,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2827,latency,latency,2827,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['latency'],['latency']
Performance,"; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Resource temporarily unavailable: '/usr/bin/python'; > ; > real 19m19.271s; > user 1084m5.580s; > sys 17m12.750s; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; > app.run(main); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; > _run_main(main, args); > File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:2378,load,loader,2378,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,2,['load'],['loader']
Performance,"; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:85148,cache,cache,85148,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"<module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103329,cache,cache,103329,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"=========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,; Now I got here, please check these two things on your side:. 1. Can you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:3434,load,load,3434,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,2,['load'],['load']
Performance,"=============; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71606,cache,cache,71606,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"====================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107002,cache,cache,107002,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"=======================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117909,cache,cached,117909,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"==============================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78127,cache,cache,78127,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"=============================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100942,cache,cache,100942,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10498,cache,cache,10498,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['cache'],['cache']
Performance,"> @ZuyaoLiu ,; > ; > Can you please give some insight on how you merged the VCFs for the analysis? Would be great if you have something where we can look how the post-processing was done to generate the stats. Hi @kishwarshafin ,. Sure.; I used GLnexus and default DeepvariantWGS settings to merge GVCFs from Deepvariant. And no further processing was performed. Best,; Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/726#issuecomment-1818540715:352,perform,performed,352,,https://github.com/google/deepvariant/issues/726#issuecomment-1818540715,1,['perform'],['performed']
Performance,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated.; > ; > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/280#issuecomment-598767292:25,perform,performs,25,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292,1,['perform'],['performs']
Performance,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735450709:271,perform,performance,271,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709,1,['perform'],['performance']
Performance,"> Do you recommend read trimming before alignment using tools such as fastp?. No - we don't perform read trimming and in fact, this can hurt model performance (because we trained on untrimmed data). We observed this when running our models on processed with Opossum (which performs trimming):. <img width=""1167"" alt=""image"" src=""https://github.com/google/deepvariant/assets/1536935/4e0cd335-0818-4cec-ae0c-ef294e98da39"">. > I can see that this is the repository for the human reference genome, which DeepVariant recommends. You can use a reference with or without alt contigs. With ALT contigs we have previously observed a slight increase in error rates with other models. I have not investigated whether they have an impact with our RNA-seq model. __Note:__ I have updated this after speaking with my teammates regarding this question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/791#issuecomment-1997884080:92,perform,perform,92,,https://github.com/google/deepvariant/issues/791#issuecomment-1997884080,3,['perform'],"['perform', 'performance', 'performs']"
Performance,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types − that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes − I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:365,optimiz,optimizes,365,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762,2,"['load', 'optimiz']","['load', 'optimizes']"
Performance,"> Hi @gambalab,; > ; > Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts.; > ; > In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:; > . thank you! this is a great solution for me.; you should add in the README page as an alternative rootless installation :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307:244,perform,performs,244,,https://github.com/google/deepvariant/issues/669#issuecomment-1604068307,1,['perform'],['performs']
Performance,"> Hi @husamia; > ; > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/408#issuecomment-766349948:312,perform,performance,312,,https://github.com/google/deepvariant/issues/408#issuecomment-766349948,1,['perform'],['performance']
Performance,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:1122,optimiz,optimized,1122,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"> Hi @pichuan Thank you for your response.; > ; > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:135,perform,performed,135,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,1,['perform'],['performed']
Performance,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). ; @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983:510,perform,performs,510,,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983,1,['perform'],['performs']
Performance,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:1702,perform,perform,1702,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,1,['perform'],['perform']
Performance,"@B10inform I'm not sure phasing is necessary for a haploid genome - because there is only one set of chromosomes the expectation is that identified variants are all already in phase. Additionally, DeepVariant is designed to perform germline variant calling in diploid organisms, and requires a reference genome. If the species you are working with does not have a reference genome than as @husamia suggested, this sounds like more of an assembly issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/418#issuecomment-773782606:224,perform,perform,224,,https://github.com/google/deepvariant/issues/418#issuecomment-773782606,1,['perform'],['perform']
Performance,"@Suke-fudan sorry for the earlier confusion regarding pileup height. You should use the latest version of DeepVariant (1.2.0), but this version is not used to perform phasing. Phasing is performed by whatshap. However, DeepVariant 1.2.0 is required in order to make use of phased bams when calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-930473071:159,perform,perform,159,,https://github.com/google/deepvariant/issues/488#issuecomment-930473071,2,['perform'],"['perform', 'performed']"
Performance,"@aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated. . By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data. `--emit_realigned_reads` - enables writing out of realigned reads; `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/280#issuecomment-593558131:23,perform,performs,23,,https://github.com/google/deepvariant/issues/280#issuecomment-593558131,1,['perform'],['performs']
Performance,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/705#issuecomment-1708685880:20,perform,performance,20,,https://github.com/google/deepvariant/issues/705#issuecomment-1708685880,1,['perform'],['performance']
Performance,"@akolesnikov This is what I get in the terminal when trying to build the binaries even without modifying the `make_examples.py` file:. ```; (18:45:18) WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".; (18:45:18) INFO: Current date is 2018-12-17; (18:45:40) INFO: Analysed target //:binaries (88 packages loaded).; (18:45:40) INFO: Found 1 target...; (18:45:40) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (18:45:41) ERROR: /home/moshvm/DeepVariant/deepvariant/third_party/nucleus/protos/BUILD:424:1: //third_party/nucleus/protos:reads_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //:binaries failed to build; (18:45:41) ERROR: /home/moshvm/DeepVariant/deepvariant/third_party/nucleus/protos/BUILD:424:1 1 input file(s) do not exist; (18:45:41) INFO: Elapsed time: 22.910s, Critical Path: 0.43s; (18:45:41) INFO: 0 processes.; (18:45:41) FAILED: Build did NOT complete successfully; (18:45:43) WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".; (18:45:43) INFO: Current date is 2018-12-17; (18:45:48) INFO: Analysed target //:licenses_zip (14 packages loaded).; (18:45:48) INFO: Found 1 target...; Target //:licenses_zip up-to-date:; bazel-genfiles/licenses.zip; (18:45:49) INFO: Elapsed time: 6.388s, Critical Path: 0.10s; (18:45:49) INFO: 0 processes.; (18:45:49) INFO: Build completed successfully, 1 total action; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-447918220:382,load,loaded,382,,https://github.com/google/deepvariant/issues/128#issuecomment-447918220,2,['load'],['loaded']
Performance,"@anands-repo my understanding of GroupByKey is what you have observed: all the data will be loaded into memory. If you are not able to use additional workers / use a larger machine with more memory, a workaround could be to run multiple shuffle jobs, each for a smaller subset of the data, rather than one global shuffle. I would also suggest contacting [Beam support](https://github.com/apache/beam#contact-us) to see if they can suggest any further optimization of this step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-720681308:92,load,loaded,92,,https://github.com/google/deepvariant/pull/365#issuecomment-720681308,2,"['load', 'optimiz']","['loaded', 'optimization']"
Performance,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:904,optimiz,optimizations,904,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['optimiz'],['optimizations']
Performance,@bcantarel The error suggests there are no quality scores for the current read. . * Can you double check that the file is not truncated? (`samtools quickcheck SAMN02990337.star.bam` can help with this). ; * Can you provide any details on how it was aligned?; * Which version of STAR did you use to perform the alignment?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/752#issuecomment-1856369666:298,perform,perform,298,,https://github.com/google/deepvariant/issues/752#issuecomment-1856369666,1,['perform'],['perform']
Performance,"@crazysummerW DeepVariant works well for identifying SNVs and indels at 50% and higher in the sample, with the exception that DeepVariant misses variants in the first/last ~100 bp of chrM because of the window size. For calling the high frequency variants, you can also use `gatk HaplotypeCaller`, but since the tool was optimized for short reads, there's a lot of additional tweaking both for `HaplotypeCaller` and `VariantFiltration` to get good results for HiFi. Callers that have been specifically designed and optimized for short reads, like Mutect2, do a great job of identifying low frequency heteroplasmic SNVs, but struggle with separating low frequency indels from sequencing errors. We've had some success with more general purpose low frequency variant callers like [lofreq](https://csb5.github.io/lofreq/) and [freebayes](https://github.com/freebayes/freebayes), but we still need to explore some parameters before sharing our recommended workflow. Because this is a HiFi question and not really directly a DeepVariant question, can you follow up on https://github.com/PacificBiosciences/pb-human-wgs-workflow-snakemake/issues/106?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/686#issuecomment-1654257037:321,optimiz,optimized,321,,https://github.com/google/deepvariant/issues/686#issuecomment-1654257037,2,['optimiz'],['optimized']
Performance,"@crazysummerW mtDNA variant analysis usually requires more specialized steps, as you need to worry about NUMT and heteroplasmy among other things, especially since the number of mitochondria vary for different cell types. DeepVariant I don't believe has the models trained for that, as it is usually geared for autosomal DNA. So, if you just want a VCF file without the large amount of analysis that is required for dealing with mtDNA, you can use the mitochondria mode of Mutect2 in GATK like this, which performs a lot of it for you:; ; ```; gatk Mutect2 \; -R reference.fa \; -L chrM \; --mitochondria-mode \; -I mitochondria.bam \; -O mitochondria.vcf.gz; ```. You can read more about it here:. https://gatk.broadinstitute.org/hc/en-us/articles/13832710384155-Mutect2. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/686#issuecomment-1652962088:506,perform,performs,506,,https://github.com/google/deepvariant/issues/686#issuecomment-1652962088,1,['perform'],['performs']
Performance,"@danielecook If you have available resources through which I can help you out to perform this analysis, feel free to let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1708709661:81,perform,perform,81,,https://github.com/google/deepvariant/issues/701#issuecomment-1708709661,1,['perform'],['perform']
Performance,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:296,Load,Load,296,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['Load'],['Load']
Performance,"@dbrami have you seen the case study?. https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md. __Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5)__. The commands we run are very similar to what you see in the case study - we just run it separately for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of tr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538:551,perform,perform,551,,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538,1,['perform'],['perform']
Performance,"@dkurt Sorry for the confusion. I meant:; ```; $ sudo docker run deepvariant:latest ls -lh /opt/models/wgs/; total 449M; -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin; -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001; -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index; -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta; -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping; -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml; ```. These are the extra files after enabling OpenVINO:; ```; -rw-r--r-- 1 root root 84M Nov 30 03:49 model.bin; -rw-r--r-- 1 root root 94K Nov 30 03:49 model.mapping; -rw-r--r-- 1 root root 276K Nov 30 03:49 model.xml; ```. Our regular Estimator code paths currently uses these files (these are what I meant by ""old ckpt format""):; ```; -rw-r--r-- 1 root root 333M Nov 10 17:09 model.ckpt.data-00000-of-00001; -rw-r--r-- 1 root root 19K Nov 10 17:09 model.ckpt.index; -rw-r--r-- 1 root root 33M Nov 10 17:09 model.ckpt.meta; ```. If both code paths can use the new (and smaller!) files, that will be very nice. I also noticed there is an intermediate `*.pb` format. If it possible to load that instead, that might be nice too. (assuming it's smaller too. I actually haven't checked.) I looked up yesterday but haven't found out how yet. If you have a pointer, please let me know. Thank you for all the work!; (And even if not, the new files are not too big. I'll experiment with building with openvino on.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735551532:1152,load,load,1152,,https://github.com/google/deepvariant/pull/363#issuecomment-735551532,1,['load'],['load']
Performance,"@dkurt we noticed some slight differences in the output VCF with and without OpenVINO. The quality scores are different in the example below (46 vs. 46.1) for an internal dataset. These quality scores are derived from the output probabilities. Are slight differences in output probabilities expected with and without OpenVINO? In the past, I've noticed such slight differences for the same hardware when EMA is not loaded in correctly at inference time. I wanted to bring this to your attention in case EMA is the reason for these differences. ```; -chr1 16895912 . G A 46 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0; +chr1 16895912 . G A 46.1 PASS . GT:GQ:DP:AD:VAF:PL 1/1:24:61:10,51:0.836066:46,23,0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736149821:415,load,loaded,415,,https://github.com/google/deepvariant/pull/363#issuecomment-736149821,1,['load'],['loaded']
Performance,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-700196639:346,optimiz,optimizations,346,,https://github.com/google/deepvariant/issues/346#issuecomment-700196639,1,['optimiz'],['optimizations']
Performance,"@hagen-wende ,. Thank you for looking into this. In your use-case, variant calling would be extremely difficult as mapping is difficult. Also, you are operating at 5x coverage. We only see the performance of ONT that are considered to be good from 15x. With that said, you can use DeepVariant create alleles for your use. Just drop the values under the threshold and all alleles will be reported so it at least gives you some idea of where the variants could be.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/892#issuecomment-2403049910:193,perform,performance,193,,https://github.com/google/deepvariant/issues/892#issuecomment-2403049910,1,['perform'],['performance']
Performance,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746:139,tune,tune,139,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746,12,"['perform', 'tune']","['performance', 'tune']"
Performance,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767885510:98,perform,performs,98,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510,4,"['perform', 'queue']","['performs', 'queue']"
Performance,"@jguhlin glad to hear that training curves look reasonable! I want to mention that DV is currently written to be a diploid variant caller. In case you are retraining with data from polyploid organisms, it is not yet clear how DeepVariant will perform. I'll close this issue for now, but feel free to reopen if you have any other questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251#issuecomment-566180701:243,perform,perform,243,,https://github.com/google/deepvariant/issues/251#issuecomment-566180701,2,['perform'],['perform']
Performance,"@kishwarshafin ; Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619:106,optimiz,optimize,106,,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619,1,['optimiz'],['optimize']
Performance,"@leorippel since your data is diploid and of similar variant density to human data, you could try applying the released DeepVariant models. That said, we can't be sure how well the models will perform on this data. I'll close this issue for now, but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357#issuecomment-698738740:193,perform,perform,193,,https://github.com/google/deepvariant/issues/357#issuecomment-698738740,1,['perform'],['perform']
Performance,"@leorippel. This specification:. > --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz. Will not match the gvcf splits you created. When paths contain an `@` symbol, they are referencing sharded files - so the path above gets expanded to a collection of files to input:. ```; /input/gvcf.tfrecord-00000-of-00030.gz; /input/gvcf.tfrecord-00001-of-00030.gz; /input/gvcf.tfrecord-00002-of-00030.gz; ...; ```. But your paths contain different prefixes (`SPLIT1` and `SPLIT2`). Regarding your original question:. > I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move?. Although I think this is technically possible I would advise against renaming sharded files. Sharded files are not designed to be manually renamed. Instead, if you are looking to speed up DeepVariant further I would recommend parallelizing the work across multiple CPUs. DeepVariant already parallelizes across cores when performing variant calling, but you can chunk the genome (e.g. by chromosome), and run each chunk on a different CPU, then combine the results at the end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413#issuecomment-771818067:1107,perform,performing,1107,,https://github.com/google/deepvariant/issues/413#issuecomment-771818067,1,['perform'],['performing']
Performance,@nlopez94 can you cat validation_set.pbtxt and see how many examples you have in the tune data? It looks like everything ended regularly but there's too little data.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101202221:85,tune,tune,85,,https://github.com/google/deepvariant/issues/819#issuecomment-2101202221,1,['tune'],['tune']
Performance,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:697,Load,Loads,697,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Load'],['Loads']
Performance,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:663,Load,Load,663,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['Load'],['Load']
Performance,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```; /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h; /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h; /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h; /usr/include/c++/v1/support/ibm/limits.h; /usr/include/c++/4.8/tr1/limits.h; /usr/include/c++/5/tr1/limits.h; /usr/include/limits.h; /usr/include/linux/limits.h; /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h; /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h; /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h; /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h; ```. ```; includes = [; include_htslib,; ""."",; ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",; ]; ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351020472:1448,cache,cache,1448,,https://github.com/google/deepvariant/issues/12#issuecomment-351020472,1,['cache'],['cache']
Performance,"@pgrosu I could not compile the library on my server . I followed the suggestion [here](https://stackoverflow.com/questions/847179/multiple-glibc-libraries-on-a-single-host/851229#851229). I added CFLAGS=""-O2"" to address an optimization request error but still the make command fails to compile; ```; mkdir glibc && cd glibc; wget https://ftp.gnu.org/gnu/glibc/glibc-2.23.tar.gz; tar xvzf glibc-2.23.tar.gz; mkdir glibc-build && cd glibc-build; mkdir ../install; ../glibc-2.23/configure CFLAGS=""-O2"" --prefix $HOME/glibc/install; make -j `nproc`; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453764783:224,optimiz,optimization,224,,https://github.com/google/deepvariant/issues/137#issuecomment-453764783,1,['optimiz'],['optimization']
Performance,"@pgrosu I got tensorflow on a different folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:499,cache,cache,499,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,2,['cache'],['cache']
Performance,"@pgrosu Thank you for the prompt response. Correct me if I'm wrong. ; taskset need to specify the specific cpu cores the process wants to occupy. Since my Spark cluster is multi-tenant, some processes may be running in the cluster and occupy some CPU cores. In addition, the dynamic resource allocation is enabled in my Spark cluster, so I can't assume all of my tasks can be equally assigned to each computing node. If my data are stored in 200 partitions, it mean that my program will launch 200 tasks by using pipe() to call taskset. I can't make sure which partition will be assigned to which computing node. Round-Robin assignment is a way, but it's violated the policy of the resource management (like Spark standalone or YARN). For example, I have 8 computing node with 4 cores per each. My Spark process might allocate 24 cores. It might be 8 computing nodes with 3 cores per each or 6 computing nodes with 4 cores per each. Furthermore, there is no information to let me know which task is done or which cpu core is available in my Spark program. the resource allocation might be unbalance and performance might be impacted severely, especially when several iterations of task assignment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-416891480:1103,perform,performance,1103,,https://github.com/google/deepvariant/issues/90#issuecomment-416891480,1,['perform'],['performance']
Performance,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:1004,optimiz,optimized,1004,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623,4,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361436703:320,optimiz,optimizations,320,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703,2,['optimiz'],['optimizations']
Performance,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:821,Load,Load,821,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Load'],['Load']
Performance,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:665,Load,Loads,665,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461,1,['Load'],['Loads']
Performance,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:104,perform,performed,104,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780,2,['perform'],['performed']
Performance,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512127253:194,perform,performing,194,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253,4,['perform'],"['perform', 'performing']"
Performance,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:218,Load,Load,218,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['Load'],['Load']
Performance,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393#issuecomment-742247654:184,perform,perform,184,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654,1,['perform'],['perform']
Performance,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735388012:362,perform,performance,362,,https://github.com/google/deepvariant/pull/363#issuecomment-735388012,1,['perform'],['performance']
Performance,"@ramcn I'm delighted you've gotten your build sorted out.; @ramcn If you are building DeepVariant from scratch, and in particular TF's wheels directly, I'd recommend looking into the exact version of TF you want to use with DeepVariant. In particular, if you are intending to run on CPUs, we've found that the MKL extensions to TF make call_variants 3-4x faster. It may be worth building yourself an optimized TF wheel for DeepVariant to maximize performance. @pgrosu Thank you for all of your insights into these issues Paul. It is much appreciate by myself and the rest of the team here at Google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423269790:400,optimiz,optimized,400,,https://github.com/google/deepvariant/issues/94#issuecomment-423269790,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"@raphaelbetschart if you do have any opportunity to perform a comparison between RNA-seq and WGS data, we would be very interested in the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1701070307:52,perform,perform,52,,https://github.com/google/deepvariant/issues/701#issuecomment-1701070307,1,['perform'],['perform']
Performance,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437426660:103,perform,performance-testdata,103,,https://github.com/google/deepvariant/issues/116#issuecomment-437426660,1,['perform'],['performance-testdata']
Performance,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/39#issuecomment-358337849:249,optimiz,optimization,249,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849,3,['optimiz'],"['optimization', 'optimized', 'optimized-os']"
Performance,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603:86,tune,tune,86,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603,26,['tune'],['tune']
Performance,"@sophienguyen01 For DeepVariant production models we generally train on chr1-19, tune on chr21-22, and save chr20 for final ""test"" or ""inference"" evaluations. When we have enough samples we'll leave out a whole sample; for most (maybe all) of our production models this is currently HG003 that is never seen during training (or tune), only kept for inference. It's important to do this train/tune/test split at the sample level and/or chromosome level so DeepVariant can't overfit to the specific variants it sees, which it could if you for example left out one of two bam files that came from the same sample.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1681392580:81,tune,tune,81,,https://github.com/google/deepvariant/issues/698#issuecomment-1681392580,3,['tune'],['tune']
Performance,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565:283,perform,performance,283,,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565,2,"['perform', 'tune']","['performance', 'tune']"
Performance,"@sophienguyen01 the logs indicate checkpoints are output:. ```; I0423 18:41:59.026870 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.9114237 step=3352 epoch=1 path=model_train/checkpoints/ckpt-3352; I0423 18:44:53.215049 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.91949123 step=6704 epoch=2 path=model_train/checkpoints/ckpt-6704; I0423 18:47:47.292658 139913113728832 train.py:456] Saved checkpoint tune/f1_weighted=0.92320794 step=10056 epoch=3 path=model_train/checkpoints/ckpt-10056; ```. But as @kishwarshafin suggests, the warnings at the end are normal and can be ignored.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073372104:133,tune,tune,133,,https://github.com/google/deepvariant/issues/802#issuecomment-2073372104,3,['tune'],['tune']
Performance,"@splaisan The intermediate directory is mainly used to generate and store [`TFRecord`](https://www.tensorflow.org/tutorials/load_data/tfrecord) files that are used among the `make_examples`, `call_variants` and `postprocess_variants` steps (performed by the `run_deepvariant` command above) to eventually generate the VCF and gVCF files used for downstream analysis. So you don't need them after a successfully completed DeepVariant run, as most folks are only interested in the resulting VCF and gVCF files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-1624413952:241,perform,performed,241,,https://github.com/google/deepvariant/issues/296#issuecomment-1624413952,1,['perform'],['performed']
Performance,"@xianyu0623 ,. Looks like the custom model file you are loading is not valid. Can you give a little more information on how you generated the custom model?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857#issuecomment-2261006352:56,load,loading,56,,https://github.com/google/deepvariant/issues/857#issuecomment-2261006352,1,['load'],['loading']
Performance,"@zyxue I did an analysis a while ago, that posted at the following link:. [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). The thing that dominated the processing at that time was the aligner, and would require more surgery to determine possibilities for optimization even though it got recently updated with the [FastPassAligner](https://github.com/google/deepvariant/blob/r0.7/deepvariant/realigner/fast_pass_aligner.cc) - which would require re-profiling. Basically a bunch of profiling tools would need to be built for you to then run, in order to determine for your scenario what the best optimization path would be. You probably noticed the following lines in [make_examples.py](https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1150-L1153) regarding the core restriction:. ```Python; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Though there are possibilities around that like @pichuan mentioned. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428686329:87,perform,performance,87,,https://github.com/google/deepvariant/issues/99#issuecomment-428686329,3,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355441693:321,optimiz,optimized,321,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693,8,"['load', 'optimiz', 'perform']","['load', 'optimization', 'optimized', 'performance', 'performant']"
Performance,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-488573130:257,optimiz,optimizations-,257,,https://github.com/google/deepvariant/issues/21#issuecomment-488573130,1,['optimiz'],['optimizations-']
Performance,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479522653:1043,perform,perform,1043,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653,2,['perform'],['perform']
Performance,"An interesting feature seems jump up when profiling of DV 1.5 in Docker, which looks to be heavy on I/O more than anything else:. ![image](https://github.com/google/deepvariant/assets/6555937/a912b74c-3daa-44f1-a0d4-ef7ecc91bcce). Then with sampling the I/O, the distribution of types of operations hint at some bottleneck opportunity:. ![image](https://github.com/google/deepvariant/assets/6555937/9b6e2296-789b-4eff-852b-f053214f291a). Maybe optimizing for I/O via balancing by having equal number of resources for `|GPU| == |CPU|` with significant on-board memory could do the trick, though not sure if all the code has been ported to Keras, which should help with the distribution strategy in that situation to make it all click together.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1549350965:312,bottleneck,bottleneck,312,,https://github.com/google/deepvariant/issues/650#issuecomment-1549350965,2,"['bottleneck', 'optimiz']","['bottleneck', 'optimizing']"
Performance,"At present we do not have a specific recommendation for joint genotyping DeepVariant gVCFs. Given the accurate genotype likelihood calibration of single-sample DeepVariant calls it may be better to simply merge calls without computing genotype posteriors based on population allelic frequencies and then altering the genotypes. We are actively investigating the performance of different methods to be able to provide a set of best practices. That said, the gVCF outputs of DeepVariant are syntactically and semantically equivalent to those produced by other tools like GATK, so can be used by any existing joint genotyping tools.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/45#issuecomment-363913008:362,perform,performance,362,,https://github.com/google/deepvariant/issues/45#issuecomment-363913008,2,['perform'],['performance']
Performance,"CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:9797,Load,Loads,9797,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['Load'],['Loads']
Performance,D in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/p,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119998,cache,cache,119998,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"Dear @ESDeutekom,. There are a few things to unpack here. The goal is to make sure that the BAM file and reference FASTA file match in their contigs and bases to ensure correct variants get called between the two, and if local realignment is required that the regions match appropriately. Thus, a couple of steps get performed:. $`1)`$ So in order to be sure there is proper consensus between the two, a first pass over the reference gets processed to remove contigs based on an internal list of excluded contigs found in the following file:. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. This is done in the reference first, because most likely these would not exist in most BAM files. $`2)`$ Now given the remaining contigs from the reference, these are used to find common contigs that exist in the BAM file as well, to ensure there is proper overlap. All of this (including $`Step \; 1`$ above) is performed by the [`_ensure_consistent_contigs` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L345-L383). The two steps summarize the statement you noticed in the documentation. Now the only excluding option I see is for [`exclude_regions`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L94-L100), though the rest of the above excluded contigs are hard-coded as [`exclude_contigs=exclude_contigs.EXCLUDED_HUMAN_CONTIGS`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L349). To exclude a region, that would come as a space-separated list of regions, which can be region literals (e.g., `chr20:10-20`) or paths to BED/BEDPE files. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-1673451441:317,perform,performed,317,,https://github.com/google/deepvariant/issues/37#issuecomment-1673451441,2,['perform'],['performed']
Performance,"Dear Cheng,. Yes, there can be some differences between DeepVariant-GLNexus (with optimization) and GATK-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both ap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:82,optimiz,optimization,82,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876,3,"['optimiz', 'scalab']","['optimization', 'scalable']"
Performance,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-352893787:272,tune,tuned,272,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787,1,['tune'],['tuned']
Performance,"Does this fix the mis-linked `libnvinfer_plugin.so.7` dlerror? . Seems like both `libnvinfer_plugin.so.7` and `libnvinfer_plugin.so.8` are in `LD_LIBRARY_PATH`, but the binary `run_deepvariant` is linked against `libnvinfer_plugin.so.7`, which complains about missing `libcublas.so.12`. Here's the error I'm getting:; ```stdout; 2024-10-07 09:10:29.222934: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-10-07 09:11:35.925528: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-10-07 09:11:35.925571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; ```. Here's where I found `libnvinfer.so.7`:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ls /usr/local/nvidia/lib; libnvinfer.so.7 libnvinfer.so.8 libnvinfer_plugin.so.7 libnvinfer_plugin.so.8; ```. Here's my `ldd` call to see what it's linked to:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif ldd /usr/local/nvidia/lib/libnvinfer_plugin.so.7; 	linux-vdso.so.1 (0x0000155555524000); 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0000155553038000); 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x0000155553032000); 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000); 	libcublas.so.12 => not found; 	libcublasLt.so.12 =>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060:436,optimiz,optimized,436,,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060,3,"['load', 'optimiz', 'perform']","['load', 'optimized', 'performance-critical']"
Performance,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437712479:805,perform,performance-testdata,805,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479,1,['perform'],['performance-testdata']
Performance,"Following up on my previous comment,; I confirmed that hap.py results in `happy.output.summary.csv` are the same. @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. ```. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp0gfwv278/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp0gfwv278/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276922:751,optimiz,optimized,751,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"GIAB used a combination of different kinds of sequencing data to get the truth VCF, including long-range technologies that give better results than using short-read sequencing alone. Learning from this truth set with more context is why DeepVariant performs better than purely looking at AD and VAF alone. DeepVariant learns to balance false positives and false negatives, so when it labels some high-AD loci as '0' it is because they look like sites that GIAB labeled as hom-ref. STRs in particular can cause false positives when using simple allele depth approaches. Since DeepVariant sees the base sequence in the pileup image, it effectively already has an STR channel, so it can ""learn"" about STRs and take their presence into account. While the GIAB truth set is not perfect, it has generally performed far better by employing multiple technologies relative to using short-read allele depth alone. I would say it's more likely that these sites were determined by other technologies to be hom-ref than that they were missed by GIAB.; Does that answer your question?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/317#issuecomment-644965403:249,perform,performs,249,,https://github.com/google/deepvariant/issues/317#issuecomment-644965403,4,['perform'],"['performed', 'performs']"
Performance,"Glad it helped Sophie! Currently yes, though DeepVariant can easily be ported to multiple GPUs with many other optimizations, that would probably be at a later time. Even if you think about the called variants, those are locus-specific and multiple regions can run across multiple GPUs. For counting alleles in the `make_examples` stage, that would be the same thing that can be taken advantage of [as illustrated by the benchmark of its sub-stages](https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md) -- and there are GPU-based Smith-Waterman aligners that are 8x-20x faster than CPU. Distributing for speedup the collecting/transforming/sorting in the last stage of post-processing the variants is only natural, and that can subsequently also provide logarithmic combines -- including other optimizations, which would be aided by utilizing changes in the previous stage. So the future is very bright, but again that would be something for a later time :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458:111,optimiz,optimizations,111,,https://github.com/google/deepvariant/issues/696#issuecomment-1679436458,2,['optimiz'],['optimizations']
Performance,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here ; [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: ; ```-config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=32 \; ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073289661:179,tune,tune,179,,https://github.com/google/deepvariant/issues/802#issuecomment-2073289661,1,['tune'],['tune']
Performance,"HONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; random_seed=random_seed)); ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; paul@gubuntu:~/deepvariant/bazel-bin$; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:2467,cache,cache,2467,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,6,['cache'],['cache']
Performance,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2116915079:315,perform,performance,315,,https://github.com/google/deepvariant/issues/820#issuecomment-2116915079,1,['perform'],['performance']
Performance,"Hello @shalabhsuman,. We are currently working on recommendations for best practices to jointly call and merge cohorts of this size. We have a few similar size projects that we are conducting ourselves to be able to assess accuracy and scalability. We have done some preliminary work in joint calling and merging cohorts (this was a component of a recent blog from our group https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). Our early investigations have used GLnexus (https://www.biorxiv.org/content/10.1101/343970v1) to combine gVCFs. Because the final recommendations will use gVCFs, if you generate the gVCF files now you should be able to merge these with the final recommended approach. If you would be interested, we would be interested to collaborate you in the joint calling of these samples. This will help you get the best calls sooner, and will also help us evaluate and improve the merging methods we are developing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-459440356:236,scalab,scalability,236,,https://github.com/google/deepvariant/issues/142#issuecomment-459440356,1,['scalab'],['scalability']
Performance,"Hello @sidharthgoel . Thank you for your help with this issue - I was able to build deepvariant! Tests failed, below, and I am happy to open a separate issue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```; FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:378,cache,cache,378,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,4,['cache'],['cache']
Performance,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:149,perform,perform,149,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772,1,['perform'],['perform']
Performance,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1671,load,load,1671,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398,1,['load'],['load']
Performance,"Hello C,. Thank you for taking the time to investigate DeepVariant. . With respect to DP and AD, reads considered have some minimum thresholds for inclusion. You can see these in deepvariant/make_examples.py lines 274-277. The default is a filter at MAPQ 10. You can see this manifest in the calculation of allele depth in deepvariant/allelecounter.cc line 289. . Above these thresholds, DeepVariant will count all of the reads present. However, there is an additional complexity. DeepVariant generates local a local of the ref and alt alleles by constructing a De Bruijn graph of each. It then performs a Smith-Waterman re-alignment of reads to the assemblies. You can think of this as a more exhaustive version of the candidate haplotype generation performed in GATK. As a result of this re-assembly, there may be some differences between the alleles that DeepVariant constructs and those constructed in GATK (and this may contribute to differences in AD). See the deepvariant/realigner folder for all of the associated code. With respect to GT and GQ, these are the primary outputs of the convolutional neural network classifier. The classifier estimates the probability of HOM REF, HET, and HOM ALT states. The GT is the most probably state as determined by the classifier. The GQ should correspond to the likelihood calculated for that GT (and as a result, this should correspond to PL). With respect to the calibration of GQ and recommendations for filtering. One observation we have about DeepVariant is that the genotype qualities seem to quite accurately reflect the empirical error probability (see Figure 2 Panel C of - https://www.nature.com/articles/nbt.4235). This fact, combined with the observation that the GQ scores produced by DeepVariant are quite normally distributed, means that you have flexibility to shift them slightly higher or lower if you prefer higher precision or higher recall. . If anything, DeepVariant seems to be slightly on the conservative side outside of the con",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-450712530:595,perform,performs,595,,https://github.com/google/deepvariant/issues/135#issuecomment-450712530,2,['perform'],"['performed', 'performs']"
Performance,"Hello, ; Thanks for your reply.; I don't want to perform assembly. I just want to obtain a VCF file for the mitochondria based on the aligned BAM file.; I am unsure if the results for chrM in the VCF generated by DeepVariant are reliable.; PacBio mitochondrial data:; ![1690423792423](https://github.com/google/deepvariant/assets/70870741/f6a18fa3-a432-4d53-9824-20a9e309298c). If DeepVariant is not suitable for calling mitochondrial variants on PacBio mitochondrial data, are there any other software recommendations for calling variants at mitochondrial loci without assembly?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/686#issuecomment-1652817371:49,perform,perform,49,,https://github.com/google/deepvariant/issues/686#issuecomment-1652817371,1,['perform'],['perform']
Performance,"Hello, here is a VAF vs GQ plot. English is not my native language so I am not sure how to say it appropriately but what the?. It seems GQ is not correlated with VAF; ![histogram2d](https://github.com/google/deepvariant/assets/81575666/9dc535f1-8337-4d51-b82b-7d58ddeec006). I am not sure what to expect since we selected for RefCall in the end, maybe it's actually correct.; I am gonna redo the call with Clair3 and see if it gives results as weird as those ones. . EDIT: side note, does make_examples perform any kind of realignment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650079569:503,perform,perform,503,,https://github.com/google/deepvariant/issues/682#issuecomment-1650079569,1,['perform'],['perform']
Performance,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:900,load,loading,900,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056,1,['load'],['loading']
Performance,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:622,optimiz,optimization,622,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['optimiz'],['optimization']
Performance,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB; #SBATCH --qos=maxjobs100. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs; HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed; OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154:795,load,load,795,,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154,2,['load'],['load']
Performance,"Hi @ASLeonard ,; A quick update: We think this is because we need to adjust our PL in our gVCF as well.; I'm working on a code change, but it might take a bit longer than I thought.; I just want to let you know that this is still in my queue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/811#issuecomment-2097115031:236,queue,queue,236,,https://github.com/google/deepvariant/issues/811#issuecomment-2097115031,1,['queue'],['queue']
Performance,"Hi @ASLeonard . This is an interesting question. The ability to sort reads and label them with a phasing tag (specifically HP) is general to DeepVariant (meaning that on the code level it is straightforward to add to DeepVariant). This feature is only used for long reads. We performed experiments in non-trio phasing of short reads, but found there is not enough information for local phasing to add information. . Trio phasing can be much more informative for short reads over long ranges. The main obstacle is that we do not have pre-trained models which have learned how to use this information as we have for long reads. It would, in theory, be possible to train models for this (though it would be a reasonable amount of work). One of the obstacles for us to do this is that we don't know what to recommend as the best practices for the trio binning process. . I don't think that variant calling on an assembly is necessarily a good idea, because the assembly itself will have errors, and the expected distribution of REF, HET, and HOM calls will be quite different from the typical variant calling problem. Assemblies are usually less complete than the reference, especially with short read data, and this is likely to create a lot of mapping artifacts. . I don't have any good recommendations for how to incorporate trio haplotype information at this time, but if you have reasonable suggestions on how to do so, we are happy to consider using them within DeepTrio. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/451#issuecomment-830342295:276,perform,performed,276,,https://github.com/google/deepvariant/issues/451#issuecomment-830342295,2,['perform'],['performed']
Performance,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:1162,perform,performance,1162,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398,1,['perform'],['performance']
Performance,"Hi @AldoCP,. We have not comprehensively investigated performance on RNA-Seq samples yet. Because some components of the RNA-Seq problem more closely resemble variant calling in DNA-Seq exomes, I recommend that you use the exome model instead of the WGS one. A proper investigation would probably involve looking at the variant calls that result from the same sample with both DNA-Seq and RNA-Seq and investigating the concordance of calls. I expect that sort of comparison is probably worth a paper. . In general, we observe that when DeepVariant is applied in other domains beyond its training data, it tends to undercall as opposed to call false positive events, so if you are getting calls in DeepVariant and not HaplotypeCaller, those are worth looking into. If you do so, we would be interested in your feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-457857605:54,perform,performance,54,,https://github.com/google/deepvariant/issues/115#issuecomment-457857605,2,['perform'],['performance']
Performance,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox...; > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756; > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS; > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:; > unifier_config:; > drop_filtered: false; > min_allele_copy_number: 1; > min_AQ1: 10; > min_AQ2: 10; > min_GQ: 0; > max_alleles_per_site: 32; > monoallelic_sites_for_lost_alleles: true; > preference: common; > genotyper_config:; > revise_genotypes: true; > min_assumed_allele_frequency: 9.99999975e-05; > snv_prior_calibration: 0.600000024; > indel_prior_calibration: 0.449999988; > required_dp: 0; > allow_partial_data: true; > allele_dp_format: AD; > ref_dp_format: MIN_DP; > output_residuals: false; > more_PL: true; > squeeze: false; > trim_uncalled_alleles: true; > top_two_half_calls: false; > output_format: BCF; > liftover_fields:; > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:867,Load,Loading,867,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['Load'],['Loading']
Performance,"Hi @AndrewCarroll When i ran variant calling on the same bam with other tool Im getting more variants than while running deepvariant. Also, some of the important variants are missed in the final output in the deepvariant. I tried 1.4.0 and i'm getting the same output. Let me know if there is any way to optimize the parameter or the code I'm trying is correct.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/775#issuecomment-1960767240:304,optimiz,optimize,304,,https://github.com/google/deepvariant/issues/775#issuecomment-1960767240,1,['optimiz'],['optimize']
Performance,"Hi @Axze-rgb,. Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types $`-`$ that's the basic hypothesis here. So here's a quick way you can fix the multiallelic issue above:. $`1)`$ First split the multiallelic sites into biallelic records like this:. ```; bcftools norm -m - multi_allelic.vcf > biallelic.vcf; ```. $`2)`$ Then parse for the `0/0` and `./.` genotypes $`-`$ I'm assuming your genotypes are not phased:. ```; bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; ```. Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119:357,optimiz,optimizes,357,,https://github.com/google/deepvariant/issues/682#issuecomment-1648597119,2,"['load', 'optimiz']","['load', 'optimizes']"
Performance,"Hi @DanJeffries ,. Can you please paste the output for the following command here:. ```bash; cat /home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt; ```; and also separately run the following and paste the output:; ```bash; cat /home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2322168099:282,tune,tune,282,,https://github.com/google/deepvariant/issues/876#issuecomment-2322168099,1,['tune'],['tune']
Performance,"Hi @DiableJambe ,. Sorry for my late response, I do not have a docker image yet. And I attached the detailed steps here for your reference. It's a bit long :). # DeepVariant. ## Environment. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$C",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:439,load,load,439,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['load'],['load']
Performance,"Hi @Fred-07 . When we investigated Element sequencing for exomes with the out-of-the-box Illumina models, we generally observed accuracy that was as good or better as that observed with Illumina without any retraining. With v1.5 the whole genome models do include joint training with Element data, and including element in WGS training does make Element accuracy somewhat further improved. I expect we'll try to get Element exomes and add them into the exome model training in the future, but for now I think you will see good performance with the existing exome model. If you do have any feedback on performance with Element exomes, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423:527,perform,performance,527,,https://github.com/google/deepvariant/issues/703#issuecomment-1705737423,4,['perform'],['performance']
Performance,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032:429,perform,performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications,429,,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032,1,['perform'],['performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications']
Performance,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491:193,tune,tuned,193,,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491,3,"['perform', 'tune']","['perform-analysis-with-happy-against-', 'performance', 'tuned']"
Performance,"Hi @HamiltonG. The one-step script whose usage is shown in https://github.com/google/deepvariant#how-to-run-deepvariant will work on a cluster, just note that giving it something like 64 threads will help it run faster.; Our case study [metrics](https://github.com/google/deepvariant/blob/r1.1/docs/metrics.md) are from runs with 64 CPU cores and no GPU, so those numbers should give you an idea if that works for your purposes. For the simple run_deepvariant case, if Docker isn't available to you on your cluster, the same container and commands can be used with Singularity. This is I think what most people do when running on a cluster. If you really want to optimize a process to run DeepVariant many times, it can be worth running the 3 stages separately and giving them different resources because make_examples wants many CPUs, call_variants runs faster on GPUs, and postprocess_variants really just needs 1 CPU. The [external solutions](https://github.com/google/deepvariant#external-solutions) do variations of this plus their own special sauce. I hope that helps answer your question,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474#issuecomment-883613731:663,optimiz,optimize,663,,https://github.com/google/deepvariant/issues/474#issuecomment-883613731,2,['optimiz'],['optimize']
Performance,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275:230,scalab,scalable,230,,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275,1,['scalab'],['scalable']
Performance,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:587,perform,performance,587,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466,1,['perform'],['performance']
Performance,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996:575,perform,perform,575,,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996,1,['perform'],['perform']
Performance,"Hi @SHuang-Broad . This is a good question. There have been a number of changes to DeepVariant over releases. However, I don't think we've deeply revisited the timing advantages between CPU and GPU. I think that in our investigations, we see that in the GCP machines we use CPU instances still have some degree of cost advantage relative to GPU. I agree that it's likely we're not optimally using the GPU, and I don't think we've specifically optimized for higher utilization of it. Although that's something we could look more into over time, right now we're prioritizing other feature improvements. . For GPUs, how do you consider Parabricks as an option? I believe that with their most recent release, they will be able to run v1.5. For the optimization of GPU performance, I expect they are much better in using the GPU than our out-of-the-box code. By the way, $100/sample seems high to me relative to our current experience. I hope that might be a function of the older method being Call-Phase-Call. If you have new benchmarks for this I would be curious to know. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112:443,optimiz,optimized,443,,https://github.com/google/deepvariant/issues/650#issuecomment-1548690112,3,"['optimiz', 'perform']","['optimization', 'optimized', 'performance']"
Performance,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:549,optimiz,optimizing,549,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050,4,"['bottleneck', 'optimiz', 'perform']","['bottleneck', 'optimizations', 'optimizing', 'performance']"
Performance,"Hi @Sami-St,. If you're really keen on using Singularity, then you can add the following two lines to your `Snakefile`:; ```; threads: 1; singularity: ""/location_of_your_sif_file/deepvariant_1.5.0.sif""; ```. And then launch it from the command-line with the following to ensure that you map (bind) shared folders within the container:. ```; snakemake -c 1 --use-singularity --singularity-args ""--bind /folder_to_share_with_singularity""; ```. It might be better if you use `/opt/deepvariant/bin/run_deepvariant` instead of `make_examples` $`-`$ unless you really know what your are doing $`-`$ as it performs the proper setup for you. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1635046221:599,perform,performs,599,,https://github.com/google/deepvariant/issues/677#issuecomment-1635046221,1,['perform'],['performs']
Performance,"Hi @WeiYang-BAI . I would recommend using GLnexus to merge gVCFs of GATK and DeepVariant. GLnexus has been optimized for both GATK and DeepVariant outputs. There are different presets for GLnexus, to combine multiple methods I would recommeng using unfiltered settings. We observe that the GATK joint genotyper doesn't seem to handle DeepVariant gVCFs well, and the accuracy is much lower after using GATK on those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-1978021002:107,optimiz,optimized,107,,https://github.com/google/deepvariant/issues/778#issuecomment-1978021002,1,['optimiz'],['optimized']
Performance,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|; |------------|----------|--------------|--------|------------|---------------|--------|; |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|; |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/809#issuecomment-2067922240:649,perform,performance,649,,https://github.com/google/deepvariant/issues/809#issuecomment-2067922240,2,['perform'],['performance']
Performance,"Hi @aardes, we have reported [this runtime](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md#runtime) for running DeepVariant WGS on a GPU. The specs for that GPU are listed at the bottom of the [page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform). I'm not sure how your specific GPU type would perform, but those numbers should give you a point of comparison.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/240#issuecomment-558701300:407,perform,perform,407,,https://github.com/google/deepvariant/issues/240#issuecomment-558701300,1,['perform'],['perform']
Performance,"Hi @aderzelle ,; the current design is that you do need to specify both the `model_type` and `customized_model` flag.; The reason is that we're using the `model_type` flag to set a few assumptions in the run_deepvariant.py script. Specifically, if you specify `PACBIO`, you get these set for you:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L183-L185. In the current version, if you set model_type to either `WGS` or `WES`, there's actually no additional assumption being made (that just means everything will be default value from the code). So, one strategy for you is to set:. ```; --model_type=WGS \; --customized_model=/input/your.model.ckpt \; ```. (or WES, they should be the same); Then the behavior is that the call_variants step should be loading your customized model. Let me know if this works for you. I'm pretty sure this answer will resolve your issue, so I'm closing this bug. But feel free to follow up. And I'm very curious to know how your own trained model works for your task!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268#issuecomment-586408803:817,load,loading,817,,https://github.com/google/deepvariant/issues/268#issuecomment-586408803,1,['load'],['loading']
Performance,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/224#issuecomment-540821357:129,tune,tune,129,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357,2,['tune'],['tune']
Performance,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383#issuecomment-727745570:266,load,loaded,266,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570,2,['load'],['loaded']
Performance,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-546050008:470,perform,perform,470,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008,2,['perform'],['perform']
Performance,"Hi @anands-repo . You are correct, subsampling is static. A nice effect of this is that it reduces the complexity in terms of training reproducibility. We have not deeply investigated whether dynamic resampling (making a different image per epoch) would benefit training. It's an interesting question. It could potentially reduce overfitting that might occur at the read level and therefore allow training to progress through more epochs before a model is selected. . I think it is unlikely that this would improve the current production training setup, but it is not impossible. For the WGS training curves, there is little overfitting apparent in training graphs over a large number of epochs. For the WES training curves, some overfitting is apparent, but we suspect this is less due to signal from the read level and more due to the smaller number of regions represented in the exome. This is one reason that we currently train the exome model by warmstarting from the WGS model. It is probably worth us taking a look at some point, but likely isn't the lowest hanging fruit for us to improve performance. Thank you for the suggestion and discussion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-553653477:1097,perform,performance,1097,,https://github.com/google/deepvariant/issues/230#issuecomment-553653477,1,['perform'],['performance']
Performance,"Hi @andrewrech and @shalabhsuman. I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md). Although this case study is a trio, we have optimized parameters for cohorts scaling into the 1000's, so we feel this will work well for your use cases. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-553665228:348,optimiz,optimized,348,,https://github.com/google/deepvariant/issues/142#issuecomment-553665228,1,['optimiz'],['optimized']
Performance,"Hi @bopohdr . I cannot speak to the dataset referenced in this paper, as it is not available, and I am not sure how merging would have been performed, and the paper references an earlier DeepVariant version. However, a higher de novo rate of DeepVariant relative to other callers is not something that we observe in the investigations we have conducted. I am attaching (if they will upload) two VCF files for the Oslo University HG002-HG003-HG004 trio available in Genome in a Bottle FTP. The DeepVariant trio is called with DeepVariant exome model and merged with GLnexus. The GATK4 trio is called with HaplotypeCaller and merged with GenotypeGVCFs. In these trios, DeepVariant has 87 de novo calls where the child is not 0/0 but both parents are 0/0; The GATK4 trio has 270 de novo cases with the same criteria. We have a set of documentation for best practices in merging a trio that we are hoping to release in the near future. If you would like me to share that with you privately now, you can reach out directly at awcarroll@google.com. Thanks,; Andrew. [deepvariant.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623783/deepvariant.cohort.vcf.gz); [gatk4.cohort.vcf.gz](https://github.com/google/deepvariant/files/3623784/gatk4.cohort.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/220#issuecomment-532442476:140,perform,performed,140,,https://github.com/google/deepvariant/issues/220#issuecomment-532442476,1,['perform'],['performed']
Performance,"Hi @brentp . This is a very good question, but I am not sure I have a good answer for you. For you and @kokyriakidis the fact that window selector can cause inconsistency across a pedigree was not something we previously appreciated when considering the trade-offs between speed and accuracy. In the intermediate future, we hope to profile performance of make_examples to improve speed and at that time we will revisit the decision to enable this by default. This will not occur for the upcoming release, but possibly the following one. It is possible to create a BED file covering de novo sites and force window selector=false across all of them, this should be fast and it could be an exercise we would want to do on the 1KG pVCF. . We are also working on some more involved methods for calling in a trio, but that is also in the intermediate timescale.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-588045756:340,perform,performance,340,,https://github.com/google/deepvariant/issues/272#issuecomment-588045756,1,['perform'],['performance']
Performance,"Hi @crazysummerW . Thank you for your question. The most recent release of DeepVariant (v1.5) has been trained with both Illumina and Element data for the WGS model, and we found a single model performs well for both data. Even in earlier releases before training with Element data, we observe that the DeepVariant Illumina model doesn't have issues operating on Element data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397:194,perform,performs,194,,https://github.com/google/deepvariant/issues/623#issuecomment-1482167397,1,['perform'],['performs']
Performance,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:183,perform,performance,183,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,3,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Hi @dkurt . First, thank you for your interest in DeepVariant, and for the substantial work that you have put into these modifications. I have some questions for you, and I suspect @pichuan may add some questions and comments as well. 1. We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. 2. Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. 3. If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). 4. We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. . I suspect that we will try to run with these changes and see how the performance changes. If you are able to answer some of these questions, it could be helpful for us to understand how to prioritize their assessment. Thank you again for the work you have put into this. It's quite impressive, and we appreciate your effort. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709920659:1033,perform,perform,1033,,https://github.com/google/deepvariant/pull/363#issuecomment-709920659,2,['perform'],"['perform', 'performance']"
Performance,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:249,perform,performed,249,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568,2,['perform'],"['performed', 'performing']"
Performance,"Hi @elcortegano . Billy has provided some excellent, detailed answers. I'll just add a little bit of context to your question . ""Our understanding ```deepvariant``` is designed for read alignments (and not assembly-to-reference alignments as achieved with ```minimap2 -ax asm```"". This is correct, DeepVariant is for read alignments not assembly-to-reference alignment. Because minimap2 has original written when most PacBio data was CLR, the choice of the word ```map-pb``` was chosen for this present. When CCS became more common, a new recommendation was made by minimap2 authors to use the parameters for assembly with an sequence divergence tuned to HiFi data (this is the 20 part of ```--asm20```). If you search for the string ``ccs``` in the minimap2 github, this should pull up the line. So in short, DeepVariant is for read alignments, and the parameter for read mapping HiFi reads happens to manifest in minimap2 as ```minimap2 -ax asm20```.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815982608:646,tune,tuned,646,,https://github.com/google/deepvariant/issues/434#issuecomment-815982608,1,['tune'],['tuned']
Performance,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815421572:932,perform,perform,932,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572,3,"['optimiz', 'perform']","['optimized', 'perform']"
Performance,"Hi @fengcong3 . DeepVariant has been applied to plant species. In the case of rice, there was good evidence of high accuracy superior [some results in this blog](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So in cases with polyploid organisms (which includes wheat), it is not yet clear how DeepVariant will perform. However, I am also not sure how other variant callers perform on polyploid samples. If you (or anyone) knows current polyploid callers for wheat, I would like to know and to run benchmarks between the two. Finally, it is possible to train DeepVariant models for a specific genome. We have a previous example of this in mosquitos [see our blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). We hope to explore training a general plant model or a general non-human model in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/200#issuecomment-513411694:529,perform,perform,529,,https://github.com/google/deepvariant/issues/200#issuecomment-513411694,4,['perform'],['perform']
Performance,"Hi @gambalab,. Thank you for the information. I tried using a Debian 11 docker image, and installed conda with python 3.8 but there seems to be other library conflicts. In the meantime if you want to have fun while your sysadmin performs a proper Docker install, you can run Docker in user-space using `udocker` in the following way:. https://indigo-dc.github.io/udocker/installation_manual.html. You don't need to install it as it say in the instructions. You can just do the following:. ```; wget https://github.com/indigo-dc/udocker/releases/download/1.3.9/udocker-1.3.9.tar.gz; tar xzvf udocker-1.3.9.tar.gz; cd udocker-1.3.9/udocker/; ./udocker pull google/deepvariant:1.5.0; ./udocker run google/deepvariant:1.5.0; ```. All the downloaded information is saved under the ` $HOME/.udocker` folder. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351:229,perform,performs,229,,https://github.com/google/deepvariant/issues/669#issuecomment-1603444351,1,['perform'],['performs']
Performance,"Hi @gunjanbaid , I am a novice when it comes to beam. So I will defer to you on https://github.com/google/deepvariant/pull/365#discussion_r512315819. My setup works with Spark/Flink so I can test it out. I am looking at another part of the code that is potentially fatal for execution at the moment:; ``` ; return (input_examples; | 'Randomize' >> beam.Map(lambda x: (sha1(x), x)); | 'Groupby' >> beam.GroupByKey(); | 'DropKey' >> beam.FlatMap(lambda x: x[1])); ```. I notice that GroupByKey is loading all of the data into the memory of the worker. Is this not a problem for DataflowRunner?. I am trying to run shuffle on tfrecords produced from 6 BAM files. The gzipped tfrecords are approximately 120GB in total, and GroupByKey quickly runs out of memory when the machine has over 600GB of RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-717368605:495,load,loading,495,,https://github.com/google/deepvariant/pull/365#issuecomment-717368605,1,['load'],['loading']
Performance,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356#issuecomment-699007209:520,perform,performing,520,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209,1,['perform'],['performing']
Performance,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355#issuecomment-697093712:455,cache,cache,455,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712,1,['cache'],['cache']
Performance,"Hi @husamia ,; To load the Keras model with GPU, you'll need enough memory. ; In the future we can see if there are ways we could improve this, but for the current version, I don't think there is a lower-memory solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/759#issuecomment-1882059065:18,load,load,18,,https://github.com/google/deepvariant/issues/759#issuecomment-1882059065,1,['load'],['load']
Performance,"Hi @husamia,. Unfortunately, 6GB is too low. The data needs to load to RAM before being loaded to GPU. So there needs have sufficient memory for that and to load the model too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/759#issuecomment-1882065583:63,load,load,63,,https://github.com/google/deepvariant/issues/759#issuecomment-1882065583,3,['load'],"['load', 'loaded']"
Performance,"Hi @jianqi-chen,. I'm assuming you are working with germline diploid samples. $`1)`$ Ti/Tv ratio is calculated as follows. For every variant that is a biallelic SNP, it will check as follows:. ```Python; is_transition = if {'A' <-> 'G'} or {'C' <-> 'T'}; is_transversion = not( is_transition ); else is_transition = is_transversion = False; ```. The Ti/Tv ratio is calculated as `float(variant_counts(is_transition)) / variant_counts(is_transversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196:914,perform,perform,914,,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196,1,['perform'],['perform']
Performance,"Hi @jkalleberg ,; Thank you for reaching out. The model ckpt you're using was older than v1.4. And you're right: in v1.4 we added an extra channel, and we haven't trained a new allele frequency model with v1.4. So we actually don't yet have a model that has both insert_size as well allele_frequency!. Two things:. 1. If you want to run v1.4.0 code with the older model (which didn't have the insert_size channel), you can add `,channels=''` to the end of your make_examples_extra_args. I added a section to my public gist here:; https://gist.github.com/pichuan/64d73bc965300645470eb29a66116593#update-if-youre-using-our-v140-docker-codebase; 2. I'm currently training a new WGS AF model that will have both the insert_size channel, as well as the allele_frequency channel. So, stay tuned! I can give you an update when I have it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143:783,tune,tuned,783,,https://github.com/google/deepvariant/issues/568#issuecomment-1251633143,1,['tune'],['tuned']
Performance,"Hi @jordimaggi @splaisan . I will add on a little to Pi-Chuan's answer with respect to filtering and quality scores. We consistently find that the genotype quality (GQ and PL fields) are extremely well-calibrated with the empirical probability of a call being correct. This is quantified in Figure 2 of the [original DeepVariant paper](https://www.biorxiv.org/content/10.1101/092890v6). This value is the best to use when determining whether a call is likely correct. Both ourselves and other external groups who we work with have tried to identify other metrics of standard INFO and FORMAT fields which are more predictive of call quality or even additionally informative in a subset of contexts and cases. For basically everything we and these groups have looked at, GQ is more predictive of call correctness. . If you are able to identify an annotation which is additionally informative beyond GQ (and also not already perfectly captured in the GQ field), it would be quite interesting to know, and we could consider incorporating it as an output field, or providing the annotation as an input during calling. . In general, I'd encourage you to look at GQ and PL as the most informative fields if you would like to tune between sensitivity and specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659:1218,tune,tune,1218,,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659,1,['tune'],['tune']
Performance,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:569,load,loaded,569,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,4,"['load', 'optimiz']","['loaded', 'optimizer']"
Performance,"Hi @karthick1087. DeepVariant will be able to run on a BAM file for one microbial species relative to its reference genome and should be able to produce a VCF file with variants. However, DeepVariant is designed with diploid variant calling in mind, so it will call HET variants, and it also not trained with subclonal variant fractions in mind. In addition, the training data for DeepVariant is diploid human data. So it hasn't really been designed to address the problem of calling on haploid microorganisms. External groups have benchmarked DeepVariant for bacterial variant calling alongside many other callers designed for this purpose (https://academic.oup.com/gigascience/article/9/2/giaa007/5728470). Given the limitations to DeepVariant mentioned above, it performs surprisingly well, but is not the most accurate pipeline. . It does, however, seem to have a high precision (DeepVariant with NextGenMap had the highest precision of any of the methods), but with lower recall. So I would not recommend DeepVariant as the only method used to analyze microbial data. However, given its high precision and given that it is a very different approach to other methods you might run, it could be of use to use DeepVariant as a secondary caller. In this use case, you would have a high degree of confidence in variants called by both DeepVariant and the alternative method. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/343#issuecomment-688064227:766,perform,performs,766,,https://github.com/google/deepvariant/issues/343#issuecomment-688064227,1,['perform'],['performs']
Performance,"Hi @kishwarshafin ,. Sure. Just a quick note first to explain the outputs, and maybe its relevant to the problem. Given the number of examples I couldn't easily perform the shuffling step on my local cluster (using DirectRunner) due to memory and wall time limits. So I performed a 2-step shuffle. I.e. I split the examples in half (parts 1 and 2), shuffled each half, then randomly split the outputs from each of the first shuffles into two halves (parts 3 and 4) and ran a second round of shuffling. . I then edited the path in the pbtxt file to accommodate all file names. So `All_samples_training_examples.dataset_config.pbtxt` now contains the following:. ```; > cat /home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py; # class0: 1454377. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt?-?????-of-?????.tfrecord.gz""; num_examples: 1454377. #name: ""Shuffle_global""; #tfrecord_path: ""/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training/examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt3-?????-of-?????.tfrecord.gz""; #num_examples: 727189. #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training/examples_shuffled/train/shuffle_2_inputs/All_samples_all_training_examples_inc_downsampled_05_pt3.shuffled-000*-of-00020.tfrecord.gz; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training/examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt3; #. # Generated by shuffle_tfrecords_beam.py. #name: ""Shuffle_global""; #tfrecord_path: ""/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training/examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:161,perform,perform,161,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,2,['perform'],"['perform', 'performed']"
Performance,"Hi @kishwarshafin,. Thank you for explaining the details of the down sampling process. - a. We perform a downsampling of reads on a given window to ensure the processing time is manageable. Some reads may be discounted during this process.; - b. For a specific variant, reads with lower mapping quality will not be counted.; - c. For a specific variant, reads that contain that variant with lower base quality than the set threshold are discounted. For positions with higher than 100x coverage, is the downsampling performed sequentially as mentioned above? ; If not, is it randomly or specific process?. Thank you for your assistance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/836#issuecomment-2230517419:95,perform,perform,95,,https://github.com/google/deepvariant/issues/836#issuecomment-2230517419,2,['perform'],"['perform', 'performed']"
Performance,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465225098:182,perform,performance,182,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098,4,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Hi @kyleoconnell in order to train a model you will need to establish ground truth data. This can be used both for training and evaluation purposes. Previously, DeepVariant has been applied to mouse data with a human model and performed well. However, even here you will want some basis upon which to determine the quality of the genotypes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459#issuecomment-850009279:227,perform,performed,227,,https://github.com/google/deepvariant/issues/459#issuecomment-850009279,1,['perform'],['performed']
Performance,"Hi @leorippel, DeepVariant has previously been applied to plant species. In the case of rice, there was good evidence of high accuracy. You can see [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant). However, these rice genomes were diploid and with a similar variant density of humans. DeepVariant is currently written to be a diploid variant caller. So if the plant species you are working with is polyploid, it is not yet clear how DeepVariant will perform. That said, I am also not sure how other variant callers perform on polyploid samples. It would be possible to train DeepVariant models for a specific genome, but this would require a gold set for the training. We have a previous example of this in mosquitos [in this blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357#issuecomment-698486292:546,perform,perform,546,,https://github.com/google/deepvariant/issues/357#issuecomment-698486292,4,['perform'],['perform']
Performance,"Hi @lucasbrambrink ,. I tried to recreate the building process, so here are some changes I made to the scripts:; ```; diff --git a/settings.sh b/settings.sh; index 9d5f58c0..649b1bb8 100755; --- a/settings.sh; +++ b/settings.sh; @@ -89,18 +89,18 @@ export DV_GPU_BUILD=""${DV_GPU_BUILD:-0}""; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; -export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; +export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-0}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""; -export DV_TF_NUMPY_VERSION=""1.19.2"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:384,optimiz,optimized,384,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['optimiz'],['optimized']
Performance,"Hi @may199128 ,. There can be several reasons for that:. > 1. PacBio Direct Data (top track): Despite having reads shown in both the IGV and VCF files, the genotype (GT) is marked as ""./.,"" and the genotype quality (GQ) is very low. This indel locus has higher coverage (141x) than the average coverage across the genome (128x). The lower GQ can be caused by several issues. If this is a variant dense region, then the model can be less confident in correctly calling the variant. Given that it looks like a structural variant, have you considered using a structural variant caller like [PBSV](https://github.com/PacificBiosciences/pbsv) for these cases?. > 2. For the PacBio Capture data (bottom track) has an average genome coverage of 2897x. Even after setting the mapping quality to >30, the read count at this indel locus is still 3000x, which is significantly higher than the read count indicated in the VCF file. Why is there such a discrepancy between the read counts in IGV and VCF?. The could be several reasons for seeing the difference: ; a. We perform a downsampling of reads on a given window to make sure the processing time is manageable. So, some reads can get discounted during that process.; b. For a specific variant, reads with lower mapping quality will not be counted.; c. For a specific variant, reads that contain that variant with lower base quality then set is discounted. Please note that our pileup image height is 100, so we can only represent 100 reads per variant within an example. Having 3000x would mean the reads will be downsampled to a fraction that can be represented in the example. Please let me know if you have any other questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/836#issuecomment-2189741503:1057,perform,perform,1057,,https://github.com/google/deepvariant/issues/836#issuecomment-2189741503,1,['perform'],['perform']
Performance,"Hi @mdriller . My answer to your question will depend on what exactly you will need from Plink and what sort of cohort approach you have. . If you just want to be able to run Plink on the joint genotype results, I wonder if you can try following the process which was performed for UKBiobank to convert their DeepVariant exome joint calls into PLINK format. That is the section **Conversion of pVCF to PLINK and BGEN files** [from the UKBiobank WES Protocol](https://biobank.ctsu.ox.ac.uk/crystal/ukb/docs/UKB_WES_Protocol.pdf). I hope this will work, as it is not generally our preference to replicate the functionality of -ERC BP_RESOLUTION, and this is likely to make writing output much slower. If, instead, you want calls at specific sites (similar to a genotyping chip approach but with NGS data), I would say that is is possible to force genotyping at a given set of alleles with one of the modules of DeepVariant (VCF candidate importer). I suspect this isn't what you want though. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172:268,perform,performed,268,,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172,1,['perform'],['performed']
Performance,"Hi @mikael-christensen . For somatic calling, we have a version of DeepVariant designed to call somatic variants. This performs well in simulated benchmarks and in benchmarks with COLO829. However, we want to get more feedback from external groups about its performance across real data for more cancer types before we feel comfortable generally releasing it. The time to train DeepVariant internally is a function of the amount and prioritization of training jobs on internal infrastructure. Generally it takes a few days to train the WGS model and is shorter for the WES and PacBio models. Training uses TPUs. I am not sure of the exact resource requirements in the production runs. However, external groups who have successfully trained DeepVariant have generally done so with much less data than what we train for in production, and often warmstart from one of our production models and re-train for their data. External groups have shown improvements for their workflows using GPUs and (I believe) on the order of a day or less of re-training time. The candidates created by the very sensitive caller are not genotyped, so comparing accuracy of the sensitive caller in the same way isn't quite possible. DeepVariant's VCF output will write every position which it generated a candidate for. For a 50x PCR-Free Illumina HG002 run, DeepVariant produced:. 0/0 calls - 934,097 ; ./. calls - 150,238; 0/1 calls - 2,793,521; 1/1 calls - 1,851,566; 1/2 calls - 78,194. This means that for about 19% of the positions which the sensitive caller proposes, DeepVariant does not believe there is sufficient evidence for a variant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241#issuecomment-559228691:119,perform,performs,119,,https://github.com/google/deepvariant/issues/241#issuecomment-559228691,4,['perform'],"['performance', 'performs']"
Performance,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488147736:18,Perform,Performance,18,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736,7,"['Perform', 'perform']","['Performance', 'perform', 'performance']"
Performance,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-524686835:1289,perform,performance,1289,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835,1,['perform'],['performance']
Performance,Hi @oschwengers . Thank you for your suggestion. I am curious if you have tried to run DeepVariant on haploid bacteria yet? We have investigated in-bred rice strains and found that it has a strong tendency to call homozygous sites as expected from the genome structure. I was curious if you have evaluated running DeepVariant on bacterial sequencing and seeing whether the results are reasonable. This could be valuable feedback to understand the differences in current performance versus expectations of the field. There may be some additional complexity in understanding how to express subclonal variants (these might manifest as HET calls). I don't have an intuition about the preferences of the field.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-490179510:470,perform,performance,470,,https://github.com/google/deepvariant/issues/183#issuecomment-490179510,2,['perform'],['performance']
Performance,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338#issuecomment-681126260:1095,bottleneck,bottleneck,1095,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260,2,['bottleneck'],['bottleneck']
Performance,"Hi @pgrosu,. After checking my region of interest, I came to the conclusion that is doesn't make sense to perform variant calling at all. Most of my SNVs fail several filters. Maybe at a later point I can get my hands on WGS data from the same samples to perform some comparisons.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1700437113:106,perform,perform,106,,https://github.com/google/deepvariant/issues/701#issuecomment-1700437113,2,['perform'],['perform']
Performance,"Hi @pichuan . Here is a helper shell script for that: https://raw.githubusercontent.com/anands-repo/deepvariant/r1.0/tools/post_eval.sh. Steps to use:; 1. Create a target empty working directory: `$WORKDIR`; 2. Launch model_eval with `--checkpoint_dir=$WORKDIR`, and direct output logs to `$WORKDIR/eval.log`; 3. Launch post_eval.sh as: `post_eval.sh $TRAINING_DIRECTORY $WORKDIR` where (`$TRAINING_DIRECTORY` is the original directory where checkpoints were dumped by model_train). The post_eval script copies one checkpoint after another to $WORKDIR from $TRAINING_DIRECTORY and modifies the ""checkpoint"" file in that directory, in coordination with model_eval, so that model_eval runs on one checkpoint after another just like when it is launched concurrently with training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378#issuecomment-721485772:750,concurren,concurrently,750,,https://github.com/google/deepvariant/issues/378#issuecomment-721485772,1,['concurren'],['concurrently']
Performance,"Hi @pichuan ; Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1780570310:130,perform,performed,130,,https://github.com/google/deepvariant/issues/720#issuecomment-1780570310,1,['perform'],['performed']
Performance,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]; This TensorFlow binary is optimized with oneAPI Deep Neural Network Library; (oneDNN) to use the following CPU instructions in performance-critical operations:; AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate comp; iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740#issuecomment-1826444464:400,optimiz,optimized,400,,https://github.com/google/deepvariant/issues/740#issuecomment-1826444464,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"Hi @pichuan, thanks for the explanation! I've been working on using DeepVariant to train my own sequencing-type specific model, and I'm getting to the point of wanting to mess with some of the different options available to improve performance. It sounds like `vcf_candidate_importer` would be less relevant for this purpose, but what about `unspecified_caller`? How does that option differ from `very_sensitive_caller`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/433#issuecomment-807115854:232,perform,performance,232,,https://github.com/google/deepvariant/issues/433#issuecomment-807115854,1,['perform'],['performance']
Performance,"Hi @pichuan,. Thanks for replying so quickly. I have tried both of those things. Doing `export TMPDIR=""$PWD/tmp_dir""` causes the same as **Error 1** above. . ```; INFO: Using cached SIF image; I0404 17:45:45.958600 23016861075264 run_deepvariant.py:345] Re-using the directory for intermediate results in /tmp/tmp_ui_t3bd. ***** Intermediate results will be written to /tmp/tmp_ui_t3bd in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmp_ui_t3bd/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /scratch1/rrautsa/deepvariant_test/tmp_dir/parXXXXX.par: Parent directory (/scratch1/rrautsa/deepvariant_test/tmp_dir/) does not exist at /usr/bin/parallel line 3889. real	0m0.257s; user	0m0.121s; sys	0m0.125s; ```. And perhaps I don't understand singularity enough, but it seems that no matter how I change the `--bind` input it gives me the following error:. ```; INFO: Using cached SIF image; FATAL: failed to open /bin/sh for inspection: failed to open elf binary /bin/sh: open /bin/sh: no such file or directory; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366:175,cache,cached,175,,https://github.com/google/deepvariant/issues/533#issuecomment-1088051366,2,['cache'],['cached']
Performance,"Hi @pioneer-pi . DeepVariant trains on all of chr1-chr19 and shuffles all together, as opposed to progressively going across chr1, chr2, etc... Because of some aspects of how training works, it's better to have the training batches be as uniformly sampled as possible. That corresponds to number 2 in your list. By ""clean"" DeepVariant, I assume that you mean to initialize from an InceptionV3 model that has been trained on ImageNet (that is only general images). This is possible, but you will need a large number of training examples, likely at least a few full whole genomes of examples, maybe something like 30 million created training examples, to get good performance. For most training experiments or setups, you will get better results training from an existing DeepVariant model checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/858#issuecomment-2261202112:662,perform,performance,662,,https://github.com/google/deepvariant/issues/858#issuecomment-2261202112,1,['perform'],['performance']
Performance,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:; `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687:101,load,load,101,,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687,2,['load'],['load']
Performance,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-821020591:476,optimiz,optimizing,476,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591,2,['optimiz'],['optimizing']
Performance,"Hi @splaisan,. That is a very complex experimental setup. I initially proposed freebayes + DV in order to merge and observe them together, for making proper selection in study-specific regions as a follow-up among the two results. Of course it would easier to write a program to compare between the two, filtered on thresholds appropriate to your design. In any case, even though [freebayes has gVCF output](https://github.com/freebayes/freebayes/blob/master/README.md#usage), let's ignore that for now and look closer at the GLnexus algorithm - as it gives nice insights about how variants are processed from gVCF files. Thus when it generates the pVCF, unified variant sites are collected from variants spanning overlapping regions. Since it performs $`\arg \max_{g} Pr(Genotype = g) Pr(Data | Genotype = g)`$, having multiple possible MNPs in the unified site (across gVCFs) dilutes the probability of each genotype, and thus having them as individual SNPs ensures the GQ and PL are maximally contributed to for each base. This is also because for such unified sites, it considers all discovered QC-filtered alleles greedily in descending order of frequency, and might be exacerbated with rare alleles when calling sites. Therefore given the above, utilizing the combined output of the VCF for post-processing into MNPs might be optimal. Ideally, having phasing information would also provide more confidence in the quality of MNPs from combined adjacent SNPs belonging to a haplotype. If possible, limiting via a BED file to regions that are more relevant for your study - especially under heavy multi-threaded conditions - might shorted the analysis time. With many threads you might be thrashing, thus limiting your performance by spending more time context-switching. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-1703129794:744,perform,performs,744,,https://github.com/google/deepvariant/issues/238#issuecomment-1703129794,3,"['multi-thread', 'perform']","['multi-threaded', 'performance', 'performs']"
Performance,"Hi @spz1st ,; Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that?; I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740#issuecomment-1828442518:202,optimiz,optimized,202,,https://github.com/google/deepvariant/issues/740#issuecomment-1828442518,1,['optimiz'],['optimized']
Performance,"Hi @tzcoolman,. A few things:. $`1)`$ Yes, it can take a long time, as shown here:. https://github.com/google/deepvariant/blob/r1.5/docs/metrics.md#pacbio-hifi. $`2)`$ Yes, `make_examples` is single-threaded and has multiple stages. You can adjust the parallelism distribution indirectly through the number of shards, which can either match the number of chromosomes (or if more then the sub-regions):. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#make_examples. https://github.com/google/deepvariant/blob/r1.5/docs/runtime-by-region.md. $`3)`$ Be careful when adjusting the `vsc_min_*` values $`-`$ yes, higher values will make it faster $`-`$ but the reason those were adjusted in post #578 is because the reference was also lower quality, and you might miss some candidates doing so. $`4)`$ One way to make things faster would be to balance the CPU core and thread workloads. CPUs with operating systems have limits at how much they can context switch, before they spend more time resource managing these threads, which is called [thrashing](https://blog.netdata.cloud/understanding-context-switching-and-its-impact-on-system-performance/). A trick you could do is to run DeepVariant with the `--dry_run` parameter, in order to retrieve the individual commands being run. Then you can run the `make_examples` step, adjusting the parameters for `parallel` for either CPU cores or threads, as now the number of jobslots (-j) is its preferred method. `parallel` loves threads and the jobslot (`-j`) argument tries to balance CPU/threads, but the key word here is $`tries`$. In fact, you can force it one way or anther, but you will have to test that empirically in order to see what gets you the best results for your machine. The list of the parameters for parallel are shown on the following page:. https://man.linuxreviews.org/man1/parallel.1.html. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683#issuecomment-1644260839:1160,perform,performance,1160,,https://github.com/google/deepvariant/issues/683#issuecomment-1644260839,1,['perform'],['performance']
Performance,"Hi @westonelison ,. Thank you for the question. We have experimented with single cell models, but not with ATAC seq specifically. I think the most similar model should be the RNAseq model. I don't have an instinct for how this might perform.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/895#issuecomment-2422850974:233,perform,perform,233,,https://github.com/google/deepvariant/issues/895#issuecomment-2422850974,1,['perform'],['perform']
Performance,"Hi @yexiao2016z, that's very exciting to hear. The best place to start is https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image.py#L244 which is the python function build_pileup() that we use to create the pileup tensor. Going down the function call stack will show you everything needed to build the tensors. There's a high-performance C++ piece of code that actually constructs the tensor from a pile of reads here as https://github.com/google/deepvariant/blob/r0.4/deepvariant/pileup_image_native.cc that will ultimately be called by pileup_image.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/23#issuecomment-353986978:344,perform,performance,344,,https://github.com/google/deepvariant/issues/23#issuecomment-353986978,1,['perform'],['performance']
Performance,"Hi @zoeward-nz . We're close to a next release of DeepVariant and DeepTrio that will make some improvements to each, some of which will be relevant here. I assume you are using PacBio data here. . DeepTrio with PacBio will still require you to haplotag the BAM files with whatshap in the two-stage process as before. DeepTrio will give better overall accuracy than DeepVariant for the trios, especially at lower coverages. The coming update will also improve performance for de novo variants. It should be the most accurate way to analyze the sample, but because you will need to do the call-phase-call approach, and because you will need to manage the non-PAR regions, it will be slower and less convenient. To run on non-PAR with DeepTrio, the recommendation is to run DeepTrio with a --regions that covers all autosomes and the PAR region with both samples. For the non-PAR regions, it would be to use the --regions flag to run the non-PAR ChrX with only the child and mother sample (omit the father's BAM). You won't need to modify the BAM files themselves by using the --regions flag, but you will need to run twice. You may find this too cumbersome, running DeepVariant on the samples, especially with 30x coverage, should give highly accurate results as well. In the coming release, there will be flags which enable you to specify non-PAR regions for male samples. . In terms of making a decision of which to run, you might want to start with DeepVariant and look at the performance characteristics that you care most about (e.g. number of de novos calls). If you are finding too many de novo calls difficult to wade through, you might want to go ahead and try the more involved trio process. . Thank you and please let me know if you have any questions,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/712#issuecomment-1741642049:459,perform,performance,459,,https://github.com/google/deepvariant/issues/712#issuecomment-1741642049,2,['perform'],['performance']
Performance,"Hi @zyxue ; since your questions seem to be no longer relevant to the original question, I'll close this issue. Internally we've made the code updates to address the original issue, and will come out in a next release. It also seems like the local changes that @depristo suggested works for you now. Regarding your latest questions:; (1) There are areas in the genome that do take longer to run. This is usually areas where more reads piled up. We've made a lot of speed optimization over time, but it's still certainly true that will be regions that seem to run for much longer.; (2) Case study says ""no GPU"" in the sentence you quoted. Currently the case studies (WGS and WES) were both CPU only.; (3) For how to run with GPU, see this previous issue: https://github.com/google/deepvariant/issues/81",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428646375:471,optimiz,optimization,471,,https://github.com/google/deepvariant/issues/99#issuecomment-428646375,1,['optimiz'],['optimization']
Performance,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/705#issuecomment-1708658232:352,perform,performed,352,,https://github.com/google/deepvariant/issues/705#issuecomment-1708658232,1,['perform'],['performed']
Performance,"Hi Amy,. As Pi-Chuan mentioned is good. I only see a total of 14 reads supporting at position 41,570,158 of chromosome 15 -- hopefully its the same BAM file as the one used in DeepVariant. . Now regarding IGV here are a few things:. $`1)`$ Under View > Preferences > Alignments set the following three:. Uncheck ""Downsample reads"" like this:. ![image](https://github.com/google/deepvariant/assets/6555937/4f10a4d9-a26f-432b-adef-79095c0536b2). Check ""Show mismatch bases"":. ![image](https://github.com/google/deepvariant/assets/6555937/d0543c66-0737-4a42-bf17-35e45c48ed50). Check ""Label indels > threshold"", and have a value of 0 (and uncheck Hide indels):. ![image](https://github.com/google/deepvariant/assets/6555937/c9354639-0915-470d-b073-35817549c50c). $`2)`$ Under View > Preferences > Third Gen use these settings:. Here again perform the following:. * Uncheck ""Downsample reads""; * Check ""Label indels > label threshold"", and have a value of 0 ; * Uncheck ""Hide indels < indel size threshold"" . ![image](https://github.com/google/deepvariant/assets/6555937/46777d27-0098-4899-ba9b-8c6fc0e3baa6). Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1662692855:836,perform,perform,836,,https://github.com/google/deepvariant/issues/691#issuecomment-1662692855,1,['perform'],['perform']
Performance,"Hi Amy,. Great! IGV is fine. Now just look in your VCF file for the region in question, and try to confirm with what you see in IGV using the realigned BAMs. For example, you had the following variant the last time:. chr15 | 41570158 | . | T | C | 6.5 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:7:23:14,9:0.391304:5,0,46; -- | -- | -- | -- | -- | -- | -- | -- | -- | --. Do you still see that variant in your current VCF file, and do the number of variations match (or exceed because of base quality) in IGV for that position? Given the counts for the alternate allele (C) the last time, it would need to have been at least 9, but I just see 1 in your picture. Does the VCF this time also reflect 1 (or less), or does it still say 9 for the alternate allele (C) as the last time?. Basically the numbers have to confirm each other between the realigned BAM and current VCF if realignment has been performed for that region, or the original BAM and current VCF if no realignment was performed for that region. Ideally as a first pass check multiple regions to be sure, but start with our original variant (T/C at chr15:41570158) as a known starting point. If they confirm each other then that's awesome, otherwise we would need to investigate the issue deeper. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1669660043:891,perform,performed,891,,https://github.com/google/deepvariant/issues/691#issuecomment-1669660043,2,['perform'],['performed']
Performance,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454:1175,load,loaded,1175,,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454,1,['load'],['loaded']
Performance,"Hi Andrew and Steve,. That's a good idea, and probably shifting to pass-by-reference might minimize the overhead between functions or even through minimizing the number of functions. A shared memory approach operated upon by the different functions could be ideal with maximizing performance, where even utilizing [NVidia's Unified Virtual Memory (UVM)](https://developer.nvidia.com/blog/maximizing-unified-memory-performance-cuda/) could help, but requires some care. Currently the `call_variants` memory profile looks something like this, with minimal changes across the running process:. ```; types | # objects | total size; ================================================================ | =========== | ============; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.clie",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:280,perform,performance,280,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,4,['perform'],"['performance', 'performance-cuda']"
Performance,"Hi Andrew,. I think we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:157,perform,performed,157,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,8,"['optimiz', 'perform']","['optimization', 'optimize', 'performed', 'performs']"
Performance,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:753,perform,perform,753,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,2,['perform'],['perform']
Performance,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391790774:437,cache,cache,437,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774,2,"['cache', 'optimiz']","['cache', 'optimized']"
Performance,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483500506:611,optimiz,optimized,611,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506,2,['optimiz'],['optimized']
Performance,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:1359,perform,perform,1359,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604,1,['perform'],['perform']
Performance,"Hi Charles,; there are a few 3rd party Cloud orchestrations for DeepVariant that you can consider. I'll provide the links below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:368,optimiz,optimized,368,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['optimiz'],['optimized']
Performance,"Hi Edoardo, in the current implementation, `DP=0` (which is used in variant records) will imply that there is no read covering that position, while `MIN_DP=0` (used in hom. ref. band in gVCF) does not necessarily imply that there is no read covering the reference band, due to consolidation of multiple hom.ref. records into one record for the reason stated in my previous response (as the name suggests, it means the *minimum* of DPs in all of the consolidated records were 0). Please also note that reads that don't satisfy the minimum mapping quality threshold (set to 5 by default [here](https://github.com/google/deepvariant/blob/r1.0/deepvariant/make_examples.py#L219)), don't count for DP/MIN_DP as far as I know. Again, we agree with your suggestion about separating out the `DP=0` regions into a separate record and setting the genotype to `./.`. The performance issue you mentioned is not expected as far as I know, and I asked my colleague to follow up on that aspect. I hope this help and please let me know if I can help with anything else. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-695121760:860,perform,performance,860,,https://github.com/google/deepvariant/issues/346#issuecomment-695121760,1,['perform'],['performance']
Performance,"Hi Fra,. If it cannot find candidates, that happens during the `make_examples` stage. `make_examples` goes through an allele counter, and very sensitive variant caller to determine if there is enough read support for a candidate. Was the same genome (fasta) used for the alignment phase (from fastq) when generating the BAM files? When you load your BAM file in IGV to how many chromosomes and chromosomal regions does it show alignment for? . In the [quickstart tutorial](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md#run-deepvariant-with-one-command) it uses the WGS model not the WES one (as indicated by `--model_type=WES` above). Also I believe your BAM file is only 300 Mb, which a bit small compared to other WES ones:. https://ega-archive.org/datasets/EGAD00001005247/files. Is your dataset a WES or WGS dataset? . Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1651377100:340,load,load,340,,https://github.com/google/deepvariant/issues/675#issuecomment-1651377100,1,['load'],['load']
Performance,"Hi Fra,. So let me go through a few items:. $`1)`$ It fails to find variant candidates in the `make_examples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:850,perform,perform,850,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175,1,['perform'],['perform']
Performance,"Hi Josh,. There are multiple possible explanations.; 1. DeepVariant performs a local realignment which may change how reads are aligned. There is a section in FAQ which describes how to output a realigned BAM file [here](https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work). For that you can run make_examples.py with --region flag that would generate examples for your specific region. I recommend to set the region to 1000 bases interval (for example chr1:2001-3000). Then you may examine a realigned BAM with IGV.; 2. DeepVariant does not take into account positions with base quality lower than the threshold. What can happen is that certain positions are low quality and therefore not counted towards allele depth. Hopefully this will help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/516#issuecomment-1034107962:68,perform,performs,68,,https://github.com/google/deepvariant/issues/516#issuecomment-1034107962,1,['perform'],['performs']
Performance,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-839642012:158,perform,performed,158,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012,3,['perform'],"['performed', 'performing', 'performs']"
Performance,"Hi Maria,. Thank you for your suggestions. I am getting closer to running but I have not quite succeeded yet. Here is where I am at the moment :; - I pulled the deepvariant image of : https://cloud.sylabs.io/library/_container/6009c4bebccfe9cf45792add; - Our cluster runs pbs so I need to create a submission script for this.; - We also have singularity; - I am trying to conduct variant calling of a pacbio dataset against a reference. Lets say my 'deepvariant_v1.0.0.sif' file is sitting in path_a; my reference sequence in path_b; my pacbio bam in path_c. Could you advise on the singularity command to execute this job?. Below is what i've tried but I must be missing and misunderstanding a few key elements.; =====; module load chpc/singularity. singularity run -B /mnt/lustre3p/groups/CBBI0843:/mnt/lustre3p/groups/CBBI0843 /mnt/lustre3p/groups/CBBI0843/deepvariant_v1.0.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO; --ref=""References/Panu3.0_X_Y_Mito.fa"" \; --reads=/mnt/lustre3p/groups/CBBI0843/IB_Sequel_IIe_data/r64187e_20210614_132743/3_C01/m64187e_210617_061402.reads.bam \; --output_vcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.vcf.gz \; --output_gvcf=/mnt/lustre3p/users/hganesan/210614_Cell3_MT18_output.g.vcf.gz \; ====. Thank you for your insights. Kind regards,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474#issuecomment-885520428:728,load,load,728,,https://github.com/google/deepvariant/issues/474#issuecomment-885520428,1,['load'],['load']
Performance,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-637264990:445,tune,tune,445,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990,1,['tune'],['tune']
Performance,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355440557:273,optimiz,optimization,273,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557,5,"['Optimiz', 'optimiz', 'perform']","['Optimization', 'optimization', 'optimizations', 'performance']"
Performance,"Hi Masaru,. Check out my analysis from a while ago on the link below, and I think you'll see where this is stemming from :). [Generalized performance analysis between the versions](https://github.com/google/deepvariant/issues/50). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/147#issuecomment-460334066:138,perform,performance,138,,https://github.com/google/deepvariant/issues/147#issuecomment-460334066,1,['perform'],['performance']
Performance,"Hi Masaru,; we have not tried loading the model checkpoints this way before.; Since it's a colab, can you share the colab so we can try it out as well?. I might have some older colab lying around that loads the checkpoint, but probably not using Keras. Would that be helpful? If so I can try to find it again. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-445917254:30,load,loading,30,,https://github.com/google/deepvariant/issues/127#issuecomment-445917254,2,['load'],"['loading', 'loads']"
Performance,"Hi Min-Zhi,. 1. DeepVariant works well across a wide range of coverages. [This blog post](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/) provides some analysis on what performance looks like across different coverages. 2. There are a few questions here, so I'll refer you to 2 docs that I believe answer all of them:; 	- [Blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) ; 	- [Training Case Study](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md). 	But to briefly address the individual questions:; 	- How data is organized: [this doc](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md) outlines the data that we are using.; 	- Your understanding is essentially correct, but see blog post linked above (and maybe [this one too](https://blog.dnanexus.com/2019-02-19-deep-dive-into-deepvariant/)) for detail.; 	- The case study gives numbers on how long it takes to train a model with certain computational resources. 3. We provide [a few different ways](https://github.com/google/deepvariant#official-solutions) to use DeepVariant, and our recommended way is to use one of our pre-trained models through Docker. [This section](https://github.com/google/deepvariant/tree/r0.10/docs#quick-start-and-case-studies) shows you how to use our Docker images to get started with calling right away. I know I've linked to a lot of docs, but there's a lot of context around each question you asked, so it would probably be easiest to go through those first. If you want more detail on a specific question, please feel free to follow-up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/320#issuecomment-647629571:264,perform,performance,264,,https://github.com/google/deepvariant/issues/320#issuecomment-647629571,1,['perform'],['performance']
Performance,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993:137,perform,perform,137,,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993,1,['perform'],['perform']
Performance,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353701703:677,optimiz,optimization,677,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703,2,['optimiz'],['optimization']
Performance,"Hi Pichuan,. You can close this issue now. I will try with different samples. I tried to lower the learning rate but it still does not exceed the performance of default model. . I will have to train on different samples. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016:146,perform,performance,146,,https://github.com/google/deepvariant/issues/802#issuecomment-2057623016,2,['perform'],['performance']
Performance,"Hi Ram,. You really do not want to have any errors show up in any of your tests, especially when performing variant calling. It's an easily fixable issue where the Google folks would just need to update their TensorFlow Python package distributed with DeepVariant, which was probably compiled with an earlier version of protobuf, while currently the Tensorflow requires `3.6` of protobuf and the Protobuf version being compiled with DeepVariant is `3.6`. Looking at your log file, you are using the CPU version. The current version on PyPI of TensorFlow is 1.10.1, and DeepVariant uses version 1.9. Here's the process by which I went about to determine what is happening:. 1. If you download both `whl` files of Tensorflow like this:. ```; wget https://storage.googleapis.com/deepvariant/packages/tensorflow/tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl. wget https://files.pythonhosted.org/packages/1a/c4/8cb95df0bf06089014259b25997c3921a87aa08e2cd981417d91ca92f7e9/tensorflow-1.10.1-cp27-cp27mu-manylinux1_x86_64.whl; ```. 2. Next rename each of the `whl` files to `zip`, and then uncompress them in separate directories. If you then look at one of the version `1.10.1` files that has the `serialized_options` such as `purelib/tensorflow/core/framework/resource_handle_pb2.py`, you will see the following difference between the versions, where the `serialized_options` keyword is present in the `1.10.1` version, but not in the `1.9.0` version:. #### _*For version 1.9*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423037408:97,perform,performing,97,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408,1,['perform'],['performing']
Performance,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:637,optimiz,optimized,637,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771,2,['optimiz'],['optimized']
Performance,"Hi Shalabh,. We have performed internal benchmarks on merging of gVCFs with GATK and do not recommend this approach. I think it will be better to wait for recommendations on merging with GLnexus, or to collaborate directly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-460945051:21,perform,performed,21,,https://github.com/google/deepvariant/issues/142#issuecomment-460945051,1,['perform'],['performed']
Performance,"Hi Sophie,. So tuning a model is both an art and a science. Yes, accuracy can be lower initially, as in the following result described [by Pi-Chuan in another post](https://github.com/google/deepvariant/issues/185#issuecomment-494919509):. ![image](https://github.com/google/deepvariant/assets/6555937/bf862317-f323-47eb-bd69-6fe255e3223e). Thus no training parameters are the same for any model, since the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:503,optimiz,optimizing,503,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,5,"['Optimiz', 'optimiz', 'perform']","['Optimization', 'optimizing', 'performed']"
Performance,"Hi Sophie,. This is great news! Yes, usually that is the expected behavior after several rounds of training. Let me answer each aspect separately:. $`1)`$ So given a trained model, you can specify a custom checkpoint file via `--customized_model`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051:719,perform,performing,719,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051,2,['perform'],['performing']
Performance,"Hi Sophie,. You have a few of options:. 1) The first option is like Andrew mentioned [in a previous post](https://github.com/google/deepvariant/issues/377#issuecomment-720908040), by running DeepVariant on each of them and use GLnexus to merge them and identify the de novo mutations from the joint call file. 2) DeepTrio outputs the child VCF as noted by the flag `--output_vcf_child`. Then you would need to compare those variants across multiple samples (with some truth sets) against the parents, to ensure they are truly DNM and are not false positives. That is quite a bit of work to perform properly. 3) You can use things external tools, of which there are many :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1684463157:590,perform,perform,590,,https://github.com/google/deepvariant/issues/699#issuecomment-1684463157,1,['perform'],['perform']
Performance,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/42#issuecomment-360510853:155,perform,performed,155,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853,1,['perform'],['performed']
Performance,"Hi!. Made some optimizations and tested on chr20 from https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-case-study.md. Can you please tell me if it's a representative launch?. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 3m7.950s | 5m59.221s | 1m5.724s |; | OpenVINO | 3m6.239s | 3m46.756s (x1.58) | 1m7.640s |. (""real"" times are in the table). ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./reference/GRCh38_no_alt_analysis_set.fasta \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr20"" \; --call_variants_extra_args=""use_openvino=True""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-724316304:15,optimiz,optimizations,15,,https://github.com/google/deepvariant/pull/363#issuecomment-724316304,1,['optimiz'],['optimizations']
Performance,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916:118,Queue,Queue,118,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916,7,"['Queue', 'queue']","['Queue', 'queues']"
Performance,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:504,optimiz,optimized,504,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"Hi, glad to see my issue is helping you.; Since DeepVariant performs local-realignment on indel sites, it is possible that DeepVarianat sees a variant site differently with CIGAR in BAM file.As shown in channel 6 of this site, there's another site very close, sometimes DeepVariant treats an insertion-deletion haplotype as an snp-snp one.; It would be helpful if you can provide the variant information along with the referred example.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379#issuecomment-723720026:60,perform,performs,60,,https://github.com/google/deepvariant/issues/379#issuecomment-723720026,1,['perform'],['performs']
Performance,"Hi, pichuan.; Thanks for your help. I just change the name of tfrecord file to shuffle the training dataset. Is it OK to use this way to shuffle the training dataset? This way is introduced from ""Improve DeepVariant for ; BGISEQ germline variant calling"" file. The accuracy of trained model have raised compared with trained model without shuffling. But I'm not sure whether it is the highest accuracy performance. I don't know how to shuffle on Spark. Maybe I would try to shuffle on Spark in future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91#issuecomment-423019414:402,perform,performance,402,,https://github.com/google/deepvariant/issues/91#issuecomment-423019414,1,['perform'],['performance']
Performance,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/174#issuecomment-487190949:263,perform,performed,263,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949,1,['perform'],['performed']
Performance,"Hi, thanks for reporting this issue. If you used a GCE instance, can you share the command you used to start a cloud machine? . And, if you look at your `""${LOG_DIR}/call_variants.log""`, you should be able to see lines like these:; ```; I0405 16:03:16.308625 140490269800192 call_variants.py:353] Processed 4680192 examples in 146256 batches [0.67 sec per 100]; I0405 16:03:16.524780 140490269800192 call_variants.py:353] Processed 4680224 examples in 146257 batches [0.67 sec per 100]; ```. Can you tell me what your ""sec per 100"" is? This will also confirm your speed for call_variants. I'm guessing it's much slower than 0.67 sec per 100. You should also watch your systems resources -- is there enough RAM, etc. And, I would also suggest that you check out the [cost- and speed-optimized, Docker-based pipelines](https://cloud.google.com/genomics/deepvariant) created for Google Cloud Platform. Case studies are created so that the users can understand the key components of DeepVariant, but if you want to look for production-grade performance, you should consider the Cloud pipelines instead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391092118:782,optimiz,optimized,782,,https://github.com/google/deepvariant/issues/74#issuecomment-391092118,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"Hi, thanks for responding. This is very helpful. Definitely will be interested in collaborating on this. I will get back to you with more information on that.; Also, apart from GLnexus, have you performed any testing using GATK4 to merge gVCFs (from Deepvariant) into final VCF? . Shalabh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-460720523:195,perform,performed,195,,https://github.com/google/deepvariant/issues/142#issuecomment-460720523,1,['perform'],['performed']
Performance,"Hi,. Couple of points:; * DeepVariant model is created specifically for a human genome. It may not perform optimally on a non-human data. ; * It looks like candidate was not created at all so this is not a model issue. I cannot say exactly why candidate was not created but having the exact command line may help. ; * By default DeepVariant performs a local realignment when creating candidates. With this coverage it may not work as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/366#issuecomment-716197107:99,perform,perform,99,,https://github.com/google/deepvariant/issues/366#issuecomment-716197107,2,['perform'],"['perform', 'performs']"
Performance,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383347052:435,perform,perform,435,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052,2,['perform'],['perform']
Performance,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204#issuecomment-518518311:250,perform,perform,250,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311,4,"['optimiz', 'perform']","['optimizing', 'perform']"
Performance,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:13018,cache,cachedCount,13018,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,12,"['Cache', 'cache', 'load']","['CacheDB', 'cache', 'cachedCount', 'cachedDuration', 'loadCpus', 'loadMemory']"
Performance,"I assign this to my self, but --; since we're not using Keras in DeepVariant, it will take a uncertain amount time for me to prioritize this on my list of things. I will also open an internal bug to track, but this is something that we likely won't be able to help with in the short term. If you want to take a look at how we load the checkpoint, you can find examples in the inference code here:; https://github.com/google/deepvariant/blob/r0.7/deepvariant/call_variants.py#L346; And for training, in order to start from a checkpoint, you can see code in this:; https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L408. We use the Estimator API. So, another possibility to find more information for things that might help with converting any TensorFlow models to a Keras model, such as:; https://github.com/keras-team/keras/issues/5273; (I don't know if this one helps, but worth a look). I will keep this issue open for a while. Please feel free to share back if you find anything. I'll also update if I find anything useful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/127#issuecomment-446036697:326,load,load,326,,https://github.com/google/deepvariant/issues/127#issuecomment-446036697,1,['load'],['load']
Performance,"I get the same error. There appears to be Cuda 12 modules being used even though Cuda 11 is installed? Adding '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_nvrtc/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cudnn/lib' lets the image see all these libraries and stops the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file"" error. . But now during the call_variants step, I get a new error --> ""2024-03-11 23:23:22.756430: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:433] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED; 2024-03-11 23:23:22.756490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Possibly insufficient driver version: 470.57.2"". Just doing '--env LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/lib/python3.8/dist-packages/nvidia/cublas/lib' also seems to get past the ""Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12:"" error, but now I get ""could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error"" during the call_variants step. The 1.6.0-gpu image appears to have some mixed cuda 11 & cuda 12 module library conflict issues. Some of these issues may be suppressed if you have a sufficient nvidia driver for both cuda 11 and cuda 12 (I'm still waiting to test that though).; Note that 1.5.0 has no issues running for me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785:479,load,load,479,,https://github.com/google/deepvariant/issues/761#issuecomment-1990592785,2,['load'],['load']
Performance,"I got the following to work (I haven't tested to the end, but successfully reached calling variants). I'm going to keep iterating and seeing how much of these commands I can remove. Opening singularity through a shell; `singularity shell --no-home --cleanenv --containall -B tmp:/tmp,input:/input,output:/output deepvariant_1.1.0.sif`. Changing directory to something I've bound (e.g. ouptut), and then running call_variants; ```; cd ouput; ../opt/deepvariant/bin/call_variants \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@24.gz"" \; --outfile ""/output/intermediate_results_dir/call_variants_out.tfrecord.gz"" \; --checkpoint ""/opt/models/pacbio/model.ckpt"" \; --use_openvino; ```; At this point, we made progress with the following; ```; Model Optimizer version: 	. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /output/./model.xml; [ SUCCESS ] BIN file: /output/./model.bin; [ SUCCESS ] Total execution time: 30.04 seconds. ; [ SUCCESS ] Memory consumed: 699 MB; ```; With persistent model.bin/mapping/xml files in my output folder. I tried unsuccessfully with different binds to be able to write files to the `/` directory in the container, which is where the openvino graph is being written. It may be related to the directory structure on our cluster not meshing well with the container.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-763490283:769,Optimiz,Optimizer,769,,https://github.com/google/deepvariant/issues/404#issuecomment-763490283,1,['Optimiz'],['Optimizer']
Performance,"I guess the naming pattern is related to second question.; Say I have a 50x depth BAM file that I want to downsample to 20%. I can squeeze about 5 downsampled BAMs out of 50x.; Given I assume i will perform make_examples with ""--training"" within a loop of say 5 iterations, what would the naming scheme look like? Hence the importance of the seed parameter if downsampling same BAM mulitiple times within a loop; i would change seed each time... I hope this clears up the questions and motivations behind them....",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188:199,perform,perform,199,,https://github.com/google/deepvariant/issues/765#issuecomment-1909090188,2,['perform'],['perform']
Performance,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:; ```; docker run \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:; ```; docker run \; -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/index/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251:107,load,load,107,,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251,1,['load'],['load']
Performance,"I think the easiest way to do it is as follows:; 1. Install VirtualBox on your laptop, then Ubuntu, then Docker.; 2. Pull the DeepVariant image, and then follow the instructions at the following link on how to modify the image and committing it:; https://www.techrepublic.com/article/how-to-commit-changes-to-a-docker-image/; 3. Perform `docker save` to be able to transfer it so you can convert it to a Singularity image - with a link provided below on how to use `docker save`:; https://docs.docker.com/engine/reference/commandline/save/. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458642985:329,Perform,Perform,329,,https://github.com/google/deepvariant/issues/132#issuecomment-458642985,1,['Perform'],['Perform']
Performance,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting!; I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it.; It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256#issuecomment-568673024:308,optimiz,optimizing,308,,https://github.com/google/deepvariant/issues/256#issuecomment-568673024,1,['optimiz'],['optimizing']
Performance,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-464903381:17,perform,performing,17,,https://github.com/google/deepvariant/issues/157#issuecomment-464903381,1,['perform'],['performing']
Performance,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU @ 2.30GHz; Stepping: 0; CPU MHz: 2300.000; BogoMIPS: 4600.00; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 46080K; NUMA node0 CPU(s): 0-15; ```. I then get 16 messages like this:; `; Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461090328:646,cache,cache,646,,https://github.com/google/deepvariant/issues/150#issuecomment-461090328,4,['cache'],['cache']
Performance,"I'm seeing an OOM in the logs:; ```; OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]; ```; It also shows your training params:; ```; Training Examples: 8264746; Batch Size: 16384; Epochs: 1; Steps per epoch: 504; Steps per tune: 1500000; Num train steps: 504; ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696:290,tune,tune,290,,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696,1,['tune'],['tune']
Performance,"ILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:13491,cache,cache,13491,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['cache'],['cache']
Performance,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:2909,cache,cached,2909,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725,2,['cache'],['cached']
Performance,"If you could describe your environment in more detail (which OS, CPU), and the full sequence of operations you performed (copy and paste the text from the terminal), it will help us to diagnose the problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-351298307:111,perform,performed,111,,https://github.com/google/deepvariant/issues/16#issuecomment-351298307,1,['perform'],['performed']
Performance,"In the current release, training occurs on chr1-19 using the v3.3.2 benchmark set for HG001, HG005-HG006 using BAM files mapped to GRCh37 (it's important to use GRCh37 when benchmarking with v3.3.2 as there are liftover artifacts on GRCh38 since v3.3.2 was not constructed on this truth set. v4.2 regions are used in training with HG002-HG004 in the current version with GRCh38. With the next version, we plan to fully withhold HG003 from training for all data types. chr20 is never used for training or model selection or anything that would influence model performance. It is a fully held-out test set. chr21 and chr22 are not used for training, but the performance on chr21 and chr22 are used in a tune set that identifies at what checkpoint to select the model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728683346:559,perform,performance,559,,https://github.com/google/deepvariant/issues/381#issuecomment-728683346,3,"['perform', 'tune']","['performance', 'tune']"
Performance,"Interestingly, if I run `cd bin; ./run-prereq.sh` (without `sudo`) I get a permission error:; ```; ========== [Fri Jan 26 11:13:59 EST 2018] Stage 'Install TensorFlow pip package' starting ; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel ; Copying gs://deepvariant/packages/tensorflow/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl...; OSError: Operation not permitted. ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-360833579:224,optimiz,optimized,224,,https://github.com/google/deepvariant/issues/41#issuecomment-360833579,1,['optimiz'],['optimized']
Performance,"It looks like you are trying to analyze the graph outside of DeepVariant, which we don't currently have documentation on, since it isn't needed to run DeepVariant or even train it. But we do have some interpretability research we are working on, so stay tuned for that on the blog!. In the meantime, can you give a bit more context on what you are trying to do? Where did you find that code snippet?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339#issuecomment-681163396:254,tune,tuned,254,,https://github.com/google/deepvariant/issues/339#issuecomment-681163396,1,['tune'],['tuned']
Performance,"It seems like not having connection to ""storage.googleapis.com"" is the issue. And it seems like it's trying to get an optimized tensorflow wheel.; What if you try with `DV_USE_GCP_OPTIMIZED_TF_WHL=0` ?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416439549:118,optimiz,optimized,118,,https://github.com/google/deepvariant/issues/89#issuecomment-416439549,1,['optimiz'],['optimized']
Performance,"Just following up on this, is the CNN optimized solely for human variant calling or should it deliver similar success on non-model organisms such as bacteria or protists?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/3#issuecomment-350808367:38,optimiz,optimized,38,,https://github.com/google/deepvariant/issues/3#issuecomment-350808367,1,['optimiz'],['optimized']
Performance,"Merging VCFs can be done using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for use with DeepVariant gVCFs. The process is described in the DeepTrio case studies ([DeepTrio whole genome sequencing case study](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-wgs-case-study.md) and [Using DeepTrio for small variant calling from the trio sequenced with PacBio HiFi](https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md)), and in the manuscript, [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). --output_gvcf_merged flag in run_deeptrio.py script is not supported.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/544#issuecomment-1179387163:97,optimiz,optimized,97,,https://github.com/google/deepvariant/issues/544#issuecomment-1179387163,2,"['optimiz', 'scalab']","['optimized', 'scalable']"
Performance,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428649613:223,load,loads,223,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613,1,['load'],['loads']
Performance,"OK. I noticed that my `pyclif_proto` is in /usr/local/bin/, not /usr/local/clif/bin. Not knowing if that really is an issue, I did the following:; ```; sudo ln -sf /usr/local/bin/pyclif_proto /usr/local/clif/bin/pyclif_proto; ```. And added that to my [experimental build-prereq.sh](https://gist.github.com/pichuan/7928d101a730c03167b6d80c9c3c58ac). Now I'm seeing a different error:; ```; (06:15:00) INFO: Found 80 targets and 33 test targets...; (06:15:00) ERROR: /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/external/nsync/BUILD:441:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 1): gcc failed: error executing command; (cd /home/pichuan/.cache/bazel/_bazel_pichuan/01047f0bd74be1f8c2eae71c8557726c/execroot/com_google_deepvariant && \; exec env - \; PATH=/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/pichuan/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python2.7 \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/site-packages \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -Wno-maybe-uninitialized -Wno-unused-function -msse4.1 -msse4.2 -mavx -O3 -MD -MF bazel-out/k8-opt/bin/external/nsync/_objs/nsync_cpp/external/nsync/internal/common.pic.d -fPIC -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/bazel_tools/tools/cpp/gcc3 -x c++ '-std=c++11' -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/gcc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386513685:481,cache,cache,481,,https://github.com/google/deepvariant/issues/29#issuecomment-386513685,2,['cache'],['cache']
Performance,"Okay, I see.; Yes, the PacBio model is meant for HiFi reads only, but also DeepVariant is not going to perform well with a single read. For one thing, the very sensitive caller that finds candidates looks for 2 reads showing the same potential variant allele, but also the models are trained with coverages only as low as about 15X, so even if you changed the thresholds for candidate generation to 1 read, the model would probably call it not-a-real-variant (class 0).; When you saw that DeepVariant finished but created 0 candidates, that is the expected behavior since the coverage is too low to create any candidates with 2+ supporting reads. You might want to check with PacBio what they recommend for calling variants from LAA, since I don't think DeepVariant would be your best choice for this type of data. Good luck!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-839953175:103,perform,perform,103,,https://github.com/google/deepvariant/issues/457#issuecomment-839953175,1,['perform'],['perform']
Performance,"Okay, I will try this with my exomes.bed file. I was already able to successfully run the pipeline with the following params; ```; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --bam gs://canis/CNR-data/TLE_a_001.reheader.bam \; --bai gs://canis/CNR-data/TLE_a_001.reheader.bam.bai \; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. It is odd that the bed file needs to lie in a public bucket, while the genomic data does not.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-438034821:357,perform,performance-testdata,357,,https://github.com/google/deepvariant/issues/116#issuecomment-438034821,1,['perform'],['performance-testdata']
Performance,"Oo likely my fault! If I am adding the particular region around that variant such as ""chr15:41,132,484-42,007,831"". Do I still have to provide a WES bed file? I have this as my current script and last time it seemed like the bed file was the problem.. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-2; #SBATCH --mem-per-cpu=68GB; #SBATCH --qos=maxjobs500. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/checking_variant_deepvariant/exome_ID_file; HG38_REFERENCE=/scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; REGIONS=""chr15:41,132,484-42,007,831""; OUTPUT_VCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkingvariantregion.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set_checkvariantregion.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/checking_variant_deepvariant/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$REGIONS \; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:693,load,load,693,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113,2,['load'],['load']
Performance,"PUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:2506,cache,cached,2506,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725,1,['cache'],['cached']
Performance,"PacBio reads are too long to perform a local assembly on them, therefore --norealign_reads flag has to be added.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208#issuecomment-523135630:29,perform,perform,29,,https://github.com/google/deepvariant/issues/208#issuecomment-523135630,1,['perform'],['perform']
Performance,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386869866:777,cache,cache,777,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866,1,['cache'],['cache']
Performance,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386074270:805,load,loading,805,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270,1,['load'],['loading']
Performance,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-604195181:107,perform,performance,107,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181,4,"['optimiz', 'perform']","['optimization', 'optimizations', 'performance']"
Performance,"Regarding loading the checkpoint, it worked for me. I wonder if the issue is because of the gotcha of the filenames. The --checkpoint argument to call_variants is actually supposed to be the model prefix name, not an actual file name. For example I just tried . `( time python bin/call_variants.zip --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ~/case-study/output/HG002.examples.tfrecord-00004-of-00008.gz --checkpoint /tmp/deepvariant/model.ckpt-726 --batch_size 32; ) >""${LOG_DIR}/call_variants.log"" 2>&1`. and it ran as expected. Let me know if that works for you. Another thing to keep in mind is that if you run model_train again without specifying the 'train_dir' arg then it will attempt to pick up where it left off.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364545868:10,load,loading,10,,https://github.com/google/deepvariant/issues/46#issuecomment-364545868,1,['load'],['loading']
Performance,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364533837:843,load,loaded,843,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837,2,['load'],"['loaded', 'loading']"
Performance,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-372143466:894,cache,cache,894,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466,1,['cache'],['cache']
Performance,"See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42751,cache,cache,42751,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/107#issuecomment-430072409:445,perform,performed,445,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409,2,['perform'],"['performance-of-ngs-pipelines-on-noisy-wgs-data', 'performed']"
Performance,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used?; You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843:148,load,load,148,,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843,2,['load'],['load']
Performance,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4612,optimiz,optimization,4612,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918,1,['optimiz'],['optimization']
Performance,"Thank you - I will look into this. I was definitely not using 128 GB of RAM on AWS, so I will keep increasing this. I think you almost have to use a Cloud Resource or High-Performance Computing Cluster/Server for that: definitely not my personal home computer :(. While I also have WGS data, this is Exome data. I thought the _call_variants_ output seemed kind of small, but I don't know what is the typical size for this program.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479365566:172,Perform,Performance,172,,https://github.com/google/deepvariant/issues/167#issuecomment-479365566,1,['Perform'],['Performance']
Performance,"Thank you @AndrewCarroll. If the read is treated as a minimal [first-class object](https://en.wikipedia.org/wiki/First-class_citizen) (just a simple map/dictionary) would suffice, then you should be able to perform realignment after mapping. This way, it can become reactive without slowing down the analysis. Basically the information would not reside in a file, but rather encapsulated in the read itself. @PengJia6 The thing is that DeepVariant is position-focused at a specific base. That is how `make_examples` generates the variants from the allele counter for a region of a sample, which gets updated every time a new read is added to it. For instance, if you explore the allele counts, you'll get something like this for the different positions (the output is 0-based, and used the 1-based to identify each one):. ##### For Position: 89013075:. ```; position {; reference_name: ""chr10""; position: 89013074; }; ref_base: ""T""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""TC""; type: INSERTION; count: 1; }; }; ...; ```. ##### For Position: 89013076: . ```; position {; reference_name: ""chr10""; position: 89013075; }; ref_base: ""C""; ref_supporting_read_count: 35; read_alleles {; key: ""m64154_210327_091530/103023686/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/128910218/ccs/0""; value {; bases: ""CA""; type: DELETION; count: 1; }; }; ...; ```. ##### For Position: 89013077: ; ```; position {; reference_name: ""chr10""; position: 89013076; }; ref_base: ""A""; read_alleles {; key: ""m64154_210327_091530/142213575/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; read_alleles {; key: ""m64154_210327_091530/4130912/ccs/0""; value {; bases: ""C""; type: SUBSTITUTION; count: 1; }; }; ...; ```. Given that the allele type (indel/substitution) changes ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828:207,perform,perform,207,,https://github.com/google/deepvariant/issues/660#issuecomment-1590833828,2,['perform'],['perform']
Performance,"Thank you Andrew, I'm looking forward to reading. > 19, at 19:17, Andrew Carroll <notifications@github.com> wrote:; > ; > ﻿; > Hi @andrewrech and @shalabhsuman; > ; > I wanted to update this issue with recent developments for cohort merging. With the DeepVariant v0.9 release, we recommend Best practices for multi-sample variant calling with DeepVariant; > ; > Although this case study is a trio, we have optimized parameters for cohorts scaling into the 1000's, so we feel this will work well for your use cases.; > ; > Thank you.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub, or unsubscribe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-553890845:406,optimiz,optimized,406,,https://github.com/google/deepvariant/issues/142#issuecomment-553890845,1,['optimiz'],['optimized']
Performance,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:904,optimiz,optimize,904,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017,3,['optimiz'],"['optimization', 'optimize']"
Performance,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-489377484:479,optimiz,optimizations-on-modern-intel-architecture,479,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484,14,"['optimiz', 'perform', 'throughput']","['optimization', 'optimizations', 'optimizations-on-modern-intel-architecture', 'performs', 'throughput']"
Performance,"Thank you for the question! . Take a look at how we instantiate InceptionV3 [keras_modeling.py](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/keras_modeling.py#L275-L364). The [input_shape](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/train_inceptionv3.py#L278-L280) is inferred from the examples. InceptionV3 can handle any number of channels provided you are not using the `imagenet` classifier. . [documentation](https://keras.io/api/applications/inceptionv3/) mentions 3 channels if you are using the `imagenet` preset, and load the weights pre-trained on ImageNet. This is not the case if you set `weights=None`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801#issuecomment-2033087003:561,load,load,561,,https://github.com/google/deepvariant/issues/801#issuecomment-2033087003,1,['load'],['load']
Performance,"Thank you for the quick response!; I'd be glad to provide more information (the information listed below is from the original datasets / metrics provided from the genetic testing company that performed the tests utilizing their own tooling.); What type of sequencing data? Diagnostic Testing / XomeDx / Clinical Exome Sequence Analysis (provided in cram format with md5 hg19 as reference and the accompanying vcfs from said company) ; what depths - 152x mean depth of coverage with a quality threshold of 98.6; what organism - Humans using hg19 as reference (family of 4 two affected female probands two unaffected parents); Anything noticeably different from the data we used in case studies - nothing that i'm aware of at this time. It'll be great if you can also report: how many number of calls in your VCFs, how many GTs are in each calls (e.g., how many calls are ./.)? -I'm not completely sure where this is located but I believe I can find it. And, if your data is public, you can point to us and we can take a look. - The data is not available publicly yet but I would be glad to share it with you and your team. It has been submitted for public sharing with mygene2 (at least I'm pretty sure they are not online yet). I do, however, plan on sharing them online with Genome Connect and the rest of genetic community in the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/311#issuecomment-636996628:192,perform,performed,192,,https://github.com/google/deepvariant/issues/311#issuecomment-636996628,1,['perform'],['performed']
Performance,"Thank you for the reply. . Following are the cpuinfo of my machine. processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:243,cache,cache,243,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,1,['cache'],['cache']
Performance,"Thank you for trying. I'm not able to reproduce on any cloud instance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-603439134:126,perform,performance,126,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134,1,['perform'],['performance']
Performance,"Thank you for your reply!; 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1210,cache,cached,1210,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260,2,['cache'],"['cache', 'cached']"
Performance,"Thank you very much, that finally resolved the issue. What happens now is that after performing the evaluation (the training, test, and validation sets are the sequencing of the NA12878 sample from the Coriell Institute sequenced three times), I’m getting low recall and precision values. For indels, recall is 0.41 and precision is 0.24, and for SNPs, recall is 0.57 and precision is 0.72. I tried the model you provided for exome sequencing, but I didn’t achieve better results (which is why I decided to create my own model). However, with typical tools (like GATK HaplotypeCaller), I get much better results (indels with 0.8 recall and 0.62 precision, and SNPs with 0.89 recall and 0.97 precision). Do you have any idea why this might be happening and any advice on how to solve it? I really believe that using a variant caller trained with my data should yield better results than GATK HaplotypeCaller",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827:85,perform,performing,85,,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827,1,['perform'],['performing']
Performance,"Thank you, I agree with you about v4.2 being more comprehensive and correct. However, the reason why I am interested in a model trained on v3.3.2 is because I wanted to test DeepVariant's performance on v4.2 benchmark variants, for which it would be better to use a model that was not already trained on v4.2 benchmark variants. Just to get final clarification, PacBio model in the current v.1.0.0 release is trained on chr1-22 (except chr20 which is withheld) of all three Ashkenazim trio genomes, or just HG002 and HG004?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381#issuecomment-728645413:188,perform,performance,188,,https://github.com/google/deepvariant/issues/381#issuecomment-728645413,1,['perform'],['performance']
Performance,"Thank you,. How exactly does it perform the realignment? How does it differ from, let's say, bwa mem?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/280#issuecomment-593561282:32,perform,perform,32,,https://github.com/google/deepvariant/issues/280#issuecomment-593561282,1,['perform'],['perform']
Performance,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866205653:609,optimiz,optimization,609,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"Thanks @DLPerf .; Sounds good. I'll file an internal issue to track this. It might end up taking a while until this gets prioritized, but I'll give an update when I have an idea on the performance differences.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-903990739:185,perform,performance,185,,https://github.com/google/deepvariant/issues/479#issuecomment-903990739,1,['perform'],['performance']
Performance,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:494,cache,cache,494,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705,1,['cache'],['cache']
Performance,"Thanks @ekofman for the update. Good to know that it was resolved. In terms of whether bigger BAM files affect the run time -- overall it would increase the run time, since we'll be dealing with more reads per region on average, which will be more expensive to realign, build pileup image, etc.; But in make_examples, we sample subset of reads before performing these expensive operations. (We try to minimize the effect on accuracy if any). So it shouldn't be too bad.; Empirically, we do still sometimes see more expensive regions. It'll be great to identify a few regions like this on any public BAM we can get, so we can understand it better and improve.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461253040:351,perform,performing,351,,https://github.com/google/deepvariant/issues/150#issuecomment-461253040,1,['perform'],['performing']
Performance,"Thanks @kishwarshafin . ; But I think I am not even able to import TF in Python environment through DV container. . ```; (base) [tahmad@gcn4 ~]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu python3; INFO: Using cached SIF image; Python 3.8.10 (default, Mar 15 2022, 12:22:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import tensorflow as tf; 2022-08-28 09:48:47.608744: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE; ```. on my local system, I have TF 2.5.2; ```; >>> import tensorflow as tf; 2022-08-28 09:51:55.901400: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; >>> print(tf.__version__); 2.5.2; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555#issuecomment-1229403857:254,cache,cached,254,,https://github.com/google/deepvariant/issues/555#issuecomment-1229403857,1,['cache'],['cached']
Performance,"Thanks @pgrosu ,. @PengJia6 ,. Here's how my IGV looks like:. <img width=""2546"" alt=""Screenshot 2023-06-13 at 11 11 39 AM"" src=""https://github.com/google/deepvariant/assets/10559039/79761e5e-5781-4094-93cd-477d00b8d445"">. And here are the variant calls with DeepVariant 1.5:; ```bash; chr10	89013075	.	T	TC	43.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:63:35,28:0.444444:43,0,47; chr10	89013076	.	CA	C	47.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:45:63:35,28:0.444444:47,0,49; chr10	89013077	.	A	C	43.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:41:34:0,34:1:43,0,43; ```. This means you need to fix your IGV setup. On IGV version 2.16.1 go to ""View > Preference > Third Gen"" and Uncheck the ""Hide indels < show indels threshold"". It should look like this:; <img width=""896"" alt=""Screenshot 2023-06-13 at 11 15 44 AM"" src=""https://github.com/google/deepvariant/assets/10559039/89f9aa13-9233-4921-9374-9449c6870c73"">. Once done, load your bams one more time and the variants will appear.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589808839:897,load,load,897,,https://github.com/google/deepvariant/issues/660#issuecomment-1589808839,1,['load'],['load']
Performance,"Thanks @pgrosu @kishwarshafin, I appreciate your swift reply!. Yes, I am using a cluster, and the data have been obtained from precisionFDA https://data.nist.gov/od/id/mds2-2336. **What chemistry is this data? Is it R9 or R10?**; This data was generated using R9.4 flow cells. **What is the basecaller version you used for basecalling this data?**; The basecalling process was performed using Guppy Version 3.6. **What is the average read length of the reads?**; 85X. **Have you tried first going through the DeepVariant Quick Start guide to check if a smaller DeepVariant run completes successfully on your system?**; Yes, I have successfully run it. **How much free memory do you have?**; 1.3T . **How much free disk space do you have?**; I have approximately 14T of free disk space. **How many CPU cores do you have, and what is their occupancy level?**; 16 CPU cores. **Do you have any NVIDIA GPUs available on your system?**; No.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274:377,perform,performed,377,,https://github.com/google/deepvariant/issues/681#issuecomment-1641137274,2,['perform'],['performed']
Performance,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:377,cache,cache,377,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,2,['cache'],['cache']
Performance,"Thanks Andrew,. This work was specifically motivated by recent approaches like those in the VGP of using SNP/indels to polish large assemblies, and deepvariant being a top performing tool for this. Unfortunately, they don't use triobinning, and triobinning papers generally only polish with the binnable long reads, so perhaps this problem is currently unsolvable. I'll keep trying out some different ideas in case anything works, but thanks for the discussion.; Alex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/451#issuecomment-831232855:172,perform,performing,172,,https://github.com/google/deepvariant/issues/451#issuecomment-831232855,1,['perform'],['performing']
Performance,"Thanks Paul for your answer,. That's clear now. That means I need to choose EC2 instance type with 1 GPU because instance with more than 1 GPU does not have any better impact on DeepVariant's performance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052:192,perform,performance,192,,https://github.com/google/deepvariant/issues/696#issuecomment-1679371052,2,['perform'],['performance']
Performance,"Thanks for clearing that up! I appreciate it. I did use hap.py to compare the customized model to the WGS model and it appears to have performed slightly worse, so I'll keep this in mind for future tests.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909:135,perform,performed,135,,https://github.com/google/deepvariant/issues/797#issuecomment-2038129909,2,['perform'],['performed']
Performance,"Thanks for the added code.; Here are my follow-up questions:; - what is the pattern to use for file naming if I’m processing multiple BAM files?; - If downsampling same source BAM multiple times, do I perform loop function myself?; - Is there a seed parameter for downsampling fraction?; Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1908687832:201,perform,perform,201,,https://github.com/google/deepvariant/issues/765#issuecomment-1908687832,1,['perform'],['perform']
Performance,"Thanks for the question!; We recommend you use DeepVariant v1.5.0 to get a model trained with Revio data.; See https://github.com/google/deepvariant/releases/tag/v1.5.0 ; ""Incorporated PacBio Revio training data in DeepVariant PacBio model. In our evaluations this single model performs well on both Sequel II and Revio datatypes. Please use DeepVariant v1.5 and later for Revio data.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815:278,perform,performs,278,,https://github.com/google/deepvariant/issues/641#issuecomment-1535452815,1,['perform'],['performs']
Performance,"Thanks for the reply. I think it solves my problem.; I also agree that loading all variables is not the best, but I'd like to try the suggested code and check if the model would be the same first.; But still I think some vars like the EMA ones should be loaded in the warm-up stage, and I'll try to figure out what vars are needed.; I'll reply here if I make any further progress.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-495054046:71,load,loading,71,,https://github.com/google/deepvariant/issues/185#issuecomment-495054046,2,['load'],"['loaded', 'loading']"
Performance,"That paper is an early version of the code - so some things changed and some didn't. So let's parse this out:. 1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the genera",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512112524:644,optimiz,optimized,644,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524,2,['optimiz'],['optimized']
Performance,"That's a good question. We use this BED file to provide to the calling programs where possible so that we don't use compute performing calling in regions we won't be evaluating on. You are correct, doing the intersection in hap.py will work just fine in terms of the final results. Here is the non-intersected BED file: . *edited by adding file attachment; [agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz](https://github.com/google/deepvariant/files/3875984/agilent_sureselect_human_all_exon_v5_b37_targets.bed.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/15#issuecomment-351194795:124,perform,performing,124,,https://github.com/google/deepvariant/issues/15#issuecomment-351194795,1,['perform'],['performing']
Performance,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1755,Load,Loads,1755,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377,1,['Load'],['Loads']
Performance,"The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") ; -- Configuring done; -- Generating done; -- Build files have been written to: /root/clif/build; ```; which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash; root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build; root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm ha",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:2683,Perform,Performing,2683,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,2,['Perform'],['Performing']
Performance,"The `postprocess_variants` may OOM because it loads all variant calls into memory in order to sort them. We are actively working on addressing this. That said, you can re-start each of the 3 steps individually by calling them directly. Instead of running `run_deeptrio`, e.g.; ```; singularity run; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```; you can run `postprocess_variants`; ```; singularity run; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/postprocess_variants; ```; Please note that the [flags](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L633-L668) will differ. At minimum you will need to set `--ref`, `--infile` and `--outfile`. The `--infile` is the output from `call_variants`, which has the following [pattern](https://github.com/google/deepvariant/blob/r1.6.1/scripts/run_deeptrio.py#L790-L794), e.g. `call_variants_output_parent1.tfrecord.gz`. You should be able to find them in the tmp directory containing all the make_examples files. Hopefully that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/803#issuecomment-2038538498:46,load,loads,46,,https://github.com/google/deepvariant/issues/803#issuecomment-2038538498,1,['load'],['loads']
Performance,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488762439:931,optimiz,optimizations-,931,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439,1,['optimiz'],['optimizations-']
Performance,The error comes from the line `output_queue = multiprocessing.Queue()`; Could you try a simple test? ; Run docker in CLI model: `docker run -it <DeepVariant image> bash`; Inside docker start Python3 and execute:; ```; import multiprocessing; q = multiprocessing.Queue(); ```; Please let us know if that works.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777:62,Queue,Queue,62,,https://github.com/google/deepvariant/issues/733#issuecomment-1816864777,2,['Queue'],['Queue']
Performance,"The error reported in the `docx` file is:. ```; I1219 05:41:37.963743 139694699493120 call_variants.py:338] Shape of input examples: [100, 221, 9]; 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf; W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_mas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:255,optimiz,optimized,255,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"The reason why it is taking so long is because of the emulation component in `qemu`. Docker Desktop performs the qemu emulation of those instructions, when you run it with `--platform`. The reason I wanted to try `colima` is because it will use the installed `qemu` on the Mac rather the one specifically shipped with Docker. The newest `qemu` has the AVX instructions. . How come you don't want to run it on the Cloud directly, as DeepVariant is suboptimal on a laptop with large datasets. Digging through my notes, I noticed I had to write my own customizations to build DeepVariant from scratch for non-AVX platforms, but that's TensorFlow-specific. It's a lot of work. What specific issues are you seeing under Ubuntu when you're trying to install it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535:100,perform,performs,100,,https://github.com/google/deepvariant/issues/657#issuecomment-1575672535,1,['perform'],['performs']
Performance,"The start seems to be quite slow, after about 27s it exits, here is the output; ```; root@52e2e7bb0093:/opt/deepvariant/bin/unzip-post/runfiles/com_google_deepvariant/deepvariant# time python -mtrace --trackcalls postprocess_variants.py --ref /home/zxue/deepvariant_exp/tcga-data/hg19.fa --infile /home/zxue/deepvariant_exp/output/cvo.tfrecord.gz ; --outfile /home/zxue/deepvariant_exp/output/output.vcf.gz --nonvariant_site_tfrecord_path /home/zxue/deepvariant_exp/output/gvcf.tfrecord@8.gz ; Traceback (most recent call last):; File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; ""__main__"", fname, loader, pkg_name); File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; exec code in run_globals; File ""/usr/lib/python2.7/trace.py"", line 819, in <module>; main(); File ""/usr/lib/python2.7/trace.py"", line 807, in main; t.runctx(code, globs, globs); File ""/usr/lib/python2.7/trace.py"", line 513, in runctx; exec cmd in globals, locals; File ""postprocess_variants.py"", line 46, in <module>; from third_party.nucleus.io import fasta; ImportError: No module named third_party.nucleus.io. real 0m27.273s; user 0m27.133s; sys 0m0.904s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/103#issuecomment-429462894:619,load,loader,619,,https://github.com/google/deepvariant/issues/103#issuecomment-429462894,1,['load'],['loader']
Performance,"There is a slight change in performance, but since v1.5.0 already has slightly different output/performance to v1.4.0, I don't think there is any negative consequence to using RNA model v1.4.0 on DV v1.5.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624#issuecomment-1498623984:28,perform,performance,28,,https://github.com/google/deepvariant/issues/624#issuecomment-1498623984,2,['perform'],['performance']
Performance,"These mostly run on nodes with Intel Xeon Gold 6140, occasionally on Intel Xeon E5-2697v4, but these are much slower anyway. . I had noticed the same speed improvement in v1.1.0 with openvino as your metrics, but I haven't tested the new version extensively with and without. However, when rerunning identical samples (both with openvino flags), I've noticed that v1.2.0 takes longer wall clock time compared to v1.1.0, but less CPU time. Maybe it is just node variation or other jobs bottlenecking IO, so hopefully will see a clearer result after more samples run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/416#issuecomment-889821634:485,bottleneck,bottlenecking,485,,https://github.com/google/deepvariant/issues/416#issuecomment-889821634,2,['bottleneck'],['bottlenecking']
Performance,"This can not be the point. I am working in a cluster environment using slurm and the dir ""."" is the job specific scratch dir, which is located at ""/scratch/SlurmTMP/JobSpecificFolder"" (${TMPDIR}); ```. cd ${TMPDIR}; BIN_VERSION=""1.6.1""; module load singularity/3.5.2. #####################################################################; # singularity pull docker://google/deepvariant:""${BIN_VERSION}"". ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150. # --model_type=PACBIO \ ##Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**; # docker://google/deepvariant:""${BIN_VERSION}"" \. if ! [ -f ""${WORKINDIR}/${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]; then; cp ""${THEREF}""* ./; cp ""${WORKINDIR}/${ALIGNMENTNAME}.bam""* .; chmod 666 `basename ""${THEREF}""`*; chmod 666 ""${ALIGNMENTNAME}.bam""*; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=`basename ""${THEREF}""` \; --reads=""${ALIGNMENTNAME}.bam"" \; --sample_name=${SAMPLENAME} \; --output_vcf=""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \; --output_gvcf=""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \; --intermediate_results_dir . \; --num_shards=8 \; --logging_dir=.; ; if ! [ -f ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" ]; then; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; /my/path/software/deepVariant/deepvariant_${BIN_VERSION}.sif \; /opt/deepvariant/bin/postprocess_variants \; --ref=`basename ""${THEREF}""` \; --infile ""./call_variants_output@$(ls ./call_variants_output*.tfrecord.gz | wc -l).tfrecord.gz"" \; --outfile ""./${ALIGNMENTNAME}.deepVariant.vcf.gz"" \; --cpus ""8"" \; --gvcf_outfile ""./${ALIGNMENTNAME}.deepVariant.g.vcf.gz"" \; --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@$(ls ./gvcf.tfrecord*.gz | wc -l).gz"" \; --sample_name=${SAMPLENAME}; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2106784467:244,load,load,244,,https://github.com/google/deepvariant/issues/818#issuecomment-2106784467,1,['load'],['load']
Performance,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1497,perform,performs,1497,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612,2,['perform'],['performs']
Performance,"This was really puzzling me, so I started with a smallest possible reproducible setting and started debugging. . Now I understand why the results are different. . This update will be about the technical explanation of why different number of shards result in different result:. This is because when we're processing reads in a region, we (by default) first do a sampling of the reads down to `max_reads_per_partition` (default=1500) reads. Here is the code snippets that does that:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L842-L844. ```; if self.options.max_reads_per_partition > 0:; reads = utils.reservoir_sample(; reads, self.options.max_reads_per_partition, self.random); ```. This was originally an optimization for speed. A while ago we noticed that some regions in the genome took really long time to create examples. And this is because some regions have a lot of reads piled up.; Because of the way that germline DeepVariant is set up, the final pileup image has only a image height of 100 anyway, so we don't really need to process all the reads. But at this stage we still need to go through the realigner step later, so we decided to sample enough reads here so that the following steps can still use more information, but we cut off at the number of `max_reads_per_partition` (default=1500) reads. The randomization here is done with `self.random`, which is initialize with the same seed at the beginning. Because the seed is the same at the beginning, we're able to generate deterministic and reproducible results if you fix the number of tasks. However, when you run make_examples with different number of tasks, that means each task has its own `self.random`. Even though each of them started with the same seed, when you have different number of tasks(shards), these RandomState ends up progressing differently. This means when you look at a same region in two different runs (for example, I was looking at `20:3208913-3209912` in the exome), the",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433598823:742,optimiz,optimization,742,,https://github.com/google/deepvariant/issues/112#issuecomment-433598823,1,['optimiz'],['optimization']
Performance,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:869,perform,performs,869,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407,4,['perform'],"['performance', 'performs']"
Performance,"Unfortunately, it's not... Providing what I can:; It's a high coverage sample (~72X). We subset the BAM into 21 regions that are roughly equal in size (approximately 160-200M bases), three of them are showing this behavior:; ```; chr10+chr14-p; chr4-p+chr5-p_chr11-p; chr7-q+chr16-q; ```; all three BAMs are in the range between 15-20GB. I am actually suspecting that somewhere in these BAMs, there are stupidly crazy high coverage regions, and that's triggering something in DV to generate loads of temporary files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960089166:491,load,loads,491,,https://github.com/google/deepvariant/issues/491#issuecomment-960089166,1,['load'],['loads']
Performance,Update:; I can confirm that I'm able to reproduce your error. We're working on a fix. Stay tuned!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-388450895:91,tune,tuned,91,,https://github.com/google/deepvariant/issues/62#issuecomment-388450895,1,['tune'],['tuned']
Performance,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255#issuecomment-568131537:985,perform,performance-testdata,985,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537,3,"['load', 'perform']","['load', 'performance-testdata']"
Performance,"We have not tried DeepVariant on an asexual diploid organism, but I see no reason why it shouldn't work. However, we have observed that the models we have trained on human data are good but not optimal for non-human genomes. See this blog post for an example in mosquitos where training a species-specific model improved performance: https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/. Now we are curious, which organism are you working on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-569115062:321,perform,performance,321,,https://github.com/google/deepvariant/issues/257#issuecomment-569115062,1,['perform'],['performance']
Performance,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1186,perform,performance,1186,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189,2,['perform'],"['perform', 'performance']"
Performance,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:35,perform,performance,35,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538,1,['perform'],['performance']
Performance,"__Is there a seed parameter for downsampling fraction?__. I don't believe this is currently possible. What you can do though is you can downsample using samtools. . From `samtools view` you can specify:; ```; -s FLOAT subsample reads (given INT.FRAC option value, 0.FRAC is the; fraction of templates/read pairs to keep; INT part sets seed); ```; So you can subsample each bam like this:. ```bash; for i in `seq 1 5`; do; # i sets the seed.; samtools view -s ${i}.20 input.bam > input.${i}.20.bam; ```; Then run make_examples + shuffle to combine the results. Note that you want your training data to resemble the data you plan to run your model on. Subsampling can help improve performance in lower coverage regions, but if you plan to run this model at the original coverage level you'll want to train on that type of data as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1911329739:679,perform,performance,679,,https://github.com/google/deepvariant/issues/765#issuecomment-1911329739,1,['perform'],['performance']
Performance,"__what is the pattern to use for file naming if I’m processing multiple BAM files?__. This can be whatever you like. When you perform shuffling, you want to specify a glob or list of patterns that will match. __If downsampling same source BAM multiple times, do I perform loop function myself?__. I'm not sure what you mean here. Can you provide more information?. __Is there a seed parameter for downsampling fraction?__. I don't think so - but I will double check on this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1908918159:126,perform,perform,126,,https://github.com/google/deepvariant/issues/765#issuecomment-1908918159,2,['perform'],['perform']
Performance,_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10045,tune,tune,10045,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00015-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00016-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00017-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10378,tune,tune,10378,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00015-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00016-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00017-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00018-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10489,tune,tune,10489,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=un",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25834,cache,cache,25834,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72481,cache,cache,72481,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s; user 0m0.037s; sys 0m0.013s; ```. [train.log](https://github.com/google/deepvariant/files/15253",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1492,tune,tune,1492,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['tune'],['tune']
Performance,"```; I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-08-15 14:02:47.618984: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-08-15 14:02:47.619048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-08-15 14:02:50.434353: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NOT_FOUND: named symbol not found; []; ```I am encountering errors while testing deepvariants calling with gpu. It seems that some libraries for cuda are missing causing it to only work with cpu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900:84,optimiz,optimized,84,,https://github.com/google/deepvariant/issues/820#issuecomment-2291334900,3,"['load', 'optimiz', 'perform']","['load', 'optimized', 'performance-critical']"
Performance,"`tf.data.Options.experimental_deterministic`.; WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; INFO:tensorflow:Calling model_fn.; I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn.; WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; INFO:tensorflow:Done calling model_fn.; I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finaliz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:4973,optimiz,optimizations,4973,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['optimiz'],['optimizations']
Performance,"`tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1008,Load,Load,1008,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Load'],['Load']
Performance,"`types_to_alt_align` refers to the type of variants in which we perform alignments against the alternative variant, when you have also set the `alt_aligned_pileup` flag. . You might be able to accomplish something like this by making use of the vcf candidate importer. See `--truth_variants` + `--variant_caller=vcf_candidate_importer`. I would expect that if you perform filtering similarly on your training and test data, that this could be a way to develop a model specific to certain size INDEL variants, but we have never tried to do something like this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2087375034:64,perform,perform,64,,https://github.com/google/deepvariant/issues/813#issuecomment-2087375034,2,['perform'],['perform']
Performance,a024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/a,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121190,cache,cache,121190,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"a024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:101518,cache,cache,101518,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"a024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:98344,cache,cache,98344,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,ache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121872,cache,cache,121872,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"alizedDType'`. I dont have the output saved from the test data run to retrieve the full command output with the error prior to installing nucleus as user, and will re-run that and update this comment with it in a few hours. I did, however, get the same error attempting to run deepvariant with my own data (prior to installing nucleus as user), and the output and command from that are below:. ```; for bam in $READS; do; 	echo ""running deepvariant on $bam""; 	run_deepvariant --model_type=PACBIO --ref=$REF --reads=$bam --output_vcf=$OUTDIR/$bam.vcf.gz --output_gvcf=$OUTDIR/$bam.g.vcf.gz --logging_dir=$LOGDIR --num_shards=$CORES; 	echo ""finished with $bam""; done. #output in block comment below. # running deepvariant on /work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam; # 2024-04-23 11:42:51.281492: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; # To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; # I0423 11:42:57.943745 140073410221888 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmpkmab_2kw. # ***** Intermediate results will be written to /tmp/tmpkmab_2kw in docker. ****. # ***** Running the command:*****; # time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/work/cjm124/SWFst/lvar3ref/Lvar_scaffolds.fasta"" --reads ""/work/cjm124/SWFst/VarCalling/reads/bc2001_aligned_sorted.bam"" --examples ""/tmp/tmpkmab_2kw/make_examples.tfrecord@12.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/tmp/tmpkmab_2kw/gvcf.tfrecord@12.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_frac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946:1232,optimiz,optimized,1232,,https://github.com/google/deepvariant/issues/812#issuecomment-2075116946,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"an omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distributio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2377,cache,cached,2377,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['cache'],['cached']
Performance,"and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1885,perform,performed,1885,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,2,['perform'],['performed']
Performance,"ant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124007,cache,cache,124007,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125882,cache,cache,125882,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42127,cache,cache,42127,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100556,cache,cache,100556,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:40374,cache,cache,40374,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125357,cache,cache,125357,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7704,optimiz,optimization,7704,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['optimiz'],['optimization']
Performance,"asons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most rece",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:98052,cache,cache,98052,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,azel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_de,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121420,cache,cache,121420,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14745,cache,cachetools,14745,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['cache'],['cachetools']
Performance,"b0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:108495,cache,cache,108495,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/v,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120240,cache,cache,120240,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/u",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1948,cache,cache,1948,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"cent call last):; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: /opt/software/GCCcore/6.4.0/lib64/libstdc++.so.6: version `CXXABI_1.3.11' not found (required by /mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. I upgraded to a higher version of GNU and re-ran but I got a nother error . ```; module load GNU/7.3.0-2.30. python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:2657,load,load,2657,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,1,['load'],['load']
Performance,che/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca0,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121642,cache,cache,121642,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ckages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:12487,cache,cached,12487,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['cache'],['cached']
Performance,"ckages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Ru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:9662,cache,cached,9662,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['cache'],['cached']
Performance,"ckages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117371,cache,cached,117371,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"command which @pichuan provided but it still print nothing on terminal. ; And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice.; Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info; ```text; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 63; model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:613,cache,cache,613,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['cache'],['cache']
Performance,"d aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext perfctr_core vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save. When I run run_wes_case_study_docker.sh on the Intel instance make_examples.py load avg is 48, call_variants.py load avg is 36 (takes about 1.5 minutes), and the it completes in about 13-15 minutes. Using the AMD instance make_examples.py load avg is 48, call_variants.py load avg is 86 (takes about 6 minutes), and it completes in about 20-23 minutes. It looks like when using AMD it slows down considerably during the call_variants.py step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1884,cache,cache,1884,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134,8,"['cache', 'load']","['cache', 'load']"
Performance,d; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00015-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10156,tune,tune,10156,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1727,optimiz,optimized,1727,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['optimiz'],['optimized']
Performance,"dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-lin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10486,cache,cached,10486,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['cache'],['cached']
Performance,"dex; 2024-08-23 15:16:56.766309: I deepvariant/postprocess_variants.cc:94] Read from: /Output/call_variants_output-00000-of-00001.tfrecord.gz; 2024-08-23 15:18:01.806248: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 10880665; I0823 15:20:45.074391 139658307389248 postprocess_variants.py:1313] CVO sorting took 3.805263650417328 minutes; I0823 15:20:45.077561 139658307389248 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0823 15:20:45.077694 139658307389248 postprocess_variants.py:1318] Using 32 CPUs for parallelization of variant transformation.; I0823 15:20:51.014987 139658307389248 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: CoriellIndex. real	8m32.455s; user	7m11.835s; sys	1m25.577s; Process ForkPoolWorker-2:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449:2101,queue,queues,2101,,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449,1,['queue'],['queues']
Performance,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5742,Perform,Performing,5742,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,['Perform'],['Performing']
Performance,"dule('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117481,cache,cached,117481,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82643,cache,cache,82643,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:89988,cache,cache,89988,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"e 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:94885,cache,cache,94885,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"e beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 saver.py:1293] Restoring parameters from /opt/models/wes/model.ckpt; I0622 21:25:24.567713 140301916206848 call_variants.py:454] Processed 1 examples in 1 batches [1678.300 sec per 100]; I0622 21:26:59.442872 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:5276,Tune,Tune,5276,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2568,cache,cached,2568,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['cache'],['cached']
Performance,"e the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:26144,cache,cache,26144,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"e_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9601,tune,tune,9601,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13142,Perform,Performing,13142,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['Perform'],['Performing']
Performance,"ed here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make su",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1563,load,load,1563,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['load'],['load']
Performance,edBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBU,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2252,cache,cache,2252,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['cache'],['cache']
Performance,"eepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11813,cache,cache,11813,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"eepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ──────────────────────────────────────────────────────────────────────────────; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pasta 0.2.0 pyh8c360ce_0 conda-forge; grpcio 1.38.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1804,cache,cached-property,1804,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['cache'],['cached-property']
Performance,"ely for each bam, then combine the training examples via shuffling. See the make_examples command in the case study. Run this for each bam you intend to train on. Then perform shuffling:. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Here, you would substitute the `input_pattern_list` with a glob that captures examples across multiple bam inputs. __Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?__. This is doable, but will require a lot of work to manage. But - you could generate training examples, shuffle, warmstart your model, train for a bit, then delete and repeat (warmstarting from that model). This approach has a lot of downsides though - you have to manage the files, and its not going to be as good as training the full dataset at once. __Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5)__. If you are training from scratch, a large number of training examples is helpful. However, if you are warmstarting, it is possible to get away with far fewer training examples. The case study highlights an example that uses only 342,758 training examples, and leads to modest SNP improvement and a nice increase in INDEL accuracy in only ~1.5 hours of training. . More data is better, but the quality of the training labels is more impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538:1349,perform,perform,1349,,https://github.com/google/deepvariant/issues/765#issuecomment-1906964538,1,['perform'],['perform']
Performance,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1967,optimiz,optimized,1967,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,2,['optimiz'],['optimized']
Performance,"enerate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123003,cache,cache,123003,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_intern",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23687,cache,cache,23687,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_intern",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:110574,cache,cache,110574,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106306,cache,cache,106306,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ental_deterministic`.; W1219 05:41:38.154152 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.; WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; INFO:tensorflow:Calling model_fn.; I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn.; WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; W1219 05:41:38.779899 139694699493120 deprecation.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:4459,optimiz,optimizations,4459,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['optimiz'],['optimizations']
Performance,"er the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:3745,perform,perform,3745,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,2,['perform'],['perform']
Performance,"er); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15281,cache,cached,15281,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['cache'],['cached']
Performance,"examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9712,tune,tune,9712,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,f-00030.gz; -rw-r--r-- 1 root root 52K Sep 24 15:47 make_examples.tfrecord-00000-of-00030.gz; -rw-r--r-- 1 root root 5.9K Sep 24 15:47 gvcf.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 5.7K Sep 24 15:47 gvcf.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 3.6K Sep 24 15:47 gvcf.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 4.5K Sep 24 15:47 gvcf.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 22K Sep 24 15:47 make_examples.tfrecord-00004-of-00030.gz; -rw-r--r-- 1 root root 31K Sep 24 15:47 make_examples.tfrecord-00003-of-00030.gz; -rw-r--r-- 1 root root 16K Sep 24 15:47 make_examples.tfrecord-00022-of-00030.gz; -rw-r--r-- 1 root root 6.2K Sep 24 15:47 gvcf.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 56K Sep 24 15:47 make_examples.tfrecord-00008-of-00030.gz; -rw-r--r-- 1 root root 7.5K Sep 24 15:47 make_examples.tfrecord-00025-of-00030.gz; -rw-r--r-- 1 root root 4.6K Sep 24 15:47 gvcf.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 38K Sep 24 15:47 make_examples.tfrecord-00026-of-00030.gz; -rw-r--r-- 1 root root 5.2K Sep 24 15:47 gvcf.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 32K Sep 24 15:47 make_examples.tfrecord-00002-of-00030.gz; -rw-r--r-- 1 root root 7.0K Sep 24 15:47 gvcf.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 3.0K Sep 24 15:47 gvcf.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 44K Sep 24 15:47 make_examples.tfrecord-00007-of-00030.gz; -rw-r--r-- 1 root root 11K Sep 24 15:47 make_examples.tfrecord-00024-of-00030.gz; -rw-r--r-- 1 root root 5.3K Sep 24 15:47 gvcf.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 27K Sep 24 15:47 make_examples.tfrecord-00013-of-00030.gz; -rw-r--r-- 1 root root 4.9K Sep 24 15:47 gvcf.tfrecord-00005-of-00030.gz; -rw-r--r-- 1 root root 20K Sep 24 15:47 make_examples.tfrecord-00005-of-00030.gz. ```; This is how my `free -h` is looking like right now:. ```; total used free shared buff/cache available; Mem: 125G 123G 776M 224M 933M 354M; Swap: 101G 100G 1.7G. ```. Thank you. Cheers.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358#issuecomment-703346071:9843,cache,cache,9843,,https://github.com/google/deepvariant/issues/358#issuecomment-703346071,1,['cache'],['cache']
Performance,"f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:12419,cache,cache,12419,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the “truth” set was defined. They mentioned that they **focused on regions where variants could be made most confidently** (genome-wide), and I’m assuming that is why most of the numbers are so high. Otherwise, they are more in the range of using [my same WGS sample and variant caller (DeepVariant) while only changing the alignment](https://precision.fda.gov/comparisons/3437), which isn’t really an independent verification (matching my original concern that the percentages being reported seemed unrealistically high). **In other words, I would say DeepVariant performed well, *along with other strategies tested***. I genuinely believe it is good to have a variety of freely available programs to use, but I believe the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to creat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:6544,perform,performed,6544,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['perform'],['performed']
Performance,"ffle_2_pt4-00016-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00017-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00018-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00019-of-00020.tfrecord.gz; ```. And for the tuning set (which was shuffled normally using just one step):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9112,tune,tune,9112,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"fle_2_pt4-00019-of-00020.tfrecord.gz; ```. And for the tuning set (which was shuffled normally using just one step):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9379,tune,tune,9379,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"g for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117839,cache,cached,117839,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"g upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:02:09 --:--:-- 0curl: (7) Failed to connect to storage.googleapis.com port 443: Connection timed out",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:15532,optimiz,optimized,15532,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['optimiz'],['optimized']
Performance,"gner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122687,cache,cache,122687,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"hared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:47217,cache,cache,47217,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"hared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53804,cache,cache,53804,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:85892,cache,cache,85892,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"hen asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:33407,cache,cache,33407,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:; ```bash; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841#issuecomment-2197206085:62,optimiz,optimized,62,,https://github.com/google/deepvariant/issues/841#issuecomment-2197206085,1,['optimiz'],['optimized']
Performance,"hi @pichuan,. That makes a lot of sense. . I already lowered the threshold but it seem like this is not a way to go. My plan is to retrain the model on our dataset and hopefully it performs better.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1663136809:181,perform,performs,181,,https://github.com/google/deepvariant/issues/690#issuecomment-1663136809,1,['perform'],['performs']
Performance,"hink the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would be the most effective approach. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:1922,bottleneck,bottleneck,1922,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177,1,['bottleneck'],['bottleneck']
Performance,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:2549,tune,tune,2549,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['tune'],['tune']
Performance,"iant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125707,cache,cache,125707,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ibcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106654,cache,cache,106654,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4402,Load,Load,4402,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['Load'],['Load']
Performance,"ies_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:9954,cache,cache,9954,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['cache'],['cache']
Performance,"ig=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9243,cache,cached,9243,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['cache'],['cached']
Performance,"ig=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8766,cache,cached,8766,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['cache'],['cached']
Performance,"ile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11126,Load,Loading,11126,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,4,"['Load', 'load']","['Loading', 'loaded']"
Performance,"iled. And the following is the error message that I got:. ```; FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labele",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1095,cache,cache,1095,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['cache'],['cache']
Performance,ileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepva,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119753,cache,cache,119753,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"in/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git Open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6827,cache,cache,6827,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['cache'],['cache']
Performance,"ino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:5083,cache,cache,5083,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,4,['cache'],['cache']
Performance,"internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100310,cache,cache,100310,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"iption); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:17155,cache,cache,17155,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 495,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:1003,perform,performance-testdata,1003,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['perform'],['performance-testdata']
Performance,"izations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods); 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence); 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}; 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}; 1276 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6977,load,loads,6977,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['load'],['loads']
Performance,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1099,optimiz,optimized,1099,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,2,['optimiz'],['optimized']
Performance,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2267,cache,cache,2267,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['cache'],['cache']
Performance,"kg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7333,optimiz,optimization,7333,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['optimiz'],['optimization']
Performance,"l.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]; I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1128 03:46:59.458546 139674856871680 call_variants.py:452] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1128 03:47:01.112938 139674856871680 call_variants.py:452] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1128 03:47:02.774386 139674856871680 call_variants.py:452] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1128 03:47:04.426402 139674856871680 call_variants.py:452] Processed 90001 examples in 176 batches [0.011 sec per 100]",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276922:2763,optimiz,optimizations,2763,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922,1,['optimiz'],['optimizations']
Performance,"l_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72306,cache,cache,72306,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"l_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72131,cache,cache,72131,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,l_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_trai,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71956,cache,cache,71956,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,l_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvari,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119097,cache,cache,119097,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"l`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### WES; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ##### WGS; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051:1353,perform,performing,1353,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051,2,['perform'],['performing']
Performance,"las.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71256,cache,cache,71256,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"le>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32675,cache,cache,32675,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"le_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", li",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:26449,cache,cache,26449,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,ll/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9934,tune,tune,9934,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"low.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14226,cache,cache,14226,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lp and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11440,cache,cache,11440,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993155459:662,cache,cache,662,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459,4,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:19314,cache,cache,19314,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:28652,cache,cache,28652,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35931,cache,cache,35931,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:38086,cache,cache,38086,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:51519,cache,cache,51519,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64696,cache,cache,64696,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66851,cache,cache,66851,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:69006,cache,cache,69006,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"lude the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117769,cache,cached,117769,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"make_examples.py:648] Task 0/8: 5800 candidates (6229 examples) [14.57s elapsed]; I0622 21:25:03.072478 140528910345984 make_examples.py:648] Task 0/8: Found 5825 candidate variants; I0622 21:25:03.072677 140528910345984 make_examples.py:648] Task 0/8: Created 6254 examples; ```. Here is a snapshot of `htop` while `make_examples` is running:; ```; 1 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 5 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running; Swp[ 0K/0K] Load average: 8.09 7.59 4.84 ; Uptime: 01:04:14; ```. make_examples took:; ```; real 32m36.929s; user 253m55.294s; sys 1m1.715s; ```. ## call_variants; At the beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:4149,Load,Load,4149,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['Load'],['Load']
Performance,"may 2023, ; running on a dell 730 with 88 cores using google/deepvariant:1.5.0. I do not have a GPU on this server and my cpu are avx2. I get the following messages:. ```; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-04 12:48:39.049123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; ```. how can I get the docker to match my architecture. The quickstart code runs with many such warnings and generates data; is this data OK?. ```; sudo docker run \; -u $(id -u) \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:372,optimiz,optimized,372,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"mber=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF refinement based on pedigree information:. * [dv-trio](https://github.com/VCCRI/dv-trio) with [FamSeq](https://github.com/wwylab/FamSeq) (This is an earlier version approach of DeepVariant before DeepTrio); * [Octopus](https://github.com/luntergroup/octopus) ([doc example](https://luntergroup.github.io/octopus/docs/guides/models/trio/)); * [GATK HaplotypeCaller + GenotypeGVCFs + CalculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html); * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is hap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:1932,perform,perform,1932,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,4,['perform'],['perform']
Performance,"mmand>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2644,cache,cached,2644,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['cache'],['cached']
Performance,"mmon reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71431,cache,cache,71431,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master':",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:5481,Tune,Tune,5481,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117127,cache,cached,117127,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"mples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necessary. Would you be willing to share the BAM file to get a better idea what is happening?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:1074,perform,perform,1074,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175,2,['perform'],['perform']
Performance,"msiu used the .sif file, I'll also do something similar:; So, then I ran:. ```bash; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_deeptrio-1.6.0-gpu.sif \; /opt/deepvariant/bin/deeptrio/run_deeptrio; ```. The command above gave me:; ```. ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpsho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:3075,optimiz,optimized,3075,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,4,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,n-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonit,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:3483,cache,cache,3483,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['cache'],['cache']
Performance,"n_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1228,cache,cache,1228,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"nal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14581,cache,cache,14581,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"nce_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplot",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1624,cache,cache,1624,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['cache'],['cache']
Performance,"nclude the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:83332,cache,cache,83332,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"nclude the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:90677,cache,cache,90677,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"nclude the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:95574,cache,cache,95574,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2169,perform,perform,2169,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['perform'],['perform']
Performance,"ndex: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2573,cache,cache,2573,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,2,['cache'],['cache']
Performance,"ng, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to make models that will generally take this into account. I would be curious in your feedback about whether the short term solution is acceptable, whether you find it insufficient, or if you would recommend other approaches for us to address the issue. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:2277,perform,performed,2277,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,2,['perform'],['performed']
Performance,"no-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10205,load,loaded,10205,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['load'],['loaded']
Performance,"no. I1128 03:33:00.717135 139674856871680 call_variants.py:338] Shape of input examples: [100, 221, 6]; 2020-11-28 03:33:00.726560: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 AVX512F FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-11-28 03:33:00.742278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz; 2020-11-28 03:33:00.748254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x419c1f0 executing computations on platform Host. Devices:; 2020-11-28 03:33:00.748310: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batchin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276922:1510,Tune,Tune,1510,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"nt/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126057,cache,cache,126057,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,nt/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118673,cache,cache,118673,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"nt_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123485,cache,cache,123485,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"o docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s; user 0m0.037s; sys 0m0.013s; ```. [train.log](https://github.com/google/deepvariant/files/15253217/train.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1539,tune,tune,1539,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,21,['tune'],['tune']
Performance,"odel.ckpt"". I0911 02:28:44.682443 139937686464256 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-09-11 02:28:44.695721: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-09-11 02:28:44.721599: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:6414,Tune,Tune,6414,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"oftware.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2034,Load,Load,2034,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Load'],['Load']
Performance,"ogle_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`.; I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48; W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation.; I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn.; W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parame",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:8313,optimiz,optimizations,8313,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['optimiz'],['optimizations']
Performance,oke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118469,cache,cache,118469,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ome common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106828,cache,cache,106828,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2574,perform,performs,2574,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,2,['perform'],['performs']
Performance,"on3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:3825,Load,Loads,3825,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Load'],['Loads']
Performance,"onal] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1745,load,loading,1745,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['load'],['loading']
Performance,"ons_to_include, contig_dict)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 464, in from_regions; for elt in reader(region):; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 429, in bed_parser; with bed.BedReader(filename) as fin:; File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 211, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/mnt/google/.google/tmp/Bazel.runfiles_34p88R/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open gs://canis/CNR-data/exomes.bed; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; ```. I've added the BED file to the public bucket:; gs://public-debug/exomes.bed",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437689349:4106,perform,performance-testdata,4106,,https://github.com/google/deepvariant/issues/116#issuecomment-437689349,1,['perform'],['performance-testdata']
Performance,"oot/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2187,cache,cache,2187,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"orflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:106480,cache,cache,106480,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"orflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:70906,cache,cache,70906,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"orflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87841,cache,cache,87841,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"orflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:33081,cache,cache,33081,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80791,cache,cache,80791,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ot/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally.; There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126407,cache,cache,126407,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,4,['cache'],['cache']
Performance,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:2897,perform,performs,2897,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,2,['perform'],['performs']
Performance,"output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_init_op.; 2021-09-11 15:27:06.477138: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2); INFO:tensorflow:Reloading EMA...; I0911 15:27:06.498517 139825053296448 modeling.py:416] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:06.499074 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:17.529355 139825053296448 call_variants.py:452] Processed 1 examples in 1 batches [1611.487 sec per 100]; ```. ```; real	0m53.331s; user	0m35.346s; sys	0m22.537s; ```; It seems there is an error that I don't find or understand",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917442547:2514,Optimiz,Optimization,2514,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547,1,['Optimiz'],['Optimization']
Performance,"ow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_roo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117711,cache,cached,117711,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"ow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77341,cache,cache,77341,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71081,cache,cache,71081,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"p.s. My coworker let me know that the reason the first three records in your quoted gVCF are split into three rather than a single one is an implementation detail, where the genome is processed by concurrent processes operating on 1,000 bp chunks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/282#issuecomment-954075521:197,concurren,concurrent,197,,https://github.com/google/deepvariant/issues/282#issuecomment-954075521,1,['concurren'],['concurrent']
Performance,"packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30544,cache,cache,30544,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"per; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:83022,cache,cache,83022,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"per; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:90367,cache,cache,90367,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"per; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Tracebac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:95264,cache,cache,95264,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"pgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash; $ pip3 freeze; absl-py==2.1.0; apache-beam==2.50.0; astunparse==1.6.3; cachetools==5.3.3; certifi==2024.2.2; charset-normalizer==3.3.2; cloudpickle==2.2.1; crcmod==1.7; Deprecated==1.2.14; dill==0.3.1.1; dnspython==2.6.1; docopt==0.6.2; fastavro==1.9.4; fasteners==0.19; flatbuffers==24.3.7; gast==0.4.0; google-api-core==2.17.1; google-apitools==0.5.31; google-auth==2.28.2; google-auth-httplib2==0.1.1; google-auth-oauthlib==1.0.0; google-cloud-aiplatform==1.44.0; google-cloud-bigquery==3.19.0; google-cloud-bigquery-storage==2.24.0; google-cloud-bigtable==2.23.0; google-cloud-core==2.4.1; google-cloud-datastore==2.19.0; google-cloud-dlp==3.16.0; google-cloud-language==2.13.3; google-cloud-pubsub==2.20.2; google-cloud-pubsublite==1.9.0; google-cloud-recommendations-ai==0.10.10; google-cloud-resource-manager==1.12.3; google-cloud-spanner==3.44.0; google-cloud-storage==2.16.0; google-cloud-videointelligence==2.13.3; google-cloud-vision==3.7.2; google-crc32c==1.5.0; google-pasta==0.2.0; google-resumable-media==2.7.0; googleapis-common-protos==1.63.0; grpc-google-iam-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:5682,cache,cachetools,5682,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['cache'],['cachetools']
Performance,"pled_05.shuffle_2_pt4-00013-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00014-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00015-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00016-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00017-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00018-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00019-of-00020.tfrecord.gz; ```. And for the tuning set (which was shuffled normally using just one step):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:8544,tune,tune,8544,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"portError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117553,cache,cached,117553,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,pvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_tra,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71781,cache,cache,71781,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117421,cache,cached,117421,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46837,cache,cache,46837,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53424,cache,cache,53424,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"r; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instruct ions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate comp iler flags.; 2020-03-27 13:32:13.625441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2095014999 Hz; 2020-03-27 13:32:13.625754: I tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:4725,optimiz,optimized,4725,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,"rations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_34o2lccw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:2158,optimiz,optimized,2158,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"re is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec per 100]; I1019 09:14:09.554191 140673783289600 call_variants.py:538] Processed 60001 examples in 118 batches [0.011 sec per 100]; I1019 09:14:11.247823 140673783289600 call_variants.py:538] Processed 75001 examples in 147 batches [0.011 sec per 100]; I1019 09:14:12.950735 140673783289600 call_variants.py:538] Processed 90001 examples in 176 batches [0.011 sec per 100]; ...; ```. The st",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-712402658:1426,optimiz,optimizations,1426,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658,1,['optimiz'],['optimizations']
Performance,"rectory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117639,cache,cached,117639,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"rent folder than deepvariant-r1.5 folder.; Just fyi, there are multiple installation of bazel: `/home/user/.bazel/bin` and `/usr/bin/`; The one that is working is /usr/bin/bazel. The command `/usr/bin/bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries` inside the deepvariant-r1.5 folder gives another error:; ```; ERROR: An error occurred during the fetch of repository 'local_config_python':; Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:979,cache,cache,979,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1847,Load,Load,1847,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Load'],['Load']
Performance,"rflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:100134,cache,cache,100134,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"riant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124181,cache,cache,124181,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"riant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125532,cache,cache,125532,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"riant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 557, in validate_reference_contig_coverage; ref_bp, common_bp, coverage, format_contig_matches())); ValueError: Reference contigs span 3095677412 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""1"" is 249250621 bp and IS MISSING, ""2"" is 243199373 bp and IS MISSING, ""3"" is 198022430 bp and IS MISSING, ""4"" is 191154276 bp and IS MISSING, ""5"" is 180915260 bp and IS MISSING, ""6"" is 171115067 bp and IS MISSING, ""7"" is 159138663 bp and IS MISSING, ""8"" is 146364022 bp and IS MISSING, ""9"" is 141213431 bp and IS MISSING, ""10"" is 135534747 bp and IS MISSING, ""11"" is 135006516 bp and IS MISSING, ""12"" is 133851895 bp and IS MISSING, ""13"" is 115169878 bp and IS MISSING, ""14"" is 107349540 bp and IS MISSING, ""15"" is 102531392 bp and IS MISSING, ""16"" is 90354753 bp and IS MISSING, ""17"" is 81195210 bp and IS MISSING, ""18"" is 78077248 bp and IS MISSING, ""19"" is 59128983 bp and IS MISSING, ""20"" is 63025520 bp and IS MISSING, ""21"" is 48129895 bp and IS MISSING, ""22"" is 51304566 bp and IS MISSING, ""X"" is 155270560 bp and IS MISSING, ""Y"" is 59373566 bp and IS MISSING; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed; Using mount point: /input-gcsfused-0; Opening GCS connection...; Opening bucket...; Mounting file system...; File system has been successfully mounted.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437596560:3635,perform,performance-testdata,3635,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560,1,['perform'],['performance-testdata']
Performance,"root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:108115,cache,cache,108115,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ror: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77783,cache,cache,77783,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ry the suggestion in https://stackoverflow.com/a/54746150 and let me know if that worked for you?. ---. For completeness, here is what I tried:. I got a AMD machine:. ```; gcloud compute instances create ""${USER}-amd"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""n2d-standard-16"" --boot-disk-size ""100"" \; --zone ""europe-west4-b""; ```. On that machine, I ran:; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: AuthenticAMD; CPU family: 23; Model: 49; Model name: AMD EPYC 7B12; Stepping: 0; CPU MHz: 2249.998; BogoMIPS: 4499.99; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 512K; L3 cache: 16384K; NUMA node0 CPU(s): 0-15; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save umip rdpid; ```. Then, I just directly run the WES case study with docker:; ```; $ curl https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wes_case_study_docker.sh | bash -x ; ```. Once make_examples started running, I used “top” to look at the jobs:; ![image](https://user-images.githubusercontent.com/471813/76821677-da7f0180-67cb-11ea-9caa-5cdd01378c00.png). Because I have 16 cores, as expected, 16 python jobs were running. All seem to take close to 100% CPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-599870096:1313,cache,cache,1313,,https://github.com/google/deepvariant/issues/274#issuecomment-599870096,4,['cache'],['cache']
Performance,"ry_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configure",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2869,cache,cache,2869,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3898,Optimiz,Optimizer,3898,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['Optimiz'],['Optimizer']
Performance,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:5564,optimiz,optimize,5564,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['optimiz'],['optimize']
Performance,"s with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint best checkpoint metric: tune/f1_weighted=0.21346085. real 1m12.933s; use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1403,Tune,Tune,1403,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['Tune'],['Tune']
Performance,"s, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1260,perform,perform,1260,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['perform'],['perform']
Performance,"s. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1148,optimiz,optimizations,1148,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['optimiz'],['optimizations']
Performance,"s_inc_downsampled_05.shuffle_2_pt4-00018-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00019-of-00020.tfrecord.gz; ```. And for the tuning set (which was shuffled normally using just one step):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9222,tune,tune,9222,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"ser/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2192,cache,cache,2192,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"shuffle_2_pt4-00014-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00015-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00016-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00017-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00018-of-00020.tfrecord.gz; ./examples_shuffled/train/shuffle_2_outputs/All_samples_all_training_examples_inc_downsampled_05.shuffle_2_pt4-00019-of-00020.tfrecord.gz; ```. And for the tuning set (which was shuffled normally using just one step):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./exam",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:8703,tune,tune,8703,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"sion). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1230,perform,perform,1230,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['perform'],['perform']
Performance,"sorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:74872,cache,cache,74872,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"sorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:75218,cache,cache,75218,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"stall development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7227,optimiz,optimization,7227,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['optimiz'],['optimization']
Performance,"sting //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/testdata.py"", line 39, in <module>; from third_party.nucleus.testing import test_utils as nucleus_test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:78468,cache,cache,78468,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e61",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117317,cache,cached,117317,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"t.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most rece",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:101226,cache,cache,101226,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,t/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_e,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119311,cache,cache,119311,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1596,cache,cache,1596,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,t/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (sh,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1422,cache,cache,1422,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:99791,load,load,99791,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:13876,load,load,13876,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16334,load,load,16334,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:18571,load,load,18571,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:20726,load,load,20726,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labele",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32308,load,load,32308,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_par",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25093,load,load,25093,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:27909,load,load,27909,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23018,load,load,23018,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30064,load,load,30064,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35188,load,load,35188,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37343,load,load,37343,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:41784,load,load,41784,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44193,load,load,44193,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_tes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:39498,load,load,39498,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:50776,load,load,50776,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:48627,load,load,48627,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46342,load,load,46342,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57363,load,load,57363,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55214,load,load,55214,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:52929,load,load,52929,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:59512,load,load,59512,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:63953,load,load,63953,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_trai",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:61661,load,load,61661,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66108,load,load,66108,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68263,load,load,68263,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:70418,load,load,70418,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_goog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:76962,load,load,76962,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:74525,load,load,74525,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nuc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82285,load,load,82285,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_traine",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:79946,load,load,79946,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debrui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:84779,load,load,84779,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nuc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:89630,load,load,89630,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87350,load,load,87350,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nuc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:94527,load,load,94527,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97021,load,load,97021,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92124,load,load,92124,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:102965,load,load,102965,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; fro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:109905,load,load,109905,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:105821,load,load,105821,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from thi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:111980,load,load,111980,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114488,load,load,114488,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:116635,load,load,116635,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['load'],['load']
Performance,"t_calling/. Inside this directory are two subdirectories: inputs (in which the assembly and bam files are) and outputs (in which the results go). 1. The commands run were:. BIN_VERSION=""1.5.0""; docker pull google/deepvariant:""${BIN_VERSION}"". INPUT_DIR=""${PWD}/inputs""; OUTPUT_DIR=""${PWD}/outputs"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa --reads=/input/NC_045426.1_A_filt_fixed_markdup_csort.bam --output_vcf=/output/NC_045426.1_A.vcf.gz --output_gvcf=/output/NC_045426.1_A.g.vcf.gz --intermediate_results_dir /output/NC_045426.1_A_intermediate_results_dir --num_shards=1 &. 3. The complete output of errors is:. 2023-08-30 21:45:18.422516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0830 21:45:20.409601 140002879141696 run_deepvariant.py:364] Re-using the directory for intermediate results in /output/NC_045426.1_A_intermediate_results_dir. ***** Intermediate results will be written to /output/NC_045426.1_A_intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/GCF_902635505.1_mSarHar1.11_genomic_baicompat.fa"" --reads ""/input/NC_045426.1_A_filt_fixed_markdup_csort.bam"" --examples ""/output/NC_045426.1_A_intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/NC_045426.1_A_intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {}. 2023-08-30 21:45:21.348267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625:1088,optimiz,optimized,1088,,https://github.com/google/deepvariant/issues/184#issuecomment-1699892625,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"t_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44932,cache,cache,44932,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:49366,cache,cache,49366,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:58102,cache,cache,58102,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55953,cache,cache,55953,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"t_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:60251,cache,cache,60251,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ta/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; > with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/core/genomics_io.py"", line 143, in make_sam_reader; > random_seed=random_seed)); > ValueError: Not found: No index found for /home/paul/data/luisa/ENCFF528VXT.bam; > paul@gubuntu:~/deepvariant/bazel-bin$; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/go",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2958,cache,cache,2958,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,2,['cache'],['cache']
Performance,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10009,cache,cached,10009,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['cache'],['cached']
Performance,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-603439134:747,cache,cache,747,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134,4,['cache'],['cache']
Performance,"te-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Trace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16812,cache,cache,16812,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; Fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92469,cache,cache,92469,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"tep):. ```; >cat ./examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt. # Generated by shuffle_tfrecords_beam.py. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 202421; # class0: 202421; #; # --input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9490,tune,tune,9490,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,"ternal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:85560,cache,cache,85560,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ternal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117248,cache,cached,117248,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118046,cache,cache,118046,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:104356,cache,cache,104356,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"th; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ──────────────────────────────────────────────────────────────────────────────; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pasta 0.2.0 pyh8c360ce_0 conda-forge; grpcio 1.38.1 py36h8e87921_0 conda-forge; h5py 3.1.0 nompi_py36hc1bc4f5_100 conda-forge; hdf5 1.10.6 no",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1898,cache,cachetools,1898,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['cache'],['cachetools']
Performance,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:829,perform,performance-testdata,829,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['perform'],['performance-testdata']
Performance,"that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1941,tune,tune,1941,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['tune'],['tune']
Performance,then I found the call_variants.py is listed in /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants.py,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/448#issuecomment-827132888:54,cache,cache,54,,https://github.com/google/deepvariant/issues/448#issuecomment-827132888,1,['cache'],['cache']
Performance,"this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:104046,cache,cache,104046,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"thon library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/use",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2441,cache,cache,2441,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:21223,cache,cache,21223,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"thon/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:62158,cache,cache,62158,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"thon3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack); 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype); 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.47",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:5979,optimiz,optimizations,5979,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['optimiz'],['optimizations']
Performance,timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:ha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118879,cache,cache,118879,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"tire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:113039,cache,cache,113039,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"to run separate calling on the non-PAR regions of ChrX and ChrY, where only the mother sample is provided as the parent for calling of the son, and (less importantly as it is unclear whether this is an issue with chrY) only the father sample is provided for calling chrY on the son. In the documentation, this is expressed in the following statement: ""Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be perfor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1235,perform,perform,1235,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,2,['perform'],['perform']
Performance,"tp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4567,Load,Load,4567,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,3,"['Load', 'load']","['Load', 'load']"
Performance,"ts.py:452] Processed 570001 examples in 1114 batches [0.011 sec per 100]; I1128 03:47:59.149574 139674856871680 call_variants.py:452] Processed 585001 examples in 1143 batches [0.011 sec per 100]; I1128 03:48:00.813269 139674856871680 call_variants.py:452] Processed 600001 examples in 1172 batches [0.011 sec per 100]; I1128 03:48:02.468808 139674856871680 call_variants.py:452] Processed 615001 examples in 1202 batches [0.011 sec per 100]; I1128 03:48:04.122274 139674856871680 call_variants.py:452] Processed 630001 examples in 1231 batches [0.011 sec per 100]; I1128 03:48:05.762554 139674856871680 call_variants.py:452] Processed 645001 examples in 1260 batches [0.011 sec per 100]; I1128 03:48:07.409487 139674856871680 call_variants.py:452] Processed 660001 examples in 1290 batches [0.011 sec per 100]; I1128 03:48:08.445094 139674856871680 call_variants.py:455] Processed 669335 examples in 1308 batches [0.011 sec per 100]; I1128 03:48:08.445318 139674856871680 call_variants.py:458] Done calling variants from a total of 669335 examples. real 15m12.564s; user 763m44.970s; sys 58m35.140s; ```. You can see these lines:; ```; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; ```. Before the `03:46:54.531774` timestamp, the last timestamp was `03:33:02.980482`. I don't know if this expected or not. I'm curious to run this on the whole genome and see whether the speedup will be more noticeable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276922:9214,optimiz,optimizations,9214,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922,1,['optimiz'],['optimizations']
Performance,"ttplib; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""xx/anaconda/envs/Python27/lib/python2.7/httplib.py"", line 80, in <module>; import mimetools; File ""xx/anaconda/envs/Python27/lib/python2.7/mimetools.py"", line 6, in <module>; import tempfile; File ""xx/anaconda/envs/Python27/lib/python2.7/tempfile.py"", line 35, in <module>; from random import Random as _Random; File ""xx/anaconda/envs/Python27/lib/python2.7/random.py"", line 45, in <module>; from math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil; ***File ""math.py"", line 79, in <module>***; import numpy as np; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import add_newdocs; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/add_newdocs.py"", line 13, in <module>; from numpy.lib import add_newdoc; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/__init__.py"", line 8, in <module>; from .type_check import *; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/lib/type_check.py"", line 11, in <module>; import numpy.core.numeric as _nx; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/core/__init__.py"", line 74, in <module>; from numpy.testing.nosetester import _numpy_tester; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/__init__.py"", line 12, in <module>; from . import decorators as dec; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/decorators.py"", line 20, in <module>; from .utils import SkipTest, assert_warns; File ""xx/anaconda/envs/Python27/lib/python2.7/site-packages/numpy/testing/utils.py"", line 15, in <module>; from tempfile import mkdtemp, mkstemp; ImportError: cannot import name mkdtemp; >>> ; ```; As you can see, it attempts to load the local `math.py`, shadowing the standard `math` module.; On the other hand, cd to another path and `import httplib` does not have any problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32#issuecomment-355522771:2546,load,load,2546,,https://github.com/google/deepvariant/issues/32#issuecomment-355522771,2,['load'],['load']
Performance,ue for this or take it somewhere else this is TensorFlow-specific. It seems that TensorFlow `r1.12` installed duing the deepvariant build is looking for CUDA 9:. ```; FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_t,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1074,cache,cache,1074,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ught TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tune/true_positives_1=1557.0; I0508 17:54:10.083408 140534986602304 train.py:394] Warmstart checkpoint be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1327,tune,tune,1327,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['tune'],['tune']
Performance,"un_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1618,cache,cached,1618,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['cache'],['cached']
Performance,une/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00015-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00016-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00017-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00018-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00019-of-00020.tfrecord.gz; ```; Hope thats useful!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10600,tune,tune,10600,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,9,['tune'],['tune']
Performance,"user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3162,cache,cache,3162,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"y"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114841,cache,cache,114841,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"y.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1328,load,loaded,1328,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['load'],['loaded']
Performance,"ython/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:39994,cache,cache,39994,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ython2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117188,cache,cached,117188,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,"ywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23359,cache,cache,23359,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:110246,cache,cache,110246,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"ywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92809,cache,cache,92809,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"zel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122419,cache,cache,122419,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 2 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 6 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 3 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 7 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; 4 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%] 8 [||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||100.0%]; Mem[||||||||||||||||||||||||||||||||| 5.45G/51.0G] Tasks: 67, 77 thr; 8 running; Swp[ 0K/0K] Load average: 8.09 7.59 4.84 ; Uptime: 01:04:14; ```. make_examples took:; ```; real 32m36.929s; user 253m55.294s; sys 1m1.715s; ```. ## call_variants; At the beginning of `call_variants`, I see:; ```; 2021-06-22 21:25:07.709510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-06-22 21:25:07.731210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000170000 Hz; 2021-06-22 21:25:07.731675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5e87820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2021-06-22 21:25:07.731713: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2021-06-22 21:25:07.734891: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; ```; which confirms that I'm using AVX optimization. The log for call_variants is pretty short, because WES has fewer examples to run on. My `call_variants` log look like this:; ```; I0622 21:25:17.009006 140301916206848 sa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:4458,optimiz,optimized,4458,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Safety," ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9325,predict,predict,9325,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['predict'],['predict']
Safety," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1191,detect,detects,1191,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,2,['detect'],['detects']
Safety," GT from the two runs indicates that the neural network assesses the probability of 0/1 and 1/1 calls to be very similar (in the first entry they are rounded identically in the PL field). Your collaborator calls do have a small lean toward 1/1. DeepVariant should give identical results when the same version is run on the same hardware platform (e.g. CPU-CPU). However, there can be floating point differences with very minor (almost never at the level of the variant call, but mostly at the GQ level) between compute platforms. Can you confirm that you and your collaborator ran the exact same version of DeepVariant on the same compute platform, or if there might be a difference (e.g. CPU vs Parabricks GPU). 2) **Why is the call here 0/1 given the pileup**. The IGV screenshot you've attached certainly looks 1/1, and all experts will assess it as a 1/1 from what is shown. We have observed that in rare circumstances, DeepVariant will occasionally call such positions as 0/1 or 0/0 or to decrease the confidence in the calls of certain positions. The signature for this seems to be when DeepVariant assesses a region to be likely to be a segmental duplication or structural variant. Signatures for that often involve a haplotype with dense variants while another haplotype is almost entirely reference, or a high amount of discordant read mapping or low MAPQ. Although your pileup does have a discordantly mapped read present, those patterns generally aren't present. For some reason, in both your and your collaborator's run, DeepVariant seems to think that this region is difficult to call. . In these cases, I'm always interested to see whether this could highlight a bug in DeepVariant, or if it reflects some learning of the model. Would there be any chance for you to share the small window of the BAM file here (maybe +/- 500 bp from the position). People in the team would be interested in whether this could detect any sort of edge case DeepVariant isn't handling well. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716:2112,detect,detect,2112,,https://github.com/google/deepvariant/issues/592#issuecomment-1332875716,2,['detect'],['detect']
Safety," directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict; hooks=all_hooks) as mon_sess:; File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:8900,predict,prediction,8900,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,4,['predict'],"['prediction', 'predictions']"
Safety," for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4713,recover,recovery,4713,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety," line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1317, in _request; (response, content) = self._conn_request(conn, request_uri, method, body, headers); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1252, in _conn_request; conn.connect(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1018, in connect; sock.connect((self.host, self.port)); File ""/home/ydliu/anaconda3/envs/py2.7/lib/python2.7/socket.py"", line 228, in meth; return getattr(self._sock,name)(*args); socket.timeout: timed out. return code: 1. ()",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549130970:7485,timeout,timeout,7485,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970,1,['timeout'],['timeout']
Safety," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1715,avoid,avoid,1715,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466,2,['avoid'],['avoid']
Safety," to this variant call, there are a few things that I see: . First, DeepVariant has a very low confidence in this call. The GQ of 4 corresponds to a 40% chance of being incorrect. Interestingly, the second most likely call at this position according to DeepVariant is 0/0 (HomRef). This is in spite of a 0.27 variant allele frequency (something that most callers would probably consider as either 0/0 HomRef or 0/1 HET). . This is an indication that DeepVariant may think that one of the haplotypes (either the Ref one or the Alt one) are unreliable (e.g. that they are reads which map from a different part of the genome), but doesn't know which to consider. Some other lines of evidence DeepVariant might use for that is the higher coverage (1200 is a coverage that would often be seen in duplicated parts of the genome) and the unusual allele frequency ratio (70% Ref, 30% Alt). Another piece of evidence we can't see but DeepVariant may use is the Insert Size channel if this is Illumina data. . A few questions -. 1) What is the sequencing technology used here, and which type of instrument. Is this PacBio or Illumina here? ; 2) Is this some form of panel sequencing targeting the region? . One suggestion to try (especially if this is panel short read sequencing) - downsample the region to ~80-100 coverage and see if the call changes. Especially look for the GQ confidence to go up as 4 is very low. If you want to avoid such situations more, you might want to put a higher GQ threshold for downstream filtering, and for a specific case like this look for cases where the PL values show higher probability for HOMREF and ALT than HET. A GQ threshold of 10 would be a 90% probability the call is correct, a GQ threshold of 20 would be a 99% threshold. . Just from the evidence presented here, this site is going to be difficult to call, as just on the VAF this look more like a HET (though the other factors mentioned here about copy number variants could be the factor in allowing a REF call.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832:1553,avoid,avoid,1553,,https://github.com/google/deepvariant/issues/655#issuecomment-1570674832,2,['avoid'],['avoid']
Safety,"(https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), bu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1857,recover,recovery,1857,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"**Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2520,recover,recovery,2520,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:571,abort,abort,571,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,2,['abort'],['abort']
Safety,". **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Gen",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1766,recover,recovery,1766,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/A2LCPRQWWLAYOZXICW5LXSDVXQAGNANCNFSM55QXIB6A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:2183,avoid,avoid,2183,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158,1,['avoid'],['avoid']
Safety,"/github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details.md) might help, but it's a bit general. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1416,predict,predict,1416,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524,4,['predict'],['predict']
Safety,"/hs37d5.fa.gzi; ```. Then, I ran `make_examples` similar to the way you did in your original post:; ```; ## Run `make_examples`; ( time seq 0 $((N_SHARDS-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""hs37d5.fa.gz"" \; --reads ""151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --regions ""agilent_sureselect_human_all_exon_v5_b37_targets.bed"" \; --gvcf ""HG002.gvcf.tfrecord@${N_SHARDS}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; This took on 13m33.192s a 64-core, 128GB RAM machine. Before I proceeded with call_variants, I first checked that the output files from make_examples exist:; ```; ls HG002.examples.tfrecord*.gz | wc -l; ```; I see 64 of them here.; A common issue is that if the make_examples step failed but you didn't notice, then the next step will fail.; Common failure modes I've seen before:; - if you were running make_examples, but abort in the middle by ctrl-c. Sometimes not all the make_examples in the background were killed. If you just re-run make_examples without killing all background make_examples first, the output might be corrupted.; - if make_examples failed completely without outputting HG002.examples.tfrecord*.gz at all, it'll also cause a failure. Our hope is that you'll notice this in the errors that make_examples displayed. If you're creating some kind of workflow yourself, you will need to make sure you check the error code of the runs. If make_examples died, you shouldn't proceed with call_variants. After my make_examples run and confirming that I have the output files, I ran call_variants:; ```; ## Run `call_variants`; ( time \; /opt/deepvariant/bin/call_variants \; --outfile ""HG002.cvo.tfrecord.gz"" \; --examples ""HG002.examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""model.ckpt"" \; ) 2>&1 | tee ""call_variants.log"" &; ```; When I run this on the same 64-core, 128GB RA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461411282:3916,abort,abort,3916,,https://github.com/google/deepvariant/issues/151#issuecomment-461411282,2,['abort'],['abort']
Safety,"/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_ses",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15591,predict,predict,15591,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['predict'],['predict']
Safety,"18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work; async-timeout==3.0.1; attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work; blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work; brotlipy==0.7.0; cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work; cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work; certifi==2021.5.30; cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work; chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work; charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work; click @ fi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:2073,timeout,timeout,2073,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,1,['timeout'],['timeout']
Safety,"29842433856 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_varia",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3234,sanity check,sanity check,3234,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['sanity check'],['sanity check']
Safety,"299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15421,predict,predict,15421,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['predict'],['predict']
Safety,"2], [1], [1, 2], [2]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 962, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 807, in _transform_call_variants_output_to_variants; multiallelic_model=multiallelic_model); File ""/tmp/Bazel.runfiles_fbg2uft9/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 680, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/341#issuecomment-686397941:3639,sanity check,sanity check,3639,,https://github.com/google/deepvariant/issues/341#issuecomment-686397941,2,['sanity check'],['sanity check']
Safety,"3, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9445,predict,predict,9445,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['predict'],['predict']
Safety,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5067,Detect,Detecting,5067,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,8,['Detect'],['Detecting']
Safety,"5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:3828,predict,prediction,3828,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918,1,['predict'],['prediction']
Safety,"6 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2172,safe,safe,2172,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['safe'],['safe']
Safety,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3692,abort,aborted,3692,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['abort'],['aborted']
Safety,"====================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2227,Timeout,Timeout,2227,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,1,['Timeout'],['Timeout']
Safety,"> @liukeweiaway are these human samples? Looks like the program is running fine, it's just not finding any variants. Can you please explain a bit more to what exactly is your data?. It is a human sample, and the generated data is the same as the reference genome. When no mutation is detected, the program will not stop and will continue to run. You need to stop the program manually.; [chr6_CYP21A2.bwa.read1.fastq.gz](https://github.com/user-attachments/files/16420817/chr6_CYP21A2.bwa.read1.fastq.gz); [chr6_CYP21A2.bwa.read2.fastq.gz](https://github.com/user-attachments/files/16420818/chr6_CYP21A2.bwa.read2.fastq.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855#issuecomment-2257306781:284,detect,detected,284,,https://github.com/google/deepvariant/issues/855#issuecomment-2257306781,1,['detect'],['detected']
Safety,"> Hi @ZuyaoLiu ,; > ; > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file?; > ; > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue!; > ; > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already.; > ; > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1823460228:550,avoid,avoid,550,,https://github.com/google/deepvariant/issues/725#issuecomment-1823460228,2,"['avoid', 'sanity check']","['avoid', 'sanity check']"
Safety,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-577247342:922,avoid,avoiding,922,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342,1,['avoid'],['avoiding']
Safety,"> Hi @linlin-coder , Thank you for bringing up this issue.; > ; > I noticed that you're working on PacBio data.; > ; > The reason why this is happening is:; > ; > In v1.3.0 or older, DeepVariant didn't have the read haplotagging functionality built in. If you see https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md , you'll see that we asked our users to run DeepVariant --> WhatsHap --> DeepVariant with HP information to get the best accuracy.; > ; > Starting from v1.4.0, **DeepVariant** has the read haplotagging functionality built in. However, **DeepTrio** doesn't have this yet, even in the latest version. That's why in https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-pacbio-case-study.md , you'll see this sentence; > ; > > The `--use_hp_information` arg makes use of a phased reads, thus allowing a further improvement of the accuracy. You can use tools like whatshap to phase.; > ; > So, for now, if you want to get the best accuracy using DeepTrio, our suggestion will be: Run DeepVariant first --> Use WhatsHap to get HP tags for your read --> Run DeepTrio with the `--use_hp_information` tag.; > ; > I'm sorry that we were not being a lot more clear in our documentation. In our next release (plan to be out before the end of this year), we will have the read haplotagging feature directly built in in DeepTrio as well, so that you won't have to worry about having to run WhatsHap in between, starting from our next release.; > ; > @linlin-coder Let me know if you have more questions that I can help clarify. Thanks!. Thank you very much for your description of all the details of Deeptrio in the mutation detection environment. In the future, I will follow the process you suggested to redo the mutation detection. If there are no accidents, I will reply to you in the next two days. Thank you again",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840:1676,detect,detection,1676,,https://github.com/google/deepvariant/issues/689#issuecomment-1661404840,4,['detect'],['detection']
Safety,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736278644:374,safe,safer,374,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644,1,['safe'],['safer']
Safety,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:486,safe,safer,486,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,2,['safe'],['safer']
Safety,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct?. Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471716420:337,sanity check,sanity check,337,,https://github.com/google/deepvariant/pull/159#issuecomment-471716420,1,['sanity check'],['sanity check']
Safety,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383:78,avoid,avoid,78,,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383,1,['avoid'],['avoid']
Safety,"@AndrewCarroll Thank you for the information. In the description above, you mentioned that deepvariant sees two candidates:. - CAGCAGCGCT -> C; - C -> T. However, I thought deepvariant would detect the following as the two candidates (Applying vt decompose produces the same as the following):; - CAGCAGCGCT -> C; - CAGCAGCGCT -> T. Am I misunderstanding this?. Going back to the GT field, can it be that this behavior is changed in version 1.5.0?; Looking results of deepvariant version 1.2.0, the GT field for all multi-allelic variants only supports the first alternate allele. I tried running deepvariant version 1.5.0 on the same alignment file, and from 47 multi-allelic variants, almost all have a GT field that supports both of the alternate alleles. Here is an example:. Here is an example:; Result of deepvariant version 1.2.0:; ```; NC_000001.11	6545786	.	C	A,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:40:91:0,54,37:0.593407,0.406593:50,44,53,44,0,61; ```; Result of deepvariant version 1.5.0:; ```; NC_000001.11	6545786	.	C	A,T	57	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:48:91:0,54,37:0.593407,0.406593:57,52,61,52,0,66; ````. However, from 47 multi-allelic variants, 4 have a GT field presented as 0/1. Is it safe to further process the VCF file to only keep the first allele for these 4 variants since deepvariant did not call them?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/618#issuecomment-1461845240:191,detect,detect,191,,https://github.com/google/deepvariant/issues/618#issuecomment-1461845240,2,"['detect', 'safe']","['detect', 'safe']"
Safety,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:44,detect,detecting,44,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491,2,['detect'],['detecting']
Safety,"@George-du That's a connection issue with Docker's CDN and/or Docker the way it operates on your side. Here's a link to possible solutions:. https://forums.docker.com/t/pulling-docker-images-i-o-timeout/740/13. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668#issuecomment-1602330299:195,timeout,timeout,195,,https://github.com/google/deepvariant/issues/668#issuecomment-1602330299,1,['timeout'],['timeout']
Safety,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729:1012,detect,detection,1012,,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729,1,['detect'],['detection']
Safety,"@Rofidagamal it looks like this is your error:. ```; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted; ```. Which suggests that the reference may be incomplete or truncated. Can you double check that the reference file is complete? Can you verify that all the contigs are present?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1298853697:341,Abort,Aborted,341,,https://github.com/google/deepvariant/issues/581#issuecomment-1298853697,1,['Abort'],['Aborted']
Safety,"@SirKuikka ,. DeepVariant will try to ""genotype"" each candidate variant observed. Currently it does not have the ability to differentiate between ""somatic"" vs ""germline"". If a variant has high allele frequency then the likelihood to be classified as ""het"" or ""hom-alt"" is high. However, DeepVariant does not use strict heuristics for such classifications so it is difficult to answer what would happen to such variants. However, rare variant detection with high-sensitivity has been reported with DeepVariant (https://www.nature.com/articles/s41525-021-00227-3).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/864#issuecomment-2278841689:442,detect,detection,442,,https://github.com/google/deepvariant/issues/864#issuecomment-2278841689,1,['detect'],['detection']
Safety,"@baozg. After carefully bisecting your BAM file, it looks like the region that throws an error is chr12:7721068-7735636. Looking at the pileup, there are 5 large (~11k) deletions in that region of 3 different lengths:; ![image](https://github.com/google/deepvariant/assets/8753889/18e84dd4-27df-4059-aced-f6f9573e1f9a). One is length `11,843`, two are `11,844` and two are `11,845`. It looks like the trouble comes from attempting to represent and realign those INDEL candidates with 2 reads each. DeepVariant can't actually call deletions that long. If you set the [vsc_min_count_indel](https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/make_examples_options.py#L292-L299) to 3, the problem goes away. So adding `--make_examples_extra_args=vsc_min_count_indels=3` should fix the issue. If desired, you can run DeepVariant on just that region with `--regions=chr12:7721068-7735636`. We will work on fixing this on our end as well in our next release. @yangxin-9 To avoid mixing issues may or may not be related, please create a new issue that shows the command you ran and the output. Also, if possible, please send us the input files used so we can try to reproduce the issue ourselves.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794#issuecomment-2035806347:977,avoid,avoid,977,,https://github.com/google/deepvariant/issues/794#issuecomment-2035806347,1,['avoid'],['avoid']
Safety,"@cmclean Thanks for the additional explanation. So the cases to be filtered out are those with more than 2 alleles predicted per reference position. Also, 300 out of millions of variants does seem to indicate that these cases are really rare after the use of this functionality. @AndrewCarroll @cmclean Thank you for your patience and the helpful explanations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/247#issuecomment-564413332:115,predict,predicted,115,,https://github.com/google/deepvariant/issues/247#issuecomment-564413332,1,['predict'],['predicted']
Safety,"@crazysummerW ; I had the same issue here. It turned out to be the problem of the h5 file in the tmp dir.; If multiple programs open the h5 simultaneously, the error would occur. So I avoided this by creating a unique tmp dir for each sample, which used a lot of file handles. @pichuan @kishwarshafin Could you please take a look at this? Probably renaming the h5 file to keep it unique would be a simple and easy solution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733:184,avoid,avoided,184,,https://github.com/google/deepvariant/issues/725#issuecomment-1820853733,2,['avoid'],['avoided']
Safety,@danielecook yes，to avoid version issues.i develop within the docker image.Previously I thought that the docker file had already executed build-prereq.sh and build_and_test.sh. I can debug it without having to do it again. But that doesn't seem to work either.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865339760:20,avoid,avoid,20,,https://github.com/google/deepvariant/issues/756#issuecomment-1865339760,1,['avoid'],['avoid']
Safety,"@dkurt With all chromosomes of WGS, the call_variants runtime change is 266m46.183s --> 198m46.734s.; So the runtime reduction is about 25% as well. Thanks for the latest change for tracking progress. I'll try it out and let you know if there's any issues. In terms of getting the code in, I'll see if I can get the code through internal review before the next release (r1.1). If not, it'll be in the the one after. If this gets in in time the next release (r1.1), I still don't plan to build our release Docker image with this on by default yet, because I'm not exactly sure what's the effect on all use cases. . @dkurt For future releases, do you think it's safe to turn on OpenVINO by default? What do you expect to happen on non-Indel machines?; Thanks!!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735510277:660,safe,safe,660,,https://github.com/google/deepvariant/pull/363#issuecomment-735510277,1,['safe'],['safe']
Safety,"@ejc043 , . Channel 5, read supports variants highlight each read that supports the allele we are trying to predict.; Channel 6, base differs from ref highlights all the bases, irrespective of alleles that do not agree with the reference at that position. Channel 5 captures read status vs reference against the proposed allele while channel 6 captures the base status against the reference irrespective of the allele proposed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/861#issuecomment-2271709401:108,predict,predict,108,,https://github.com/google/deepvariant/issues/861#issuecomment-2271709401,1,['predict'],['predict']
Safety,"@leorippel here is some information that might be relevant from [the docs](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md):. > Because postprocess_variants combines and sorts the output of call_variants, it needs to see all of the outputs from call_variants for a single sample to merge into a final VCF. postprocess_variants is single-threaded and needs a non-trivial amount of memory to run (20-30 GB), so it is best run on a single/dual core machine with sufficient memory. It seems like you don't have much memory left on your machine, so that could be the issue. If you are able to free up some memory, you can run a sanity check by trying one of our [case studies](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-exome-case-study.md), which use smaller files and should consume much less memory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358#issuecomment-704411609:651,sanity check,sanity check,651,,https://github.com/google/deepvariant/issues/358#issuecomment-704411609,1,['sanity check'],['sanity check']
Safety,"@pgrosu Thank you for your reply. This is the error (which started the whole troubleshooting):; ```; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped; /opt/deepvariant/bin/run_deepvariant: line 2: 8 Aborted python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178:239,Abort,Aborted,239,,https://github.com/google/deepvariant/issues/657#issuecomment-1575575178,2,['Abort'],['Aborted']
Safety,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:118,predict,predicted,118,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888,2,['predict'],['predicted']
Safety,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:230,detect,detected,230,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['detect'],['detected']
Safety,"@pichuan . As you know, I am running the shuffle script using Spark. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as well: https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/360#issuecomment-713149241:611,sanity check,sanity check,611,,https://github.com/google/deepvariant/issues/360#issuecomment-713149241,2,['sanity check'],['sanity check']
Safety,"@pichuan @pgrosu This improves my understanding substantially. After reading the paper, my initial impression was that DeepVariant consists of two parts - one part following traditional callers performing local assembly, haplotype detection and candidate allele generation based on the top-two haplotypes, and a second part which is a DNN used to perform candidate filtration. In such a scheme, the second part would be presented a single image per site and it makes a determination among homozygous-ref, heterozygous, and homozygous-alt in an allele-agnostic way - hence it needs a single image per site. In such a caller, 3 allele candidates at a site wouldn't be possible. I was also surprised that such a method could outperform all others, since this approach simply (approximately) replaces the VQSR step in GATK or the RandomForest in Strelka, and a lot would be riding on heuristic-based algorithms for candidate selection. I understand now that all (reasonable) candidates at a site are evaluated and scored against each other using the DNN, which seems to me to be a much more satisfying approach. I am still interested in how different predictions are combined into a single variant quality score, since they are not normalized; hence my request for guidance towards the code source that combines multiple predictions into a variant call. I will go through these resources over the next day. Thanks a lot! Much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512127253:231,detect,detection,231,,https://github.com/google/deepvariant/issues/197#issuecomment-512127253,6,"['detect', 'predict']","['detection', 'predictions']"
Safety,@pichuan Can you inspect more closely the convolutions in inception v3 to trace out what is happening? How about building an ensemble model where the prediction runs through multiple learned models by either majority vote or a more fine-grained decision boundary.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/109#issuecomment-431125198:150,predict,prediction,150,,https://github.com/google/deepvariant/issues/109#issuecomment-431125198,2,['predict'],['prediction']
Safety,"@pichuan I just stumbled upon the same thing, and it took me quite a while to figure out what's going on there. I am running deepvariant v0.9.0 (docker container), and I found that there is quite a lot of files left behind for failed jobs under /tmp on the execution host. Can you comment on deepvariant's behavior when two (or more) DV jobs are scheduled to the same execution host in a cluster setup? Should that be avoided?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175#issuecomment-560473436:418,avoid,avoided,418,,https://github.com/google/deepvariant/issues/175#issuecomment-560473436,1,['avoid'],['avoided']
Safety,"@pichuan, It might be an effect of current implementation - all the processing is done at iterator initialization and then `__getitem__` returns predicted results without delay. We will take a look at overall efficiency and check what can be improved here, thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-712794761:145,predict,predicted,145,,https://github.com/google/deepvariant/pull/363#issuecomment-712794761,1,['predict'],['predicted']
Safety,"@pichuan, thank you! It should be safe to build Docker image with OpenVINO backend and just keep it disabled by default, so users can turn on it only manually by `--call_variants_extra_args=""use_openvino=True""`. OpenVINO import is surrounded by try-catch and I guess that it won't crash on non-Intel CPU:; ```python; try:; from openvino.inference_engine import IECore, StatusCode; except:; pass; ```. Anyway, I'll try to run on some public CI to confirm.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735539751:34,safe,safe,34,,https://github.com/google/deepvariant/pull/363#issuecomment-735539751,1,['safe'],['safe']
Safety,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266:398,predict,prediction,398,,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266,3,['predict'],['prediction']
Safety,"A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FANUP4ZC4YG3AONVCKJ7RT53VW27C7ANCNFSM55E3G3QA&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Ci6wL9MBJUn9xtmb4eLWiMAZi%2BFEFI03EvFd2jZp0rc%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:1465,safe,safelinks,1465,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320,2,['safe'],['safelinks']
Safety,"Amazing, thank you!; Amy. Get Outlook for iOS<https://aka.ms/o0ukef>; ________________________________; From: Kishwar Shafin ***@***.***>; Sent: Sunday, July 31, 2022 7:15:11 PM; To: google/deepvariant ***@***.***>; Cc: Amy Houseman ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] What hg38 would you suggest? (Issue #549). Hi @amyhouseman<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscri",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:376,safe,safelinks,376,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320,2,['safe'],['safelinks']
Safety,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761#issuecomment-1890184989:264,detect,detected,264,,https://github.com/google/deepvariant/issues/761#issuecomment-1890184989,1,['detect'],['detected']
Safety,"And there's bacteria with 4 chromosomes.... On Mon, 31 Jul 2023, 17:50 Joe, ***@***.***> wrote:. > You said it was random clonal organism, il have a look when the reads are; > on the sra database, was just trying to help,; >; > And yes I don't care about SNPs sorry.; >; > (Google people) I'll be here to offer random advice to other people if I'm; > still allowed,; >; > Joe; >; > On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:; >; >> Ok so you have no idea what we are talking about, this is not a; >> prokaryote. I would suggest you avoid hopping into conversations for making; >> completely ignorant comments, not even bothering to check about what; >> organisms we are dealing with.; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; >> .; >> You are receiving this because you commented.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720:546,avoid,avoid,546,,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720,1,['avoid'],['avoid']
Safety,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566496481:370,avoid,avoid,370,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481,1,['avoid'],['avoid']
Safety,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486#issuecomment-984133927:453,predict,predictors,453,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927,4,['predict'],"['predict', 'predictors']"
Safety,"As another sanity check, what's the output of `ls -l ${INPUT_DIR}`?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402#issuecomment-757014380:11,sanity check,sanity check,11,,https://github.com/google/deepvariant/issues/402#issuecomment-757014380,1,['sanity check'],['sanity check']
Safety,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:313,sanity check,sanity check,313,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489,1,['sanity check'],['sanity check']
Safety,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-352893787:609,detect,detect,609,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787,1,['detect'],['detect']
Safety,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480628311:96,avoid,avoids,96,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311,1,['avoid'],['avoids']
Safety,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385706487:571,safe,safe,571,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487,1,['safe'],['safe']
Safety,"Gottcha. Basically, my wish would be for DV to emit MNPs as single VCF line. This is because basically all effect prediction algorithms operate that way. It is technically possible to analyse phased VCFs and get protein-level effect predictions that will look at in-phase SNPs within a coding sequence (Haplosaurus, Bcftools CSQ), but the available tools for that are very limited in what they can annotate and report back. And they are generally not used (much) in (clinical) variant interpretation for that reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1558742268:114,predict,prediction,114,,https://github.com/google/deepvariant/issues/520#issuecomment-1558742268,2,['predict'],"['prediction', 'predictions']"
Safety,"Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling specifically.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364524990:356,predict,predict,356,,https://github.com/google/deepvariant/issues/47#issuecomment-364524990,1,['predict'],['predict']
Safety,"HI!; any update on this topic?. We called multiple genomes recently and found a first example of 4 adjacent base substitution AACT => GGTC being called 4 separate SNVs. The support information is very similar for the 4 calls and I wonder why deepvariant did not call them as one single MNP. This makes that the annotation and effect prediction downstream are wrong. Thanks for your support. ![failed_MNP-call](https://github.com/google/deepvariant/assets/858516/a02c3b5a-0362-4030-be8a-8b3c49129a8f). here is the extract of the gVCF at that location for one sample. ```; chrNN 51225801 . T <*> 0 . END=51225807 GT:GQ:MIN_DP:PL 0/0:50:27:0,81,809; chrNN 51225808 . A G,<*> 30.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:26:17,9,0:0.346154,0:30,0,37,990,990,990; chrNN 51225809 . A G,<*> 31.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,38,990,990,990; chrNN 51225810 . C T,<*> 31.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:26:17,9,0:0.346154,0:31,0,39,990,990,990; chrNN 51225811 . T C,<*> 32.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:28:17,9,0:0.321429,0:32,0,37,990,990,990; chrNN 51225812 . C <*> 0 . END=51225881 GT:GQ:MIN_DP:PL 0/0:48:26:0,87,869; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315:333,predict,prediction,333,,https://github.com/google/deepvariant/issues/238#issuecomment-1700653315,1,['predict'],['prediction']
Safety,"Happy to see the reply,actually, i run this on hpc and the usage of the gpu is very low , but the cpu and memory usage is extremely high , then i run this command line on my laptop with gtx 1050ti and compare the time of the prediction one batch ,the time in the hpc is longer than my laptop , but the truth is the performance of the hpc gpu is better than gtx1050ti. So, the gpu don't work. I will post what you want later.Thx!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2116915079:225,predict,prediction,225,,https://github.com/google/deepvariant/issues/820#issuecomment-2116915079,1,['predict'],['prediction']
Safety,"He Andrew. It's a diploid organism with an ancient tetraploid structure. That's ancient enough it should avoid any mapping issue.. so, diploid it is. . Yes ... What we saw in the experimental evolution results, is that some independent lines get the same SNP. It doesn't seem random, like there were genetic clusters in the clonal ""or so we thought"" amcestral population. Still, du the mode of reproduction and very ""obbious signal"" I would not expect anything less than 10% of all alleles to be relevant. Does this answer your question? . For this question to be complete: I forgot to link the answer of Clair3 authors https://github.com/HKU-BAL/Clair3/issues/210#issuecomment-1642527205. I don't favour one over the other nor am I trying to start a conflict ^^ but I think it's an interesting technical discussion! . Thank you ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1642808771:105,avoid,avoid,105,,https://github.com/google/deepvariant/issues/682#issuecomment-1642808771,1,['avoid'],['avoid']
Safety,"Hello Charles,. Let me take your questions point by point. 1) The PrecisionFDA entry is rpoplin-dv42. The corresponds to a substantially earlier version of DeepVariant, but the core elements (deep neural network classification of examples) is the same. Ryan Poplin and Mark DePristo were at Verily at the time, and since transferred to Google Brain. (Verily is partially owned by Google). 2) In the PrecisionFDA Truth Challenge, the HG002 truth set was characterized by NIST but these calls were fully with-held. As a result, this challenge represents the only time that a well-characterized genome was hidden from all developers and offered a unique opportunity to avoid over-fitting. To preserve the validity of this, we never train on HG002 in Illumina data. As noted above, the version of DeepVariant submitted in PrecisionFDA was early, and subsequent improvements improved accuracy both on this sample and for other instruments and PCR preparations. With these accuracy improvements, DeepVariant unambiguously outperforms other entries in both SNP and Indel:. There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476392585:666,avoid,avoid,666,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585,2,['avoid'],['avoid']
Safety,"Hello Ryan,. Thanks for the information. We have about 500 curated vcf on a gene panel we use here. I will try to create a model for that panel. We want variants as low as 1%. Have you tried something similar?. Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364650047:846,predict,predict,846,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047,1,['predict'],['predict']
Safety,"Hello, I tried but this returns ; ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/pyth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268#issuecomment-586584341:593,predict,prediction,593,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341,3,['predict'],"['predict', 'prediction', 'predictions']"
Safety,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385701252:607,safe,safe,607,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252,1,['safe'],['safe']
Safety,"Hello,. I did make 64 examples from the GIAB exome data mentioned on the github site. I encountered the same problem I mentioned. I’ve attached an archive, bundle.zip that has important files. The file nohup.out shows what was returned when I ran model_train from the command line. Examples were made using the shell script in the bundle: testModeExamples.sh. I’ve included the two python scripts I’ve altered for my deep sequencing project. I appreciate your help. Let me know if there is more I should provide. Thank you,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this beca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381247700:997,safe,safe,997,,https://github.com/google/deepvariant/issues/62#issuecomment-381247700,1,['safe'],['safe']
Safety,"Hello,. Unfortunately the data I’m using are restricted by Federal regulations and also are proprietary. Apart from sharing data, what can I provide that might help figure this out?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 12, 2018 3:34 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; From a quick look of your error, it doesn't look like anything I've ever; encountered before. If you could potentially set up a reproducible setting; that I can very quickly run, I can see if I can try it out and tell you; what might could have gone wrong. We don't currently have a tutorial for; training, unfortunately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com<mailto:notifications@github.com>> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:656,safe,safe,656,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['safe'],['safe']
Safety,"Hello,. With make_examples I believe I have made examples I can use in model_train. The 64 files are named like this: 5PRR-RD_S86.examples.tfrecord-00000-of-00064. My protobuffer file contains this:. name: ""my-training-dataset""; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord""; num_examples: 64. When I run model_train I see this error:. ValueError: Cannot find matching files with the pattern ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord"". How should I specify the tfrecord_path to get model_train to use the files?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual name",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380158183:1033,safe,safe,1033,,https://github.com/google/deepvariant/issues/62#issuecomment-380158183,2,['safe'],['safe']
Safety,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381621757:496,safe,safe,496,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757,1,['safe'],['safe']
Safety,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox...; > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756; > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS; > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:; > unifier_config:; > drop_filtered: false; > min_allele_copy_number: 1; > min_AQ1: 10; > min_AQ2: 10; > min_GQ: 0; > max_alleles_per_site: 32; > monoallelic_sites_for_lost_alleles: true; > preference: common; > genotyper_config:; > revise_genotypes: true; > min_assumed_allele_frequency: 9.99999975e-05; > snv_prior_calibration: 0.600000024; > indel_prior_calibration: 0.449999988; > required_dp: 0; > allow_partial_data: true; > allele_dp_format: AD; > ref_dp_format: MIN_DP; > output_residuals: false; > more_PL: true; > squeeze: false; > trim_uncalled_alleles: true; > top_two_half_calls: false; > output_format: BCF; > liftover_fields:; > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:745,detect,detected,745,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['detect'],['detected']
Safety,"Hi @AndrewCarroll . Thank you very much for your promptness and attention.; I verified that DeepVariant has in several parts of the code, parameters for diploid organisms and two alleles. Like the small example function below:. `def most_likely_genotype(predictions, ploidy=2, n_alleles=2):`. The mentioned function is used in the post-processing process of calling variants.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/562#issuecomment-1234855822:254,predict,predictions,254,,https://github.com/google/deepvariant/issues/562#issuecomment-1234855822,1,['predict'],['predictions']
Safety,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:959,predict,predict,959,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['predict'],['predict']
Safety,"Hi @DLPerf , thanks for looking into this.; I can look into your suggestions above, test out the changes, and report back.; If you want to just create a PR with your fixes suggested above, that will certainly make my job easier and avoid miscommunication. Note that our repo isn't quite setup to directly take external pull requests, but if you create one, I can create a corresponding internal commit and point to your PR when we commit it. Let me know if you want to create a PR, or just for me to try to follow your suggestion above. Either way works. If you'll create a PR, I'll wait for that first.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-903356421:232,avoid,avoid,232,,https://github.com/google/deepvariant/issues/479#issuecomment-903356421,1,['avoid'],['avoid']
Safety,"Hi @FraSilver; As a sanity check, can you try running this quick start with no modifications at all? Try just copy-pasting it exactly as written without using your own files or making any other changes to the command:; https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1631154925:20,sanity check,sanity check,20,,https://github.com/google/deepvariant/issues/675#issuecomment-1631154925,1,['sanity check'],['sanity check']
Safety,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491:117,detect,detection,117,,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491,1,['detect'],['detection']
Safety,"Hi @MariaNattestad , thanks for the reply. I found there's much more STRs inside the GIAB VCFs than the ""missed"" ones, so I believe GIAB has treated the STRs quite seriously.; However in many sites I found, the repeating units seems to be inserted or deleted as a group, for example, ATATATATAT -> ATATATAT is observed, but ATATATATAT -> ATATTAAT or other cases is not.; Since the allele depths are also high enough, the probability should be very small that such cases are caused by sequencing errors.; I'm not very familiar with sequencing or sample preparation, could other source of errors have more dominate contributions?. On the other hand, STRs can indeed be detected by DeepVariant in the pileup images, but could it help by inputting the information explicitly? I believe that would separate mixed examples in an earlier stage in the data space.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/317#issuecomment-645258677:667,detect,detected,667,,https://github.com/google/deepvariant/issues/317#issuecomment-645258677,1,['detect'],['detected']
Safety,"Hi @Modernism-01 . For ONT data can you try the merge set DeepVariant_unfiltered. The presets for DeepVariantWGS were determined based on Illumina WGS. I hope that will help recover ONT variants that are too aggressively filtered. If this is not the case, you could please report back here. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2035863304:174,recover,recover,174,,https://github.com/google/deepvariant/issues/778#issuecomment-2035863304,1,['recover'],['recover']
Safety,"Hi @Npaffen,. To expand a bit on my previous explanation, regarding the way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:854,detect,detect,854,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['detect'],['detect']
Safety,"Hi @Qianwangwoo ,; First of all, DeepVariant is a germline variant caller - all our release models are trained for germline variant calling. But if I read your question correctly, your question is more about ""why does DeepVariant call this image as HET rather than HOM-ALT"". To answer that question, it'll be similar to this FAQ here: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. Once a candidate is identified, DeepVariant uses a classifier on it to generate a probability distribution for the 3 classes (0: HOM-REF, 1: HET, 2: HOM-ALT). ; ; From the `PL` field, it would look like HOM-ALT has lower probability than HET, but not necessarily by much `33,0,1`.; And, the classifier takes into account many factors here, which is why the prediction is not always intuitive (and, not always right). . Let me know if this helps and if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303:813,predict,prediction,813,,https://github.com/google/deepvariant/issues/528#issuecomment-1067120303,2,['predict'],['prediction']
Safety,"Hi @Sh1von ,. Thanks for reporting this. Currently that's how our pipeline is designed unfortunately. We don't automatically resume. However, if make_examples and call_variants has completed successfully, you can manually run postprocess_variants separately. The way to figure out the commands to use : You can run your original command with `--dry_run`, and then it'll break down the sequence of commands you need. Then, you can manually rerun the postprocess_variants command. I understand that being able to resume can be a useful feature. To automatically detect existing data robustly will be challenging, because we don't know if there might be corrupted data. But, I think it'll be possible to add a flag for users to skip steps. I will think about it a bit more. For now, please try manually re-run postprocess_variants, and see if that works for you. Sorry for the inconvenience!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059:560,detect,detect,560,,https://github.com/google/deepvariant/issues/587#issuecomment-1319519059,2,['detect'],['detect']
Safety,"Hi @Suke-fudan , ; to answer your question about `./.`, please refer to https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md , specifically this section and the first sentence:; ""Missing variants where a candidate is generated:"". As explained there, DeepVariant makes a prediction which is probabilistic, based on the evidence it has, it might not always be as confident about its prediction. Our postprocessing logic can change a less confident call to a `./.` call. See this flag here:; https://github.com/google/deepvariant/blob/r1.3/deepvariant/postprocess_variants.py#L86-L89. As for ""RNC"" , I don't think that comes from DeepVariant. I wonder if it comes from GLnexus? But I'm not sure actually.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1018054666:277,predict,prediction,277,,https://github.com/google/deepvariant/issues/494#issuecomment-1018054666,2,['predict'],['prediction']
Safety,"Hi @Yousuk-Song ,. There might be some confusion here. Let me clarify:. You asked ""Can I regard the variant as a true somatic mutation and proceed ?"". Please note that you're running **DeepVariant**, which is a **germline** variant caller. So, it does not call **somatic** mutations. If you're interested in detecting somatic variants instead of germline variants, please make sure to use DeepSomatic (https://github.com/google/deepsomatic) instead. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877#issuecomment-2330722256:308,detect,detecting,308,,https://github.com/google/deepvariant/issues/877#issuecomment-2330722256,1,['detect'],['detecting']
Safety,"Hi @ZuyaoLiu ,. Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file? . If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately.; Currently our code is:; https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run.; We'll make sure to create a more unique filepath in the future to avoid issue!. On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already. @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1823372443:540,avoid,avoid,540,,https://github.com/google/deepvariant/issues/725#issuecomment-1823372443,2,"['avoid', 'sanity check']","['avoid', 'sanity check']"
Safety,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-569131100:902,avoid,avoiding,902,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100,1,['avoid'],['avoiding']
Safety,"Hi @ajsa-nukovic ,; Sorry for the confusion. Starting from v1.1.0, we added an additional channel to our PacBio model, and tried to simplify the flags in the one-step `run_deepvariant` by adding just one flag `--use_hp_information`, which you can set to false if you're BAM is not phased, and set to true if your BAM is phased. Example:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. However, from your command, I see that you're running make_examples directly, which is a reasonable use. But you're specifying your own make_examples flag for 1.1.0 code and model, you'll need to add `--add_hp_channel` to make sure the last channel is added. So, to summarize, if you're specifying your own flags for the make_examples step:; - If your BAM is NOT phased: use `--sort_by_haplotypes=false --parse_sam_aux_fields=false --add_hp_channel=true`.; - If your BAM is phased: use `--sort_by_haplotypes=true --parse_sam_aux_fields=true --add_hp_channel=true`.; Basically, in v1.1.0, `add_hp_channel` needs to always be set to true when you're running make_examples for PacBio. . I'll see if I can update the r1.1 documentation to avoid future confusions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458#issuecomment-844317545:1222,avoid,avoid,1222,,https://github.com/google/deepvariant/issues/458#issuecomment-844317545,2,['avoid'],['avoid']
Safety,"Hi @anands-repo . The major factor was the length of chromosomes, selecting sizes that would allow enough data for comparison of tune and test while still maximizing the data available for training. The tuning and test chromosomes should also be roughly representative of the overall genome content. There are some chromosomes that should probably be avoided for tune or test choices - specifically ones that have regions quite different from the rest of the genome. Chromosome 6 is an example, since it contains the MHC regions. So it is mostly arbitrary, though other methods such as Clairvoyante seem to have adopted the convention as well. It is nice for the choices to align, since it allows comparisons with the same regions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/224#issuecomment-540821357:351,avoid,avoided,351,,https://github.com/google/deepvariant/issues/224#issuecomment-540821357,1,['avoid'],['avoided']
Safety,"Hi @anitagh . Thank you for putting effort into detailed analyses. However, there is a key mistake. DeepVariant is not a somatic caller. It is not designed to detect subclonal variants. In the same way that for GATK you would not use HaplotypeCaller, you would use a different tool, Mutect2, you would not use DeepVariant for this problem. Before DeepVariant's neural net, there is a human-written candidate generation component which finds candidate positions. There is a threshold for this to even nominate a candidate for later classification. This is set to 12% (based on tuning for germline calling), so we would not expect that DeepVariant would nominate a 10% mix as candidates. . We do have a somatic calling tool in early access that we are making available to trusted partners. If you would be interested in that method, you can email me (awcarroll@google.com). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-536320209:159,detect,detect,159,,https://github.com/google/deepvariant/issues/222#issuecomment-536320209,1,['detect'],['detect']
Safety,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844209552:774,avoid,avoid,774,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552,2,['avoid'],['avoid']
Safety,"Hi @egnarora . The way you are running DeepVariant (run on individual samples then genotype jointly with GLnexus) is correct and what we recommend. Thank you @pgrosu which is in agreement with the recommendation. @egnarora some external groups have performed analysis on strategies which use more extensive joint calling processes with DeepVariant (for example, discovering all variants in a cohort and then experimentally performing force calling on candidate positions). Regeneron is one example of a group that has conducted this analysis. Their conclusion is that there are not variant calls which are missed in the individual process that can be recovered by the more extensive joint calling, and their conclusion was that the recommendation to use GLnexus will not result in missed variants that another approach would capture. Hopefully this answers your question. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568:651,recover,recovered,651,,https://github.com/google/deepvariant/issues/680#issuecomment-1641138568,1,['recover'],['recovered']
Safety,"Hi @ekofman , with this issue, I think we might have left at a place where there is still a mystery. Now the v0.8.0 is out, do you mind trying this again and see if you're still seeing the same issue? Given that I didn't fully understand what the problem last time was, at least a sanity check on whether we're still seeing the same thing will be helpful. . If I might be forgetting something that I could have followed up from last time, please also feel free to remind me. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164#issuecomment-482431632:281,sanity check,sanity check,281,,https://github.com/google/deepvariant/issues/164#issuecomment-482431632,1,['sanity check'],['sanity check']
Safety,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/772#issuecomment-1954778964:575,detect,detect,575,,https://github.com/google/deepvariant/issues/772#issuecomment-1954778964,1,['detect'],['detect']
Safety,"Hi @gushenweiz . Just to add a bit to Kishwar's answer. Refcall here means that the neural network evaluated the most probable genotype of this position to be homozygous reference. It's not always possible to infer the predictive reason for a specific call being made. . The GenotypeQuality here is 12, which means that DeepVariant's confidence in the call is something like 94%. . Generally, when there is a RefCall with high coverage, we tend to observe that DeepVariant seems to suspect that there is a lot of mismapped reads from some other part of the genome. If you have WGS and are using the WGS model, this makes a lot of sense as that's a very high coverage for standard sequencing. If you are using exome or panel sequencing, it's good to make sure you are using the exome model. Ref calls can be made for this, but I presume the model will be more conservative about making them.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/750#issuecomment-1854758289:219,predict,predictive,219,,https://github.com/google/deepvariant/issues/750#issuecomment-1854758289,1,['predict'],['predictive']
Safety,"Hi @jackycsie, there is currently no way to run `call_variants` on multiple GPUs. The codebase uses the TensorFlow Estimator API, which does not support prediction using multiple GPUs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/227#issuecomment-542798708:153,predict,prediction,153,,https://github.com/google/deepvariant/issues/227#issuecomment-542798708,1,['predict'],['prediction']
Safety,"Hi @japhill , to give you an update, I plan to add this section to our FAQ in the next release:. ---. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for more details. ---. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951:294,avoid,avoid,294,,https://github.com/google/deepvariant/issues/530#issuecomment-1079509951,1,['avoid'],['avoid']
Safety,"Hi @jordimaggi @splaisan . I will add on a little to Pi-Chuan's answer with respect to filtering and quality scores. We consistently find that the genotype quality (GQ and PL fields) are extremely well-calibrated with the empirical probability of a call being correct. This is quantified in Figure 2 of the [original DeepVariant paper](https://www.biorxiv.org/content/10.1101/092890v6). This value is the best to use when determining whether a call is likely correct. Both ourselves and other external groups who we work with have tried to identify other metrics of standard INFO and FORMAT fields which are more predictive of call quality or even additionally informative in a subset of contexts and cases. For basically everything we and these groups have looked at, GQ is more predictive of call correctness. . If you are able to identify an annotation which is additionally informative beyond GQ (and also not already perfectly captured in the GQ field), it would be quite interesting to know, and we could consider incorporating it as an output field, or providing the annotation as an input during calling. . In general, I'd encourage you to look at GQ and PL as the most informative fields if you would like to tune between sensitivity and specificity.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659:613,predict,predictive,613,,https://github.com/google/deepvariant/issues/531#issuecomment-1082325659,2,['predict'],['predictive']
Safety,"Hi @jrvanalstine! Unfortunately, the TensorFlow Estimator API does not support running prediction on multiple GPUs, so `call_variants.py` currently uses only 1 GPU at prediction time. That said, the API does support running training and evaluation (both steps require labels) with multiple GPUs through distribution strategies (i.e. `MirroredStrategy`). We hope to look into this and will post an update if we are able to run these steps using multiple GPUs. I'll close this issue for now but feel free to reopen if you have any other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/163#issuecomment-486883166:87,predict,prediction,87,,https://github.com/google/deepvariant/issues/163#issuecomment-486883166,2,['predict'],['prediction']
Safety,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:340,predict,predict,340,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703,2,['predict'],['predict']
Safety,"Hi @karoliinas ,. The format `gvcf.tfrecord@19.gz` is referring to files gvcf.tfrecord-[00000-00018]-of-00019.gz. So I don't think your commands are wrong. I think your call_variants_output-[00000-00015]-of-00016.tfrecord.gz files likely contain prediction values that are unexpected. I'm out of office now. I'll give you some examples to debug the call_variants_output next week!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2225681439:246,predict,prediction,246,,https://github.com/google/deepvariant/issues/849#issuecomment-2225681439,1,['predict'],['prediction']
Safety,"Hi @karoliinas ,; From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2221028135:80,predict,prediction,80,,https://github.com/google/deepvariant/issues/849#issuecomment-2221028135,1,['predict'],['prediction']
Safety,"Hi @karoliinas,. Given that you're having weird numerical prediction values from call_variants output, and that you mentioned your GPU version is newer that what we used in DeepVariant 1.6, I strongly suspect your GPU+DeepVariant setting is producing unexpected output. Would it be possible for you to:; 1. Use the compatible GPU driver version? (I understand this is annoying. We've made the CUDA update internally already, and it'll be out in the next version. But if it's possible to test with a compatible one, that might be easier for you); 2. Just to confirm whether it's the hardware issue: Can you run with CPU and see if it still crashes with the same error? That will help us identify whether it's the hardware, or actually something unexpected with your input file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2233667709:58,predict,prediction,58,,https://github.com/google/deepvariant/issues/849#issuecomment-2233667709,1,['predict'],['prediction']
Safety,"Hi @kishwarshafin,. I am trying to add --haploid_contigs=""chrX,chrY"" in a module from nf-core that I am using. However, when running the command line, I am only detecting chrX variants from the test data. When I try to run the command using a BED file with only chrY, I get an empty VCF file with headers as the result. I also tried using --regions as a parameter, but without success.; Could you please suggest some ideas on how to resolve this issue?; Thank you for your assistance.; ```; /opt/deepvariant/bin/run_deepvariant \; --ref=GRCh38_no_alt_analysis_set.fasta \; --reads=sample1-lane_1.converted.cram \; --output_vcf=sample1-lane_1.deepvariant.vcf.gz \; --output_gvcf=sample1-lane_1.deepvariant.g.vcf.gz \; --haploid_contigs=""chrX,chrY"" \ . --regions=""chrX chrY"" \ ; --model_type PACBIO \; --regions=chrX_10001-44821.bed \; --intermediate_results_dir=tmp \; --num_shards=12; ```; [chrY.vcf.gz](https://github.com/user-attachments/files/16334591/chrY.vcf.gz); [sample1-lane_1.deepvariant.vcf.gz](https://github.com/user-attachments/files/16334613/sample1-lane_1.deepvariant.vcf.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853#issuecomment-2243024446:161,detect,detecting,161,,https://github.com/google/deepvariant/issues/853#issuecomment-2243024446,1,['detect'],['detecting']
Safety,"Hi @maryawood ,; `customized_classes_labeler` is an internal, experimental feature that we didn't provide documentation because we don't plan to provide support on it at this moment. Can you provide a bit more detail on your problem definition - E..g, what are you trying to predict? ; You mentioned `callsets` in INFO -- does that mean you're trying to build a variant classifier that could predict these callsets values? If so, if you don't mind sharing, what's the problem that you're trying to solve? . In general, DeepVariant utilizes a general classification algorithm (InceptionV3), however, our codebase serves a much more specialized purpose than InceptionV3, so the codebase does have a lot more hard-coded constraint. For example, the codebase assumes that there are 3 class: 0, 1, 2. [This blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) has more details on what the inputs and outputs are for the InceptionV3 model. It is possible to make enough changes to DeepVariant to get it work on other problems. But, if you're solving a very different problem from DeepVariant, I strongly suggest that you look into more general libraries - such as other image classification libraries in TensorFlow or other frameworks you like. If you need to process files such as VCFs, you can consider using [Nucleus](https://github.com/google/nucleus) which is what we used to process common genomics file formats in DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454#issuecomment-834808144:275,predict,predict,275,,https://github.com/google/deepvariant/issues/454#issuecomment-834808144,2,['predict'],['predict']
Safety,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-608968890:812,safe,safest,812,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890,1,['safe'],['safest']
Safety,"Hi @observer2735 . I will second Maria's question about the application you are trying to do. I did want to mention one things about your statement. ```; This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation.; ```. The VCF for DeepVariant does not report every single variant present in the reads, it reports the candidate positions which pass the minimum support (as @pgrosu mentioned). So you should see that the other possible variants in the read don't make the threshold required to generate a support. DeepVariant is operating from an assumption of a diploid organism, if you want to try to do things like detect subclonal variants at lower allele frequencies, that would be good to know, as there would be different recommendations we would make.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683129229:923,detect,detect,923,,https://github.com/google/deepvariant/issues/697#issuecomment-1683129229,1,['detect'],['detect']
Safety,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-524686835:310,sanity check,sanity check,310,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835,1,['sanity check'],['sanity check']
Safety,"Hi @pichuan, thanks for your response! Yes, ideally I'd like to build a variant classifier that could predict those callset values that I could then use as a proxy for the ""confidence level"" of a variant, i.e. a variant identified in multiple callsets might be more likely to be a ""real"" variant than one predicted in just one or two. . Based on your description of the 3-class system in the codebase, would it theoretically be more feasible to feed the algorithm an edited VCF files that bins the callset values into three categories? (E.g. 0 = 1 callset, 1 = 2-4 callsets, 2 = >5 callsets). Thank you for sharing the blog post and other resources, I'll take a look through those to try to learn more as well!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454#issuecomment-835842617:102,predict,predict,102,,https://github.com/google/deepvariant/issues/454#issuecomment-835842617,4,['predict'],"['predict', 'predicted']"
Safety,"Hi @pichuan,. sorry I was on vacation. Mhm strange I've checked your test command on my Debian 10 machine and it's showing the container nvidia-smi output without any problems. . I've checked your script. Did you install any NVIDIA drivers before CUDA? If I remember right, I did something like ""nivida-detect"" and installed the proposed package. What is your version of nvidia-container-toolkit (mine is 1.1.2-1) ? I saw in your mentioned issue that people there are using a newer one. So maybe it is worth a try to downgrade. You can also try something like ""sudo modprobe nvidia-uvm"". I have had this issue with another tool that wanted to use the GPU. Somehow the card was not ready and this fixed my problem. I hope this helps. I'm quite new in the GPU world. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-674744222:303,detect,detect,303,,https://github.com/google/deepvariant/issues/321#issuecomment-674744222,1,['detect'],['detect']
Safety,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads; ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms?. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1697167719:165,recover,recovering,165,,https://github.com/google/deepvariant/issues/701#issuecomment-1697167719,1,['recover'],['recovering']
Safety,"Hi @sen1019san ,. Are you starting a fresh instance everytime? In my experience singularity fails to load all the modules for GPUs to be detected. So you can try this before your singularity command:; `nvidia-modprobe -u -c=0`. This will load all the required modules for singularity to see the GPUs. Otherwise, you can run one of the CUDA samples before running the singularity command. Let me know if this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687:137,detect,detected,137,,https://github.com/google/deepvariant/issues/619#issuecomment-1471943687,1,['detect'],['detected']
Safety,"Hi @shadrinams . Thank you for the plot. This is expected the the found de novo calls are lower in confidence (because DeepTrio has learned that de novo events are rare). Given that a call is a de novo (0/1-0/0-0/0), the higher GQ values will still indicate higher confidence, so more confident de novo calls should be more likely to be true. For calls which are 1/1-0/0-1/1 (or permutations of this), the parent and child models of DeepTrio do not coordinate, so they aren't optimizing for consistency. There is a property of some regions which look like potential segmental duplications where a call that appears to a human as a 1/1/ or 0/1 is actually some kind of CNV. DeepTrio has learned some parts which are predictive of this (generally a variant-dense haplotype and a reference haplotype with higher depth). There may be cases where the child model gives a call a 1/1 and the parent gives a 0/0 when the site itself is similar, but the context is different. If you are looking for homozygous Mendelian violations, you may want to filter regions of high variant density, as apparent calls will come from this phenomenon. . In addition, if you are looking at the sex chromosomes, it would be good to separately call the non-PAR chrX for male samples. For a male sample, running chrX providing only the mother sample provides best results. (chrY should already only have paternal reads outside of the PAR). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-821020591:715,predict,predictive,715,,https://github.com/google/deepvariant/issues/440#issuecomment-821020591,2,['predict'],['predictive']
Safety,"Hi @situssog . To expand on Pi-Chuan's answer - DeepVariant will expect quality scores to be present in the BAM file (and to have the same length as the bases for each read). In order to run DeepVariant, you would need to add QUAL scores of all one value. It is unclear how DeepVariant's accuracy and behavior in variant calling would change as a result. . I believe that a value of ""!"" corresponds to the lowest base qual value. DeepVariant may be conservative in calling reads if Base Qualities are given that as the confidence. A value like ""@"" would be more balanced, but it is difficult to predict how DeepVariant will behave either way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270#issuecomment-587936807:595,predict,predict,595,,https://github.com/google/deepvariant/issues/270#issuecomment-587936807,1,['predict'],['predict']
Safety,"Hi @sophienguyen01 , can you specifically point out the line of the error? All the lines in the logs are API warnings, you can safely ignore those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073368951:127,safe,safely,127,,https://github.com/google/deepvariant/issues/802#issuecomment-2073368951,1,['safe'],['safely']
Safety,"Hi @sophienguyen01 . I am not familiar with https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard so I clicked on it and read a bit on the page. From https://horizondiscovery.com/-/media/Files/Horizon/resources/Product-data/Notification_Tru-Q_update_effective_from_31st_March_2021.pdf which listed the Allele Frequency, it seems to me that these variants you're try to detect are NOT germline variants?. If so, that would explain why DeepVariant wasn't able to detect many of them, especially given the default thresholds. Note that DeepVariant (and our released models) are trained for germline variant calling use cases, we don't currently support non-germline variant calling. You're welcome to tweak the thresholds and try out different ways of using the codebase, but please be aware that our models are not designed for that. Hopefully this is helpful. Feel free to reopen if you have further questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748:409,detect,detect,409,,https://github.com/google/deepvariant/issues/690#issuecomment-1662683748,2,['detect'],['detect']
Safety,"Hi @wangsb111 . For filtering, it remains the case that filtering on the sample GQ field is the best way to increase specificity while losing the smallest number of true variants. . Are you able to run the same postprocess tool you are using on the GLnexus calls? If so, here are some thoughts on your parameters:. **-filter ""QUAL <50.0"" --filter-name LowQ** The QUAL field can be used. DeepVariant is much more conservative and realistic about the QUAL values. Filtering at 50 will filter too many real calls. Figure 1 in our [DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) has a chart of the Predicted and Real qualities for GATK and DeepVariant. Matching the predicted-real accuracy by quality between the two would put the comparable QUAL threshold at 12-17. Note that DeepVariant's GQ values are well-calibrated over the range of 0-60, so you can map them directly onto your desired specificity (e.g. Q10 is 90% confidence in call, Q20 is 99% confidence, Q30 99.9% confidence, and so on). **--filter-name LowQ --filter ""DP <200 "" --filter-name LowD** This field implies to me you are doing panel sequencing or deep exome sequencing, as this is a high value. The sample info should have DP present and it will not differ between GATK and DeepVariant output. You can use the same filter. . **--filter ""DP> 10000 "" --filter-name HigD** This is again the DP field, which is present in the call file and can also be used. **MQ<40.0 "" --filter-name SedT** This filters low mappability regions. This is actually a fairly high value, which means this will aggressively filter low mappability regions. MQ is seen by DeepVariant and used in calling to determine the variant call and confidence, so it will be reflected in the GQ value. However, MQ is not reported into the VCF, so this filter cannot be used for filtering without additional annotation of the VCF. If you want to filter out low mappability regions, I would recommend intersecting with the hig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/642#issuecomment-1535446429:642,Predict,Predicted,642,,https://github.com/google/deepvariant/issues/642#issuecomment-1535446429,2,"['Predict', 'predict']","['Predicted', 'predicted-real']"
Safety,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815783472:556,safe,safely,556,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472,2,['safe'],['safely']
Safety,"Hi Andrew,; Thanks for answering my questions.It is very informative.; Currently I am using the GATK pipeline to identify any novel strains for a; particular bacteria genus. So far it makes sense, meaning the known mutated; locations on the genome can be found using the pipeline. However, there are; a lot of possible false positives, which could be either potential new; SNPs, or just sequencing error. That's what prompted me to explore; DeepVarient in the first place, to minimize false positives from sequencing; error.; As I understand that GATK has a step to incorporate known variants to; correct for downstream analysis. But bacteria in general do not have those; information. I am wondering do you have any toolbox on this topic?; I'd love to perform some comparisons between different variant detection of; bacteria genomes in the near future. Will keep you posted if I find; anything interesting. On Wed, Oct 31, 2018 at 7:13 PM Andrew Carroll <notifications@github.com>; wrote:. > This is a very interesting question, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the bo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:804,detect,detection,804,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,2,['detect'],['detection']
Safety,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:113,sanity check,sanity check,113,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751,1,['sanity check'],['sanity check']
Safety,"Hi Fra,. That's great to hear! Hopefully you were able to detect some expected variant candidates, and ideally discover some novel and relevant ones as well. Glad this helped you out -- it was a fun team effort!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1633300854:58,detect,detect,58,,https://github.com/google/deepvariant/issues/675#issuecomment-1633300854,1,['detect'],['detect']
Safety,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:164,sanity check,sanity check,164,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046,1,['sanity check'],['sanity check']
Safety,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-637264990:151,avoid,avoid,151,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990,1,['avoid'],['avoid']
Safety,"Hi Paul,. Thank you very much for responding to my questions and providing detailed explanations, and it's give me some very import tips tha I ignore before, so very appreciate for your reply. A few hours ago I tried to change command --regions chx:xxxxx-xxxxxx to --regions chx, deleted the locations of rigion and it output excepted files. But somethings confused me again, for example, my research topic is about repeat times in the NGS data, and my original intention in using Deepvariant is to detect the sequencing noise and delete them or find the true data in noised data. However ,what I saw is that the Deepvariant only can print one mutation such like following string:. chx xxxxxx .	TAAA	T	1.6	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:5:490:36,81:0.165306:0,3,25. This is very contrary to common sense, because in this loci it have at least 490 reads to cover it (it maybe more because Deepvariant can't recognize all reads due to empty align), Deepvariant only give one mutation that is TAAA to T, but when I open IGV to focus on the loci, it shows that they are more than one mutation, in fact, it's far more than one mutation. I mentioned in the previous paragraph that I am very grateful for your reply because you give me one tip is that :"" Keep in mind, the pileup images for DeepVariant are ~100 reads, so it will randomly down-sample from the reads to fit into the maximum of the allowed pileup image."" So I think maybe what confuse me is that Deepvariant only can detect 100x depth data? or somethings else I can do to detect truth sequence data in NGS data?. Very sorry to bother you, wish you a pleasant work and life,; Ji",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857:499,detect,detect,499,,https://github.com/google/deepvariant/issues/697#issuecomment-1680308857,3,['detect'],['detect']
Safety,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:512,predict,predicted,512,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['predict'],['predicted']
Safety,"Hi all,; it has recently be reported again that the crashing issue on empty shard for `call_variants` wasn't fully resolved last time. I just released v0.6.1 that should really resolve this issue now:; https://github.com/google/deepvariant/releases/tag/v0.6.1. The issue was that I didn't properly return in the if branch where an empty shard was detected:; https://github.com/google/deepvariant/commit/12f9e67f9a246dcf2209c98f7792a4c728469bc9; (And the unit test I had for it was flawed. We'll fix the unit test in a later release.). This time I've tested it manually on an empty shard, and confirmed that call_variants works when there is zero record. . Please feel free to report if you see any issues again. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-385593624:347,detect,detected,347,,https://github.com/google/deepvariant/issues/27#issuecomment-385593624,1,['detect'],['detected']
Safety,"Hi,. So if there would be a rare somatic variant with e.g. 0.5 VAF, would that be classified as germline, artefact or something else? I'm just trying to understand what happens if there are mutations that are not true germline mutations. Can the tool really detect only germline mutations and not somatic mutations? I don't mean it would need to distinguish between the two types.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/864#issuecomment-2278810058:258,detect,detect,258,,https://github.com/google/deepvariant/issues/864#issuecomment-2278810058,1,['detect'],['detect']
Safety,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12594,abort,aborted,12594,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,4,['abort'],"['aborted', 'abortedCount']"
Safety,"I guess the core question for the variant caller is, (whether or not fully normalized as you have written) whether the raw calls can be interpreted without contradictions by a downstream method (notwithstanding the VCF standard). Considering GT=0 to be ""non-ALT"" instead of ""REF allele"" for such call clusters does seem to work in these cases (though I am not aware of a standardization of such an interpretation). The code I have referenced from DeepVariant above seems to try to filter out cases that do not make sense for such clusters of calls, so a downstream method that assumes GT=0 to imply ""non-ALT"" might work smoothly almost all the time (except perhaps when the algorithm aborts). I also didn't have an idea as to whether, as a method developer, you considered these to be corner cases, and your comment above has provided an answer to that. I myself do not have any measurements on how prevalent these types of cases are, but I encountered some of these cases recently. I will make another post here if I learn anything further about this, but I understand the point-of-view behind the original piece of code that I posted. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/247#issuecomment-563285549:684,abort,aborts,684,,https://github.com/google/deepvariant/issues/247#issuecomment-563285549,2,['abort'],['aborts']
Safety,"I have copy-n-paste the wrong script. ; I have already written -v ""${OUTPUT_DIR}"":""/output"", declared my $FQ variable and used backslash.; It doesn't work.; In my ""/input"" directory I have .fasta file, .fasta.fai, bam and .bai file. I suppose I don't require nothing else. (maybe, also VCF tools if the aim is the vcf output ""prediction"". . Any suggestion?; singularity run doesn't work as well.; I get the ""deepvariant_1.5.0.sif "" image file after running singularity pull... Thanks in advance,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618:326,predict,prediction,326,,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618,1,['predict'],['prediction']
Safety,"I ran using the reference genome mentioned earlier, but it seems that the reference has the chromosomes named without the ""chr"" prefix, which the BAM file has for the contig names, it would be nice if the package could detect and correct this automatically -- do you think this makes sense, if so I am happy to submit a patch:; ```; 2018-11-10 01:31:37.340729: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:37.359072 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; I1110 01:31:37.383352 140105147934464 make_examples.py:1075] Preparing inputs; 2018-11-10 01:31:38.214366: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1110 01:31:38.223048 140105147934464 genomics_reader.py:213] Reading /input-gcsfused-0/CNR-data/TLE_a_001.bam with NativeSamReader; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1076, in make_examples_runner; regions = processing_regions_from_options(options); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 990, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/mnt/google/.google/tmp/Bazel.runfiles_Mu9e5m/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 485, in _ensure_consistent_contigs; min_coverage_fraction); File ""/mnt/google/.google/tmp/Bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437596560:219,detect,detect,219,,https://github.com/google/deepvariant/issues/116#issuecomment-437596560,1,['detect'],['detect']
Safety,"I second this feature request. We typically run DV in WDL (backed by Cromwell, on GCP) workflows. We try our best to take advantage of the spot machines but as you know, spot VMs can be preempted non-predictably and the more resource reserved for that machine, the chance is higher for it to be preempted within a certain period (i.e. smaller VMs survive longer before taken away). OTH smaller VMs take longer to finish the job on the same chunk of data, hence the overall preemption rate is likely the same as the bigger ones. So if there's a way to do check-pointing, even during the execution of a step, and resuming from that after recovery from the preemption, it'd be a huge saving for us. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749#issuecomment-1930787197:200,predict,predictably,200,,https://github.com/google/deepvariant/issues/749#issuecomment-1930787197,2,"['predict', 'recover']","['predictably', 'recovery']"
Safety,"I thought the ""very sensitive"" has something to do with the indel threshold, but the result turned out to be more exciting!; I just wondered if it's possible for deepvariant to generate candidates according to an existing VCF, and now you have been working for it.; It would be a great feature for uses like optimizing deepvariant models with on-the-flow data, or ensemble calling in somatic-variant detecting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/256#issuecomment-568673024:400,detect,detecting,400,,https://github.com/google/deepvariant/issues/256#issuecomment-568673024,1,['detect'],['detecting']
Safety,"I've figured out what's going on here and have some good news and bad news. . First, the bad news is that setting the height to 2000 isn't going to work in the short run. This is a limitation coming from inception_v3 itself. At such large image sizes, we would have to run with spatial_squeeze=False to avoid this exception. By doing so we'd essentially end up with a ""tile"" of deepvariant predictions every 64 rows in the image, and then have to pool them together somehow, which makes sense in the general object detection case but not for us in DeepVariant. . The good news is that the maximum supported depth is 362. So you can get a lot more information into your images than the default 100 value. Give 362 a try and let us know if that works. . I should point out that we use a reservoir sampler to create these images. So a height of 362 means you'll get a random sampling of 362 - 5 [for the reference] reads from your very deep sequencing. It's not ideal if you want to detect things occurring in only 1 or 2 reads, but you get a reasonable number of reads if you are looking for things >1% or so frequency in the reads. Hope that helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-389567694:303,avoid,avoid,303,,https://github.com/google/deepvariant/issues/62#issuecomment-389567694,4,"['avoid', 'detect', 'predict']","['avoid', 'detect', 'detection', 'predictions']"
Safety,"I’m generating a set from the GIAB exome data as you described. I’ll see what happens with it when I try to train with it. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addresse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381209312:595,safe,safe,595,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312,1,['safe'],['safe']
Safety,"Jillian;; Awesome, thanks so much. Here is a branch with where we're at right now:. https://github.com/chapmanb/bioconda-recipes/tree/deepvariant-compile/recipes/deepvariant. Lots of hacking in there to reference the conda python with pyclif but that works and then should get stuck on not detecting zlib during the htslib compile. Let me know if you have any questions and thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-391324483:290,detect,detecting,290,,https://github.com/google/deepvariant/issues/29#issuecomment-391324483,1,['detect'],['detecting']
Safety,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:213,safe,safety,213,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579,1,['safe'],['safety']
Safety,"My current investigation shows that this went down the path here:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L460-L463. which ended up identify this file as an FAI format. And then this format gets into this switch:; https://github.com/samtools/htslib/blob/1.10.2/hts.c#L1069-L1105; But FAI format doesn't get read here, so it got into the last branch of the switch:; ```; default:; errno = EFTYPE;; goto error;; ```. One potential solution here is that we can change the Nucleus BedReader to always read file as BED (and not using htslib's format detection). This could cause other issues (for example, unable to read bed.gz. (Although, I personally have not tried using bed.gz files yet). Another potential solution : maybe newer version of htslib would work. I have not tried that either. For now, please preprocess the file and remove the last column. I'll discuss with teammates to see if it makes sense to change the BedReader implementation in Nucleus for this. Thank you for reporting the issue!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374#issuecomment-723752207:559,detect,detection,559,,https://github.com/google/deepvariant/issues/374#issuecomment-723752207,1,['detect'],['detection']
Safety,"OK – that proceeded further, I think. Now the error is; ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380197853:799,safe,safe,799,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853,1,['safe'],['safe']
Safety,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428649613:285,predict,prediction,285,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613,1,['predict'],['prediction']
Safety,"Ok so you have no idea what we are talking about, this is not a prokaryote. I would suggest you avoid hopping into conversations for making completely ignorant comments, not even bothering to check about what organisms we are dealing with. I find this kind of behaviour extremely irritating, especially since the answer to your questions are deducible from reading the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658759224:96,avoid,avoid,96,,https://github.com/google/deepvariant/issues/682#issuecomment-1658759224,1,['avoid'],['avoid']
Safety,"Peter;; Thanks for following up and glad to hear that this got it installed for you. I've been trying to replicate to fix the issue and avoid the manual pinning but can't seem to do on my system. Perhaps this was a temporary download issue with the google-cloud-sdk package, but it seems okay now. If anyone else stumbles across this same issue we can dig more but hopefully it was just something transient and we're good going forward. Thanks again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-487313448:136,avoid,avoid,136,,https://github.com/google/deepvariant/issues/177#issuecomment-487313448,1,['avoid'],['avoid']
Safety,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486163437:256,avoid,avoid,256,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437,1,['avoid'],['avoid']
Safety,"Peter;; Thanks for the report and apologies about the install issues. It looks like you're installing deepvariant in a python 3 environment, and it only works with python 2.7, which might be the source of the problems. Hopefully if you try installing with `'python=2.7'` as an additional requirement that will help avoid it. If you still hit issues, it might be a legit problem with the latest `google-cloud-sdk` and you could also add `'google-cloud-sdk<243.0.0'` as another requirement. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486033904:315,avoid,avoid,315,,https://github.com/google/deepvariant/issues/177#issuecomment-486033904,1,['avoid'],['avoid']
Safety,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430171385:314,avoid,avoid,314,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385,2,['avoid'],['avoid']
Safety,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385485505:82,avoid,avoid,82,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505,1,['avoid'],['avoid']
Safety,"Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:2905,Detect,Detecting,2905,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,8,['Detect'],['Detecting']
Safety,"Right now DeepVariant does not have the ability to report such a region by itself and skip it. You will need to exclude the problematic regions before running DeepVariant, or use `vsc_min_count_indels` to avoid candidate generation in these cases.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794#issuecomment-2040326853:205,avoid,avoid,205,,https://github.com/google/deepvariant/issues/794#issuecomment-2040326853,1,['avoid'],['avoid']
Safety,"So it looks like the VCF parsing is failing here:. ```bash; I0911 20:23:50.839811 140713511122752 positional_labeler.py:163] Multiple matches detected; no good match found. Fall back to first. variant: reference_bases: ""G""; alternate_bases: ""A""; info {; key: ""BAM_FNAME""; value {; values {; string_value: ""SR_male_1.fixmate.coordsorted.bam""; }; }; }; calls {; info {; key: ""AD""; value {; values {; int_value: 0; }; values {; int_value: 22; }; }; }; info {; key: ""DP""; value {; values {; int_value: 22; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 1.0; }; }; }; genotype: -1; genotype: -1; call_set_name: ""SR_male_1""; }; end: 14057936; reference_name: ""NC_053213.1_chromosome_2""; start: 14057935; : matches: [reference_bases: ""G""; alternate_bases: ""A""; alternate_bases: ""<NON_REF>""; ```. Looks like your truth contains alleles like ""<NON_REF>"" and when positional lableler tries to match something it fails. Is your truth VCF consistent with the regular VCF standards? You may need to filter non-standard calls like ""NON_REF"" as an alt allele from the truth to fix this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2347131916:142,detect,detected,142,,https://github.com/google/deepvariant/issues/876#issuecomment-2347131916,1,['detect'],['detected']
Safety,"So we were unable to reproduce this specific error. Regardless, we are overhauling how multiprocessing is used in `postprocess_variants` with our next release, which will very likely avoid this type of error. I am closing this issue for now. If someone is experiencing this issue and would like an experimental docker container to run, please comment on this issue and we will provide one!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2113514063:183,avoid,avoid,183,,https://github.com/google/deepvariant/issues/804#issuecomment-2113514063,1,['avoid'],['avoid']
Safety,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-499709284:143,ABORT,ABORT,143,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284,1,['ABORT'],['ABORT']
Safety,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4529,predict,prediction,4529,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918,1,['predict'],['prediction']
Safety,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:221,predict,prediction,221,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952,1,['predict'],['prediction']
Safety,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:737,avoid,avoid,737,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356,1,['avoid'],['avoid']
Safety,"Thank you! I added the base quality scores to the bam file and finally it worked!. Additional question: ; I detected a mutation at a query position in the bam file using igv browser. However, deep variant didn't call it as a variant in output vcf file, but only recored it as a non-variant position in the output gvcf file. <img width=""424"" alt=""스크린샷 2024-09-05 오후 1 07 05"" src=""https://github.com/user-attachments/assets/3e0f4a5d-5c37-489c-b8e7-6673a04cb9c5"">; <img width=""760"" alt=""스크린샷 2024-09-05 오후 1 07 42"" src=""https://github.com/user-attachments/assets/9c76e316-6131-4117-964d-5134f9e92505"">. Are there any ways to adjust filtering option of deep variant to make deep variant detect it?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877#issuecomment-2330557917:108,detect,detected,108,,https://github.com/google/deepvariant/issues/877#issuecomment-2330557917,2,['detect'],"['detect', 'detected']"
Safety,"Thanks Michael, I'd love to know what conclusions about how the accuracy; and background noise detection you have on the TB genome. If it is not too; much trouble, can you keep me posted? Or you have some blog/post or paper I; can follow up on?. You guys are totally rocking it!! Very exciting study!!. On Thu, Nov 8, 2018 at 2:47 PM Michael <notifications@github.com> wrote:. > We are also interested in exploring the possibility of using DeepVariant; > to call SNPs in bacterial genomes. For our current pipeline we are using; > BWA to map reads to the reference and then PILON from the Broad to call; > variants. Specifically we are working on the Tuberculosis genome.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-437149890>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP00NuLoTAyzg9zh_dKzzgjzby9ytMks5utJhlgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-437155211:95,detect,detection,95,,https://github.com/google/deepvariant/issues/114#issuecomment-437155211,1,['detect'],['detection']
Safety,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-638091561:405,avoid,avoid,405,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561,2,['avoid'],['avoid']
Safety,Thanks for reporting this. ; I removed the gs://deepvariant/singularity_images directory to avoid future confusion.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/353#issuecomment-696330532:92,avoid,avoid,92,,https://github.com/google/deepvariant/issues/353#issuecomment-696330532,1,['avoid'],['avoid']
Safety,"Thanks for the feedback. I went back to my files and just realized that my previous comment was inaccurate: the locus I analyzed on RNASeq was ""chr20:10,000,000-10,040,000""; the same exonic variant (chr20:10019093) was detected by both GATK and DeepVariant (WGS model) in my sample. As mentioned, I didn't do extensive tests at all (it was just that one locus) -- I'm happy to do further analysis if relevant,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-462075143:219,detect,detected,219,,https://github.com/google/deepvariant/issues/115#issuecomment-462075143,2,['detect'],['detected']
Safety,"Thanks for your help! I can ignore the warning about ""height"".; However,when I ran Deepvariant 1.1.0 and DeepTrio 1.2.0,the only error was about ""height"". When I ran Deepvariant 1.2.0 and DeepTrio 1.2.0,the errors were not only about ""height"",but also about ""ValueError"" like this:; **raise ValueError('call_variants_outputs did not pass sanity check.'); ValueError: call_variants_outputs did not pass sanity check. raise Exception('One or more postprocess_variants failed.'); Exception: One or more postprocess_variants failed.**. How should I deal with these errors？",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-940586642:338,sanity check,sanity check,338,,https://github.com/google/deepvariant/issues/488#issuecomment-940586642,2,['sanity check'],['sanity check']
Safety,"Thanks! I haven't tried running with gvcf since we messaged last, but yes I; never got it to work. We'll let you know if we get a chance to get it; working with the newest version. On Fri, Apr 12, 2019 at 12:20 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > Hi @ekofman <https://github.com/ekofman> , with this issue, I think we; > might have left at a place where there is still a mystery.; >; > Now the v0.8.0 is out, do you mind trying this again and see if you're; > still seeing the same issue? Given that I didn't fully understand what the; > problem last time was, at least a sanity check on whether we're still; > seeing the same thing will be helpful.; >; > If I might be forgetting something that I could have followed up from last; > time, please also feel free to remind me. Thanks!; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/164#issuecomment-482431632>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFMFwhn7aSNq8hIAj0rjxAgqHRnx7D1zks5vgAmCgaJpZM4cBchL>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164#issuecomment-482616033:592,sanity check,sanity check,592,,https://github.com/google/deepvariant/issues/164#issuecomment-482616033,1,['sanity check'],['sanity check']
Safety,"Thanks, I’m giving that a try today. vsc_min_count_snps, vsc_min_count_indels are already small numbers (2), so I changed only the fraction flags from their defaults, 0.12, to 0.01 which the fraction I want. Is that reasonable?. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Thursday, April 5, 2018 6:56 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercept",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379857500:701,safe,safe,701,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500,2,['safe'],['safe']
Safety,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430634265:361,predict,predict,361,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265,2,"['predict', 'safe']","['predict', 'safe']"
Safety,"This is impressive work! Though it looks pretty cool, let's ask ourselves what is happening here. The biological system went through clonal events (let's suppose under some non-random selective conditions), where the sequencer hopefully captured the state of those events as complete as possible (i.e. with minimal error). Then we pass those through two processes (DeepVariant and Clair3), which respond with areas of variation mirroring previous signals it was trained with to maximally respond upon. The key is, are these events biologically meaningful? So if you look in the literature or known databases -- for your organism -- would these changes be reflective of something significant within exomic (gene) regions, or pathways that might be activated? If so why, and what might be those non-random selective conditions that make these genomic changes both significant and predictable?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1655336903:878,predict,predictable,878,,https://github.com/google/deepvariant/issues/682#issuecomment-1655336903,1,['predict'],['predictable']
Safety,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541:32,detect,detecting,32,,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541,1,['detect'],['detecting']
Safety,"This worked but now another problem appeared that seems to be related to this one. Seems the samtools index does not work:. Current thread 0x00007f283e6c9740 (most recent call first):; File ""/tmp/Bazel.runfiles_qz56zi29/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2023-06-21 07:21:53.632147: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""1"" start: 2295000 end: 2330000; Fatal Python error: Aborted. ```; Current thread 0x00007fe7f6689740 (most recent call first):; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_tfz52rn8/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976 in _make_allele_counter_for_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1548 in candidates_in_region; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1318 in process; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183 in make_examples_runner; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186 in main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_rexajgra/runfiles/com_google_deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1600094995:619,Abort,Aborted,619,,https://github.com/google/deepvariant/issues/666#issuecomment-1600094995,1,['Abort'],['Aborted']
Safety,"Update : just now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:424,Detect,Detected,424,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759,1,['Detect'],['Detected']
Safety,"Use could build docker image with this command. It needs to be run from 'deepvariant' directory.; I used arbitrary values for PROJECT_ID and VERSION_NUMBER, you may replace them. . PROJECT_ID=my-deepvariant-docker; VERSION_NUMBER=0.7.2 ; gcloud builds submit --project ""${PROJECT_ID}"" --config cloudbuild.yaml --substitutions TAG_NAME=""${VERSION_NUMBER}"" --timeout 2h .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428366972:357,timeout,timeout,357,,https://github.com/google/deepvariant/issues/99#issuecomment-428366972,1,['timeout'],['timeout']
Safety,"When you refer to the method GATK has to incorporate known variants for quality control, I believe that you are referring to VQSR (variant quality score recalibrator). You are correct that this requires a set of ground truth information. It seems likely to me that VQSR and similar systems would not behave correctly on bacterial data even if ground truth were available. To understand whether events are false positives or real SNPs, I think you will want to get some orthogonal measure of whether they are accurate. It probably makes sense to look at the reads data for some of these sites in a tool like IGV to get a feel for whether they look reliable or not. One nice thing about DeepVariant is that the quality scores for variants calls are generally normally distributed and predict the human variant quality confidence quite well. One approach you can take is to see what the distribution of DeepVariant quality scores is for the variants you know are real and see whether the new variants have a similar confidence distribution.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-436379034:782,predict,predict,782,,https://github.com/google/deepvariant/issues/114#issuecomment-436379034,1,['predict'],['predict']
Safety,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433250645:236,predict,predictability,236,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645,1,['predict'],['predictability']
Safety,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725:397,timeout,timeout,397,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725,2,['timeout'],['timeout']
Safety,"Yes, the truth_variants have to have genotypes, as these are used to compute the labels that DeepVariant trains to predict. It's not sufficient to just have sites. . As a general comment, we are aware of the need for more information about how to train DeepVariant, which we hope to produce in a reasonable timeframe.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/64#issuecomment-380240790:115,predict,predict,115,,https://github.com/google/deepvariant/issues/64#issuecomment-380240790,1,['predict'],['predict']
Safety,"You are right I've found the error: ; ""[E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2021-09-11 06:41:18.592077: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 4655405 end: 4655474; Fatal Python error: Aborted"" ; It is written few lines above the parallel error. I get this error with the original files of the WES run. (parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917353570:328,Abort,Aborted,328,,https://github.com/google/deepvariant/issues/483#issuecomment-917353570,1,['Abort'],['Aborted']
Safety,"You said it was random clonal organism, il have a look when the reads are; on the sra database, was just trying to help,. And yes I don't care about SNPs sorry. (Google people) I'll be here to offer random advice to other people if I'm; still allowed,. Joe. On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:. > Ok so you have no idea what we are talking about, this is not a; > prokaryote. I would suggest you avoid hopping into conversations for making; > completely ignorant comments, not even bothering to check about what; > organisms we are dealing with.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468:417,avoid,avoid,417,,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468,1,['avoid'],['avoid']
Safety,"]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9276,predict,predict,9276,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['predict'],['predict']
Safety,"_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9125,predict,prediction,9125,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,8,['predict'],"['prediction', 'predictions']"
Safety,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1102,predict,predictions,1102,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942,2,"['Predict', 'predict']","['Predictions', 'predictions']"
Safety,"al stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict; hooks=all_hooks) as mon_sess:; File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_sessio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:9042,predict,predict,9042,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['predict'],['predict']
Safety,"ant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1718,recover,recovery,1718,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"ariants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5292,sanity check,sanity check,5292,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['sanity check'],['sanity check']
Safety,"ceback (most recent call last):; File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:14330,predict,predict,14330,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['predict'],['predict']
Safety,"chuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:2008,Detect,Detecting,2008,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,8,['Detect'],['Detecting']
Safety,"comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3215,recover,recovery,3215,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486821218:407,avoid,avoid,407,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218,1,['avoid'],['avoid']
Safety,"d (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1810,recover,recovery,1810,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"d to replicate this issue, and here is what I found. I created an instance from [this CentOS7 VM](https://console.cloud.google.com/marketplace/details/centos-cloud/centos-7). I chose the default location for all installations. When asked if I wanted to update my PATH during installations, I chose to do so. I was able to install DeepVariant through Bioconda using the below steps. . I ran into a particular error with `gsutil`. After running `source ~/.bashrc`, I saw an error when I ran `gsutil`. `gsutil` is used by the DeepVariant installation, so that failed as well. To address this, I referenced [this post](https://stackoverflow.com/questions/38783140/importerror-no-module-named-google-compute-engine) and ran `export BOTO_CONFIG=/dev/null` before installing DeepVariant again. Running these commands in order allows me to successfully install on the VM. ```; # install gsutil; curl https://sdk.cloud.google.com | bash; exec -l $SHELL; # verify that gsutil is working; gsutil. # install wget and bzip2, which are both needed to download miniconda; sudo yum install bzip2 wget; wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh; bash Miniconda2-latest-Linux-x86_64.sh ; source ~/.bashrc. # gsutil is failing now; gsutil; export BOTO_CONFIG=/dev/null; # gsutil should be working again; gsutil. # create new conda env, add channels, install deepvaraint; conda create -n dv python=2.7; conda activate dv; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda install -n dv deepvariant -v; ```. In the output from running `conda install -n dv deepvariant -v`, I see the first error you posted even with a successful installation. I was not able to replicate the second error. Some sanity checks for you:. * Are you able to successfully run `gsutil`?; * Did you add all conda channels in the correct order?; * Could you post the entire output from running `conda install -v deepvariant`?. CC @melkerdawy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-452921108:1795,sanity check,sanity checks,1795,,https://github.com/google/deepvariant/issues/137#issuecomment-452921108,1,['sanity check'],['sanity checks']
Safety,"dn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as sho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3545,recover,recovery,3545,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,4,['recover'],['recovery']
Safety,"e exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper). Regarding seeing the pileup images, the command is the following -- which is based [on the following document](https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md):. ```; INPUT_DIR=""${PWD}/YOUR_INPUT_PATH""; OUTPUT_DIR=""${PWD}/YOUR_OUTPUT_PATH"". BIN_VERSION=""1.5.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup --num_records=20. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. There are many reasons why candidate variants are not detected:. 1. The quality of reads in the BAM file.; 2. The reference file used to generate the BAM is different than the one used with DeepVariant.; 3. There might not be many SNPs left supported by informative reads.; 4. Besides requiring that you have a model for your technology, how many variants do you see in IGV that are based on high-quality reads?. The reason for RefCall is because the genotype for those calls is 0/0, as per the postprocessing that happens as the last step in DeepVariant. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629:1827,detect,detected,1827,,https://github.com/google/deepvariant/issues/690#issuecomment-1660589629,2,['detect'],['detected']
Safety,"er combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4933,recover,recovery,4933,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"esulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https://precision.fda.gov/discussions/55-hg002-truth-dataset) on precisionFDA asking about how the “truth” set was defined. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4982,recover,recovery,4982,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1619,predict,predict,1619,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665,6,['predict'],"['predict', 'predicted']"
Safety,"exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:14995,predict,predict,14995,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['predict'],['predict']
Safety,"g back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4846,recover,recovery,4846,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"h both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are high, like the other variant calling results). I also created a [discussion group](https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4888,recover,recovery,4888,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"hi @IndyHouseGuy ,. You can add ; ```; docker run -it -v /data:/data \; -u `id -u`:`id -g`; ```; to your docker command to avoid this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1205591500:123,avoid,avoid,123,,https://github.com/google/deepvariant/issues/550#issuecomment-1205591500,1,['avoid'],['avoid']
Safety,"hi，@AndrewCarroll ; Thank you for your suggestion. This is a new platform panel data, we will consider filtering GQ to avoid this problem.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655#issuecomment-1586633037:119,avoid,avoid,119,,https://github.com/google/deepvariant/issues/655#issuecomment-1586633037,1,['avoid'],['avoid']
Safety,"ible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1629,recover,recovery,1629,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"ipts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2056,recover,recovery,2056,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"ks and other DNA damage can occur with their subsequent repair, though the papers have very different goals. If you want to be able to compare them from the point of view of damage-to-repair, it will be a bit difficult. Let me explain why through the papers:. $`\underline{In \; the \; Lab \; (from \; the \; paper)}`$. - This paper showed how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully lo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1184,safe,safeguard,1184,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342,1,['safe'],['safeguard']
Safety,"l.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 659, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5358,sanity check,sanity check,5358,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['sanity check'],['sanity check']
Safety,"led filters for DeepVariant, then that is still only about 9 _million_ sites. However, I wanted to wait until I had my own DeepVariant results before I said anything else, since it is always possible there may have been something that I wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://pre",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1379,recover,recovery,1379,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,4,['recover'],['recovery']
Safety,"les (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4757,recover,recovery,4757,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"ll_variants runs optimally on a machine which contains a single gpu, such as nvidia-A10, or nvidia-v100. The output of call_variants is a single tfrecords.gz file which contains the call probabilities for each candidates. postprocess_variants parses the output of call_varaiants and converts it to a vcf file. * In the instructions below deepVariant is being run within the docker container. ### Running make_examples. #### Generate scattered intervals; ```; mkdir out && \; picard \; IntervalListTools \; SCATTER_COUNT=200 \; SUBDIVISION_MODE=BALANCING_WITHOUT_INTERVAL_SUBDIVISION_WITH_OVERFLOW \; UNIQUE=true \; SORT=true \; BREAK_BANDS_AT_MULTIPLES_OF=100000 \; INPUT=wgs_calling_regions.hg38.interval_list \; OUTPUT=out; ```. #### Further steps describe running on a single interval. Required hardware: ; * 1CPU/shard, ; * 2.5GB RAM/shard, ; * hard disk: 50GB/shard+size_of_bam_files/num_shards; ; Select reads from only a single area of the genome. In our system it is necessary to avoid unnecessary multiple copies of large BAM/CRAM files. ```; gatk --java-options ""-Xms1G"" PrintReads \; -I {input_bam} \; -O input.bam \; -L {interval} \; -R Homo_sapiens_assembly38.fasta. # DeepVariant receives intervals in bed format; gatk IntervalListToBed -I {interval} -O interval.bed; ```. #### Run make_examples on a single resulting interval to generate examples tfrecords and an optionl gVCF tfrecords files. ```; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ~{ref_fasta} \; --reads input.bam \; --regions out/temp_0001_of_200/scattered.interval_list \; --examples interval001.tfrecord.gz \; --min_base_quality 5 \; --dbg_min_base_quality 0 \; --vsc_min_fraction_indels 0.06 \; --vsc_min_fraction_hmer_indels 0.12 \; --vsc_min_fraction_snps 0.12 \; --vsc_min_count_snps 2 \; --ws_min_windows_distance 20 \; --min_mapping_quality 5 \; --candidate_min_mapping_quality 5 \; --max_reads_per_partition 1500 \; --aux_fields_to_keep tp,t0 \; --skip_bq_channel \; --channels hmer_deletion_qu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580:2924,avoid,avoid,2924,,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580,1,['avoid'],['avoid']
Safety,"ly fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased precision for the RefSeq CDS variants (but decreased sensitivity), compared to DeepVariant (or an [unfiltered GATK variant set](https://precision.fda.gov/comparisons/3442)). **6)** If you go back to the original precisionFDA results, I think this is also consistent with HG002:; If you go to “[explore results](https://precision.fda.gov/challenges/truth/results-explore)"", and select “**func_cds**” (which is where I would expect most of the clinically actionable variants to be) DeepVariant is **not** top ranked for either F-score, Recall, or Precision (although the values are hi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4800,recover,recovery,4800,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"m 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1752,Timeout,Timeout,1752,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,1,['Timeout'],['Timeout']
Safety,"n; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; variant = next_or_none(variant_iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 998, in next_or_none; return next(iterable); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. real	15m9.448s; user	14m3.211s; sys	0m20.620s; I1010 03:07:07.182588 47710997727040 postprocess_variants.py:1233] Finished writing VCF and gVCF in 36.08515272140503 minutes.; I1010 03:07:07.600154 47710997727040 genomics_reader.py:222] Reading /DeepTrio/C1/F1.output.vcf.gz with NativeVcfReader; I1010 03:09:40.323815 48001616553792 postprocess_variants.py:1233] Finished writing VCF and gVCF in 38.57903414567311 minutes.; I1010 03:09:40.659351 48001616553792 genomics_reader.py:222] Reading /DeepTrio/C1/C1.output.vcf.gz with NativeVcfReader. real	45m21.485s; user	43m7.954s; sys	1m1.029s. real	47m46.669s; user	45m31.656s; sys	1m0.352s; Traceback (most recent call last):; F",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:5062,predict,predictions,5062,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['predict'],['predictions']
Safety,"nately. And to be honest, even if we do, it probably; wouldn't specifically cover this error case. (from my phone). On Tue, Apr 10, 2018, 11:16 AM KBT59 <notifications@github.com> wrote:. > OK – that proceeded further, I think. Now the error is; > ValueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for; > 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes:; > [64,27,1,3]; >; >; > I hate to keep bothering people about this. Is there documentation on all; > of this that I can refer to?; >; >; > Thanks,; > Brad Thomas; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1368,safe,safe,1368,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943,1,['safe'],['safe']
Safety,"ob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which is equivalent to ./. genotype. This is clarified in the [`genotype field under the VariantCall`](https://github.com/googl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:1317,predict,predictions,1317,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318,2,['predict'],['predictions']
Safety,"ors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 629, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:14187,predict,prediction,14187,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['predict'],"['prediction', 'predictions']"
Safety,"ot 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; W1009 18:31:59.770313 48004354086720 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An error will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **tail -n 50 log**; Traceback (most recent call last):; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1225, in main; merge_and_write_variants_and_nonvariants(variant_generator,; File ""/TMP_DIR/Bazel.runfiles_eyn11z72/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1012, in merge_and_write_variants_and_nonvariants; var",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-939618673:3300,sanity check,sanity check,3300,,https://github.com/google/deepvariant/issues/488#issuecomment-939618673,1,['sanity check'],['sanity check']
Safety,"partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 41450 (95.7%) partial SNP recovery; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:3171,recover,recovery,3171,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"pvariant ran for more than 3 hours and then failed on Chromosome V with this error:. ```; ...; I0402 20:41:44.332051 47425001081536 make_examples.py:535] 25500 candidates (27904 examples) [26.82s elapsed]; I0402 20:42:04.004627 47425001081536 make_examples.py:535] 25600 candidates (28010 examples) [19.67s elapsed]; I0402 20:42:27.991226 47425001081536 make_examples.py:535] 25700 candidates (28130 examples) [23.99s elapsed]; I0402 20:42:35.967661 47425001081536 make_examples.py:535] 25813 candidates (28251 examples) [7.98s elapsed]; I0402 20:42:48.188316 47425001081536 make_examples.py:535] 25911 candidates (28355 examples) [12.22s elapsed]; I0402 20:42:49.405055 47425001081536 make_examples.py:535] 26014 candidates (28458 examples) [1.22s elapsed]; [E::fai_retrieve] Failed to retrieve block. (Seeking in a compressed, .gzi unindexed, file?); 2020-04-02 20:46:28.318323: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""V"" start: 5524980 end: 5526020; Fatal Python error: Aborted. Current thread 0x00002b21fe57cac0 (most recent call first):; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 73 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_e68bsnf0/runfiles/com_google_deepvariant/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-608553986:1760,Abort,Aborted,1760,,https://github.com/google/deepvariant/issues/292#issuecomment-608553986,1,['Abort'],['Aborted']
Safety,rAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; environment: {}; flags:; - ALWAYS_RUN; imageUri: google/cloud-sdk:alpine; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; environment: {}; resources:; projectId: valis-194104; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 10; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuPlatform: ''; disks: []; labels: {}; machineType: n1-standard-1; nvidiaDriverVersion: ''; preemptible: false; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/cloud-platform; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; zones:; - us-west1-b; timeout: 604800s; startTime: '2018-11-08T14:27:06.604193Z'; name: projects/valis-194104/operations/12097970745380060156; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:9252,timeout,timeout,9252,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['timeout'],['timeout']
Safety,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help?. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196:1687,predict,predicted,1687,,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196,2,['predict'],['predicted']
Safety,"rence/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ──────────────────────────────────────────────────────────────────────────────; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1441,timeout,timeout,1441,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['timeout'],['timeout']
Safety,"rnally for improving the accuracy by [expanding the region of finding candidates](https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_core.py#L1305-L1321):. https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L255. This will not show phased reads in the VCF file. You are correct in that you will need to use a tool like [`WhatsHap`](https://whatshap.readthedocs.io/en/latest/guide.html) to update the VCF file with phasing information. `2` The output for the filter column is generated in the following way for most cases (biallelic):. `2.1` `call_variants` generates the genotype probabilities using the PACBIO model (for this case), for the candidates generated by `make_examples`. `2.2` `postprocessing_variants` operates all of this through the [`add_call_to_variant()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L304-L340) function:. `2.2.1` For each variant's predicted genotype probabilities it looks at the highest probability, denoting that being the most likely genotype. It then generates it via the [`most_likely_genotype(predictions, ploidy=2, n_alleles=2)`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L193-L273) function. `2.2.2` It then takes that most likely genotype and uses it [`compute_filter_fields()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L170-L190) function, to generate the appropriate filter. Basically it, strictly looking at the genotype and it if it sees it as [0, 0] (i.e. 0/0) it will label the Filter column as RefCall. `2.2.3` Then the [`uncall_homref_gt_if_lowqual()`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L286-L301) function will label the genotype of each variant with a GQ value < 20 (set via the [`--cnn_homref_call_min_gq`](https://github.com/google/deepvariant/blob/r1.5/deepvariant/postprocess_variants.py#L86-L89) flag) as [-1,-1] which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318:1149,predict,predicted,1149,,https://github.com/google/deepvariant/issues/666#issuecomment-1601744318,2,['predict'],['predicted']
Safety,"rnings.warn(; I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started.; I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]; I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True; I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:56.551032 47501039724352 call_variants.py:716] Predicted 982 examples in 1 batches [0.419 sec per 100].; I0510 12:13:57.403082 47501039724352 call_variants.py:779] Complete: call_variants. real	0m21.581s; user	1m40.583s; sys	0m15.744s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME"". Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_0t8uq2zt/runfiles/com_google_deepvariant/deepvariant/postproc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632:2883,Predict,Predicted,2883,,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632,1,['Predict'],['Predicted']
Safety,"runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.cr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15470,predict,predict,15470,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['predict'],['predict']
Safety,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3842,Detect,Detection,3842,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['Detect'],['Detection']
Safety,"s WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1967,recover,recovery,1967,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"s well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2104,recover,recovery,2104,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3823,predict,predictable,3823,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['predict'],['predictable']
Safety,"snps (the default is 2); * vsc_min_count_indels (the default is 2); * vsc_min_fraction_snps (the default is 0.12); * vsc_min_fraction_indels (the default is 0.06); * vsc_min_fraction_multiplier (the default is 1.0). Here is an example:. ```; --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_count_indels=3,vsc_min_fraction_snps=0.12,vsc_min_fraction_indels=0.06'; ```. You can read more details about these parameters at the following links:. https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. https://github.com/google/deepvariant/blob/r1.5/deepvariant/make_examples_options.py#L178-L202. $`4)`$ Now regarding the pileup, that basically is used to generate the GT and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:3240,predict,prediction,3240,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918,1,['predict'],['prediction']
Safety,"space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">Reasons for No Call</h3><p style=""color: rgb(0, 0, 0); font-family: &quot;Microsoft YaHei&quot;; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"">All entries in the GLnexus pVCF include a Reason for No Call (RNC) field, which is filled in if either GT entry is not called (.). The possible values of RNC include:</p>. Code | Reason | Explanation; -- | -- | --; . | N/A | corresponding allele is called; M | Missing data | input (gVCF) had no data at this genome position; P | Partial data | input only partially covered this genome position; D | Depth | read depth too low to call; – | deletion | sample carries a deletion allele that couldn't be unified into this site; there may be more information in overlapping monoallelic site(s).; L | Lost allele | ^ but other than deletion allele; U | Unphased variants | sample carries multiple non-overlapping variants at this position*, whose phase is not known, so the diploid genotype cannot be called safely. There may be more information in overlapping monoallelic site(s).; O | Overlapping variants | sample carries multiple overlapping variants at this position, so the diploid genotype cannot be called safely. (GLnexus does deal with several common, but not all, cases of overlapping variants output by gVCF callers.) There may be more information in overlapping monoallelic site(s).; 1 | monoallelic | this is a monoallelic site; no assertion about presence/absence of any allele here. <!--EndFragment-->; </body>; </html>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954:3493,safe,safely,3493,,https://github.com/google/deepvariant/issues/494#issuecomment-1018260954,4,['safe'],['safely']
Safety,thank you @pgrosu please could it had been same vpn problem with this docker pulling command?. Error response from daemon: Get https://gcr.io/v1/_ping: dial tcp 64.233.187.82:443: i/o timeout,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-417210803:184,timeout,timeout,184,,https://github.com/google/deepvariant/issues/89#issuecomment-417210803,1,['timeout'],['timeout']
Safety,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:282,Timeout,Timeout,282,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['Timeout'],['Timeout']
Safety,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:404,risk,risk,404,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051,1,['risk'],['risk']
Safety,"uch closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU train",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1475,predict,prediction,1475,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['predict'],['prediction']
Safety,"variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 41450 (95.3%) full SNP recovery; 39678 / 414",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2195,recover,recovery,2195,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"way you can view a variant coming from a neural network is through the idea of preserving $`information`$ $`propagation`$. You saw the previous description of how the different channels get encoded, but let's start with a simpler version. Let say you have a one-line matrix with the following columns:. ![image](https://github.com/google/deepvariant/assets/6555937/d46a3924-2ea4-4b92-8c9c-9fcc29bb7219). This could denote a simplified version of your read base representation, where you notice the middle denotes the variant, and the last column the channel it represents. Now I want to create new types of data from this, which will help me with identifying unique areas of patterns within it. This could be of the form of transformation of the values to ranges that are easier to detect differences among columns. One of these can be dividing all the values by 10, and labeling that channel 2. In this transformation, 10 represents a kernel I described previously and the output is a new feature map (a transformed matrix that helps with detecting unique features based on the numerical representation). Now the data would look like this:. ![image](https://github.com/google/deepvariant/assets/6555937/e52786fd-00b8-4dc9-ad60-c400a30b0f79). Now imagine I create different transformations of these rows, to expand on specific areas among these values where intriguing patterns might emerge. Suppose I create 5 different transformations having then 5 channels with multiple copies of each row, in order to have a fuller dataset that mimics the number of reads. This data is multi-dimensional as it contains different values of X. This can be pretty hard to interpret, but we can collapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:1112,detect,detecting,1112,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['detect'],['detecting']
Safety,"work.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:14843,predict,prediction,14843,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['predict'],"['prediction', 'predictions']"
Safety,"wouldn’t expecting in the VCF from DeepVariant. ***So, in terms of additional feedback***:. **5)** I compared DeepVariant on Google Cloud (v0.7.2, using code similar to [this](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_gcp_deepvariant_runner.sh)) Exome versus WGS variant calls with [a script that I wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:1673,recover,recovery,1673,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"wrote](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/VCF_recovery.pl) as well as precisionFDA. My script filters out “complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](ht",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2011,recover,recovery,2011,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"y; ```. I am omitted the indel statistics from my script because Veritas used freebayes for variant calling (and I’m not converting the indel format, causing the indel count to be quite low, presumably because most overlapped a homopolymer of at least 2 nucleotides). **Still, maybe it is interesting that the “*full*” SNP recovery was higher for the provided variants, but the “*partial*” SNP recovery was higher for DeepVariant?** The precisionFDA comparison (for my samples) shows better results for DeepVariant than the provided VCF files (but there are going to be considerably fewer indels for the overall counts). Going back to _point 4_, running DeepVariant with both my provided .bam alignment (after combining the per-chromosome alignments) and BWA-MEM re-aligned reads both resulted in a set of ~7 million reads (which is closer to the total number reported in that other study for DeepVariant). However, if you emphasize precision, perhaps that somewhat matches my filtered set (in terms of the SNP counts and more noticeable indel differences for homozygous/heterozygous recovery). Namely, I can obtain higher percentages with my script if I use GATK HaplotypeCaller (with `--dontUseSoftClippedBases`)+VariantFiltration (similar to [shown here](https://github.com/cwarden45/DTC_Scripts/blob/master/run_GATK_VarScan.py), for Exome file, but not WGS, and I used GATK version 3.x instead of 4.x) and filter for on-target reads, as shown below:. ```; 20765 / 21141 (98.2%) full SNP recovery; 20872 / 21141 (98.7%) partial SNP recovery; 243 / 258 (94.2%) full insertion recovery; 249 / 258 (96.5%) partial insertion recovery; 208 / 228 (91.2%) full deletion recovery; 213 / 228 (93.4%) partial deletion recovery; ```. That being said, maximizing those recovery statistics does decrease the total number of variants, particularly the indels. Nevertheless, [the precisionFDA comparison for that GATK quality-fitered variant set](https://precision.fda.gov/comparisons/3441) indicates increased p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:4306,recover,recovery,4306,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Safety,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355036534:316,avoid,avoid,316,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534,1,['avoid'],['avoid']
Safety,"“complex variants” (with more than one variant at a position), but .vcf files containing those variants weren’t flagged by precisionFDA (even though I did have to do some file re-processing). So, starting for my test (checking recovery of my CDS Exome variants in my WGS dataset), **the recovery numbers were very close to what I originally expected (98-99% for SNPs, ~90% for indels)**:. **Provided Exome on Provided WGS .bam Alignment**:; ```; 68759 / 72556 (94.8%) full SNP recovery; 71276 / 72556 (98.2%) partial SNP recovery; 3027 / 3648 (83.0%) full insertion recovery; 3413 / 3648 (93.6%) partial insertion recovery; 3119 / 3911 (79.7%) full deletion recovery; 3596 / 3911 (91.9%) partial deletion recovery; ```. **BWA-MEM Exome on WGS (for only on-target alignments)**:; ```; 51417 / 54229 (94.8%) full SNP recovery; 53116 / 54229 (97.9%) partial SNP recovery; 1964 / 2391 (82.1%) full insertion recovery; 2242 / 2391 (93.8%) partial insertion recovery; 2058 / 2537 (81.1%) full deletion recovery; 2349 / 2537 (92.6%) partial deletion recovery; ```. For comparison, you can see what is reported from precisionFDA (when running DeepVariant on my samples) for the [provided .bam alignment](https://precision.fda.gov/comparisons/3435) or the [BWA-MEM re-aligned comparison](https://precision.fda.gov/comparisons/3436). There are two numbers because a “*partial*” recovery just checks for the presence of a variant (either heterozygous or homozygous). Due to formatting differences (such as with indels), I didn’t feel comfortable showing comparisons between variant callers on my page with notes/code on my [Genos Exome sample](https://github.com/cwarden45/DTC_Scripts/tree/master/Genos_Exome), but I think this is OK when using the same variant caller on different samples. For comparison, the precisionFDA comparison for the provided .vcf files is [here](https://precision.fda.gov/comparisons/3434), and these are the statistics for the provided .vcf files (from my script):; ```; 39494 / 4145",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:2148,recover,recovery,2148,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['recover'],['recovery']
Security," 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5697,certificate,certificate,5697,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['certificate'],['certificate']
Security, 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3516,access,access,3516,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security," > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10608,secur,security,10608,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['secur'],['security']
Security," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1397,confidential,confidential,1397,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047,2,"['confidential', 'secur']","['confidential', 'secured']"
Security," Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1972,validat,validation,1972,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['validat'],['validation']
Security," for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2655,validat,validated,2655,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['validat'],['validated']
Security," have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the model will still try to assign probabilities given what it has seen across the whole region and all channels, to infer the genotype probabilities, but that does not mean they will be high.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424:2680,validat,validated,2680,,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424,1,['validat'],['validated']
Security," mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:2536,validat,validation,2536,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['validat'],['validation']
Security," modified file on this public s3 bucket so you can have a look on it directly from here : ; s3://dv-testfiles/hg19.fa; s3://dv-testfiles/ENCFF528VXT.bam. There you can find the genome I used for running too. Before i created the needed files ( done in the following docker container https://hub.docker.com/r/luisas/samtools/ ) :; ```; samtools index ENCFF528VXT.bam; samtools faidx hg19.fa; bgzip -c -i hg19.fa > hg19.fa.gz; samtools faidx ""hg19.fa.gz""; ```. Then I ran in the docker container you provide :; ```; mkdir shardedExamples. time seq 0 1 | parallel --eta --halt 2 python /opt/deepvariant/bin/make_examples.zip --mode calling --ref hg19.fa --regions chr20:10,000,000-10,010,000 --reads ENCFF528VXT.bam --examples shardedExamples/examples.tfrecord@2.gz --task {}. ```. ```; /opt/deepvariant/bin/call_variants --outfile call_variants_output.tfrecord --examples shardedExamples/examples.tfrecord@2.gz --checkpoint dv2/models/model.ckpt; ```. and here the error output:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 09:54:53.415692 140603705038592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 410, in module; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 401, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_vZjmn7/runfiles/genomics/deepvariant/call_variants.py"", line 324, in call_variants; examples_filename, example_format)); ValueError: The TF examples in shardedExamples/examples.tfrecord@2.gz has image/format 'None' (expected 'raw') which means you might need to rerun make_examples to genenerate the examples again.; ```. Thanks a lot, ; Luisa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371096675:3248,access,accessible,3248,,https://github.com/google/deepvariant/issues/52#issuecomment-371096675,1,['access'],['accessible']
Security,"![train_loss_and_error](https://user-images.githubusercontent.com/13111474/60637174-f2844000-9e4b-11e9-9ed0-6461041cf407.png); Hi, I looked in my log file and plotted a figure of training losses and validation f-measures, and it turned out that I might have overlooked the improves in the trained models. The improvments after 10000 steps become very small (still important though) compared with that in the first steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194#issuecomment-508325465:199,validat,validation,199,,https://github.com/google/deepvariant/issues/194#issuecomment-508325465,1,['validat'],['validation']
Security,"1) Can you confirm that you have generated training/validation data? e.g, run ; ```; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```; and ; ```; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030223849:52,validat,validation,52,,https://github.com/google/deepvariant/issues/797#issuecomment-2030223849,1,['validat'],['validation']
Security,"1) Cloud has Docker and sudo access, but you have to pay for it for the time you use it (it's basically VM created instances with storage that is pay-per-usage):. https://cloud.google.com/life-sciences/docs/tutorials/deepvariant. 2) If you have a cluster that you have access to, that would be perfect. You can use singularity, or if you don't have Docker there are ways to run Docker is some limited way without root access. 3) Regarding Ubuntu, tell me what you see when you run the following script under your DeepVariant folder (the one git cloned):. ```; #!/bin/bash; source settings.sh; ./run-prereq.sh; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575697410:29,access,access,29,,https://github.com/google/deepvariant/issues/657#issuecomment-1575697410,3,['access'],['access']
Security,"42653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_op",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1939,access,accessible,1939,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,2,['access'],['accessible']
Security,"; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1855,confidential,confidential,1855,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943,1,['confidential'],['confidential']
Security,"; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 467, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 382, in create_all_commands_and_logfiles; check_flags(); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 357, in check_flags; raise RuntimeError('The model files {}* do not exist. Potentially '; RuntimeError: The model files gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.data-00000-of-00001* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open; ```. I also get the same error when hosting the model (renamed model.ckpt) in my personal GS bucket -- I have made the storage bucket read accessible to all users so the TPU should have access:; ```bash; docker run \; -v `pwd`:`pwd` -w `pwd` \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --call_variants_extra_args use_tpu=true,tpu_name=""variantcaller-node1"",tpu_zone=""europe-west4-a"" \; --customized_model ""gs://tpu-bwb/analysis-files/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt"" \; --model_type=WGS \; --ref=""input/data/${REF}"" \; --reads=""input/data/${BAM}"" \; --output_vcf=""output/${OUTPUT_VCF}"" \; --output_gvcf=""output/${OUTPUT_GVCF}"" \; --regions chr20 \; --num_shards=$(nproc) \; --intermediate_results_dir /output/intermediate_results_dir. I0527 21:26:03.381308 140127359940416 run_deepvariant.py:341] Creating a directory for intermediate results in /output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:2440,access,accessible,2440,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['access'],"['access', 'accessible']"
Security,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:416,access,access,416,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872,1,['access'],['access']
Security,"> Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:137,validat,validation,137,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,4,['validat'],['validation']
Security,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:366,hash,hash,366,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,1,['hash'],['hash']
Security,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371#issuecomment-717204430:61,access,accessing,61,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430,1,['access'],['accessing']
Security,"@AndrewCarroll hello Andrew, thank you for your reply! I'll explain my intentions:; As a project, I need is to select a reference (doesn't really matter of what) and a dataset of long reads of that reference, successfully train the DeepVariant model with that set, and then evaluate the the trained model accuracy for the long reads. All I need to do is to use the data to make sets of training examples, validation examples and test examples, train the model with the training set, evaluate each checkpoint with the validation set, eventually choose the best checkpoint and finally evaluate it with the test set. Very similar to this [case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) DeepVariant provided, but with a dataset of long reads. Also thank you very much for pointing out that DeepVariant was not trained for non-CCS reads, I'll try using the dataset you provided! If I'll succeed training and evaluating the model with this dataset, it would help a lot to be able to compare my my final model to your trained model. Also if you can explain a little about how to work with the files in the dataset. Where can I find truth variants and confident regions file of it's reference for my training?. I hope this clarifies my purposes. It may seem like a small project, but I greatly appreciate your help with this, it is very important to me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458885417:405,validat,validation,405,,https://github.com/google/deepvariant/issues/138#issuecomment-458885417,2,['validat'],['validation']
Security,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:455,encrypt,encrypted,455,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491,2,['encrypt'],['encrypted']
Security,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2111103594:449,access,access,449,,https://github.com/google/deepvariant/issues/818#issuecomment-2111103594,1,['access'],['access']
Security,"@aizhimin . For 10x genomics data, We've previously observed lower accuracy both across many methods and DeepVariant as well. I think we will do ""OK"" on 10x data, likely not what I would recommend for 10x data. . For sc-RNA seq, I have a similar reaction, but it may also be the case that the alternatives are even fewer in number. As @danielecook and @pgrosu mention, it might be worth doing if you have some way of assessing and validating the result.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/705#issuecomment-1709038904:431,validat,validating,431,,https://github.com/google/deepvariant/issues/705#issuecomment-1709038904,1,['validat'],['validating']
Security,"@aizhimin I suspect performance will be poor, but if you have a method for validating we would be interested in seeing the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/705#issuecomment-1708685880:75,validat,validating,75,,https://github.com/google/deepvariant/issues/705#issuecomment-1708685880,1,['validat'],['validating']
Security,"@danielecook If I may with a small inquiry, and please feel free to continue adding. @mwhitesi To better understand what's possible, I had a few small questions to get a state of your running environment:. 1) What operating system are you running on, and what version is it? For Linux one of these should work from the command-line:. ```; cat /etc/lsb-release; cat /etc/redhat-release; cat /etc/os-release; ```. 2) What CPU are you operating on? If you are on Linux, you can type from your terminal: `cat /proc/cpuinfo`. 3) Do you have root access, or is this a shared compute environment (cluster)? I will try to remain in user-space. 4) Is micromamba the preferred approach, or would you be willing try other ones like miniconda if it becomes the case?. 5) Do you have Docker and/or Singularity? This will be an option if the above fail. Again these are just preliminary to gauge for possibilities. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014:541,access,access,541,,https://github.com/google/deepvariant/issues/664#issuecomment-1595877014,1,['access'],['access']
Security,@gambalab You're most likely running Python 3.6 and DeepVariant needs Python 3.8. What operating system and version of the OS are you running? Do you have Docker or Singularity access?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1602300665:177,access,access,177,,https://github.com/google/deepvariant/issues/669#issuecomment-1602300665,1,['access'],['access']
Security,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471842177:266,threat,threat,266,,https://github.com/google/deepvariant/pull/159#issuecomment-471842177,1,['threat'],['threat']
Security,"@gunjanbaid I am following the tutorial https://cloud.google.com/genomics/docs/tutorials/deepvariant - so GCP WM with Debian 9.6 (4.14.74+ GNU/Linux) running the docker container that you have provided. I have access to a CentOS 7.5.1804 (3.10.0-862.3.3.el7.x86_64 GNU/Linux) system, however, I do not have full admin rights as it a shared system and there are several post about issues with CentOS 7 and not having root permissions. Unfortunately I don´t have the time to play around with this at the moment – I’m expecting there will be a lot of trouble shooting and work arounds!. Thanks for all your help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-453430258:210,access,access,210,,https://github.com/google/deepvariant/issues/135#issuecomment-453430258,1,['access'],['access']
Security,"@helizabeth1103 For training / validation sets -- the main point here is to keep them separate. ; And then, for all data that goes into training set, they will need to be shuffle into one set of shards. So that you can get the num_examples, and a consistent path. For example, in our documentation you see something like:. ```; name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. For training, you need one `tfrecord_path` that refer to all the files (output of shulffling), and a num_examples. For validation, you need a separate file with similar format. Hope that's clear! I'll close this issue now that you're able to run shuffling!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668:31,validat,validation,31,,https://github.com/google/deepvariant/issues/793#issuecomment-2013124668,4,['validat'],['validation']
Security,"@kishwarshafin . Sorry to let you know, but the researcher is not okay to share the BAM file due to confidentiality concerns. Quick question: where can I download the complete datasets (aligned BAM files) for HG002 and HG003 datasets for R10 ONT chemistry used for benchmarking in the study ""Local read haplotagging enables accurate long-read small variant calling""? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854#issuecomment-2249065386:100,confidential,confidentiality,100,,https://github.com/google/deepvariant/issues/854#issuecomment-2249065386,1,['confidential'],['confidentiality']
Security,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-439585565:213,access,access,213,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565,1,['access'],['access']
Security,"@nmousavi I made a readonly public bucket you can access here:; http://storage.googleapis.com/public-debug. @pgrosu I am not using Docker. I am running the shell script from my local machine. Do I need to update my google-cloud-sdk version to slim? Or are you referring to the DeepVariant container image itself? If so, I am using the Cloud Genomics API -- not hosting my own image of DeepVariant... see:; https://cloud.google.com/genomics/docs/tutorials/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437189603:50,access,access,50,,https://github.com/google/deepvariant/issues/116#issuecomment-437189603,1,['access'],['access']
Security,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1639,confidential,confidential,1639,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:399,access,access,399,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457,1,['access'],['access']
Security,"@pgrosu Thank you for your response.; When you say ""the cloud"" do you mean to run it on a server/a super computer?; I predicted that I will need root access (sudo) which I don't have.; 1. Is there a way to do this? Docker is not even installed there. 2. So are you suggesting that we should not use mac (apple silicon)?. The ubuntu 20.04 that was installed on my mac (using UTM), should this work?. Thank you for your time and guidance!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888:150,access,access,150,,https://github.com/google/deepvariant/issues/657#issuecomment-1575677888,2,['access'],['access']
Security,"@pichuan ; I tried to run; ; ```; bash; BIN_VERSION=""1.4.0""; docker run --gpus all \; -v ""${INPUT_DIR}"":""/data/deepvariant/test_bam"" \; -v ""${OUTPUT_DIR}"":""/data/deepvariant/output"" \; -v ""${REF}"":""/data/PublicData/hg38"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /data/deepvariant/test_bam/*bam*; ```. Strangely, it showed that I have 3 files matching the wildcard; ```bash; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.du': No such file or directory; ls: cannot access '/data/deepvariant/test_bam/DP8400014945BR_L01_541.picard_MD.bam.220427_094718.md5': No such file or directory; ```; Then I tried to ls directory itself:; ```bash ; ls: cannot access '/data/deepvariant/test_bam/': No such file or directory; ```. I also checked all the permissions and rw perrmissions are granted recursively",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746:389,access,access,389,,https://github.com/google/deepvariant/issues/577#issuecomment-1284947746,4,['access'],['access']
Security,@pichuan Thank you for the followup. I definitely have access to the exomes.bed file and the gsutil cat works fine. . I see what you mean re: the `/input-gcsfused-0/CNR-data/` ...it seems like the parameter did not get translated properly -- I noted in the original launch shell script the bam and bai files are also sent using the gs:// format and assume this is the proper means for the bed file,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437887566:55,access,access,55,,https://github.com/google/deepvariant/issues/116#issuecomment-437887566,1,['access'],['access']
Security,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:182,password,password,182,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,4,"['password', 'secur']","['password', 'security']"
Security,"@ptrebert Basically containers operate via cgroups to limit their own internal memory resources. The container basically needs to access more memory inside of itself. Let's see if your run is successful, which otherwise might require setting other settings.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-642917555:130,access,access,130,,https://github.com/google/deepvariant/issues/304#issuecomment-642917555,1,['access'],['access']
Security,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-643473196:154,inject,injects,154,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196,2,"['expose', 'inject']","['exposed', 'injects']"
Security,"@solokopi What country is the `solokopi-All-Series` host are you trying to access the `storage.googleapis.com` domain from? Do you have root access on that machine? If you cannot access the domain, try to get some VPN access on your machine that would allow you access to the `storage.googleapis.com` domain.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416721742:75,access,access,75,,https://github.com/google/deepvariant/issues/89#issuecomment-416721742,5,['access'],['access']
Security,"@tzcoolman You're very close. Just update your `singularity run` with the following two binds (`--bind ${INPUT_DIR} --bind ${OUTPUT_DIR}`), like this:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} docker://google/deepvariant:""${BIN_VERSION}"" ...; ```. This makes those directories accessible within the container. The rest can stay the same. Let me know if there is anything else. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098:335,access,accessible,335,,https://github.com/google/deepvariant/issues/678#issuecomment-1635203098,1,['access'],['accessible']
Security,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-354748344:452,expose,expose,452,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344,2,['expose'],['expose']
Security,"According to file dv_config.py : ; ```; # If set to 0, use full validation dataset.; config.num_validation_examples = 0; ```. Also, the[ training tutorial](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md) also use `--config.num_validation_examples=0 `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073360601:64,validat,validation,64,,https://github.com/google/deepvariant/issues/802#issuecomment-2073360601,1,['validat'],['validation']
Security,"Also, @chrisfleisch is the singularity container you build for version 0.7.0 still working? If so, can you please share with me the definition file you used or any documentation you have about it?. This can be very helpful for me as when I try to build a singularity container for 0.7.0, it gives me the same error in regards of needing access to the path of site-packages. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-457987076:337,access,access,337,,https://github.com/google/deepvariant/issues/132#issuecomment-457987076,1,['access'],['access']
Security,"Also, when we tried to run `call_varaints.zip` with a training checkpoint we just found earlier, it failed with this exception below. We are not entirely sure what we did wrong. We just used your command to train and the command in the case study to run the model to produce variants; ```; root@qiuz-deepvariant-quickstart:~/case-study/output/logs# cat call_variants.log; WARNING: Logging before flag parsing goes to stderr.; I0209 02:46:47.705486 139970286499584 htslib_gcp_oauth.py:82] GCP credentials found; will be able to access non-public gs:// URIs from htslib; 2018-02-09 02:46:50.318843: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; I0209 02:46:51.237144 139970286499584 call_variants.py:325] Initializing model from /root/case-study/output/model.ckpt; 2018-02-09 02:46:51.248949: W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open /root/case-study/output/model.ckpt: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 387, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 378, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/call_variants.py"", line 326, in call_variants; model.initialize_from_checkpoint(checkpoint_path, 3, False)(sess); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/modeling.py"", line 298, in initialize_from_checkpoint; [self.n_classes_model_variable]); File ""/tmp/Bazel.runfiles_lnD8hJ/runfiles/genomics/deepvariant/tf_utils.py"", line 264, in model_sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364502198:527,access,access,527,,https://github.com/google/deepvariant/issues/46#issuecomment-364502198,1,['access'],['access']
Security,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1239,confidential,confidential,1239,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,Can you check if you have `example_info.json` output in your training data and validation data generation folders and if they are the same? If same then you can copy it to the directory and use it. The training loop is supposed to copy the `example_info.json` from training folder to the checkpoint output directory. Not sure if it was missing in your setup.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869#issuecomment-2313193016:79,validat,validation,79,,https://github.com/google/deepvariant/issues/869#issuecomment-2313193016,1,['validat'],['validation']
Security,"Can you check whether you're able to pull other public images on this machine?. From the error above it seems to have an ""Authentication error"", which doesn't quite make sense to me because our Docker hub image should be public. I just tried:; ```; singularity pull docker://google/deepvariant:""1.3.0""; ```; as a sanity check. I confirmed it worked for me (so at least hopefully there isn't an obvious mistake here on permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489:122,Authenticat,Authentication,122,,https://github.com/google/deepvariant/issues/513#issuecomment-1027495489,1,['Authenticat'],['Authentication']
Security,"D5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-rig",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2244,validat,validation,2244,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628,1,['validat'],['validation']
Security,"Dear pgrosu, thank you very much for your suggestions. I want to wait for it to finish running and then try it immediately. Although I am not sure whether it is aff53ed783a7 or 45f6c7767ff0, this is my running command; `sudo docker run \; > -v ""/home/xxh/all_data/Deepvariant/input"":""/input"" \; > -v ""/home/xxh/all_data/Deepvariant/output"":""/output"" \; > google/deepvariant:""1.5.0"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > -ref=/input/bs_filled.fasta \; > --reads=/input/aln_sort.bam \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=16 ; [sudo] password for xxh: ; Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant; 7608715873ec: Already exists ; ff9c04d6f4fd: Already exists ; ccc0633ad137: Already exists ; 31c4e73f93e5: Already exists ; 5f25a39487ec: Already exists ; 634688f8f57d: Already exists ; ce40c5e2b0ba: Already exists ; 9eaa0cfabb44: Already exists ; f71d9ff16a67: Already exists ; 27ddda9faddb: Already exists ; 6ba5fd944a25: Already exists ; bec3e4e06e4d: Already exists ; dc1469807dcc: Already exists ; ceb45df4e1ef: Already exists ; 4f6165ff322d: Already exists ; f9cefd1876c2: Already exists ; 931b80517c67: Already exists ; 7b13ecd8df6e: Already exists ; 245c9afd7ea9: Already exists ; 97c2b022ac0f: Already exists ; 4c15f3639f35: Already exists ; fa21a1eddf03: Already exists ; 6419c68a3d65: Already exists ; 8751b2539913: Already exists ; 20646815bf33: Already exists ; 25dc07245f2e: Already exists ; 6e3dea686609: Already exists ; dc216b407a52: Already exists ; c6710cf0efec: Already exists ; 6a519085af15: Already exists ; fd35c1634889: Already exists ; 0e4b2b2ad2db: Already exists ; 87e7c72faeb5: Already exists ; 690adc142e08: Already exists ; abfd217d5088: Already exists ; 30b033b0505f: Already exists ; 853ad599972a: Already exists ; f20c79af8049: Already exists ; 26703b5b7abd: Already exists ; f3b33765da79: Already exists ; c382f9fc227e: Alrea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/700#issuecomment-1687461144:625,password,password,625,,https://github.com/google/deepvariant/issues/700#issuecomment-1687461144,1,['password'],['password']
Security,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4157,confidential,confidential,4157,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"First of all, I assume that you have access to `gs://canis/CNR-data/exomes.bed` file?; If you do:; `gsutil cat gs://canis/CNR-data/exomes.bed` from the project that has the same permission, you can first double check that you have access to it. And, I don't fully understand how gcsfuse is implemented in the Google Cloud runner, so I'll defer this to our collaborator at Cloud @nmousavi -- Nima, in the command in the previous log, particularly this one:; `mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs canis /input-gcsfused-0 && /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/canis/CNR-data/deep_variant_files/examples/0/examples_output.tfrecord@8.gz --reads /input-gcsfused-0/CNR-data/TLE_a_001.reheader.bam --ref /mnt/google/.google/input/deepvariant/performance-testdata/hs37d5.fa.gz --task 0 --regions gs://canis/CNR-data/exomes.bed`. It seems weird to me that everything else has been changed to under `/input-gcsfused-0` or `/mnt`, but the BED file still has a `gs://` prefix. This seems to me like something might be wrong in how the runner is using gcsfuse on the BED file. Can you take a look?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437712479:37,access,access,37,,https://github.com/google/deepvariant/issues/116#issuecomment-437712479,2,['access'],['access']
Security,"Follow up: I looked my older work log -- I actually did test the Singularity+OpenVINO combination. I didn't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:268,access,access,268,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['access'],['access']
Security,For anyone with the same problem: my bed file had the IDs labelled as chr1 whereas other files used the accession number (e.g NC_000001.11) - I used dplyr on R to recode the IDs to the accession number and then deepvariant worked :) Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/527#issuecomment-1078989925:104,access,accession,104,,https://github.com/google/deepvariant/issues/527#issuecomment-1078989925,2,['access'],['accession']
Security,"For some reason singularity is accessing my local python libs, instead of the singularity image's libraries.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/385#issuecomment-728242962:31,access,accessing,31,,https://github.com/google/deepvariant/issues/385#issuecomment-728242962,1,['access'],['accessing']
Security,"From the log it seems like there's something to do with your authentication to the Google Cloud Platform. But usually this shouldn't fail even if you didn't set that up. Can you give us a bit more description about your machine? (OS version, if you're running on cloud, where and what type of machine, etc)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/7#issuecomment-350520174:61,authenticat,authentication,61,,https://github.com/google/deepvariant/issues/7#issuecomment-350520174,1,['authenticat'],['authentication']
Security,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1031,confidential,confidential,1031,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"Hello @andrewrech,. Thank you for you interest. We are quite interested in collaborating on this effort, and the timeframe of Q2-Q4 is a good fit for our efforts. For these WES samples, will these be germline sequencing or somatic sequencing? . I do not know the institutional requirements for access, but if you are able to add other investigators, our team may be interested to work closely with you on generating the individual calls, merging them, and evaluating the results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-463728562:294,access,access,294,,https://github.com/google/deepvariant/issues/142#issuecomment-463728562,1,['access'],['access']
Security,"Hello @pichuan, ; Thanks for response, actually I am running a SGE cluster in order to run my analysis, and the sysadmins keeps use of docker restricted(root access issue). Hence I am tryng to use conda.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2063522106:158,access,access,158,,https://github.com/google/deepvariant/issues/806#issuecomment-2063522106,1,['access'],['access']
Security,"Hello @pichuan, I ran the full deepvariant pipeline after deleting all output directories from the previous run. It seems call_variants outputs only 16 files to the intermediate dir, whereas make_examples outputs 19 (with --num_shards 19). Here's the full command:. `podman run -it --rm -e LD_LIBRARY_PATH=/usr/bin:/usr/lib/nvidia:/usr/local/nvidia/:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/bin:/usr/local/lib/python3.8/dist-packages/tensorrt_libs/ --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/run_deepvariant --model_type=WGS --regions 'chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM' --num_shards 19 --ref=/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --reads=/data/bamfiles/sample1.E250013.L1.hg38.rg.bam --output_vcf=/data/variants/sample1.vcf.gz --output_gvcf=/data/variants/sample1.g.vcf.gz --intermediate_results_dir=/data/variants/sample1.intermediate --logging_dir=/data/variants/sample1.logs`. Adding the ld_library_path -argument gets rid of the error messages about libvinfer, however I still get the cuda error:. `2024-07-16 14:14:08.323907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error`. Although call_variants did use gpu and ran in about half an hour. Then postprocess_variants halts with:; `ValueError: ptrue must be between zero and one: nan`. (Full error log in the first message) I'll try to play around with --num_shards next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994:458,secur,security-opt,458,,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994,1,['secur'],['security-opt']
Security,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:602,secur,secure,602,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772,1,['secur'],['secure']
Security,"Hello Salik,. I am sorry you are encountering issues with the Cloud Runner. I can't definitively tell you what has gone wrong, but I can spot a few things that will be issues (there could also be others). First, I think the log snippet here comes from the main log of the cloud runner. The underlying program error can often be found in another folder of the same run. You should see a ""logs"" folder and this should contain a program-specific log (like make_examples.log). This can be more informative. From your run, I can see that you provide your FASTA as an uncompressed .FA file. I believe that the cloud runner requires a BGZIP compressed reference, a samtools FAIDX, and a GZI index all in the same place. You will need to do the following operations on the FASTA file in a bucket that you control:. bgzip -i GRCh38_Verily_v1.genome.fa; samtools faidx GRCh38_Verily_v1.genome.fa. afterward, you will need to put the resulting:. GRCh38_Verily_v1.genome.fa.gz; GRCh38_Verily_v1.genome.fa.gz.gzi; GRCh38_Verily_v1.genome.fa.gz.fai. in the same bucket that the cloud runner can access. . Hopefully these instructions seem reasonable and this unblocks you from this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-436856528:1081,access,access,1081,,https://github.com/google/deepvariant/issues/116#issuecomment-436856528,1,['access'],['access']
Security,"Hello, thank you for your reply! . Not setting `--model_type WGS ` led to an error. To clarify, when you say generating the training data, you're referring to including `--channels ""insert_size""` in the make_examples steps for the training and validation sets, correct? Or do you mean the step where the custom model is trained? . Thank you! . Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2027660245:244,validat,validation,244,,https://github.com/google/deepvariant/issues/797#issuecomment-2027660245,1,['validat'],['validation']
Security,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385701252:974,confidential,confidential,974,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381621757:863,confidential,confidential,863,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"Hey, thanks for the response! I may have access to a high-quality dataset in the near future. Also, training multiple models for each level of ploidy makes sense. I've got some experience with TF and programming in general (Python and Rust, lately), so curious if you can give some feedback on specific areas that may need to be adjusted for ploidy? Or, if it would be too much for one person I understand too. I've done re-training of DV models for non-model species and have seen great improvements so far, so I'm familiar with that as well. Thanks again!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841:41,access,access,41,,https://github.com/google/deepvariant/issues/519#issuecomment-1053728841,2,['access'],['access']
Security,"Hi @Axze-rgb,. That's good, and that just tells you that there might be high GQ regions of possible correct calls, which you need to validate. Now what you want is to determine the stationary areas of variation, which would not fall within noise signatures. As Andrew mentioned, you want to look at GQ regions with their VAF, and begin to stratify confidence regions backed up by good alignment of reads (IGV, or your favorite browser). Then you want to determine what regions of high-confidence with stable high GQ (correct call) mean in terms of VAF genotype regions, which might look like this (but not exactly):. ![image](https://github.com/google/deepvariant/assets/6555937/0847f683-8d69-4bb6-a6d4-eddd7e166993). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1648181613:133,validat,validate,133,,https://github.com/google/deepvariant/issues/682#issuecomment-1648181613,1,['validat'],['validate']
Security,"Hi @BIjoy92 . The problem is that the paths are different inside the docker container than outside. The part of the command that says `-v ""${INPUT_DIR}"":""/input""` mounts the outside $INPUT_DIR/ to the inside /input/, which means the reads can be accessed at the following path: `--reads=/input/BWA_Markdup_sort.bam`. The same change is needed for all your inputs and outputs, e.g. `/output/`. See the Quick Start for an example: https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; I also noticed you're using the gcr.io docker version, which works fine, but all our newer documentation uses `google/deepvariant:""${BIN_VERSION}""`, so I just want to check: what documentation were you using for reference?. Thanks!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/337#issuecomment-679213268:246,access,accessed,246,,https://github.com/google/deepvariant/issues/337#issuecomment-679213268,1,['access'],['accessed']
Security,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032:915,validat,validation,915,,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032,1,['validat'],['validation']
Security,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491:225,validat,validate,225,,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491,1,['validat'],['validate']
Security,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1705867623:1646,validat,validate,1646,,https://github.com/google/deepvariant/issues/704#issuecomment-1705867623,2,['validat'],"['validate', 'validation']"
Security,"Hi @JakeHagen . Although we do have access to some dbGaP datasets, I don't believe that this is one of them. Let me conduct some experiments from our benchmark data and see if I can replicate the effect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1331125414:36,access,access,36,,https://github.com/google/deepvariant/issues/586#issuecomment-1331125414,1,['access'],['access']
Security,"Hi @JakeHagen . We may have identified an issue which could have affected very specifically exome runs with 100bp length (but not WGS). We have been able to both replicate your findings and train a model which seems to eliminate the effect on our replication. Would you be interested to run a with this custom model that we generated to confirm that it fixes your issue? If so, can you email awcarroll@google.com and I can send you both the model and instructions to run it. If this does seem to correct the issue and we can validate the fix, we will plan to push this out as a part of next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582:525,validat,validate,525,,https://github.com/google/deepvariant/issues/586#issuecomment-1356675582,1,['validat'],['validate']
Security,"Hi @PengJia6 @pgrosu . As @kishwarshafin indicates, when you visualize the Insertion and Deletion items, you'll see these events. . However, this is only part of the story. It looks to me like these are two equivalent ways of representing the same variant. The underlying event is that one chain of Cs is one longer and another chain of As is one shorter. This can be represented as a C -> A SNP event, or as a deletion of A and an insertion of C. The issue is that the mapper is not consistently choosing a representation for the reads. For Illumina data, DeepVariant would perform its own realignment and make a representation that is consistent within itself. But we don't do that realignment step for DeepVariant. I suspect, but have not validated, that example here relates to the use of homopolymer compression by minimap. That is, the AA in the reference is represented as only a single A during mapping, which can cause the location of the deletion not to get reconciled. It may be possible to fix this by mapping the sample with the --preset HiFi flag (which now does not use homopolymer compression). I am trying to take a bit of time to look at the BAM file itself to confirm that.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996:742,validat,validated,742,,https://github.com/google/deepvariant/issues/660#issuecomment-1590162996,1,['validat'],['validated']
Security,"Hi @SHuang-Broad , thanks for your question. This line in the log indicates that you are running with our internal phasing:; ```; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; ```; (In this case, the phasing algorithm skips some windows based on some threshold). Given that you're using run_deepvariant, you can see that in r1.5, our run_deepvariant.py enables `phase_reads` for the PacBio mode.; https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L247-L260; (Other than `phase_reads`, a few other flags are also necessary. For example: `track_ref_reads`, `sort_by_haplotypes`, `parse_sam_aux_fields`, And `partition_size` to cover enough candidates.); If you're running make_examples separately, please make sure to add these flags. You can use the `--dry_run` flag of run_deepvariant to figure out which flags to add. Note that our phasing is intended for use in DeepVariant accuracy only (so that our users don't need to add an extra step like before). We don't intend for our phasing results to be used like a standalone phasing tool. That is why we didn't explicitly expose the phased results. If you're not seeing the accuracy we reported, or the accuracy you see on other samples look unexpected, please let us know!. By the way, if you really want to double check the phasing results, you can save the make_examples results using the --intermediate_results_dir flag. And then use our https://github.com/google/deepvariant/blob/r1.5/docs/show-examples.md tools to visualize the images. You should able to see a channel with the sorted HP values. Note that not all windows would be phased, but if you look through a few of them you should be able to check. If you're curious about the phasing algorithm itself, @akolesnikov and @kishwarshafin are working on writing up a bit more of the details. That might take a while to finish, but we can let you know when we have some more details to share.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1547069475:1168,expose,expose,1168,,https://github.com/google/deepvariant/issues/649#issuecomment-1547069475,1,['expose'],['expose']
Security,"Hi @Suke-fudan ,; Thanks for finding where RNC comes from. For `./.` , it means that our classifier wasn't as confident about the call. The way that the classifier makes the probabilistic call is based on all the information from the input signals that we encoded into the channels. You can see the blog post [Looking Through DeepVariant's Eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/) for more details on that topic. In a way, you can think of the decision for us to label it as `./.` as a heuristic. We could have exposed the original call (which is 0/0), but the probability given to 0/0 wasn't high enough, so we decide to give it `./.`. In this case, if you like, you could have a postprocessing step that takes all the `./.` calls, and make a different decision, if you decide that there are overwhelming evidence to be another genotype. If you do decide to go with such a postprocessing step, I'd recommend that you evaluate the whole pipeline (including your postprocessing step) to make sure the accuracy is better systematically. I hope this helps. Feel free to follow up.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141:566,expose,exposed,566,,https://github.com/google/deepvariant/issues/494#issuecomment-1038521141,1,['expose'],['exposed']
Security,"Hi @Wenfei-Xian . DeepVariant already sees the strand of the input reads as one of the input. For more information on the inputs to DeepVariant, see: [our blog looking through DeepVariant's eyes](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant will incorporate the strand information as a part of its estimation for the confidence in the variant call (e.g. in the QUAL field and the GQ value). The GQ value is the best-calibrated value to use for filtering or evaluation. DeepVariant doesn't annotate the supporting information in the VCF, so if you want to directly access the support for strand, you will need to run some other program to annotate the VCF with the BAM information. For strand, realignment is unlikely to have a noticeable effect, as it will change the coordinates of read positions (and when doing so may alter which reads overlap a variant position), but it will not alter the forward or reverse orientation of a read. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/630#issuecomment-1506060914:622,access,access,622,,https://github.com/google/deepvariant/issues/630#issuecomment-1506060914,1,['access'],['access']
Security,"Hi @aderzelle . Today we released DeepVariant v0.9, which contains several changes to code and training models. As part of this release, we have introduced changes which fix the issue for the BAM snippets presented, and which we think will generally fix the issue that you observed in other cases. To briefly summarize what we believe to be the cause - in candidate generation, a de Bruijn graph of variant and reference haplotypes is constructed. In rare cases, some graph paths are created in which local connections are valid, but no individual read supports the entire path. In your case, this caused two similar representations to generate candidates at different positions, each of which could be locally supported. In our fix, we require at least some support for the constructed graph of the candidate haplotype. We also noticed a separate fix that resolves your case. Specifically, your case was sensitive to the kmer length used to construct the graph. By default, this is 10, but we noticed that increasing to 15 also resolved your issue. We think this may reflect local repetitiveness. We have exposed this parameter in make_examples as: --dbg_min_k. This is available when running make_examples directly, but not in the Docker image. Since the issue should be resolved in v0.9 without this change, this is mostly for your information if you want to experiment with other tweaks. We would be interested to hear your feedback confirming this case is resolved in v0.9. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/209#issuecomment-553647872:1106,expose,exposed,1106,,https://github.com/google/deepvariant/issues/209#issuecomment-553647872,2,['expose'],['exposed']
Security,"Hi @anitagh . Thank you for putting effort into detailed analyses. However, there is a key mistake. DeepVariant is not a somatic caller. It is not designed to detect subclonal variants. In the same way that for GATK you would not use HaplotypeCaller, you would use a different tool, Mutect2, you would not use DeepVariant for this problem. Before DeepVariant's neural net, there is a human-written candidate generation component which finds candidate positions. There is a threshold for this to even nominate a candidate for later classification. This is set to 12% (based on tuning for germline calling), so we would not expect that DeepVariant would nominate a 10% mix as candidates. . We do have a somatic calling tool in early access that we are making available to trusted partners. If you would be interested in that method, you can email me (awcarroll@google.com). Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-536320209:731,access,access,731,,https://github.com/google/deepvariant/issues/222#issuecomment-536320209,1,['access'],['access']
Security,"Hi @crazysummerW , so it seems like you might have a different issue. Is it possible that in your setting, you don't have write access to the directory that `tempfile.gettempdir()` gave you? I'll need more information from you to pinpoint the issue (because I can't reproduce it on my side yet). For example, on my machine:. ```bash; $ python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'; /tmp; $ export TMPDIR=${HOME}; python -c 'import tempfile; foo=tempfile.gettempdir(); print(foo)'; /home/pichuan; ```. @crazysummerW , I wonder if it's possible that you don't have write access to your /tmp? If so, can you try setting `TMPDIR` to a directory that you have write access to?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1833091479:128,access,access,128,,https://github.com/google/deepvariant/issues/725#issuecomment-1833091479,3,['access'],['access']
Security,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team.; If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests!. Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress.; If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102:331,access,access,331,,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102,3,['access'],"['access', 'accessible']"
Security,"Hi @fellen31 ,. Thanks for the suggestion. For the official Docker file, we usually don't like to modify it after we push it. This is mostly for reproducibility concern. Even when we don't change everything else on our side, rebuilding at a later time can cause confusion if some other components (not in our control) has changed, and then might result in unexpected reproducibility differences. A few possible solutions:; 1. We'll make sure to update the version number in future releases, and we'll be careful even with a PATCH version update. (But we'll do nothing right now); 2. Or, if it's absolutely necessary for you to access a Docker under our Docker Hub and/or gcr.io, I can build one for you, but it will be called something like `1.6.1-fix-version` (instead of overwriting the 1.6.1 images). To do 2, it will take some time and effort on my side, but it's doable if it's important for you. Let me know!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2168729043:627,access,access,627,,https://github.com/google/deepvariant/issues/830#issuecomment-2168729043,1,['access'],['access']
Security,"Hi @genieusbio,. This first run is just a validation. So try it like in the following script with `--regions chr1:783006-783007` and `--num_shards 1` as well, as shown below. You might need to change your input/output/reference folders reflecting what you have on your system:. ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0""; MAKE_EXAMPLE_ARGS=""variant_caller=vcf_candidate_importer,proposed_variants=/input/force_calling.candidates.vcf.gz,vsc_min_count_snps=0,vsc_min_count_indels=0,vsc_min_fraction_snps=0,vsc_min_fraction_indels=0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions chr1:783006-783007 \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards 1 \; --make_examples_extra_args=""${MAKE_EXAMPLE_ARGS}"" \; --intermediate_results_dir /output/intermediate_results_dir; ```. Let me know if there is anything I should expand for clarification. You might have to run it a couple of times to make it all go, but it should run fairly fast as the region is very small. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1719783676:42,validat,validation,42,,https://github.com/google/deepvariant/issues/708#issuecomment-1719783676,1,['validat'],['validation']
Security,"Hi @jianqi-chen,. In order to give you a more concrete answer it would be very helpful if you could provide couple of examples from your validation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1699536118:137,validat,validation,137,,https://github.com/google/deepvariant/issues/702#issuecomment-1699536118,1,['validat'],['validation']
Security,"Hi @karoliinas ,; From your log, it seems like the DeepVariant model has made a prediction with unexpected numerical value. From your log, I'm unable to tell why this has occurred. In this command:. `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""`. If you can share the `sample1.intermediate/call_variants_output.tfrecord.gz` (and optionally`sample1.intermediate/gvcf.tfrecord@19.gz` files) with me, I can able to look into the records and see which example has this issue. (Or, if you can narrow this down to a small BAM file, and if you can share that BAM file, that works too). Please email to pichuan@google.com if you can share. If you can't share the files, we can think about what we can do here to help identify which example caused the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2221028135:222,secur,security-opt,222,,https://github.com/google/deepvariant/issues/849#issuecomment-2221028135,1,['secur'],['security-opt']
Security,"Hi @lykstudio,. Ultima has a separate fork of deepvariant.; It's not completely public yet.; Please contact me at doron.shemtov@ultimagen.com for further details for early access. See usage instructions below. Best,; Doron. # Generating unfiltered germline callset. ## Requirements. The workflow below assumes that you have a sorted, duplicate marked UG BAM/CRAM file. ; We also assume that you built the UG DeepVariant docker (>=1.4.12) per separate instructions file.; Official gatk and picard installations are required. . ### Files required for the analysis (download locally); The following files are publicly available:. gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict; gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list. ### UG-specific files; DeepVariant model files ; * WGS calling (model 1.2); ```; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.data-00000-of-00001; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.index; gs://concordanz/deepvariant/model/germline/v1.2_rc2/model.ckpt-710000.meta; ```. DeepVariant-ug docker container:; ```; gcr.io/ganymede-331016/deepvariant:ug-1.4.13; ```. ## Generating whole genome callset . Instructions below require ability to run whole genome DeepVariant. DeepVariant is split into 3 stages: make_examples, call_variants, and postprocess_variants. We normally run make_examples on a genome split into 200 equal intervals in parallel through cromwell engine, but any parallelization engine will do. Each make_examples process creates examples.tfrecords.gz file which contains a single image and variant metadata for each candidate. The set of tfrecords file is read by call_variants as a single tensorflow dataset. call_variants runs optimally on a machine which contains a single g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580:172,access,access,172,,https://github.com/google/deepvariant/issues/711#issuecomment-1734891580,1,['access'],['access']
Security,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371#issuecomment-716733704:24,Access,Accessing,24,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704,1,['Access'],['Accessing']
Security,"Hi @pichuan ,. Thanks for the quick response! Regarding your questions:. 1. I have completed training and run some test calls, though this was just to make sure the models were vaguely sensible, I ran hap.py but didn't spend too much time evaluating the results because I am not yet finished optimising the training. But I take your point that real-world metrics will be more useful than the internal stats. . 2. As we are not working in a model organism, our truth set is unlikely to be of the same quality of, say, humans. Though my hope is that it is good enough. We defined confident regions and truth variants via some relatively strict alignment quality filters plus mendelian segregation patterns (the training data are from 5 trios). But no further validation, so I certainly think it is possible that there are real variants in the confident regions that are not in the truth VCF. I can see how this would lower hom_ref recall so I will explore this and see how many might be there. . I'll post back here once I have explored these points further. . Thanks!. Dan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904#issuecomment-2458942715:757,validat,validation,757,,https://github.com/google/deepvariant/issues/904#issuecomment-2458942715,1,['validat'],['validation']
Security,"Hi @prasundutta87 , given that `--haploid_contigs` and `--par_regions_bed` are arguments for postprocess_variants, you can try using:. `--postprocess_variants_extra_args=""--haploid_contigs=yourValue,--par_regions_bed=yourValue"""". If you're using Docker, you likely need to make sure the par_regions_bed path are something that the run in Docker can access.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2096843856:349,access,access,349,,https://github.com/google/deepvariant/issues/816#issuecomment-2096843856,1,['access'],['access']
Security,"Hi @splaisan ; Your question is using docker, which is a bit different from the discussion earlier, I believe. To use --intermediate_results_dir, it indicates you probably want to access the content there later. So, I'd recommend that you write it to an output file that you mounted with `-v`. For example, given that you have `-v ""$PWD/tmp_dir"":""/tmp_dir""`, maybe try seeting:; ` --intermediate_results_dir /tmp_dir`, which should write output to `$PWD/tmp_dir` once you're done?; (I noticed you wrote `--intermediate_results_dir /temp_dir`, which was not actually mounted. Not sure if that's a typo or not.). Hope this helps. I'm going to close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1091034842:180,access,access,180,,https://github.com/google/deepvariant/issues/524#issuecomment-1091034842,1,['access'],['access']
Security,"Hi Aiken,. I am assuming this is from a germline diploid sample, which is what the variant caller is designed for. Could you give a little background on your experiment, just to be sure I'm not missing anything in my assumptions below. [Based on the paper](https://academic.oup.com/bioinformaticsadvances/article/3/1/vbad062/7197031), the training was performed on RNA-seq samples that were not single cell. In theory it should work, though the 10x would be downsampled to 95 reads because of how the input to the model operates. Then first 5 row are used for representing the reference sequence, bringing the pileup image to a 100 rows. Try it with the RNA-seq model from [the case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md), given the above, though lowering the number of reads might help. I would be curious on how it validates with your data. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/705#issuecomment-1708658232:872,validat,validates,872,,https://github.com/google/deepvariant/issues/705#issuecomment-1708658232,1,['validat'],['validates']
Security,"Hi Aman,. Thank you for reaching out to us. As you've read in our [Cohort Best Practices](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration), generating a cohort variant callset using DeepVariant includes two separate steps:. 1. Running DeepVariant on all sample reads to generate gVCF files, one per each sample.; 2. Running GLnexus on the gVCFs using the provided `DeepVariantWGS` or `DeepVariantWES`. For the first step, the best way to parallelize computation would be using cloud compute resources if you have access to them. You can find instructions on how to run DeepVariant on Google Cloud Platform [here](https://cloud.google.com/life-sciences/docs/tutorials/deepvariant), or you can use any other cloud service providers using our DeepVariant docker [images](https://github.com/google/deepvariant#how-to-run-deepvariant), to run DeepVariant on each sample (or a batch of samples) separately in multiple machines. I would not recommend parallelizing DeepVariant runs over samples in a single machine though, since a single run of DeepVariant already uses multiple cores in the `make_examples` step - the number of cores to use is controlled by the `--num_shards` parameter. Once you've generated all ~200 gVCFs, one for each sample, the second step should be pretty simple. A single machine with relatively good CPU/RAM capacity should be able to handle merging 200 gVCFs using GLnexus. I hope this helps. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676:631,access,access,631,,https://github.com/google/deepvariant/issues/526#issuecomment-1067196676,2,['access'],['access']
Security,"Hi Amy. Yes, DeepVariant is free to use. A good place to start is to see if you can run the quick start: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md; You might also find this video to be an accessible introduction to DeepVariant: https://www.youtube.com/watch?v=pZEMSBmoyi0; You can ask your questions here, though we ask that you try to read the documentation first and try to see if that answers your questions. And if you have any questions you can't ask publicly, then let me know and I can email you and try to help. Otherwise it's great to have questions and answers documented in the issues so if anyone else has the same question, they can find it by searching the issues. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/502#issuecomment-1007774645:226,access,accessible,226,,https://github.com/google/deepvariant/issues/502#issuecomment-1007774645,1,['access'],['accessible']
Security,"Hi Attila,. So your account would need `sudo` privileges, which you probably already figured out. The `run-prereq.sh` itself [contains calls](https://github.com/google/deepvariant/blob/r0.4/run-prereq.sh) using `sudo`, which is why you are getting the `not permitted` OS error. If you curious to see the trace of the script to determine where the issue might be arising from, just run it like this and that will provide you with an exact control-flow:. `sudo bash -x ./run-prereq.sh`. So these suggestions are just for curiosity purposes, and Pi-Chuan's recommendation is the correct one since it has been validated on Ubuntu 16.04. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-360945398:606,validat,validated,606,,https://github.com/google/deepvariant/issues/41#issuecomment-360945398,1,['validat'],['validated']
Security,"Hi Both, thank you so much for your replies! I really appreciate it. I adjusted the settings with your additions Paul but got the same result as I put above. Definitely the same bam and vcf sample, I just double checked. Hopefully just a realignment thing like Pi-Chuan mentioned. We may end up getting them validated just to make sure we're not throwing any potentials away! Hard choice!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1662847110:308,validat,validated,308,,https://github.com/google/deepvariant/issues/691#issuecomment-1662847110,1,['validat'],['validated']
Security,"Hi Charles,. Usually `mount` (`-v`) require specialized access that the administrator can provide. ; Maybe you can show them our tests that worked for you previously. In any case, here are a few more tests:. $`1)`$ The first is to use the `mount` command explicitly:. ```; docker run \; --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input; ```. $`2)`$ This is using volumes, which is a different approach:. ```; docker volume create --name dv-vol. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. touch input-path-cont/file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run \; --mount source=dv-vol,target=""/input"" \; google/deepvariant:""1.5.0"" ls -l /input. docker rmi --force hello-world:latest. docker volume rm dv-vol; ```. If the volume removal gives you an error like this:. ```; Error response from daemon: remove dv-vol: volume is in use - [ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2]; ```; Just perform `docker container rm` on each individual of the listed container ids in the brackets, like this (before trying `docker volume rm dv-vol` again):. ```; docker container rm ba544db0a11b27dfdc6eb578c65e4a6eb09c31854a039bce010b37e2bf40f3f2; ```. Let me know the results of both steps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604:56,access,access,56,,https://github.com/google/deepvariant/issues/184#issuecomment-1701584604,1,['access'],['access']
Security,"Hi Gunjan,; and thanks a lot for your reply. Yes, the directories where all accessible and under ```root``` (I know it can be done better....).; Indeed when I added to the docker command the option ``` --user root``` the error changed and was very clear: ``` disk full```.; It remains to me to free disk or change docker location, add a docker group to allow users run it without ```sudo``` and rerun it. I am pretty confident it should work. With Regards,; -A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325#issuecomment-659193187:76,access,accessible,76,,https://github.com/google/deepvariant/issues/325#issuecomment-659193187,2,['access'],['accessible']
Security,"Hi Ji,. I understand better what you're trying to do -- glad for Maria's questions and Andrew's comments. I'm not sure DeepVariant might be optimal for that, as it was designed for germline variant-calling, and these errors are usually cleaned/corrected by other means before they arrive in DeepVariant. For STR analysis maybe [HipSTR for Illumina](https://github.com/HipSTR-Tool/HipSTR) or [DeepRepeat for Nanopore](https://github.com/WGLab/DeepRepeat) might help as a first pass, but I'm still trying to think a bit more here. . Regarding DeepVariant, I need think a bit longer about how it might be used as an inference tool given how it is designed. You can still experiment with what I mentioned to see how it reacts, though it might not be something I would put a lot of confidence until you validate it by other means. . As you mentioned, keep in mind you need to compare with a truth set you might have to generate in your lab via something like Sanger sequencing with a high-fidelity PCR enzyme. Sorry if this lead you too far astray, but I wanted to post before you spent too much time. Let us know in case there are any questions you might have of why and how DeepVariant operates. Thanks,; Paul. #### References. [1] [Genome-wide profiling of heritable and de novo STR variations (HipSTR paper)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482724/); [2] [DeepRepeat: direct quantification of short tandem repeats on signal data from nanopore sequencing](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02670-6)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1683256496:798,validat,validate,798,,https://github.com/google/deepvariant/issues/697#issuecomment-1683256496,1,['validat'],['validate']
Security,"Hi Maria,. Thanks for your kind help and time. Your linked report looks OK and shows minus symbols in chrome or safari (OSX 13.3.1 / M2) while mine are bad and were obtained on two different ubuntu22 servers with the same docker image. By contrast, the same report looks OK on the ubuntu server itself (open via VNC and using Mozilla), which points in the direction of a ubuntu 22 font specificity in my servers as you suggested. I attach here one such report for your review, the data is not confidential and comes from a SRA genome. [WT_REP1.visual_report.html.zip](https://github.com/google/deepvariant/files/11404181/WT_REP1.visual_report.html.zip). best regards",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/643#issuecomment-1535859706:493,confidential,confidential,493,,https://github.com/google/deepvariant/issues/643#issuecomment-1535859706,1,['confidential'],['confidential']
Security,"Hi Mark,. 1. Shuffling is done so that each training batch is more representative of the entire data set. It helps the training to converge faster and avoid overfitting. If you shuffle the way you described it won't help. Although you may train without shuffling, it just won't work as well. Another problem is that without shuffling step you don't have all of your training data in one sharded file (see answer (3)); 2. Shuffling is needed for tune and train data, and not needed for validation data.; 3. input_pattern_list of shuffle script takes a list of files. This way you shuffle all of your samples into one sharded file. ; 4. Will comment on this in another post.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-637264990:485,validat,validation,485,,https://github.com/google/deepvariant/issues/312#issuecomment-637264990,1,['validat'],['validation']
Security,"Hi Masaru,; It looks like you're using DirectRunner, which is fine for smaller datasets, but when we have larger datasets we instead use DataflowRunner (see where we shuffle training set [here](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each)). In the Case Study, the DirectRunner is used for the small validation set and DataflowRunner is used for the large training set. The fact that it works when you split up the data into smaller pieces suggests this may be the issue. Please try running with DataflowRunner and let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133#issuecomment-450955687:424,validat,validation,424,,https://github.com/google/deepvariant/issues/133#issuecomment-450955687,1,['validat'],['validation']
Security,"Hi Nils,. This is possibly because most of the decoy contigs are excluded by default through the following file (because of possible large incorrect mappings):. https://github.com/google/deepvariant/blob/r1.5/deepvariant/exclude_contigs.py. More of this is discussed in [issue 37](https://github.com/google/deepvariant/issues/37). In this situation, besides trying with GRCh38 -- which could be a good check -- some of your reads probably have better alignment to the decoy contigs with suboptimal alignment to chromosome Y (or vice-versa). One thing you could try is to indirectly determine the sex of the sample via a threshold that compares differences between allele depth (AD) and depth of coverage (DP) across variants in the sample. If that is not enough (in case they are equal), then a comparison between GQ and QUAL might provide better granularity. The idea is that suboptimal vs optimal read alignments for chrY might work as an inference of sex. For this to work, you would need to create a test and validation set of samples where the sex is known to extract what the threshold would be. I'm assuming these are not in the pseudo-autosomal (PAR) regions, as both chrX and chrY are identical in the PAR regions of the genome assembly. As a last resort you can rename the decoys in your BAM and reference with something different than the ones in the excluded file, so that they would be included in the VCF. What's interesting is that you see the decoy-aligned reads on chrY. As I think a bit about the realignment and how things are excluded, can you confirm that the reads that were aligned to the decoy contigs actually realign to chrY? Basically does your DP increase for regions of chrY more than the number of reads you expect there, and would account for reads from the decoy contigs? I'm only asking based on how I see the code processing the regions. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/695#issuecomment-1677027250:1013,validat,validation,1013,,https://github.com/google/deepvariant/issues/695#issuecomment-1677027250,1,['validat'],['validation']
Security,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:228,access,access,228,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973,2,['access'],['access']
Security,"Hi Paul,; there is actually some explanation in the file (below the Copyright lines):; https://github.com/google/deepvariant/blob/r0.7/cloudbuild.yaml#L28-L34. We use these yaml files to build and push docker images. You probably could re-use them to build and push DeepVariant docker images to projects where you have write access to. We didn't document these more, because we didn't think it's a very common use case.; If it's confusing to have these files on the top level, I can check with my colleagues and see if we can move them to a sub-directory in the next release. Would that help?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413716778:325,access,access,325,,https://github.com/google/deepvariant/issues/87#issuecomment-413716778,1,['access'],['access']
Security,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191#issuecomment-504481029:1068,access,accessible,1068,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029,2,['access'],['accessible']
Security,"Hi Sophie,. Ambitious endeavor, but some things to be aware of. So a low VAF would probably confuse the model training, as that is usually a homozygous reference call. With a low VAF, the other alleles would probably be automatically be selected as supporting information for training in the pileup to confirm a call, as they would be in the majority. Thus when you run your trained model in the future against new BAM files, it would not look so much for the rare variants, but rather select for the more prevalent ones in your data to call a genotype (GT) and subsequently a GQ score of supporting it. . Regarding training and validation, they always usually must be different. For example, in the original DeepVariant paper under the [Supplementary Text and Figures](https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.4235/MediaObjects/41587_2018_BFnbt4235_MOESM81_ESM.pdf), they trained on chromosomes 1-19, and validated on 20-22:. ![image](https://github.com/google/deepvariant/assets/6555937/83cdb7f3-44e9-4185-99cf-81362489dfcf). If you want to train chromosome-by-chromosome, it is possible to take a previously trained model and continue training it as noted in the [tutorial specified by `GCS_PRETRAINED_WGS_MODEL`](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md#start-model_train-and-model_eval), but that could create batch effects (meaning it will shift the model one way with one chromosome, and then back with another even under shuffling conditions). So it might be better to start with a complete set of chromosomes you select for training. Not trying to discourage you, so give it a try and let's see what happens -- in any case it will be fun learning experience. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936:629,validat,validation,629,,https://github.com/google/deepvariant/issues/698#issuecomment-1681102936,4,['validat'],"['validated', 'validation']"
Security,"Hi Sophie,. Similarly to my reply in https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:930,validat,validation,930,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['validat'],['validation']
Security,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219:1114,validat,validation,1114,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219,2,['validat'],['validation']
Security,"Hi Zhuyi,. DeepVariant is complaining here because your BAM has a record that says it has a mapped mate but mtid, which is the htslib variable holding the offset into the chromosome array, isn't set so its actual mapped location isn't present. This would normally indicate that's something corrupted with your BAM. Where did you get? How was it aligned? I'd recommend running ValidateSamFile to see if it complains. It could be we've being overly strict in Nucleus for parsing our BAMs. It'd be great if you can determine if your BAM is considered valid or not, and let us know if we need to be more permissive in our parsing or if the BAM needs to be fixed up. All the best,. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-426810736:376,Validat,ValidateSamFile,376,,https://github.com/google/deepvariant/issues/99#issuecomment-426810736,1,['Validat'],['ValidateSamFile']
Security,Hi!. I'm not sure I've seen this error before. It seems that the file is accessible given that it appears to be failing on parsing of the `DeepVariantDatasetConfig` proto. . Is is possible for you to share 1 of the files in the mounted bucket? That would allow us to try this on our end. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837#issuecomment-2187745312:73,access,accessible,73,,https://github.com/google/deepvariant/issues/837#issuecomment-2187745312,1,['access'],['accessible']
Security,"Hi, so I noticed in the Deepvariant manuscript supplementary text that the 44x NA12878 CLR data (sorted_final_merged.bam) from GIAB was used for benchmarking Deepvariant with PacBio CLR reads (specifically, that chroms 1-19 were used for training and testing was performed on chroms 20-22). Would it be possible for me to access the result VCF for chroms 20-22 (and ideally also the trained model used in the manuscript)? The reason I'm interested in this is that we have developed our own CLR variant calling method and I would like to fairly compare it with Deepvariant. If I have the VCF, then I can be sure that our precision/recall calculations on chr20-22 are done in exactly the same way, and if I have the trained model then I also have the option to run Deepvariant myself on other CLR data. Unfortunately, the training procedure for Deepvariant seems to be complicated at this time and requires significant compute resources. I would be very grateful if it would be possible to meet this request -- you can reach me by email at pedge AT eng.ucsd.edu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/174#issuecomment-487190949:322,access,access,322,,https://github.com/google/deepvariant/issues/174#issuecomment-487190949,1,['access'],['access']
Security,"Hi, thank you for checking in again. Work has been really busy so I actually haven't looked at it yet. I will try to find 30min-1hr today to take a look. But just checking -- you're saying it worked for you on another CentOS7?. Is it possible for you to check (on both machines) what's the different between the protobuff version installed?; (I'm also not sure how Singularity version access which version of protobuf, but it'll be useful to check whether that is different on the two machines)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475:385,access,access,385,,https://github.com/google/deepvariant/issues/499#issuecomment-1012497475,1,['access'],['access']
Security,"Hi, thanks for looking into it. Any news? Not sure how easy this will be reproduce and/or how Deepvariant determines whether python or c++ are used. From the code, I can only see protobuff being installed via pip - but that is probably not the whole story. . In the meantime I had access to another Centos7 cluster and the docker image works fine with the included Singularity (1.2 & 1.3). So it seems this may have something to do with the host system and pre-installed packages maybe? But I have honestly no way of even guessing what the problem could be then specifically. . /M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010:281,access,access,281,,https://github.com/google/deepvariant/issues/499#issuecomment-1012212010,1,['access'],['access']
Security,"Hi,. Thanks for brining up this issue. It'll be a bit tricky to debug this without having access to the files. Would it be possible to share the input files so we can try to reproduce this? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794#issuecomment-2019143173:90,access,access,90,,https://github.com/google/deepvariant/issues/794#issuecomment-2019143173,1,['access'],['access']
Security,"Hi,. Thanks for the reply, but it doesn't address my question.; My doubt is about how to generate the bam file for doing the variant calling. For the checking/validation approach, I would like to ask a few questions. > Ok, then this would probably be my approach:; > ; > 1. Call variants individually on all of the samples indepedently (not using a trio caller, use DeepVariant or something) and create a combined gVCF using glnexus. Set parameters like minimum depth of 15 and GQ 20. Then find blocks that you can use as ""confident regions"". I did medium coverage sequencing based on some literature, but since my species might have high genetic diversity I used depth higher than recommended (1x), about ~10X per individual.; https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079667; https://onlinelibrary.wiley.com/doi/10.1111/mec.12105; https://pubmed.ncbi.nlm.nih.gov/34250668/. Then, I can't use the minimum depth you suggest of 15; my data should be good quality, hence GQ of 20 is fine, but depth of 15 is not achievable. > 2. Pick a few samples and apply the confident regions on them and see if you get mostly variants with GQ>=20 with them. At this point you may need to make sure they are not falling within an SV for some samples. This gives you truth VCFs.; I am new in Bioinformatics, could you suggest a program to use to find those ""confident blocks"". I can't think of any program for doing it.; I have short-reads not long-reads, hence finding SV is not very reliable. Any suggestion on how to check if a variant falls within a SV?. > 3. Assess the F1-score of current DeepVariant using your truth vcf and bed and see how it looks.; > ; > Finally train a model and see if the F1-score improves. I am sure there are better ways to do this, but, this would be the simplest and least blocking path for this experiment. As mentioned before, I do not have trios data. @pichuan , Is it possible to train a model without trios data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771:159,validat,validation,159,,https://github.com/google/deepvariant/issues/878#issuecomment-2364502771,2,['validat'],['validation']
Security,"Hi,; currently the instructions are written for Ubuntu 16. I've not tried it on CentOS before.; If you have access to a Ubuntu 16 machine, can you give it a try and see if you're still seen the same issue? I can also try running on CentOS and report back, but that might take a while for me to do.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-350159832:108,access,access,108,,https://github.com/google/deepvariant/issues/6#issuecomment-350159832,1,['access'],['access']
Security,"I attached two files. One is shuffled validation set, the other one is without shuffled. These data sets are from google quick-start test dataset. Region is chr20:10,005,000-10,008,000. ; [validation_set_with_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082816/validation_set_with_shuffled.tar.gz); [validation_set_without_shuffled.tar.gz](https://github.com/google/deepvariant/files/3082817/validation_set_without_shuffled.tar.gz); @akolesnikov Thanks very much.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172#issuecomment-483490813:38,validat,validation,38,,https://github.com/google/deepvariant/issues/172#issuecomment-483490813,1,['validat'],['validation']
Security,"I do have access to a linux distributed computing system so I will re-try there and see how it goes. Thanks for letting me know, I will update should I still encounter the same error on the different OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-620715339:10,access,access,10,,https://github.com/google/deepvariant/issues/304#issuecomment-620715339,1,['access'],['access']
Security,"I got it. Looking at the source code:. <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega@5""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-lite@3.4.0""></script>; <script type=""text/javascript"" src=""https://storage.googleapis.com/deepvariant/lib/vega/vega-embed@4""></script>. unfortunately the ""https://storage.googleapis.com"" is blocked here for ""security reasons"" :( . I open in my mobile using external network and I can see the complete output. Thanks for the feedback.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-698670947:447,secur,security,447,,https://github.com/google/deepvariant/issues/290#issuecomment-698670947,2,['secur'],['security']
Security,I have published an image for the latest state of PR at https://hub.docker.com/r/dkurtaev/deepvariant. May I ask to validate it?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-734258287:116,validat,validate,116,,https://github.com/google/deepvariant/pull/363#issuecomment-734258287,1,['validat'],['validate']
Security,"I just checked the code, and you're right that the temp file names will be the same:; https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266. For now, please pass in different `intermediate_results_dir` for each run. For example:; `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175#issuecomment-560625427:546,expose,exposed,546,,https://github.com/google/deepvariant/issues/175#issuecomment-560625427,2,['expose'],['exposed']
Security,"I recently experienced a similar issue in which I kept getting the error `ValueError: NOT_FOUND: could not load fasta and/or fai for fasta` when I'd try to run DeepVariant via Docker. The command I was using looked like:; ```; docker run \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/data/bioinfo.shared/index/hg38_gatk/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```. Not having used Docker before, I wasn't aware that I needed to mount all directories I needed to access so that Docker could see them. This issue and [this article](https://towardsdatascience.com/how-to-mount-a-directory-inside-a-docker-container-4cee379c298b) helped me figure out where I went wrong (i.e. using the absolute path in the --ref parameter without having mounted it). The adjusted command (that worked) looked like:; ```; docker run \; -v ""/data/bioinfo.shared/index/hg38_gatk/"":""/index"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/bam"":""/input"" \; -v ""/data/analysis_data/ngs/r64558e_20231017_193725/result_DeepVariant"":""/output"" \; google/deepvariant:""1.6.0"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/index/hg38.fa \; --reads=/input/m64558e_231018_115114.hifi_reads.sorted.add-rg.rm-tags.bam \; --output_vcf=/output/FD800429.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251:840,access,access,840,,https://github.com/google/deepvariant/issues/653#issuecomment-1793241251,1,['access'],['access']
Security,I see that you don't bind a /scicore directory in docker command. Your local directories are not accessible from a docker. Please take a look at the FAQ [section](https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open) that explains docker binding.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515#issuecomment-1034110256:97,access,accessible,97,,https://github.com/google/deepvariant/issues/515#issuecomment-1034110256,1,['access'],['accessible']
Security,"I updated my versions of apache-beam and numpy to match yours and it seems like that worked. I was able to run the shuffle script on both my training file and the validation file. Thank you!. To clarify your other response above, if I'm using one chromosome .bam each for my training and validation sets, then I should run make_examples once for each of those and shuffle on each of those single output files? . Thanks so much for your help, I really appreciate it!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2011022724:163,validat,validation,163,,https://github.com/google/deepvariant/issues/793#issuecomment-2011022724,2,['validat'],['validation']
Security,I upload it to Google Drive: https://drive.google.com/file/d/1aZnuOlCpcDhudfJa28XhvUggOLHuJAyv/view?usp=sharing. Please let me know if you cannot access it. Thanks Andrew!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/109#issuecomment-2108524366:146,access,access,146,,https://github.com/google/deepvariant/issues/109#issuecomment-2108524366,1,['access'],['access']
Security,"I'll look into these options some more start of next week. My initial thought was the path to the model is inside the container (`/opt/models/pacbio/model.ckpt`), and I cannot write to a singularity container without sudo access. I'll try some different binds to see if I can get that to write to the current directory, as I definitely have write access there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761608040:222,access,access,222,,https://github.com/google/deepvariant/issues/404#issuecomment-761608040,2,['access'],['access']
Security,"I've got it working now. What I reported earlier was correct and I'm happy to explore this further. Oddly enough, I now have a bed file owned by root as well. I do not have root access on this machine which explains why some HPC's have banned docker. I'll investigate the differences and report back. Good new is I have it working so there is no action need on your end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1207586205:178,access,access,178,,https://github.com/google/deepvariant/issues/550#issuecomment-1207586205,1,['access'],['access']
Security,"INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. which also seems to work. This command below shows my TensorFlow version:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__version__)'; INFO: Using cached SIF image; 2022-02-10 23:13:05.337920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; 2.5.0; ```. To confirm the path:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf; print(tf.__file__)'; INFO: Using cached SIF image; 2022-02-10 23:12:22.632481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; /usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py; ```. I have to say I don't really know how Singularity works, but here are a few commands I tried:. ```; pichuan@pichuan-gpu:~$ singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" ls /usr/local/lib/python3.8/dist-packages/tensorflow; INFO: Using cached SIF image; __init__.py __pycache__ _api compiler core include keras libtensorflow_framework.so.2 lite python tools xla_aot_runtime_src; ```; ```; pichuan@pichuan-gpu:~$ ls /usr/local/lib/python3.8/dist-packages/tensorflow; ls: cannot access '/usr/local/lib/python3.8/dist-packages/tensorflow': No such file or directory; ```. Not really sure how helpful this is. @Phillip-a-richmond if you spot any differences, let me know how I can change to reproduce the error.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:3679,access,access,3679,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725,1,['access'],['access']
Security,"Interestingly, mine is a python 2.7.15 machine. Below the list of the packages installed by anaconda in the environment:; ```. Package plan for package removal in environment .../Andrea/myanaconda/deepvariant:. The following packages will be REMOVED:. _libgcc_mutex: 0.1-main conda-forge; _tflow_select: 2.1.0-gpu ; absl-py: 0.8.1-py27_0 conda-forge; astor: 0.7.1-py_0 conda-forge; backports: 1.0-py_2 conda-forge; backports.weakref: 1.0.post1-py27_1000 conda-forge; boost: 1.70.0-py27h9de70de_1 conda-forge; boost-cpp: 1.70.0-h8e57a91_2 conda-forge; bzip2: 1.0.8-h516909a_2 conda-forge; c-ares: 1.15.0-h516909a_1001 conda-forge; ca-certificates: 2019.11.28-hecc5488_0 conda-forge; certifi: 2019.11.28-py27_0 conda-forge; cffi: 1.13.2-py27h8022711_0 conda-forge; chardet: 3.0.4-py27_1003 conda-forge; cliff: 2.15.0-py27_0 conda-forge; cmd2: 0.8.6-py27_0 conda-forge; contextlib2: 0.6.0-py_0 conda-forge; crcmod: 1.7-py27_1002 conda-forge; cryptography: 2.8-py27h72c5cf5_1 conda-forge; cudatoolkit: 9.2-0 ; cudnn: 7.6.4-cuda9.2_0 ; cupti: 9.2.148-0 ; curl: 7.65.3-hf8cf82a_0 conda-forge; enum34: 1.1.6-py27_1002 conda-forge; funcsigs: 1.0.2-py_3 conda-forge; futures: 3.3.0-py27_0 conda-forge; gast: 0.3.2-py_0 conda-forge; google-cloud-sdk: 166.0.0-py27_0 bioconda ; grpcio: 1.23.0-py27he9ae1f9_0 conda-forge; h5py: 2.10.0-nompi_py27h513d04c_101 conda-forge; hdf5: 1.10.5-nompi_h3c11f04_1104 conda-forge; htslib: 1.9-h244ad75_9 bioconda ; httplib2: 0.14.0-py27_0 conda-forge; icu: 64.2-he1b5a44_1 conda-forge; idna: 2.8-py27_1000 conda-forge; intervaltree: 3.0.2-py_0 conda-forge; ipaddress: 1.0.23-py_0 conda-forge; keras-applications: 1.0.8-py_1 conda-forge; keras-preprocessing: 1.1.0-py_0 conda-forge; krb5: 1.16.4-h2fd8d38_0 conda-forge; libblas: 3.8.0-14_openblas conda-forge; libcblas: 3.8.0-14_openblas conda-forge; libcurl: 7.65.3-hda55be3_0 conda-forge; libdeflate: 1.3-h516909a_0 conda-forge; libedit: 3.1.20170329-hf8c457e_1001 conda-forge; libffi: 3.2.1-he1b5a44_1006 conda-forge; libgcc-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566427577:633,certificate,certificates,633,,https://github.com/google/deepvariant/issues/252#issuecomment-566427577,1,['certificate'],['certificates']
Security,Is there any update regarding this issue? I am also trying to build a singularity container for deepvariant 0.7.2 and I am producing the same error. Will this be fixed on later versions?. I assume a lot of teams don't have a root access when using HPC. It will be awesome if you add support for building singularity containers in your documentations as this will things a lot easier when using deepvariant on HPC either for calling or training.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-457977330:230,access,access,230,,https://github.com/google/deepvariant/issues/132#issuecomment-457977330,1,['access'],['access']
Security,"It is not a requirement to use Google Cloud or its SDK. You should be able to still use DeepVariant without having to install anything related to Google Cloud.; One issue here is that we put our data (including pre-built binaries) on Google Cloud storage. So you might not have access to them. You can try the browser version to see if you can view or download the data: https://console.cloud.google.com/storage/browser/deepvariant. But even if that doesn't work, you should still be able to build the binary from scratch:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. The example input data (such as FASTA, BAM files) can be found on their original sites. For example, in the Case Study we listed where we got the files: https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-case-study.md#test-data",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/17#issuecomment-352085708:278,access,access,278,,https://github.com/google/deepvariant/issues/17#issuecomment-352085708,1,['access'],['access']
Security,Just to add to the previous post. Your file system is not accessible from the container. Using -B option you may map (mount) some directories from your file system to be accessible from within the container. . There is a similar issue with the similar content. https://github.com/google/deepvariant/issues/506#issuecomment-1017088492,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1076926666:58,access,accessible,58,,https://github.com/google/deepvariant/issues/530#issuecomment-1076926666,2,['access'],['accessible']
Security,"K-Joint genotyping. If you look at the following paper:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8023681/pdf/btaa1081.pdf). Using parameter-optimization of GLNexus (such as minimum quality thresholds, among others listed under Supplementary Table 4), the authors were able to get a slightly different number of SNPs than via GATK-Joint:. ![image](https://github.com/google/deepvariant/assets/6555937/da3459b6-cb09-45b8-8fd9-9bbcdb0d12a7) . This is from Supplementary Figure 11 (A) found under Supplementary data, listed as [a link on this page](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So merging with GLNexus for DeepVariant gVCF output files $`-`$ as @AndrewCarroll mentioned in a [previous post](https://github.com/google/deepvariant/issues/83#issuecomment-553660314) $`-`$ by using [Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r1.5/docs/trio-merge-case-study.md), it was found to be more accurate than using those gVCFs with GATK GenotypeGVCFs. Regarding missing SNPs in individual samples, their genotype might get a no call (`./.`) as noted in [this line of the GLNexus code](https://github.com/dnanexus-rnd/GLnexus/blob/main/src/service.cc#L206):. ```; Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele; ```. Though it probably could also get called as homozygous reference, if all the QC pass. Regarding impact on downstream analysis, probably the best bet is to try both approaches (DV-GLN-OPT and GATK-Joint) in parallel, and then validate both results. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876:2060,validat,validate,2060,,https://github.com/google/deepvariant/issues/680#issuecomment-1640133876,1,['validat'],['validate']
Security,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-387445867:208,attack,attack,208,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867,2,"['attack', 'inject']","['attack', 'inject']"
Security,"Python 3.6 is meant to be ""unsupported"" by the end of this year ([pep 494](https://www.python.org/dev/peps/pep-0494/)), so would be decent future-proofing or ensuring security.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821002845:167,secur,security,167,,https://github.com/google/deepvariant/issues/441#issuecomment-821002845,1,['secur'],['security']
Security,"Since you're running this via docker this sounds like more of a docker / linux permission issue than a Deepvariant issue. I personally haven't done it but later versions of Docker do allow you to run without sudo access. Please take a look here https://askubuntu.com/questions/477551/how-can-i-use-docker-without-sudo . I would, however, additionally state that you want to make sure your account does full access to those folders / file as well. I'll let the Deepvariant team also add in any other suggestions as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/335#issuecomment-674131304:213,access,access,213,,https://github.com/google/deepvariant/issues/335#issuecomment-674131304,2,['access'],['access']
Security,"T and GQ values from a trained model. So DeepVariant first picks candidates by going through the regions of the genome you specify, and are also in your BAM file. Now given those candidates, it then picks data from your reads of roughly ~100 positions on each side of your variant (actually it is 199) to build an image to run through the prediction model. Information that is collected for building that image, is things such as the read base, base quality, if the read supports the variant, etc., all which you can read in [the following blog](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). For each of these pieces of information it can only fit 100 rows of it, as the model operates optimally with only a height of ~100, and was trained as such. So the model is only used to infer your GT with the supporting GQ, as the candidates are already selected before it even gets to the prediction portion of DeepVariant. . $`5)`$ Since you're trying to find the true candidates among noisy data, you have the option of running known truth candidates with different gradations of noise to determine what your parameters and thresholds for those parameters should be. Some of that could be done with some preprocessing of your BAM files or reads, if you know specific parameters that your data should exhibit. Regarding truth candidates, you can look at the list of samples that were used to train DeepVariant:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details-training-data.md. Given those samples you can simulate or vary the reads to determine the effect on the prediction of known variants, where you would also explore the parameter space for optimization. You can look at a sample training example to get an idea how they were used to train a model:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md. This becomes very complex, as you will need to do a lot of validation. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918:4869,validat,validation,4869,,https://github.com/google/deepvariant/issues/697#issuecomment-1681008918,1,['validat'],['validation']
Security,"Thank you - yes, I just tried in 0.7.1 and now am running into a new error. I have python and numpy installed and I am wondering how to make this accessible or if this is a Docker issue?; Thank you in advance,; Best,; ```; File ""/tmp/Bazel.runfiles_uDUZWS/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 37, in <module>; import numpy as np; ImportError: No module named numpy; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-438484727:146,access,accessible,146,,https://github.com/google/deepvariant/issues/104#issuecomment-438484727,1,['access'],['accessible']
Security,"Thank you for the acknowledgement, but more importantly as scientists we require that the experiment be complete by reflecting equivalence in the results. Let's dig a little deeper:. 1. In the article you are right with AVX-512 would give you the ability to ""operate on more information at once"", so have you tried a test where you compiled DeepVariant with just `-mavx512*` without MKL? Let's look at the following article:. https://software.intel.com/en-us/articles/tensorflow-optimizations-on-modern-intel-architecture. The increased throughput (though significant) via vectorized functions is _*only one*_ aspect of the optimizations. I would suspect you picked MKL for multiple optimization reasons, one of which performs auto-queries for code path dispatches to save space on multiple binaries for users (among many other reasons):. https://software.intel.com/en-us/mkl-linux-developer-guide-instruction-set-specific-dispatching-on-intel-architectures. 2. Yes Mark's proposal is accurate with AVX, but try running with just AVX512 optimizations - which not everyone might have access to such CPUs - and _*without MKL*_ and I think you might surmise the results. To drive the point home, look at the code references in Tensoflow for AVX512 vs MKL:. * 143 for MKL => https://github.com/tensorflow/tensorflow/search?q=mkl&unscoped_q=mkl; * 19 for AVX512 => https://github.com/tensorflow/tensorflow/search?q=avx512&unscoped_q=avx512. Now having said that, what do you think could be done to make DeepVariant even faster besides AVX/MKL/CUDA/TPU optimizations?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-489377484:1083,access,access,1083,,https://github.com/google/deepvariant/issues/21#issuecomment-489377484,2,['access'],['access']
Security,"Thank you for the quick response. I tried that and still got files by root. I tried setting the uid and; groupid explicitly and using the dynamic method you recommended. Please advise. command:. deep_dir=deep_variant_id80429g20. mkdir -p XXXXXXXXXXXXXXXXXX/$deep_dir. docker pull google/deepvariant:0.9.0. # This was ran, XXXXXXX are directories marked out for security purposes. docker run -it -u 80429:20 -v; XXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; XXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXXXXXXXXXXX/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@*",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:361,secur,security,361,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158,1,['secur'],['security']
Security,"Thank you for the reply about TensorFlow. I'll double-check when I get home (I have limited IP access to my AWS Cloud account), but the call_variants file seemed much smaller than I would expect for a file format conversion at the next step. However, I think is probably that it is a binary file. However, the proportions of run time in that case study does match my own experience (the ""call_variants"" step ran the most quickly). I apologize that it will take me a little while to look into all of these things, but I do plan on comparing Google Cloud at some point (possibly for this particular issue). Total time / cost is important, but I am not currently certain what I would recommend to be as clear as possible to users. Using Docker made a huge difference for me for usability. However, at a later point, I am very grateful that you have all of the code open-source (so, if I wanted, I could use DeepVariant to better understand how to use TensorFlow in other applications). For example, even if they don't use COHCAP directly, you can use the [source code](https://github.com/cwarden45/COHCAP/tree/master/src) to see how the Boost libraries for [statistical distributions](https://www.boost.org/doc/libs/1_67_0/libs/math/doc/html/dist.html) (rather than parallel operation) can substantially decrease the run time (relative to the R-base functions). _[I apologize for being a bit off topic, but that is the best analogy that I can think of]_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479627680:95,access,access,95,,https://github.com/google/deepvariant/issues/167#issuecomment-479627680,2,['access'],['access']
Security,"Thank you very much, that finally resolved the issue. What happens now is that after performing the evaluation (the training, test, and validation sets are the sequencing of the NA12878 sample from the Coriell Institute sequenced three times), I’m getting low recall and precision values. For indels, recall is 0.41 and precision is 0.24, and for SNPs, recall is 0.57 and precision is 0.72. I tried the model you provided for exome sequencing, but I didn’t achieve better results (which is why I decided to create my own model). However, with typical tools (like GATK HaplotypeCaller), I get much better results (indels with 0.8 recall and 0.62 precision, and SNPs with 0.89 recall and 0.97 precision). Do you have any idea why this might be happening and any advice on how to solve it? I really believe that using a variant caller trained with my data should yield better results than GATK HaplotypeCaller",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827:136,validat,validation,136,,https://github.com/google/deepvariant/issues/869#issuecomment-2315052827,1,['validat'],['validation']
Security,"Thank you! I re-ran the training and validation sets with that flag, and re-shuffled them. Now, however, when I go to train the model (using the same parameters as the example case study--I just want to test out the process) I'm not getting any checkpoints in the output training directory, just the event log and the json file. What does this mean? Is the training step failing, or do I simply need to adjust my parameters? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877:37,validat,validation,37,,https://github.com/google/deepvariant/issues/797#issuecomment-2029425877,2,['validat'],['validation']
Security,"Thanks @samanvp - unfortunately, because of confidentially issues, we won't be able to use GCP . @pichuan - I managed to run the three scripts but am seeing a surprising result. We have sequenced a panel of control amplicons with spiked inputs containing SNP at position 41. I aligned the reads to the ref using bwa mem and filtered paired reads. Then I ran GATK Mutect2, Illumina Pisces, and Deepvariants to confirm the presence of SNP at position 41. Below is my result. . ![image](https://user-images.githubusercontent.com/24441131/65831442-65545a00-e287-11e9-8c55-8541a8ef3fce.png). I'm not sure what mistake I have made. Below is the set of commands I used to generate the vcf files from the 10% spiked-in sample. BIN_VERSION=""0.8.0""; docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode=calling \; --sample_name=RHA \; --examples=/tmpdir/make_examples.tfrecord.gz \; --ref=/home/RHA.fa \; --reads=/home/SNP-Cnt-10p_S7_L001_001.bam \; --gvcf=/tmpdir/gvcf.tfrecord.gztest.gv. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --checkpoint=/opt/models/wgs/model.ckpt \; --examples=/tmpdir/make_examples.tfrecord.gz \; --outfile=/tmpdir/call_variants_output.tfrecord.gz. docker run \; -v ""${HOME_DIR}"":""/home"" \; -v ""${TMP_DIR}"":""/tmpdir"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/postprocess_variants \; --ref=/home/RHA.fa \; --infile=/tmpdir/call_variants_output.tfrecord.gz \; --nonvariant_site_tfrecord_path=/tmpdir/gvcf.tfrecord.gztest.gv \; --outfile=/home/out.vcf \; --gvcf_outfile=/home/out.gvcf. Thanks for all your help; Azita",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-536285730:44,confidential,confidentially,44,,https://github.com/google/deepvariant/issues/222#issuecomment-536285730,1,['confidential'],['confidentially']
Security,"Thanks Lucas,. I'm not exactly sure where the WORKSPACE build you shared would get used, but it currently uses `htslib` version `1.13`, which is different to the conda provided htslib version accessible within the container, which gives this information. ```; Apptainer> samtools --version; samtools 1.10; Using htslib 1.18; Copyright (C) 2019 Genome Research Ltd.; ```. Maybe there is some strange interaction between out-of-sync versions? Regardless, I already converted cram->bam to finish variant calling, so feel free to close the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741#issuecomment-1851450942:192,access,accessible,192,,https://github.com/google/deepvariant/issues/741#issuecomment-1851450942,1,['access'],['accessible']
Security,"Thanks Nima. I ran it again. Looks like the error is because it is unable to open the bed file I have provided. However, the bed file exists on gcp and my v0.6.1 code was able to access it. I am not sure what I am doing wrong. I was able to run the same code successfully if I provide the bed file in the example documentation (gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed). . Error file: gbsc-gcp-project-udn-dev-deep-variant/UDN668131_deepVariant_test4/deepvariant_staging_folder/logs/make_examples/0:. W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:51.497793: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:51.498179 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:51.518172 140612532332288 make_examples.py:1075] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bai; 2018-11-09 19:48:52.291229: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I1109 19:48:52.291625 140612532332288 genomics_reader.py:213] Reading /input-gcsfused-48/UDN668131/UDN668131-P_HGF2YBCX2_deduped.bam with NativeSamReader; I1109 19:48:52.335163 140612532332288 make_examples.py:991] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; [E::hts_open_format] **Failed to open file gs://gbsc-gcp-project-udn-dev-test/VCRome_2_1_hg19_capture_targets_chrStripped.bed**; Traceback (most recent call last):; File ""/mnt/google/.google/tmp/Bazel.runfiles_qQ5ryq/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/di",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-437503051:179,access,access,179,,https://github.com/google/deepvariant/issues/118#issuecomment-437503051,1,['access'],['access']
Security,"Thanks for following up and for the detailed error log. This looks again like a connectivity issue. The deepvariant recipe needs to pull down the training files from a Google Bucket and is timing out trying to connect. Either there was a general internet issue or there is something blocking access to the bucket. If you re-try multiple times, does it fail in the same way? If so, you may need to investigate if the machine can access Google buckets for download. Hope this helps with tracking down the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549166457:292,access,access,292,,https://github.com/google/deepvariant/issues/228#issuecomment-549166457,2,['access'],['access']
Security,"Thanks for the help. So call variants only needs to be called once on all the shards at once, running on one GPU? Not individually once per shard? And under the hood it’s designed to recognize the tfrecord naming and numbering scheme as is described in the example wgs sh script? Given that the format is 000*-of-00064 do I have to specify this or does call variants assume that format and know how to find the 64 shards if they’re in the working directory? There’s a lot of regex going on I’d love to see documented (I guess maybe it’s in the codebase but if it could be explicitly written on the documentation that’d be awesome) — I understand that there is this cloud runner but considering these other components are exposed I feel like I don’t have a good sense of the way they should be appropriately called individually. . I’m hoping to set up a version of this in FireCloud using WDL as all of our workflows are currently set up in this way. It seems like I’m almost there — if I can’t figure it out then yes I will perhaps have to use the cloud runner. . > On Feb 6, 2019, at 11:57 PM, Nima Mousavi <notifications@github.com> wrote:; > ; > Can you verify TF examples (test.gvcf.tfrecord-*) are in ${BASE} path?; > ; > If you use DeepVariant's cloud runner, you won't need to do all these steps manually. It takes care of everything and runs the pipeline on GCP. See instruction here:; > ; > https://cloud.google.com/genomics/docs/tutorials/deepvariant; > ; > Is there any reason why you don't use cloud runner?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461398822:721,expose,exposed,721,,https://github.com/google/deepvariant/issues/151#issuecomment-461398822,1,['expose'],['exposed']
Security,"Thanks for your command @akolesnikov, I've made it running. Also, I wonder how to build it locally without GCP access, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428369416:111,access,access,111,,https://github.com/google/deepvariant/issues/99#issuecomment-428369416,1,['access'],['access']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F159) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-469103606:889,authoriz,authorized,889,,https://github.com/google/deepvariant/pull/159#issuecomment-469103606,4,['authoriz'],['authorized']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F187) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/187#issuecomment-498540454:889,authoriz,authorized,889,,https://github.com/google/deepvariant/pull/187#issuecomment-498540454,4,['authoriz'],['authorized']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F234) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/234#issuecomment-554897702:898,authoriz,authorized,898,,https://github.com/google/deepvariant/pull/234#issuecomment-554897702,4,['authoriz'],['authorized']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F365) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-713317674:898,authoriz,authorized,898,,https://github.com/google/deepvariant/pull/365#issuecomment-713317674,4,['authoriz'],['authorized']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it. ----. #### What to do if you already signed the CLA. ##### Individual signers. * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/). ##### Corporate signers. * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).; * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).; 		. ℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fpull%2F424) for more info**. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/424#issuecomment-779362572:898,authoriz,authorized,898,,https://github.com/google/deepvariant/pull/424#issuecomment-779362572,4,['authoriz'],['authorized']
Security,"Thanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA). :memo: **Please visit <https://cla.developers.google.com/> to sign.**. Once you've signed, please reply here (e.g. `I signed it!`) and we'll verify. Thanks. ---. - If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).; - If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.; - In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed. <!-- need_sender_cla -->",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18#issuecomment-353062531:805,authoriz,authorized,805,,https://github.com/google/deepvariant/pull/18#issuecomment-353062531,6,['authoriz'],['authorized']
Security,"Thanks so much, that makes the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. Very much looking forward to reading your comments on warmstarting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-637523488:135,validat,validation,135,,https://github.com/google/deepvariant/issues/312#issuecomment-637523488,2,['validat'],['validation']
Security,Thanks. ; I have solved this problem. The main reason is my machine can't access Google buckets for download as you say. I will close this issue.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-550114233:74,access,access,74,,https://github.com/google/deepvariant/issues/228#issuecomment-550114233,1,['access'],['access']
Security,"That's right, `channels` are set during `make_examples` when generating training and validation sets. The model will then use those during training.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2027683933:85,validat,validation,85,,https://github.com/google/deepvariant/issues/797#issuecomment-2027683933,1,['validat'],['validation']
Security,"The `GLIBC` error indicates that you're running on an older operating system that DeepVariant doesn't support. The DeepVariant binaries are pre-compiled (unlike most other conda libraries, which get compiled on Centos 6 systems with an older glibc). Unfortunately this means that on older systems (like Centos <=7) they won't work. What operating system are you running it on? If you have access, it would be worth switching to a more up to date system or running on a cloud instance. Sorry for the issues and hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-568430043:389,access,access,389,,https://github.com/google/deepvariant/issues/252#issuecomment-568430043,1,['access'],['access']
Security,This could be due to no examples were generated for validation set. When you run make_example step there multiple files generated. Could you attach one of the text files generated during make_example validation set step?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172#issuecomment-483412986:52,validat,validation,52,,https://github.com/google/deepvariant/issues/172#issuecomment-483412986,2,['validat'],['validation']
Security,"This has a simple fix. Basically you need to replace `/output/realigned_reads` with a location you have write-access to. For example, you can do the following commands:. Assuming you have write-access to this folder `/scratch/c.c21087028/checking_variant_deepvariant`, you can create a sub-directory called `realigned_reads` like this:. ```; mkdir /scratch/c.c21087028/checking_variant_deepvariant/realigned_reads; ```. Then in your Singularity script you have two options to pick from:. #### Option 1 - update only the following line:. ``` ; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/scratch/c.c21087028/checking_variant_deepvariant/realigned_reads"" \; ```. All other lines for Option 1 remain the same. #### Option 2 - update the following lines (I used -B to make /output accessible inside the container):. ```; sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B /scratch/c.c21087028/checking_variant_deepvariant/:/output/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \. ```. The rest of the lines can stay the same for Option 2. Regarding a BED file, you don't need one as they are the same thing as regions -- which you already provide:. https://en.wikipedia.org/wiki/BED_(file_format). Let me know how this runs, and if your run into any other issues. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104:110,access,access,110,,https://github.com/google/deepvariant/issues/691#issuecomment-1667728104,6,['access'],"['access', 'accessible']"
Security,"This is a very interesting question, and the answers to it are complex. I assume for the sake of the question that you have FASTQ data from sequencing a single bacterial colony (so this is not a metagenomics question). Let's divide the question between ""can it technically be done"" and ""will the answers be scientifically valid"". To the question ""can it technically be done"", the answer is probably yes. I am not aware that we have specifically attempted this in bacteria. But if you have a FASTA file with a reference for a species and FASTQ reads, you should be able to generate variant calls for it. To the question ""will the answers be scientifically valid"", it is important to note calling variants in bacterial genomes is an area of open research. Using DeepVariant is reasonable, but I don't think you'll able to consider the output of any method (DeepVariant or other) as certain to give you fully correct results on this problem right out of the box. You'll want to use a few methods (use Freebayes and GATK) and compare between them with metrics you can independently validate, then decide what works and doesn't for your use case. One way to do this could be that for a clonal lineage you expect variants to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin used this measure in a similar way to compare DeepVariant and other methods on inbred rice strains from the 3000 Rice Genomes Project. We would be quite interested to receive your feedback on how DeepVariant performs in this use case, as this may help us understand the value of DeepVariant and improve it for the community.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-434889612:1078,validat,validate,1078,,https://github.com/google/deepvariant/issues/114#issuecomment-434889612,2,['validat'],['validate']
Security,"This seems to be a QC assay for detecting specific allele frequencies of cancer variants. As Pi-Chuan mentioned, these would be somatic variants and usually one cannot rely on the ploidy to correlate with allele frequency given there might be a mixture of cancer subpopulations. I wonder what the genotype probabilities would even mean. I'm not sure what a good VCF truth set of candidates might be for this specific subset of cancer variants to be used for training a model. It probably could be simulated as tumor purity is not consistent in its microenvironment -- unless of course you have well-validated samples. In any case I'd be curious to see where this journey might lead.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541:599,validat,validated,599,,https://github.com/google/deepvariant/issues/690#issuecomment-1663249541,1,['validat'],['validated']
Security,"To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2659,confidential,confidential,2659,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['confidential'],['confidential']
Security,"Unbelievable! Thanks for the pointing this out @obigbando! . @AlfWa you probably would need to set `intel_pstate=disable` at boot time as a kernel argument, and then `cpupower` - or if really necessary `modprobe` - according to what is listed in the directory using this command:. ```; ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; ```. You'll probably see something like the following, or close to it which might include `acpi-cpufreq`:. ```; $ ls /lib/modules/`uname -r`/kernel/drivers/cpufreq/; cpufreq_conservative.ko cpufreq_powersave.ko freq_table.ko; cpufreq_ondemand.ko cpufreq_stats.ko; $; ```; Try reading the following version 7 documentation carefully:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/power_management_guide/cpufreq_governors. I included also the version 6 with `modprobe` just in case:. https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/power_management_guide/cpufreq_setup. Let us know if you run into any issues. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-429714006:680,access,access,680,,https://github.com/google/deepvariant/issues/104#issuecomment-429714006,2,['access'],['access']
Security,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-725869282:25,access,access,25,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282,2,['access'],['access']
Security,"Unfortunately, we don't officially support running on TPU at the moment. The way you ran it when using a short model name looks correct. It could be an access control issue (there is no read access to the bucket containing the model from TPU host).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1149310849:152,access,access,152,,https://github.com/google/deepvariant/issues/537#issuecomment-1149310849,2,['access'],['access']
Security,"Was this issue fixed? I'm having the same error:; ```; [E::cram_decode_slice] MD5 checksum reference mismatch at #1:248741259-248759777; [E::cram_decode_slice] CRAM: 857e35c22e237f102003142abf430800; [E::cram_decode_slice] Ref : 550702c7390d3ead48df01da66f85854; [E::cram_next_slice] Failure to decode slice; Traceback (most recent call last):; File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign; reads = reservoir_sample_reads(; File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads; return utils.reservoir_sample(iterable_of_reads, k, random); File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample; for i, item in enumerate(iterable):; File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/state/partition1/job-44859747/Bazel.runfiles_warfxbvu/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: DATA_LOSS: Failed to parse SAM record; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741#issuecomment-2028785168:82,checksum,checksum,82,,https://github.com/google/deepvariant/issues/741#issuecomment-2028785168,1,['checksum'],['checksum']
Security,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25#issuecomment-354157667:41,access,accessible,41,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667,2,['access'],"['access', 'accessible']"
Security,"Well, I generated the BAM file using bam and samtools from fastq files. The file is too large to share. I'll add the one drive link to access the file.; [https://1001genomes.org/data/GMI-MPI/releases/v3.1/pseudogenomes/fasta/pseudo801.fasta.gz](url); [https://unipotsdamde-my.sharepoint.com/:u:/g/personal/anil_kumar_boddapati_uni-potsdam_de/EYyX5a4xEqBBrH4aXAID8KcB9__Q3s34vU3cTfav0J-VrA?e=owCGZI](url). Best,; Anil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815:135,access,access,135,,https://github.com/google/deepvariant/issues/606#issuecomment-1406588815,1,['access'],['access']
Security,"When you do ```ls /opt/models/wgs/``` you see the local content of the mounted directory which is probably not accessible from TPU host. Although, we don't officially support running on TPU there is an older version case study that shows how to run training on TPU [here](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md#start-a-cloud-tpu). In particular, there is a [link](https://cloud.google.com/tpu/docs/storage-buckets#storage_access) with instructions how to make storage bucket accessible from the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1139165175:111,access,accessible,111,,https://github.com/google/deepvariant/issues/537#issuecomment-1139165175,2,['access'],['accessible']
Security,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433250645:152,validat,validation,152,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645,2,['validat'],"['validated', 'validation']"
Security,"Yes, I definitely got each pbtxt file. Attached below are the log files from the model train step. When I ran this step before (when I had not used the --channels flag, and could not test the model), the .err file for the model training step looked as though it reached a stopping point, whereas in this run it looks like it simply stopped and did not reach that same point. It's definitely not a timeout issue, but I'm not sure what's causing it. . The pbtxt file for the validation set (training set looks similar) looks like this:; ```; # Generated by shuffle_tfrecords_beam.py; #; # --input_pattern_list=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channlesize.tfrecord.gz; # --output_pattern_prefix=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled; #. name: ""Chromosome3""; tfrecord_path: ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.with_label_channelsize.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 35759; # class1: 27257; # class0: 1777; # class2: 6725; ```; And here are the log files from the attempted model training: ; [deepvariant_modeltrain-14705863-Atlas-0031.err.txt](https://github.com/google/deepvariant/files/14828238/deepvariant_modeltrain-14705863-Atlas-0031.err.txt); [deepvariant_modeltrain-14705863-Atlas-0031.out.txt](https://github.com/google/deepvariant/files/14828239/deepvariant_modeltrain-14705863-Atlas-0031.out.txt). Thank you for your help!. Best, ; Haley",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725:473,validat,validation,473,,https://github.com/google/deepvariant/issues/797#issuecomment-2030499725,2,['validat'],['validation']
Security,"Yes, I was having the same issue using the old version 0.4.1, when I change to 0.5.1:; ```; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371288942:364,access,accessible,364,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942,1,['access'],['accessible']
Security,"_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirming new hypotheses. Given that, you use it to infer how the experiment might behave, or in your case the meaning behind your results. If your model is not well-established for your organism, your organism might respond in a unexpected ways given an experimental setup. I get the feeling we're trying to mix the engineering with the science-based approach, which might cause us to require designing additional experiments for validating previous results -- possibly becoming circular. On another note, I'm sure your already know this regarding IGV colors, you can get complete breakdown of their meaning at the following site: . https://software.broadinstitute.org/software/igv/AlignmentData. Hope it helps,; ~p. #### References. 1. [GenomeScope 2.0 and Smudgeplot for reference-free profiling of polyploid genomes](https://www.nature.com/articles/s41467-020-14998-3); 2. [GenomeScope: fast reference-free genome profiling from short reads](https://academic.oup.com/bioinformatics/article/33/14/2202/3089939?login=false); 3. [Measuring Genome Sizes Using Read-Depth, k-mers, and Flow Cytometry: Methodological Comparisons in Beetles (Coleoptera)](https://academic.oup.com/g3journal/article/10/9/3047/6060154); 4. [Kmer2SNP: reference-free SNP calling from raw reads based on matching](https://www.biorxiv.org/content/10.1101/2020.05.17.100305v1.full)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:3381,validat,validating,3381,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,2,['validat'],['validating']
Security,```; Failed to get matching files on /opt/models/wgs/model.ckpt: UNIMPLEMENTED: File system scheme '[local]' not implemented (file: '/opt/models/wgs/model.ckpt'); ``` . Could you try to save the model on the cloud (path should start with gs://)? It looks that model is not accessible from the TPU host.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1136530780:273,access,accessible,273,,https://github.com/google/deepvariant/issues/537#issuecomment-1136530780,1,['access'],['accessible']
Security,"abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1812,validat,validation,1812,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['validat'],['validation']
Security,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1600,confidential,confidential,1600,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500,8,"['confidential', 'secur']","['confidential', 'secured']"
Security,"ages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 660, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 235, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 886, in _build; build_restore=build_restore); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 510, in _build_internal; restore_sequentially, reshape); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 389, in _AddShardedRestoreOps; name=""restore_shard"")); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 336, in _AddRestoreOps; re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:10295,access,access,10295,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['access'],['access']
Security,"ages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:10703,access,access,10703,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['access'],['access']
Security,"aining and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:2236,validat,validation,2236,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,4,['validat'],['validation']
Security,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1896,confidential,confidential,1896,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312,4,"['confidential', 'secur']","['confidential', 'secured']"
Security,"arting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a better question is what exactly are the reads inferring/representing here regarding a point in time of a clone(s) state? This hints at the mapping complexities that Andrew was suggesting. Regarding the engineering approach, here you assume to have a well-established model you rely upon -- or at least backed up thoroughly by prior experiments -- of your organism's behavior under different conditions. It is more goal-driven, as you have stronger expectations of confirmi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:2058,validat,validated,2058,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,2,['validat'],['validated']
Security,"atest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1576,validat,validation,1576,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,4,['validat'],['validation']
Security,"create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:11342,access,access,11342,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,4,['access'],['access']
Security,downloading http://security.debian.org/debian-security/pool/updates/main/o/openssl/libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb followed by; ```; sudo dpkg --install libssl1.0.0_1.0.1t-1+deb8u10_amd64.deb; ```; seems to work on WSL debian 9,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-466661657:19,secur,security,19,,https://github.com/google/deepvariant/issues/41#issuecomment-466661657,2,['secur'],['security']
Security,"e second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosity will result in a higher first peak and a lower second peak"",*_ which makes sense. Then from there, you form hypotheses to test what might be the model of your organism operates by. So do I trust the reads, usually yes from HiFi, but maybe a bett",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1470,validat,validate,1470,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,2,['validat'],['validate']
Security,"el-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_A",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:3777,hash,hash,3777,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['hash'],['hash']
Security,"enetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers of the mutations and transmissions -- some of which might be more stable than others given different selection forces. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1222,validat,validate,1222,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876,2,['validat'],"['validate', 'validated']"
Security,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2216,secur,secured,2216,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943,1,['secur'],['secured']
Security,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:2731,access,access,2731,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761,5,['access'],['access']
Security,figured it out by adding more bind path for the files I need to access to,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/601#issuecomment-1363548041:64,access,access,64,,https://github.com/google/deepvariant/issues/601#issuecomment-1363548041,1,['access'],['access']
Security,"https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11296,secur,security,11296,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['secur'],['security']
Security,"https://github.com/google/deepvariant/issues/698#issuecomment-1711046219, it is best shuffle all the TFRecords across all the samples. If you have bad samples, then skip those. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture mo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1058,validat,validation,1058,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['validat'],['validation']
Security,ianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3626,access,access,3626,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,"ions to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1093,validat,validate,1093,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876,1,['validat'],['validate']
Security,"ization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3020,secur,secured,3020,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['secur'],['secured']
Security,jianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujiangl,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3969,access,access,3969,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1385,validat,validation,1385,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,2,['validat'],['validation']
Security,"l@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; > options = default_options(add_flags=True, flags_obj=FLAGS); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2416,access,accessible,2416,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,2,['access'],['accessible']
Security,"l`, as shown under the [*Pick a model*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#pick-a-model) section. Probably it would be easiest to pick the one that is specified by the `best_checkpoint.txt` file. $`2)`$ Regarding `.pb` files, these are serialized binary ProtoBuf formatted versions of the model with additional details, denoted as the [*TensorFlow's SavedModel format*](https://www.tensorflow.org/guide/saved_model). When performing variant calling, DeepVariant can only use `.ckpt` model files - not `.pb` files - as specified under the [*Convert model.ckpt to model.pb*](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md#convert-modelckpt-to-modelpb-savedmodel-format) section. May I ask why you might need the ProtoBuf formatted file? . $`3)`$ Regarding the `model.ckpt.example_info.json` file, it is used to validate the checkpointed model's JSON file with the JSON file generated for the examples files, matching against a few specs such as channels and shape (tensor image dimensions), to ensure data and model match before performing variant calling. It should have been automatically copied over by the [`copy_over_example_info_json()` function](https://github.com/google/deepvariant/blob/r1.5/deepvariant/model_train.py#L151-L162). Basically it contains the version of DeepVariant, the tensor image shape (rows, columns, channels) and enumerated channels. There are different ones for different technologies, and below is a list of the contents for each instrument:. ##### ONT R10.4; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### PacBio; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ```. ##### WES; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ##### WGS; ```Json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ``",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051:1135,validat,validate,1135,,https://github.com/google/deepvariant/issues/706#issuecomment-1732869051,2,['validat'],['validate']
Security,"me chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:2831,validat,validation,2831,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['validat'],['validation']
Security,"mp/Bazel.runfiles_od9li1rj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils'; (dv) dpipe@4de3e1b4384c:/app/dpipe$ micromamba list -n dv; List of packages in environment: ""/opt/conda/envs/dv"". Name Version Build Channel ; ──────────────────────────────────────────────────────────────────────────────; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; _tflow_select 2.1.0 gpu ; absl-py 0.15.0 pyhd8ed1ab_0 conda-forge; aiohttp 3.7.4.post0 py36h8f6f2f9_0 conda-forge; altair 4.2.0 pyhd8ed1ab_0 conda-forge; astor 0.8.1 pyh9f0ad1d_0 conda-forge; async-timeout 3.0.1 py_1000 conda-forge; attrs 22.2.0 pyh71513ae_0 conda-forge; blinker 1.5 pyhd8ed1ab_0 conda-forge; boost 1.75.0 py36h355b2fd_0 conda-forge; boost-cpp 1.75.0 hc6e9bd1_0 conda-forge; brotlipy 0.7.0 py36h8f6f2f9_1001 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.5.7 hbcca054_0 conda-forge; cached-property 1.5.2 hd8ed1ab_1 conda-forge; cached_property 1.5.2 pyha770c72_1 conda-forge; cachetools 5.0.0 pyhd8ed1ab_0 conda-forge; certifi 2021.5.30 py36h5fab9bb_0 conda-forge; cffi 1.14.6 py36hd8eec40_1 conda-forge; chardet 4.0.0 py36h5fab9bb_1 conda-forge; charset-normalizer 2.1.1 pyhd8ed1ab_0 conda-forge; click 8.0.1 py36h5fab9bb_0 conda-forge; contextlib2 21.6.0 pyhd8ed1ab_0 conda-forge; crcmod 1.7 py36h8f6f2f9_1006 conda-forge; cryptography 35.0.0 py36hb60f036_0 conda-forge; cudatoolkit 10.0.130 h8c5a6a4_11 conda-forge; cudnn 7.6.5.32 ha8d7eb6_1 conda-forge; cupti 10.0.130 0 ; curl 7.87.0 h6312ad2_0 conda-forge; deepvariant 1.5.0 py36hf3e76ba_0 bioconda ; entrypoints 0.4 pyhd8ed1ab_0 conda-forge; enum34 1.1.10 py36h9f0ad1d_2 conda-forge; gast 0.2.2 py_0 conda-forge; google-auth 2.20.0 pyh1a96a4e_0 conda-forge; google-auth-oauthlib 0.4.6 pyhd8ed1ab_0 conda-forge; google-cloud-sdk 359.0.0 py36h5fab9bb_0 conda-forge; google-pas",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:1758,certificate,certificates,1758,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['certificate'],['certificates']
Security,nevermind... solved there was a text line more in the validation pbtxt,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837#issuecomment-2188585196:54,validat,validation,54,,https://github.com/google/deepvariant/issues/837#issuecomment-2188585196,1,['validat'],['validation']
Security,"ollapse these differences to a 2D representation using [t-SNE plots](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize these differences. For example, if I do that to this dataset, I get the following plot:. ![image](https://github.com/google/deepvariant/assets/6555937/679fd1db-16bb-4534-8afb-794c978eb955). This helps me understand which channels (feature maps) might be similar, or different. Now if we look at the heatmap for the original data it would look something like this (notice the variant in the middle):. ![image](https://github.com/google/deepvariant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have se",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:2804,expose,expose,2804,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['expose'],['expose']
Security,"on, and the answers to it are complex. I; > assume for the sake of the question that you have FASTQ data from; > sequencing a single bacterial colony (so this is not a metagenomics; > question).; >; > Let's divide the question between ""can it technically be done"" and ""will; > the answers be scientifically valid""; >; > To the question ""can it technically be done"", the answer is probably yes.; > I am not aware that we have specifically attempted this in bacteria. But if; > you have a FASTA file with a reference for a species and FASTQ reads, you; > should be able to generate variant calls for it.; >; > To the question ""will the answers be scientifically valid"", it is; > important to note calling variants in bacterial genomes is an area of open; > research. Using DeepVariant is reasonable, but I don't think you'll able to; > consider the output of any method (DeepVariant or other) as certain to give; > you fully correct results on this problem right out of the box. You'll want; > to use a few methods (use Freebayes and GATK) and compare between them with; > metrics you can independently validate, then decide what works and doesn't; > for your use case.; >; > One way to do this could be that for a clonal lineage you expect variants; > to all be called as 1/1 for the non-plasmid genome sequence. Ryan Poplin; > used this measure in a similar way to compare DeepVariant and other methods; > on inbred rice strains from the 3000 Rice Genomes Project.; >; > We would be quite interested to receive your feedback on how DeepVariant; > performs in this use case, as this may help us understand the value of; > DeepVariant and improve it for the community.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/114#issuecomment-434889612>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AHCP08xLJBFnbpQzdBn9MXeFKOyacKs9ks5uqjzDgaJpZM4YEtb1>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/114#issuecomment-435084063:2128,validat,validate,2128,,https://github.com/google/deepvariant/issues/114#issuecomment-435084063,2,['validat'],['validate']
Security,"or security purposes. docker run -it -u 80429:20 -v; XXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; XXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXXXXXXXXXXX/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:1252,secur,security,1252,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158,1,['secur'],['security']
Security,"ose. If you want to see the difference among samples, train then individually, but that will generate multiple per-sample-biased models. What you really want is one good representative model of all your samples, as that will be the model that will be presented to future samples. Thus you want as many good representative samples to train that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:1180,validat,validation,1180,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['validat'],['validation']
Security,"ough there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and validation data. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:3555,validat,validation,3555,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,2,['validat'],['validation']
Security,oujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3741,access,access,3741,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1183,confidential,confidential,1183,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,"please see the error has:. ```bash; --ref is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773:195,access,accessible,195,,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773,1,['access'],['accessible']
Security,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3401,validat,validate,3401,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['validat'],['validate']
Security,"thank you, it the same, but don't know what must have gone wrong. i used a different data it ran but gave a different error again, please can you help analyze this for me? . WARNING: Logging before flag parsing goes to stderr.; I1024 02:24:26.300854 140364017985280 client.py:1004] Timeout attempting to reach GCE metadata service.; W1024 02:24:26.301438 140364017985280 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; I1024 02:24:26.349014 140364017985280 make_examples.py:911] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /TCGA-AF-6136-01A.add_rg.bam.bai; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: /performance-testdata%2FHG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz.tbi; Traceback (most recent call last):;   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>;     tf.app.run();   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run;     _sys.exit(main(_sys.argv[:1] + flags_passthrough));   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 1005, in main;     make_examples_runner(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 912, in make_examples_runner;     regions = processing_regions_from_options(options);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"", line 892, in processing_regions_from_options;     options.min_shared_contigs_basepairs);   File ""/tmp/Bazel.runfiles_CqnGJt/runfiles/genomics/deepvariant/make_examples.py"",",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/111#issuecomment-432491512:469,access,accessible,469,,https://github.com/google/deepvariant/issues/111#issuecomment-432491512,1,['access'],['accessible']
Security,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1057,Validat,Validation,1057,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,7,"['Validat', 'validat']","['Validation', 'validation']"
Security,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1246,confidential,confidential,1246,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853,2,"['confidential', 'secur']","['confidential', 'secured']"
Security,ujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3855,access,access,3855,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,ujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1019,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4091,access,access,4091,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,ujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin z,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4205,access,access,4205,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['access'],['access']
Security,"umns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:4876,validat,validation,4876,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,2,['validat'],['validation']
Security,"ve a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount it whenever I create a new instance, but I don't have to re-upload the files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:1677,access,accessible,1677,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,2,['access'],['accessible']
Security,"ve reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1318,validat,validation,1318,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['validat'],['validation']
Testability," ""data/chr20.fa"" \; --reads ""data/sorted_final_merged.bam"" \; --examples ""training-examples/training_set.with_label.tfrecord.gz"" \; --confident_regions ""data/NA12878.sorted.bed"" \; --regions ""chr20"" \; --truth_variants ""data/NA12878.sorted.vcf.gz"" \; --norealign_reads; ```. When running with this flag, I run into an error that is due to the `QUAL` field missing (output shown below). I am not sure why you are not seeing this error, but you may want to check your data files. You could try the following:; * Regenerate `data/sorted_final_merged.bam.bai` and `data/NA12878.sorted.vcf.gz.tbi` as these seem to be older than the corresponding data files. ; * Check the BAM file. Are you able to view the contents using `samtools`? I mention this because the log shows the following line: `[W::bam_hdr_read] EOF marker is absent. The input is probably truncated`. Hope this helps!. ```; 2019-01-15 00:54:33.940093: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0115 00:54:33.942667 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; I0115 00:54:33.945646 140481635538688 make_examples.py:1024] Preparing inputs; 2019-01-15 00:54:33.956298: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0115 00:54:33.958478 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; I0115 00:54:33.978430 140481635538688 genomics_reader.py:174] Reading data/NA12878.sorted.vcf.gz with NativeVcfReader; I0115 00:54:33.980473 140481635538688 make_examples.py:946] Common contigs are [u'chr20']; I0115 00:54:34.150244 140481635538688 make_examples.py:1030] Writing examples to /home/gunjanbaid/examples/training.examples.tfrecord.gz; 2019-01-15 00:54:34.151400: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-01-15 00:54:34.238521: W third_party/nucleus/io/sam_reader.cc:531] Unrecog",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-454225947:1654,Log,Logging,1654,,https://github.com/google/deepvariant/issues/138#issuecomment-454225947,1,['Log'],['Logging']
Testability," --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:1694,test,testdata,1694,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability, //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1354,test,testlogs,1354,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3304,test,testlogs,3304,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381,3,"['log', 'test']","['log', 'test', 'testlogs']"
Testability," 140301916206848 call_variants.py:454] Processed 15001 examples in 30 batches [0.744 sec per 100]; I0622 21:28:34.156948 140301916206848 call_variants.py:454] Processed 30001 examples in 59 batches [0.688 sec per 100]; I0622 21:30:08.158901 140301916206848 call_variants.py:454] Processed 45001 examples in 88 batches [0.667 sec per 100]; I0622 21:30:37.846297 140301916206848 call_variants.py:458] Processed 49760 examples in 98 batches [0.663 sec per 100]; I0622 21:30:37.846524 140301916206848 call_variants.py:461] Done calling variants from a total of 49760 examples. real 5m34.074s; user 32m1.122s; sys 0m32.211s; ```. Note that the CPU usage for `call_variants` seems to vary more than `make_examples`. Not all 8 CPUs are at 100% all the time, but I did observe that all 8 CPUs were used. RAM was also used that much (maybe about 5GB when I look).; I didn't take a snapshot of `htop` here. ## postprocess_variants; The log here is pretty short, so I'll paste below:; ```; ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""output/HG003.output.vcf.gz"" --nonvariant_site_tfrecord_path ""output/intermediate_results_dir/gvcf.tfrecord@8.gz"" --gvcf_outfile ""output/HG003.output.g.vcf.gz"" ). I0622 21:30:41.675962 140597045393152 postprocess_variants.py:1061] Using sample name from call_variants output. Sample name: HG003; 2021-06-22 21:30:41.680945: I deepvariant/postprocess_variants.cc:88] Read from: output/intermediate_results_dir/call_variants_output.tfrecord.gz; 2021-06-22 21:30:42.033617: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 49760; I0622 21:30:42.624573 140597045393152 postprocess_variants.py:1120] CVO sorting took 0.015735054016113283 minutes; I0622 21:30:42.625385 140597045393152 postprocess_variants.py:1122] Transforming call_variants_output to variants.; I0622 21:30:4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866352316:6694,log,log,6694,,https://github.com/google/deepvariant/issues/463#issuecomment-866352316,1,['log'],['log']
Testability," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44658,test,testlogs,44658,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:49092,test,testlogs,49092,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57828,test,testlogs,57828,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55679,test,testlogs,55679,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:59977,test,testlogs,59977,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," @SQ	SN:chr3	LN:198022430	M5:8abc85e73b1c75518a03743de2c2b14b; @SQ	SN:chr4	LN:191154276	M5:2d37b4e662928cc6c58b84b2a4cc8648; @SQ	SN:chr5	LN:180915260	M5:250f4e82a213269b0a0e4aebb0468470; @SQ	SN:chr6	LN:171115067	M5:409088215d77f0bc72364b390430a5a7; @SQ	SN:chr7	LN:159138663	M5:ef15cde5c82fb860694bf8f611807459; @SQ	SN:chr8	LN:146364022	M5:8fbef8c3eaaac674cc6e690d1641464b; ```; As you can see I don't get the EOF error when viewing `chr20.bam`. I tries running make_examples with the new files and I get the same error as you got here:. > @mosh305 I do not have experience with mapping, but the README for the data indicates that it was mapped using `blasr v1.3.2 to the hg19 human reference genome`. You could check out the documentation for [blasr](https://github.com/PacificBiosciences/blasr). Sorry that I cannot be of more help here.; > ...; > ; > Hope this helps!; > ; > ```; > 2019-01-15 00:54:33.940093: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; > WARNING: Logging before flag parsing goes to stderr.; > I0115 00:54:33.942667 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; > I0115 00:54:33.945646 140481635538688 make_examples.py:1024] Preparing inputs; > 2019-01-15 00:54:33.956298: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; > I0115 00:54:33.958478 140481635538688 genomics_reader.py:174] Reading data/chr20.bam with NativeSamReader; > I0115 00:54:33.978430 140481635538688 genomics_reader.py:174] Reading data/NA12878.sorted.vcf.gz with NativeVcfReader; > I0115 00:54:33.980473 140481635538688 make_examples.py:946] Common contigs are [u'chr20']; > I0115 00:54:34.150244 140481635538688 make_examples.py:1030] Writing examples to /home/gunjanbaid/examples/training.examples.tfrecord.gz; > 2019-01-15 00:54:34.151400: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; > 2019-01-15 00:54:34.238521: W third_party/nucleus/io/sam_rea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-457253153:1927,Log,Logging,1927,,https://github.com/google/deepvariant/issues/138#issuecomment-457253153,1,['Log'],['Logging']
Testability," Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help` argument:. ```; $ ./build help; ...; cloudbuild Builds Docker images of DeepVariant, and ; pushes them on the Google Container Registry (gcr.io) ; ... $ ./build cloudbuild help;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1230,test,test,1230,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,2,['test'],['test']
Testability, DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=0; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-2.11.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_TF_NUMPY_VERSION=1.19.2; ++ DV_TF_NUMPY_VERSION=1.19.2; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; ++ export PYTHON_VERSION=3.8; ++ PYTHON_VERSION=3.8; +++ which python3.8; ++ export PYTHON_BIN_PATH=/usr/bin/python3.8; ++ PYTHON_BIN_PATH=/usr/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api'; + bazel; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; + PATH=/root/bin:/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:14407,test,test,14407,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['test'],['test']
Testability," Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11601,log,log,11601,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability," Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14845,test,testing,14845,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability," HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; ./docs/deeptrio-details-training-data.md:| 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:2489,test,testdata,2489,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability," Sent: Friday, April 13, 2018 10:14 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; originally I was thinking a small/synthetic dataset could subsampled from your data. I actually don't want the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your syst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1183,log,logic,1183,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312,2,['log'],['logic']
Testability," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476392585:2933,benchmark,benchmark,2933,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585,2,['benchmark'],['benchmark']
Testability," _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97546,log,log,97546,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability," all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system with the exception of the ones in the volumes (which it's supposed to use)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:1095,test,testing,1095,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872,2,['test'],"['testing', 'tests']"
Testability," and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001701867.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output. I0622 13:06:39.176360 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:39.193307 47468847029248 make_examples.py:648] Preparing inputs; I0622 13:06:39.256251 47468847029248 genomics_reader.py:223] Reading /input/S-001701867.markdup.bam with NativeSamReader; I0622 13:06:40.155634 47468847029248 make_examples.py:648] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY', 'chrM']; I0622 13:06:53.526016 47468847029248 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-22 13:06:53.527661: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOC",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:2289,log,logs,2289,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,2,['log'],['logs']
Testability," aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:1775,log,log,1775,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['log'],['log']
Testability," can input one tfrecord file to the pbtxt file. How can we input all of the tfrecord file to train model at the same time?. **The make_examples script is:**; `BASE=""${HOME}/Documents/source""; BIN_DIR=""${BASE}/bin""; MODELS_DIR=""${BASE}/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard""; MODEL=""${MODELS_DIR}/model.ckpt""; N_SHARDS=""64""; BAM=""/sdbdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1197,log,logs,1197,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701,1,['log'],['logs']
Testability," case your highest is 19, which is borderline for DeepVariant which has it at 20. A score of 20 means, an error of 0.01, so 99% probability that the call is correct. This has a relationship to the PL value, which gives normalized Phred-scaled scores per genotype, but in this case would be too low to call confidently. - $`DP`$ $`(read`$ $`depth)`$: This tells you the filtered reads that support the call. Here your depth is a bit low for a couple of them. For research purposes the cutoff is usually 10 or higher. For clinical 30 would be nice but anything higher than 18 can be good. You will see an AD value (which includes all reads at that position) as well, but that means you are including reads that could be problematic. . - $`VAF`$ $`(variant`$ $`allele`$ $`frequency)`$: This is the percentage of reads for a specific variant divided by the overall coverage at that locus. For heterozygous it would have a value of 50%, for homozygous close to 100% while matching the reference would be 0%. You will notice one is 75% and it denotes it as heterozygous, but the GQ and PL values are very low to call it confidently. - $`QUAL`$: These are Phred-scaled values that the probability the genotype is reference (0/0). For instance a QUAL score of 20 means that your are 99% confident there is a variant at that site, but as before with much lower values you would not be. You can read more about what these columns mean, and how to interpret them at the [following site](https://gatk.broadinstitute.org/hc/en-us/articles/360035531692-VCF-Variant-Call-Format). As @pichuan mention, other metrics you can use are a tool such as [Illumina's Hap.py](https://github.com/Illumina/hap.py/tree/master), with which you can benchmark your VCF against gold standard truth datasets, to see the confidence of these calls in terms of number of F1 score, precision and recall. . More details are provided in the [Hap.py Manual](https://github.com/Illumina/hap.py/blob/master/doc/happy.md). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880:2109,benchmark,benchmark,2109,,https://github.com/google/deepvariant/issues/671#issuecomment-1610040880,1,['benchmark'],['benchmark']
Testability," extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10876,mock,mock,10876,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['mock'],['mock']
Testability," in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - Th",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3184,test,test,3184,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,2,['test'],['test']
Testability," it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1071,log,log,1071,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['log'],['log']
Testability," line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tens",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114999,log,log,114999,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability," logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.ch",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15042,test,testdata,15042,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['test'],['testdata']
Testability," min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally.; There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.; (06:29:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126338,test,testlogs,126338,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," model at the same time?. **The make_examples script is:**; `BASE=""${HOME}/Documents/source""; BIN_DIR=""${BASE}/bin""; MODELS_DIR=""${BASE}/DeepVariant/deepvariant-model-wes_and_wgs/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard""; MODEL=""${MODELS_DIR}/model.ckpt""; N_SHARDS=""64""; BAM=""/sdbdata/WESdata/CL100026859_bwa/CL100026859_L02_14/PE150.2.rmdup.bam""; REF=""/home/suanfa/Documents/source/ref/hg19.fasta""; var=${BAM##*/}; var=${var%.*}; path=""/home/suanfa/Documents/shishiming/training_WES_model""; OUTPUT_DIR=""$path/output""; EXAMPLES=""${OUTPUT_DIR}/${var}.training.examples.tfrecord@${N_SHARDS}.gz""; CONFIDENT_REGIONS=""/home/suanfa/Documents/source/exome_region_bed/new_v4.bed""; TRUTH_VARIANTS=""/home/suanfa/Documents/source/NISTv3.3_baseline/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.new_ch rID.vcf.gz""; LOG_DIR=""${OUTPUT_DIR}/logs"" . echo 'Run make_examples'; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --confident_regions ${CONFIDENT_REGIONS} \; --truth_variants ${TRUTH_VARIANTS} \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. **the part of the examples output:**; > PE150.2.rmdup.training.examples.tfrecord-00000-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00022-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00044-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00001-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00023-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00045-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00002-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00024-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00046-of-00064.gz; PE150.2.rmdup.training.examples.tfrecord-00003-of-00064.gz PE150.2.rmdup.training.examples.tfrecord-00025-of-00064.gz PE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69#issuecomment-386515701:1304,log,log,1304,,https://github.com/google/deepvariant/issues/69#issuecomment-386515701,1,['log'],['log']
Testability," of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123939,test,testlogs,123939,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability," sent, awaiting response... 200 OK; Length: 357342653 (341M) [binary/octet-stream]; Saving to: âENCFF528VXT.bamâ. ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s. 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]. paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 10",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:1653,Log,Logging,1653,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,1,['Log'],['Logging']
Testability," stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42941,test,testing,42941,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['testing']
Testability," that with ``` --regions ' ""20 21"" ' ```. However, a further question is that when should we include the chr in the region. Apparently ``` --regions ' ""chr20:10,000,000-10,001,000 chr20:10,002,000-10,003,000"" ' ``` does not work but ``` --regions ' ""20:10,000,000-10,001,000 20:10,002,000-10,003,000"" ' ``` is fine. . One more problem is about the log file of the make sample. . It appears to show variant candidate in each region and finally display the total number of variant, i.e.; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 7 candidates in 20:10000000-10000999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` . However, for region 20:10,002,000-10,003,000, I get; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ``` ; which the final candidate variant is less than the candidate in the region. When I run with region 20:10,000,000-10,001,000 20:10,002,000-10,003,000, ; ``` ; I0815 12:48:59.877676 140644793067264 make_examples.py:782] Found 9 candidates in 20:10002000-10002999 [1.56s elapsed]; I0815 12:48:59.899951 140644793067264 make_examples.py:1053] Found 7 candidate variants ; ```; ; which is the same as just running region 20:10,002,000-10,003,000 only. But when I check the call variant log file, it indicated that 16 samples are run, which is correct (7 for first region and 9 for second region). . As I am using ```parallel```, my guess is the log file and final variant number shown in make sample log file only show the total number in 1 partition instead of all. Is that the case?. I am not sure if I misunderstand the log file, or the make sample log does not really reflect the total number of variant. Should I only refer to sample number shown in Call Variant log file to get the total number of candidate variant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72#issuecomment-413179094:1587,log,log,1587,,https://github.com/google/deepvariant/issues/72#issuecomment-413179094,6,['log'],['log']
Testability," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16 \; --alsologtostderr; ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1669,log,log,1669,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['log'],['log']
Testability," used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1293,log,log,1293,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['log'],['log']
Testability," useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15446,test,testdata,15446,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['test'],['testdata']
Testability," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:2635,test,testing,2635,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,1,['test'],['testing']
Testability," with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:4628,test,test,4628,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['test'],['test']
Testability,"![train_loss_and_error](https://user-images.githubusercontent.com/13111474/60637174-f2844000-9e4b-11e9-9ed0-6461041cf407.png); Hi, I looked in my log file and plotted a figure of training losses and validation f-measures, and it turned out that I might have overlooked the improves in the trained models. The improvments after 10000 steps become very small (still important though) compared with that in the first steps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194#issuecomment-508325465:146,log,log,146,,https://github.com/google/deepvariant/issues/194#issuecomment-508325465,1,['log'],['log']
Testability,""" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m0.146s`. ```; $ cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; {""version"": ""1.6.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```. ```bash; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took `5m25.905s`. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. # This part",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:3203,log,log,3203,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['log'],['log']
Testability,""" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://gist.githubusercontent.com/pichuan/75aa5aebc961dd2c2472bcbcdd9ecaa9/raw/3c313815050f1b517a660604b222ecc7528b37e0/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```; sudo apt -y update; sudo apt -y install python3-dev python3-pip; pip3 install setuptools --upgrade; # I had to fix the version, otherwise I hit this issue: https://github.com/apache/arrow/issues/2163; pip3 install apache_beam[gcp]==2.26.0; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:3885,log,log,3885,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['log'],['log']
Testability,""", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/HG002_ONT_deeptrio.denovo.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/customized_classes.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_gvcf_output.g.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3070,test,testdata,3070,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"# Attempt to reproduce the issue. ## Get a machine with the same Linux version. I used GCP to get a machine to test. Hopefully this provides a similar environment:. `gcloud compute instances create ""${USER}-test"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family almalinux-9 --image-project almalinux-cloud --machine-type ""n1-standard-64"" --zone ""us-west1-b""`. I ssh'ed into the machine:. ```bash; gcloud compute ssh pichuan-test --zone ""us-west1-b""; ```. Check the Linux version:. ```; $ uname -a; Linux pichuan-test.us-west1-b.c.brain-genomics.google.com.internal 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. And I ran this too:. ```; $ cat /etc/os-release; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:111,test,test,111,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,11,"['LOG', 'log', 'test']","['LOGO', 'logo-icon', 'test']"
Testability,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917501282:380,test,testing,380,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282,1,['test'],['testing']
Testability,"#########################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21076,assert,assertEqual,21076,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['assert'],['assertEqual']
Testability,"##############################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-lea",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:20882,mock,mock,20882,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['mock'],['mock']
Testability,"####################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; export BAZEL_PYTHON=/home/qilibj/inst/bin/python; export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11""; # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:17897,test,test,17897,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['test'],['test']
Testability,"(modified from earlier response); 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file; * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai; ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461157985:449,test,test,449,,https://github.com/google/deepvariant/issues/149#issuecomment-461157985,1,['test'],['test']
Testability,"); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unuse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:21045,test,tests,21045,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:39817,test,tests,39817,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46661,test,tests,46661,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53248,test,tests,53248,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unuse",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:61980,test,tests,61980,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:197,log,login,197,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214,1,['log'],['login']
Testability,", expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1597,test,testMethod,1597,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['test'],['testMethod']
Testability,", in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97488,test,testlogs,97488,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,",000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2888,test,testdata,2888,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"----------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from; raise value; AssertionError: Expected call: query(reference_name: ""20""; start: 9; end: 21; ); Actual call: query(reference_name: ""20""; start: 9; end: 1 <== this ends with 1 due to the mock object returns 1; ); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:2287,mock,mock,2287,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,7,"['Assert', 'mock']","['AssertionError', 'mock']"
Testability,"--copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11""; # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19202,log,log,19202,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,"['log', 'test']","['log', 'test']"
Testability,"--scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I sshed into the machine. ```bash; gcloud compute ssh pichuan-cpu --zone us-west1-b; ```. Then, on the machine, I get DeepVariant r1.5 source first:. ```bash; git clone https://github.com/google/deepvariant.git; cd deepvariant/; git checkout r1.5; ```. And I confirmed the version:. ```; pichuan@pichuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pth",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:1617,log,log,1617,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['log'],['log']
Testability,"-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10587,mock,mock,10587,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['mock'],['mock']
Testability,"-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2697,log,logs,2697,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['log'],['logs']
Testability,"-cc328-failed-call-to-cuinit-cuda. on the machine where I just installed CUDA 11.3. First, just `sudo yum install nvidia-modprobe` didn't seem to work for me. So I had this workaround first:. ```; sudo su. cat <<EOF > /etc/yum.repos.d/nvidia.repo; [nvidia]; baseurl=https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/; enabled=1; gpgcheck=0; EOF; ```. And then; ```; sudo yum install nvidia-modprobe; ```. Check version:. ```; nvidia-modprobe --version; ```; shows:; ```; nvidia-modprobe: version 440.118.02; ```. check CUDA version again:; ```; [pichuan@pichuan-gpu2 ~]$ nvcc --version; nvcc: NVIDIA (R) Cuda compiler driver; Copyright (c) 2005-2021 NVIDIA Corporation; Built on Sun_Mar_21_19:15:46_PDT_2021; Cuda compilation tools, release 11.3, V11.3.58; Build cuda_11.3.r11.3/compiler.29745058_0; ```. Try this again:; ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}-gpu"" python -c 'import tensorflow as tf;print(tf.test.is_gpu_available())'; ```; Still false -- didn't seem to help:. ```; 2023-03-16 07:01:52.583106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 07:01:52.583190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: pichuan-gpu2; 2023-03-16 07:01:52.583209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: pichuan-gpu2; 2023-03-16 07:01:52.583304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 07:01:52.583352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 465.19.1; False; ```. I tried one more thing, which is rebooting after installing nvidia-modprobe. I did:; ```; gcloud compute instances re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355:1114,test,test,1114,,https://github.com/google/deepvariant/issues/619#issuecomment-1471426355,1,['test'],['test']
Testability,"-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.uni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1812,test,testdata,1812,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['test'],['testdata']
Testability,"-sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1843,test,test,1843,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['test'],['test']
Testability,"-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iext",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10279,test,test,10279,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,".0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1040,test,test,1040,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217,2,['test'],"['test', 'testdata']"
Testability,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-689891360:3966,log,log,3966,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360,1,['log'],['log']
Testability,".4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124462,test,testlogs,124462,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,".; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:11278,test,test,11278,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['test'],['test']
Testability,".cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126045,log,log,126045,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,".fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quick",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:16015,test,testdata,16015,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['test'],['testdata']
Testability,".pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80437,test,testlogs,80437,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,".sorted.bed"" \; --truth_variants ""data/NA12878.sorted.vcf.gz"" \; --regions ""chr20"" \; --norealign_reads; ```; And The output: (receiving the same QUAL field missing error); ```; 2019-01-29 11:46:16.329383: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.333216: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.334961 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.337215 140471159555840 make_examples.py:1024] Preparing inputs; 2019-01-29 11:46:16.340804: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.344462: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.346041 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.360527 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/NA12878.sorted.vcf.gz with NativeVcfReader; I0129 11:46:16.361952 140471159555840 make_examples.py:946] Common contigs are [u'chr20']; I0129 11:46:16.501434 140471159555840 make_examples.py:1030] Writing examples to prj-NA12878/training-examples/training_set.with_label.tfrecord.gz; 2019-01-29 11:46:16.502209: I third_party/nucleus/io/sam_reader.cc:565] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-01-29 11:46:16.550218: W third_party/nucleus/io/sam_reader.cc:125] Unknown tag pb: in header line, ignoring: @HD VN:1.5 SO:coordinate pb:3.0.1; 2019-01-29 11:46:16.554270: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring:; I0129 11:46:16.556066 140471159555840 genomics_reader.py:174] Reading prj-NA12878/testdata/alignments20_sorted.bam with NativeSamReader; I0129 11:46:16.571476 140471159555840 genomics_reader.py:174",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458487916:2020,test,testdata,2020,,https://github.com/google/deepvariant/issues/138#issuecomment-458487916,1,['test'],['testdata']
Testability,".training_examples.tfrecord.gz-00002-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.vcf_candidate_importer.calling_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.calling_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.postprocess_single_site_output.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/golden.training_examples.vcf:##DeepVariant_version=1.6.0; ./deeptrio/testdata/HG002_ONT_deeptrio.examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 199, 9], ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}; ./deeptrio/testdata/golden.vcf_candidate_importer.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/alt_aligned_pileup.golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [140, 199, 8], ""channels"": [1, 2, 3, 4, 5, 6, 9, 10]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00000-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./deeptrio/testdata/golden.training_examples.tfrecord.gz-00001-of-00003.example_info.json:{""version"": ""1.6.0"", ""shape"": [300, 221, 6], ""channels"": [1, 2, 3, 4, 5, 6]}; ./Dockerfile.deepsomatic:ARG VERSION_DEEPSOMATIC=1.6.0; ./deepvariant/testdata/golden.vcf_candidate_importer_calling_examples.tfrecord.example_info.json:{""version"": ""1.6.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040:3739,test,testdata,3739,,https://github.com/google/deepvariant/issues/830#issuecomment-2192031040,1,['test'],['testdata']
Testability,"/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123995,log,log,123995,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125870,log,log,125870,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,//deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107283,test,testlogs,107283,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/exe,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119029,log,log,119029,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"/bin/run_deepvariant --call_variants_extra_args=use_openvino=true --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done."". ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m38.326s; real 15m12.564s; real 7m15.173s; ```. 2. Use your Docker image, use_openvino=false; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..78712d8 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -65,14 +65,14 @@ aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testda; aria2c -c -x10 -s10 ""http://storage.googleapis.com/deepvariant/case-study-testdata/${REF}.fai"" -d ""${INPUT_DIR}""; ; ## Pull the docker image.; -sudo docker pull google/deepvariant:""${BIN_VERSION}""; +sudo docker pull dkurtaev/deepvariant:latest; ; echo ""Run DeepVariant...""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; - google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + dkurtaev/deepvariant:latest \; + /opt/deepvariant/bin/run_deepvariant --call_variants_extra_args=use_openvino=false --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime:; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:2359,test,testdata,2359,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['test'],['testdata']
Testability,"/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125214,log,log,125214,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,/deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca02,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120647,log,log,120647,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"/envs/py38/bin/python3.8; ++ PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel h",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:3803,test,test,3803,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['test'],['test']
Testability,/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119625,test,testlogs,119625,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15582,test,testdata,15582,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['test'],['testdata']
Testability,/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120161,log,log,120161,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; [W::hts_idx_load2] The index file is older than the data file: project-retraining/testdata/variants.vcf.gz.tbi; I1218 13:54:44.727533 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qt8ycuy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1922,test,testdata,1922,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (sha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107457,test,testlogs,107457,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124636,test,testlogs,124636,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1877,test,testlogs,1877,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107805,test,testlogs,107805,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124984,test,testlogs,124984,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107979,test,testlogs,107979,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125158,test,testlogs,125158,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107631,test,testlogs,107631,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124810,test,testlogs,124810,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2051,test,testlogs,2051,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123285,test,testlogs,123285,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118365,test,testlogs,118365,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1264,log,logs,1264,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['log'],['logs']
Testability,024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119671,log,log,119671,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122241,test,testlogs,122241,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"0f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123765,test,testlogs,123765,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9502,mock,mock,9502,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['mock'],['mock']
Testability,"1) Can you confirm that you have generated training/validation data? e.g, run ; ```; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```; and ; ```; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. 2) What do you see in the `${LOG_DIR}/train.log` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2030223849:272,log,log,272,,https://github.com/google/deepvariant/issues/797#issuecomment-2030223849,1,['log'],['log']
Testability,"17:55 make_examples_parent2.tfrecord-00026-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00027-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00028-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00029-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00030-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00031-of-00052.gz; 1.2G Mar 9 17:56 make_examples_parent2.tfrecord-00032-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00033-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00034-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00035-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00036-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00037-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00038-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00039-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00040-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00041-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00042-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00043-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00044-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00045-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00046-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00047-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00048-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00049-of-00052.gz; 1.2G Mar 9 17:54 make_examples_parent2.tfrecord-00050-of-00052.gz; 1.2G Mar 9 17:53 make_examples_parent2.tfrecord-00051-of-00052.gz. > Is there a chance you could share your BAM files or may be just a chr20? It would be easier to investigate the issue if we could reproduce it locally. I don't have cloud storage for thi. > Also, could you try to run the test on Ubuntu OS?. I can't run it on Ubuntu OS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/431#issuecomment-805135046:19701,test,test,19701,,https://github.com/google/deepvariant/issues/431#issuecomment-805135046,1,['test'],['test']
Testability,"2.pdf) it looks like '#contig' header is optional, but I think Nucleous library (that is used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:1226,test,testdata,1226,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from; raise value; AssertionError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1800,test,testing,1800,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['test'],['testing']
Testability,"2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy==1.0) (1.14.0); Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:10986,mock,mock,10986,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['mock'],['mock']
Testability,"20516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72891,log,log,72891,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,20516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120589,test,testlogs,120589,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"3085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]; I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088.; I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]; ...; I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]; I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496.; I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]; ....; ```. And there's also the relevant command information from the log below (formatted for human consumption). ```; time seq 0 0 | parallel -q --halt 2 \; --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \; --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \; --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \; --add_hp_channel \; --alt_aligned_pileup ""diff_channels"" \; --max_reads_per_partition ""600"" \; --min_mapping_quality ""1"" \; --parse_sam_aux_fields \; --partition_size ""25000"" \; --phase_reads \; --pileup_image_width ""199"" \; --norealign_reads \; --regions ""chr20"" \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels ""0.12"" \; --task {}; ...; ```. Here's a snapshot of the output. ```; chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35; chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59; chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230:1534,log,log,1534,,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230,1,['log'],['log']
Testability,"333:0,3,33; chr20	10099055	.	T	C	0	RefCall	.	GT:GQ:DP:AD:VAF:PL	0/0:28:42:28,14:0.333333:0,28,55; chr20	10099079	.	C	T	0.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:16:38:23,15:0.394737:0,15,48; chr20	10099111	.	T	TTTTGTTTG	1.9	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:49:33,15:0.306122:0,2,35; chr20	10099140	.	G	T	0.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:16:52:28,24:0.461538:0,15,46; chr20	10099190	.	G	T	9.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:9:47:20,27:0.574468:8,0,42; chr20	10099220	.	A	G	37.3	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:37:40:21,19:0.475:37,0,51; chr20	10099250	.	G	A	32.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:32:39:21,18:0.461538:32,0,46; chr20	10099535	.	G	A	45.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:46:58:32,26:0.448276:45,0,57; chr20	10099565	.	C	T	42.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:42:56:33,23:0.410714:42,0,52; chr20	10099755	.	C	T	24.8	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:25:50:25,25:0.5:24,0,47; chr20	10099832	.	A	G	29.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:29:53:31,22:0.415094:29,0,50; ```; </details>. <details>; <summary>Our test VCF (DeepVariant v0.9.0)</summary>; ; ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; #",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239#issuecomment-558061938:23915,test,test,23915,,https://github.com/google/deepvariant/issues/239#issuecomment-558061938,1,['test'],['test']
Testability,4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:121058,test,testlogs,121058,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:108277,log,log,108277,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"51,674:0,0,13,4; NC_053212.1_chromosome_1 42506 . C T,<NON_REF> 1121.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,28,0:28:84:1135,84,0,1135,84,1135:0,0,12,16; NC_053212.1_chromosome_1 46081 . A G,<NON_REF> 620.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,16,0:16:48:634,48,0,634,48,634:0,0,4,12; NC_053212.1_chromosome_1 47173 . G A,<NON_REF> 1059.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,29,0:29:87:1073,87,0,1073,87,1073:0,0,5,24; NC_053212.1_chromosome_1 47399 . TTG T,<NON_REF> 675.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,19,0:19:57:689,57,0,689,57,689:0,0,16,3; NC_053212.1_chromosome_1 47570 . G C,<NON_REF> 385.6 . . GT:AD:DP:GQ:PL:SB 0/1:11,11,0:22:99:393,0,381,426,414,841:6,5,5,6; NC_053212.1_chromosome_1 47768 . ATG A,<NON_REF> 812.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,22,0:22:66:826,66,0,826,66,826:0,0,9,13; NC_053212.1_chromosome_1 48014 . CA C,<NON_REF> 876.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,24,0:24:72:890,72,0,890,72,890:0,0,16,8; NC_053212.1_chromosome_1 48426 . A G,<NON_REF> 780.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,19,0:19:57:794,57,0,794,57,794:0,0,9,10; NC_053212.1_chromosome_1 50624 . T C,<NON_REF> 1021.03 . . GT:AD:DP:GQ:PGT:PID:PL:PS:SB 1|1:0,22,0:22:69:1|1:50616_C_T:1035,69,0,1035,69,1035:5061>; NC_053212.1_chromosome_1 50765 . TC T,<NON_REF> 1005.03 . . GT:AD:DP:GQ:PL:SB 1/1:2,26,0:28:64:1019,64,0,1024,78,1038:1,1,14,12; NC_053212.1_chromosome_1 50887 . G T,<NON_REF> 856.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,23,0:23:69:870,69,0,870,69,870:0,0,10,13; NC_053212.1_chromosome_1 50971 . A T,<NON_REF> 699.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,19,0:19:57:713,57,0,713,57,713:0,0,9,10; NC_053212.1_chromosome_1 51160 . C T,<NON_REF> 1100.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,31,0:31:92:1114,92,0,1114,92,1114:0,0,10,21; NC_053212.1_chromosome_1 53199 . TCA T,<NON_REF> 767.03 . . GT:AD:DP:GQ:PL:SB 1/1:0,20,0:20:60:781,60,0,781,60,781:0,0,7,13. ```. And I have attached the log file for this make_examples for this sample. [MAKE_EX_TRAIN_3148249-3.err.gz](https://github.com/user-attachments/files/16873655/MAKE_EX_TRAIN_3148249-3.err.gz). Thanks. Dan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2329464881:5539,log,log,5539,,https://github.com/google/deepvariant/issues/876#issuecomment-2329464881,1,['log'],['log']
Testability,"5720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/mode",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72294,log,log,72294,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"5720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72119,log,log,72119,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,5720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_tra,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71944,log,log,71944,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"5720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72469,log,log,72469,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:19037,test,testlogs,19037,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25558,test,testlogs,25558,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:28375,test,testlogs,28375,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35654,test,testlogs,35654,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37809,test,testlogs,37809,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:51242,test,testlogs,51242,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64419,test,testlogs,64419,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66574,test,testlogs,66574,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68729,test,testlogs,68729,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"6_64-linux-gnu/libdl.so.2 (0x0000155553032000); 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000); 	libcublas.so.12 => not found; 	libcublasLt.so.12 => not found; 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000); 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000); 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000); 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000); 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000); 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000); 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000); ```. When I grep for `libcublas` in the container:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}""; *** bunch more omitted output. I just wanted to show above versions ***; ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you!. Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,; Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060:3665,test,test,3665,,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060,2,['test'],['test']
Testability,720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120824,test,testlogs,120824,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"7680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing examples to /tmp/tm p63xxmwmi/make_examples.tfrecord-00000-of-00001.gz; I0327 13:32:07.296404 47175299967680 make_examples.py:535] Writing gvcf records to /tm p/tmp63xxmwmi/gvcf.tfrecord-00000-of-00001.gz; I0327 13:32:07.298243 47175299967680 make_examples.py:535] Starting from v0.9.0, --use _ref_for_cram is default to true. If you are using CRAM input, note that we will decod e CRAM using the reference you passed in with --ref; 2020-03-27 13:32:07.298857: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OP T_BLOCK_SIZE to 134217728; I0327 13:32:07.388042 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.389374 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.863366 47175299967680 make_examples.py:535] 6 candidates (6 examples) [ 0.57s elapsed]; I0327 13:32:09.857359 47175299967680 make_examples.py:535] Found 78 candidate variants; I0327 13:32:09.857484 47175299967680 make_examples.py:535] Created 86 examples. real 0m11.980s; user 0m5.478s; sys 0m3.350s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp63xxmwmi/call_variants_outp ut.tfrecord.gz"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0327 13:32:13.471893 47138345245376 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-03-27 13:32:13.524960: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with In",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:3756,test,testdata,3756,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['test'],['testdata']
Testability,"8, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112446,test,testlogs,112446,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"9a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Imp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:72833,test,testlogs,72833,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,": localimage; From: deepvariant-0.7.0.simg. # Modified from: https://gist.github.com/pansapiens/717efcdefb51fa0ce1a6abf092bcb2f4; # Install Google Cloud SDK for the gcloud tool, then:; # We grab the official Docker image, push it to a locally running container repo, then get; # Singularity to pull it back.; # docker pull gcr.io/deepvariant-docker/deepvariant:0.7.0; # docker tag gcr.io/deepvariant-docker/deepvariant:0.7.0 localhost:5000/deepvariant:0.7.0; # docker run -d -p 5000:5000 --restart=always --name registry registry:2; # docker push localhost:5000/deepvariant:0.7.0; # sudo SINGULARITY_NOHTTPS=1 singularity pull docker://localhost:5000/deepvariant:0.7.0; #; # Then use that container image to make this customized one; # sudo singularity build deepvariant-custom.simg Singularity.spec. %environment; PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin; DV_GPU_BUILD=0; export PATH DV_GPU_BUILD. %apprun download_testdata; BUCKET=""gs://deepvariant""; DATA_BUCKET=""${BUCKET}/quickstart-testdata/*""; mkdir -p input; gsutil cp -R ""${DATA_BUCKET}"" input/. %apprun make_examples; exec /opt/deepvariant/bin/make_examples \; --mode calling \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --reads /dv2/input/NA12878_S1.chr20.10_10p1mb.bam \; --examples output.examples.tfrecord \; --regions ""chr20:10,000,000-10,010,000"". %apprun call_variants; exec /opt/deepvariant/bin/call_variants \; --outfile call_variants_output.tfrecord \; --examples output.examples.tfrecord \; --checkpoint /models/wgs/model.ckpt. %apprun postprocess_variants; exec /opt/deepvariant/bin/postprocess_variants \; --ref /dv2/input/ucsc.hg19.chr20.unittest.fasta.gz \; --infile call_variants_output.tfrecord \; --outfile output.vcf. %runscript; if [ $# -eq 0 ]; then; echo '''Example Usage:. # download data to input and models; singularity run --app download_testdata deepvariant-custom.simg. # make the examples, mapping inputs; singularity run --bind input:/dv2/input/ --app make_examples deepvaria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-458208323:1292,test,testdata,1292,,https://github.com/google/deepvariant/issues/132#issuecomment-458208323,1,['test'],['testdata']
Testability,"://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. After this, examine the content in ""${OUTPUT_DIR}/intermediate_results_dir"". ```; [pichuan@pichuan-centos8 ~]$ ls -1 ""${OUTPUT_DIR}/intermediate_results_dir""; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```. Next, I want to try running with `--intermediate_results_dir tmp/`. First checking the directories in the current working directory:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; ```. Then I ran:; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir tmp/ \; --num_shards=1; ```. Now I check the current directory again:; ```; [pichuan@pichuan-centos8 ~]$ ls -1; deepvariant.sif; quickstart-output; quickstart-testdata; tmp; ```. and I see the intermediate results in the directory:. ```; [pichuan@pichuan-centos8 ~]$ ls -1 tmp/; call_variants_output.tfrecord.gz; gvcf.tfrecord-00000-of-00001.gz; make_examples.tfrecord-00000-of-00001.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:3046,test,testdata,3046,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,4,['test'],['testdata']
Testability,"://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; > ; > **Describe the issue:** Am getting the error as ""Fatal Python error: Segmentation fault""; > ; > **Setup**; > ; > * Operating system: Ubuntu 22.04.2 LTS; > * DeepVariant version: 1.6.1; > * Installation method (Docker, built from source, etc.): Docker; > * Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format.; > ; > **Steps to reproduce:**; > ; > * Command: sudo docker run ; > -v ""${INPUT_DIR}"":""/input"" ; > -v ""${OUTPUT_DIR}"":""/output"" ; > google/deepvariant:""${BIN_VERSION}"" ; > /opt/deepvariant/bin/run_deepvariant ; > --model_type=PACBIO ; > --ref=/input/RILWLs1.fasta ; > --reads=/input/Out.fastq ; > --output_vcf=/output/output.vcf.gz ; > --output_gvcf=/output/output.g.vcf.gz ; > --intermediate_results_dir /output/intermediate_results_dir ; > --num_shards=15; > * Error trace: (if applicable); > ; > **Does the quick start test work on your system?** Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md. Yes. Test data works fine. ![Screenshot from 2024-04-17 12-24-22](https://private-user-images.githubusercontent.com/68117296/323111309-41ac66ff-ff52-493f-b18f-f017921caa86.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTMzMzcyOTIsIm5iZiI6MTcxMzMzNjk5MiwicGF0aCI6Ii82ODExNzI5Ni8zMjMxMTEzMDktNDFhYzY2ZmYtZmY1Mi00OTNmLWIxOGYtZjAxNzkyMWNhYTg2LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA0MTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwNDE3VDA2NTYzMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTg3ZDQ3ZTBmNDFjYWQ4YWQyNmM4MDdmYTJiYjVjNzlhYmI1MDA2NzQxOGY3MjA1ZjU1ODY3ZDUzOTcyMTkyNzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVw",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080:1081,test,test,1081,,https://github.com/google/deepvariant/issues/807#issuecomment-2060514080,2,['test'],['test']
Testability,"://github.com/pgrosu/test/assets/6555937/f1f478fa-8ffc-4a9a-b5de-4f123658750d). ![image](https://github.com/pgrosu/test/assets/6555937/eb14b3e0-3424-4dc5-82b3-c77091c871a2). Given visual similarity, these were confirmed via Euclidean distance (0.9931127, 0.8543731 and 1.052052, respectively). This indicates the feature set might exhibit strong similarity for interpretation. . Looking at one network (PacBio), it might be possible to confirm calibration by testing for network-resiliency. Via perturbation analysis it should be possible to get insight into a channel's response under perturbation, and their binary interactions under such conditions. Keeping the variant unchanged within a window on each side for preserving the call, the inspection each channel vulnerability response to perturbation can be tested. This resulted in the following perturbation response ($`c\_*`$ denotes a channel, and $`i\_*\_*`$ represents a binary interaction between two channels):. ![image](https://github.com/pgrosu/test/assets/6555937/97c6b13e-e80b-48ae-939d-2367e7ab65c1). The above can be mapped into a network of interactions among the channels:. ![image](https://github.com/pgrosu/test/assets/6555937/cc0e1e2a-278f-4178-a124-67b0321bba3e). Based on the above mapping, by testing well-interacting channels through a probabilistically value-update -- within DeepVariant-acceptable values -- it might be possible to check for shifts in genotype mimicking Mendelian violation. Selecting `base_quality` and staying within DeepVariant's minimum acceptable value, random sampling with replacement was performed in the window outside the variant region. A shift in genotype was achieved giving a measure of network resiliency. Other channels being more strongly-connected could provide more aggressive shifts in genotype. This can be offset by restrictions in convolutional motifs within the network, or be shifted to the code via threshold limitations in `make_examples`. Thank you and Happy 4th of July!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040:1436,test,test,1436,,https://github.com/google/deepvariant/issues/666#issuecomment-1619352040,3,['test'],"['test', 'testing']"
Testability,":/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.568112 139796154570496 make_examples.py:648] Preparing inputs; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.568638 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.569230 139796154570496 make_examples.py:648] Common contigs are ['chr20']; I0629 23:43:41.686007 139796154570496 make_examples.py:648] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-29 23:43:41.686352: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.688519 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:15879,test,testdata,15879,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['test'],['testdata']
Testability,":; ```; $ grep '^real' /tmp/open; real 7m20.986s; real 21m24.429s; real 6m32.705s; ```. 3. Use v1.0.0 image.; The code diff:; ```; $ git diff; diff --git a/scripts/run_wgs_case_study_docker.sh b/scripts/run_wgs_case_study_docker.sh; index 3dc9712..88fb0c1 100755; --- a/scripts/run_wgs_case_study_docker.sh; +++ b/scripts/run_wgs_case_study_docker.sh; @@ -72,7 +72,7 @@ sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; - /opt/deepvariant/bin/run_deepvariant \; + /opt/deepvariant/bin/run_deepvariant --make_examples_extra_args=regions=chr1 \; --model_type=WGS \; --ref=""/input/${REF}.gz"" \; --reads=""/input/${BAM}"" \; @@ -100,6 +100,6 @@ pkrusche/hap.py /opt/hap.py/bin/hap.py \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${UNCOMPRESSED_REF}"" \; -o ""${OUTPUT_DIR}/happy.output"" \; - --engine=vcfeval; + --engine=vcfeval -l chr1; ) 2>&1 | tee ""${LOG_DIR}/happy.log""; echo ""Done.""; ```. Runtime; ```; $ grep '^real' /tmp/openvino.log; real 7m26.887s; real 20m40.889s; real 6m25.257s; ```. ---. # Machine details. I got the machine with this command:. ```; gcloud compute instances create ""${USER}-openvino-expt"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ## `lscpu`; ```; $ lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 64; On-line CPU(s) list: 0-63; Thread(s) per core: 2; Core(s) per socket: 32; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 85; Model name: Intel(R) Xeon(R) CPU @ 2.00GHz; Stepping: 3; CPU MHz: 2000.178; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:4220,log,log,4220,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['log'],['log']
Testability,":; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:1025,log,log,1025,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['log'],['log']
Testability,":model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_train_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71769,log,log,71769,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['log'],['log']
Testability,"; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(moni",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19937,test,test,19937,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['test'],['test']
Testability,"; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <24>(char const (&) [24])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; collect2: error: ld returned 1 exit status; make[3]: *** [clif/backend/CMakeFiles/clif-matcher.dir/build.make:147: clif/backend/clif-matcher] Error 1; make[2]: *** [CMakeFiles/Makefile2:1342: clif/backend/CMakeFiles/clif-matcher.dir/all] Error 2; make[1]: *** [CMakeFiles/Makefile2:1349: clif/backend/CMakeFiles/clif-matcher.dir/rule] Error 2; make: *** [Makefile:617: clif-matcher] Error 2; ```; I had the same error last time but it somehow worked magically when I removed `build/` folder after a while. I think these are the major changes I have ma",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:5841,Log,LogMessage,5841,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,2,['Log'],['LogMessage']
Testability,"; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1684,test,test,1684,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,2,['test'],"['test', 'testdata']"
Testability,"; > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.; The former has 77+69+25+23=194 total FNs+FPs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:1025,test,tested,1025,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,1,['test'],['tested']
Testability,"; > Length: 357342653 (341M) [binary/octet-stream]; > Saving to: âENCFF528VXT.bamâ; >; > ENCFF528VXT.bam 100%[=========================================================>] 340.79M 1.26MB/s in 5m 17s; >; > 2018-03-07 16:25:59 (1.08 MB/s) - âENCFF528VXT.bamâ saved [357342653/357342653]; >; > paul@gubuntu:~/data/luisa$ samtools index ENCFF528VXT.bam; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > [E::bgzf_read] Read block operation failed with error -1 after 143 of 246 bytes; > samtools index: failed to create index for ""ENCFF528VXT.bam""; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$; > paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; > paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; > WARNING: Logging before flag parsing goes to stderr.; > I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; > W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; > [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; > Traceback (most recent call last):; > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; > tf.app.run(); > File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; > _sys.exit(main(_sys.argv[:1] + flags_passthrough)); > File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371306075:2126,Log,Logging,2126,,https://github.com/google/deepvariant/issues/52#issuecomment-371306075,1,['Log'],['Logging']
Testability,"; ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; Fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87669,test,tests,87669,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,"['Test', 'test']","['Testing', 'tests']"
Testability,"; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112505,log,log,112505,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,3,"['Test', 'log']","['Test', 'Testing', 'log']"
Testability,"; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:85254,test,testlogs,85254,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"<module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103435,test,testlogs,103435,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"=============; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log; (06:29:16) FAIL: //deepvariant:model_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:71712,test,testlogs,71712,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"====================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_te",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107108,test,testlogs,107108,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,2,['test'],"['test', 'testlogs']"
Testability,"> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Callin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1334,test,test,1334,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,['test'],['test']
Testability,"> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:4462,test,test,4462,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,['test'],['test']
Testability,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-737655909:315,log,logging,315,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909,3,['log'],['logging']
Testability,"> @dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :). @pichuan, that's good question. Checkpoint is used to restore model training and that's why it takes a lot of size. Probably, internally it contains not just weights but also gradients and intermediate outputs for layer. `.pb` model can be used for inference but using TensorFlow 1.x API, not sure about Estimator, unfortunately. I moved OpenVINO conversion into runtime anyway - that seems now simpler and doesn't oversize an image. > @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet). Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735703602:859,test,test,859,,https://github.com/google/deepvariant/pull/363#issuecomment-735703602,2,['test'],['test']
Testability,"> @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:; > > Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users. Added a commit which let's to track `call_variants` progress with OpenVINO backend. Updated docker image correspondingly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735450709:168,log,logs,168,,https://github.com/google/deepvariant/pull/363#issuecomment-735450709,1,['log'],['logs']
Testability,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?. I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767600297:38,log,logs,38,,https://github.com/google/deepvariant/issues/412#issuecomment-767600297,1,['log'],['logs']
Testability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:200,test,test,200,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761,2,['test'],['test']
Testability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:200,test,test,200,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277,1,['test'],['test']
Testability,"> @nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:814,test,testing,814,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872,2,['test'],['testing']
Testability,"> @pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8.; > ; > I recommend trying to build without using a miniconda environment. Hi Daniel! Are there any plans to support Python 3.9?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-2307619528:144,test,tested,144,,https://github.com/google/deepvariant/issues/727#issuecomment-2307619528,1,['test'],['tested']
Testability,"> Did you run `tensorflow.test.is_gpu_available()` from the DeepVariant docker?. Yes i did , I have posted the result which shows that in python shell the gpu can be identified with tensorflow in the first comment. > Could you try the suggestion from this [thread](https://stackoverflow.com/questions/48658204/tensorflow-failed-call-to-cuinit-cuda-error-no-device). Actuallly i have tried to set ` CUDA_VISIBLE_DEVICES=0 ` in System ENV ,it did't work .So I tried to find the place where sets the value of the env in your code , and want to set the ` CUDA_VISIBLE_DEVICES=0 ` , but i did't find. So ,i turn to ask for your help. I think the reason why the error occurs may be in your code the value of the ` CUDA_VISIBLE_DEVICES` does't match with my device.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2126036319:26,test,test,26,,https://github.com/google/deepvariant/issues/820#issuecomment-2126036319,1,['test'],['test']
Testability,"> Hello Ferdinand,; > ; > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l); > ; > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers.; > ; > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error.; > ; > Thanks,; > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data.; As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/158#issuecomment-468228631:975,benchmark,benchmarking,975,,https://github.com/google/deepvariant/issues/158#issuecomment-468228631,1,['benchmark'],['benchmarking']
Testability,"> Hi @A-Tsai ,; > I think our setup on GitHub might be a bit confusing right now.; > Because the way our repo is set up, the main DeepVariant repo can't directly merge from external pull requests.; > ; > We will need to internally test your pull request, and will have to internally make a code change which will be exported to GitHub later. When we make the code change internally, we will point to your PR in the commit log.; > Please let us know if you're ok with that. If so, we'll proceed with that. Thank you. Yes, I have no problems on it. Please help to verify the PR.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-469126284:231,test,test,231,,https://github.com/google/deepvariant/pull/159#issuecomment-469126284,2,"['log', 'test']","['log', 'test']"
Testability,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types − that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes − I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:799,test,test,799,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762,1,['test'],['test']
Testability,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328#issuecomment-663306398:502,test,tested,502,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398,2,['test'],['tested']
Testability,"> Hi @ZuyaoLiu; > ; > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized.; > ; > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu; > ; > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well.; > ; > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not?; > ; > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)?; > ; > Thank you!. Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1793557479:29,test,tested,29,,https://github.com/google/deepvariant/issues/722#issuecomment-1793557479,3,['test'],"['test-the-model', 'tested', 'testing']"
Testability,"> Hi @leorippel; > ; > In your log, the error says:; > ; > ```; > 2021-01-26 17:29:22.919993: E third_party/nucleus/io/tfrecord_reader.cc:48] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; > ```; > ; > Can you confirm that you actually have this file? From what you described earlier, you're trying to rename files into this format, but I'm not sure what you actually rename them to. Yes, Sorry for the late reply. when I type the input command "" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" "" I understand that the script will look for this pattern, no? Because I tried *gvcf.tfrecord@30.gz also didn't work and as I mentioned before my entry files are ""SPLIT2.gvcf.tfrecord-00000-of-00030.gz and SPLIT.gvcf.tfrecord-00000-of-00030.gz "". The ""*"" before the pattern should work? maybe two --nonvariant_site_tfrecord_pat flags? . how Can I pass those 2 differents names to the argument?. Cheers.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413#issuecomment-771149411:31,log,log,31,,https://github.com/google/deepvariant/issues/413#issuecomment-771149411,1,['log'],['log']
Testability,"> Hi @pichuan Thank you for your response.; > ; > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:71,test,testing,71,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,3,"['benchmark', 'test']","['benchmark', 'test', 'testing']"
Testability,"> Hi @pioneer-pi ,; > ; > Can you please put some more details? Please follow the suggestions in this doc: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md, which suggests you to run `sudo su` before running `./build-prereq.sh`. One thing you can do is run:; > ; > ```shell; > ./build-prereq.sh 2>&1 | tee /tmp/dv_build.log; > ```; > ; > Then upload the `dv_build.log` here so a detailed log is available. I pull the docker image of deepvariant:1.5.0, and then I start a container through `docker run -it deepvariant /bin/bash`. In this environment(I am root), I git clone the source code and try to build it from source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:178,test,test,178,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,8,"['log', 'test']","['log', 'test']"
Testability,"> Hi @tinyfallen; > ; > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144).; > ; > So the main resource use can be estimated from the single sample runtime multiplied by sample number.; > ; > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077:296,benchmark,benchmarks,296,,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077,1,['benchmark'],['benchmarks']
Testability,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:252,log,logs,252,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584,1,['log'],['logs']
Testability,"> I'm getting your first 5 commits (up to 3cfa6c5 ) reviewed internally. We'll plan to get those 5 commits into our codebase for the upcoming release. @pichuan, May I ask to additionally take a look at Dockerfile. There is the following line I feel unsure:. ```; sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; ```; It would be safer to replace with something like this:; ```patch; diff --git a/Dockerfile b/Dockerfile; index 0432fd8..a57364d 100644; --- a/Dockerfile; +++ b/Dockerfile; @@ -67,7 +67,7 @@ RUN chmod +r /opt/models/hybrid_pacbio_illumina/model.ckpt*; # Convert model to OpenVINO format; RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; python3 -m pip install networkx defusedxml test-generator==0.1.1; \; - sed -i -E 's/from deepvariant import tf_utils//' /opt/deepvariant/deepvariant/modeling.py; \; + sed -i -E 's/from deepvariant import tf_utils/#from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; export PYTHONPATH=/opt/deepvariant:${PYTHONPATH}; \; for model in wgs wes pacbio hybrid_pacbio_illumina; do \; cd /opt/models/${model}; \; @@ -79,6 +79,7 @@ RUN if [ ""${DV_OPENVINO_BUILD}"" = ""1"" ]; then \; --scale 128; \; rm model.pb; \; done \; + sed -i -E 's/#from deepvariant import tf_utils/from deepvariant import tf_utils/' /opt/deepvariant/deepvariant/modeling.py; \; fi; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736278644:740,test,test-generator,740,,https://github.com/google/deepvariant/pull/363#issuecomment-736278644,1,['test'],['test-generator']
Testability,"> I'm happy to hear you have enjoyed my YouTube videos :); > ; > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names.; > ; > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :); > ; > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?. for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917356380:866,test,test,866,,https://github.com/google/deepvariant/issues/483#issuecomment-917356380,1,['test'],['test']
Testability,"> In your variants.vcf.txt and variants.bed.txt contigs have name 'chr' when BAM file has contig name 'NC_000913.3'; > Contig names should match. Looking at [VCF Specs](https://samtools.github.io/hts-specs/VCFv4.2.pdf) it looks like '#contig' header is optional, but I think Nucleous library (that is used by DeepVariant to read VCF) relies on that field. @akolesnikov first, thank you very much for your help!; I renamed all the contig names in the VCF and BED to ""NC_000913.3"" and added a ""##contig"" header to the VCF file, I indexed again the FASTA, BAM and BED files, but it still doesn't work. I used the command `samtools faidx sequence.fasta` to create `sequence.fasta.fai` file.; I used the command `samtools index aligned_reads.bam` to create `aligned_reads.bam.bai` file.; And I used the command `tabix -p vcf variants.vcf.gz` to create `variants.vcf.gz.tbi` file. This is my command line:. ```; python bin/make_examples.zip \; --mode training \; --ref ""project-retraining/testdata/sequence.fasta"" \; --reads ""project-retraining/testdata/aligned_reads.bam"" \; --examples ""project-retraining/training_examples"" \; --confident_regions ""project-retraining/testdata/variants.bed"" \; --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and I get the same ValueError as before (from `make_examples.log` file):. ```; 2018-12-18 13:54:44.725754: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.725832 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1218 13:54:44.726912 140314139805440 make_examples.py:1024] Preparing inputs; 2018-12-18 13:54:44.727156: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1218 13:54:44.727194 140314139805440 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-448201709:983,test,testdata,983,,https://github.com/google/deepvariant/issues/128#issuecomment-448201709,1,['test'],['testdata']
Testability,"> Just tried the image on [n2d-standard-8](https://cloud.google.com/compute/docs/machine-types#n2d_machine_types) from GCP and it works fine through OpenVINO backend (AMD EPYC 7B12). So seems like we can freely turn OpenVINO by default for CPU only environment. Shall I do it in this PR or you can switch it separately?. Thanks for testing! What do you think is the best way to change the default for GPU? I was thinking bout this, but not sure:; For building, we'll want to keep `DV_OPENVINO_BUILD=0` in Dockerfile, right? Because for building GPU, we don't want DV_OPENVINO_BUILD to be on by default. This one is easy to change - I can just change our release process for CPU image building to always add `--build-arg DV_OPENVINO_BUILD=1`. So we don't need to change the default in Dockerfile. I wonder what's a good way to change the default of the use_openvino flag, though.; Because of GPU use case, we don't really want to switch `use_openvino` to `True` in call_variants.py either.; I was thinking about optionally add --use_openvino flag in Dockerfile if building for GPU, but haven't tried whether that'll work or not. (Ideally I want users to still be able to pass in --use_openvino=false if they want to turn it off.). If you have a proposed change that works well for CPU as a default, but doesn't hurt the GPU use case, feel free to propose a commit here. Internally I'm about to get some of these code through for review first, and I can add on any incremental changes for review internally later. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735915820:332,test,testing,332,,https://github.com/google/deepvariant/pull/363#issuecomment-735915820,1,['test'],['testing']
Testability,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896293199:127,test,tested,127,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199,4,['test'],['tested']
Testability,"> Thank you @ZuyaoLiu for the question and the solution. We'll give it a try and improve this.; > ; > One question for you @ZuyaoLiu , did you have any issues using Singularity + GPU to run variant calling as well, or are you seeing this issue with training only? (I'm curious because I've tried Singularity+GPU for variant calling, and that worked fine. But I haven't personally tried Singularity+GPU training yet. So I'll first want to see if I can reproduce that issue). So far, I haven't tested the variant calling module. Version 1.6.0 was released when I was about to start the training step. Therefore, my SNP matrixes and examples are from v 1.5.0. But I will give it a try after finishing my training step and will let you know if it works. Best,; Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1781774874:492,test,tested,492,,https://github.com/google/deepvariant/issues/722#issuecomment-1781774874,1,['test'],['tested']
Testability,">We're watching developments in the area of these truth sets and hope to be able to further develop the somatic caller in the future. Hi, I think I have a decent set of multi-tumor samples that would allow us to define ground truth sets for somatic clonal and somatic sub-clonal mutations, (and also pick up on potential noise from our inhouse variant caller). ; @AndrewCarroll, @pichuan - I would be interested in testing out a prototype version of your somatic caller, or perhaps collaborating to see how it performs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983:415,test,testing,415,,https://github.com/google/deepvariant/issues/351#issuecomment-1019480983,1,['test'],['testing']
Testability,"@A-Tsai Try using `taskset` to assign which cores your process should go to, where the logical cores as enabled by hyperthreading keep an architectural state of your running process. Here's a link to the `taskset` manpage:. https://linux.die.net/man/1/taskset",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-416729239:87,log,logical,87,,https://github.com/google/deepvariant/issues/90#issuecomment-416729239,1,['log'],['logical']
Testability,"@ASLeonard , . I was able to reproduce the issue. Currently there's no way to avoid this log. I am trying to push a patch soon. Thanks for your patience.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383:89,log,log,89,,https://github.com/google/deepvariant/issues/748#issuecomment-1853175383,1,['log'],['log']
Testability,"@AndrewCarroll , what data was used for training the somatic caller? And how do you generate the simulated benchmarks? Could you point me to more information on this?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/241#issuecomment-1478100032:107,benchmark,benchmarks,107,,https://github.com/google/deepvariant/issues/241#issuecomment-1478100032,1,['benchmark'],['benchmarks']
Testability,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371#issuecomment-717204430:75,log,logit,75,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430,1,['log'],['logit']
Testability,"@AndrewCarroll hello Andrew, thank you for your reply! I'll explain my intentions:; As a project, I need is to select a reference (doesn't really matter of what) and a dataset of long reads of that reference, successfully train the DeepVariant model with that set, and then evaluate the the trained model accuracy for the long reads. All I need to do is to use the data to make sets of training examples, validation examples and test examples, train the model with the training set, evaluate each checkpoint with the validation set, eventually choose the best checkpoint and finally evaluate it with the test set. Very similar to this [case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md) DeepVariant provided, but with a dataset of long reads. Also thank you very much for pointing out that DeepVariant was not trained for non-CCS reads, I'll try using the dataset you provided! If I'll succeed training and evaluating the model with this dataset, it would help a lot to be able to compare my my final model to your trained model. Also if you can explain a little about how to work with the files in the dataset. Where can I find truth variants and confident regions file of it's reference for my training?. I hope this clarifies my purposes. It may seem like a small project, but I greatly appreciate your help with this, it is very important to me.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458885417:429,test,test,429,,https://github.com/google/deepvariant/issues/138#issuecomment-458885417,2,['test'],['test']
Testability,"@AndrewCarroll, many thanks for such quick response!. > We do not directly accept external PRs, but this is not because we do not accept community additions. The ""source of truth"" for the DeepVariant GitHub repo resides in internal systems, which are then copied onto GitHub. As a result, to incorporate your changes, we will need to change the internal code and copy that back out. We have done this in the past with community additions, and have attributed the authors for contributions in release notes and other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO rel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:880,benchmark,benchmark,880,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['benchmark'],['benchmark']
Testability,"@Axze-rgb A lot of what I'm gonna say I'm sure you already know well. As it feels you might be in a time-crunch, you're a better judge than me on these:. - Do you have a fairly complete story that you and your advisor are happy with?; - If you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:718,test,test,718,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,2,['test'],['test']
Testability,"@DanJeffries, sorry for the late reply I was traveling for a conference. Looking at the log, most of it looks like this:; ```bash; I0812 17:25:36.970339 140640080795456 make_examples_core.py:301] Task 18/20: 130000 candidates (5465 examples) [24.32s elapsed]; I0812 17:25:38.358597 140641138153280 make_examples_core.py:301] Task 17/20: 136011 candidates (5980 examples) [24.30s elapsed]; I0812 17:25:38.452311 140719761319744 haplotype_labeler.py:449] Not including more because genotype_options_product will be 157464.0, which exceeds max(=100000); I0812 17:25:39.808730 139930050549568 make_examples_core.py:301] Task 19/20: 130009 candidates (5415 examples) [19.65s elapsed]; I0812 17:25:40.161706 140719761319744 make_examples_core.py:301] Task 1/20: 130009 candidates (5656 examples) [26.55s elapsed]; I0812 17:25:39.762312 140656897058624 haplotype_labeler.py:449] Not including more because genotype_options_product will be 118098.0, which exceeds max(=100000); I0812 17:25:40.178942 139929751582528 haplotype_labeler.py:449] Not including more because genotype_options_product will be 118098.0, which exceeds max(=100000); I0812 17:25:41.815151 139685487241024 make_examples_core.py:301] Task 8/20: 134010 candidates (5603 examples) [20.32s elapsed]; I0812 17:25:42.062644 140656897058624 make_examples_core.py:301] Task 14/20: 130007 candidates (5538 examples) [26.62s elapsed]; I0812 17:25:42.944558 139916975953728 haplotype_labeler.py:449] Not including more because genotype_options_product will be 157464.0, which exceeds max(=100000); I0812 17:25:42.945389 139916975953728 haplotype_labeler.py:449] Not including more because genotype_options_product will be 944784.0, which exceeds max(=100000); I0812 17:25:42.940323 140366879631168 make_examples_core.py:301] Task 6/20: 132005 candidates (5726 examples) [25.32s elapsed]; I0812 17:25:43.504171 139929751582528 make_examples_core.py:301] Task 4/20: 132003 candidates (5481 examples) [23.80s elapsed]; I0812 17:25:43.563577 1405856798",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2334910255:88,log,log,88,,https://github.com/google/deepvariant/issues/876#issuecomment-2334910255,1,['log'],['log']
Testability,"@ErinKinghorn ,. Once you follow @danielecook 's suggestion to see if the files look good. If you in fact find the files look normal, can you please test this docker:. ```bash; docker pull google/deepvariant:CL602468145; docker pull google/deepvariant:CL602468145-gpu; ```. It seems like this issue is related to a this issue: https://github.com/google/deepvariant/issues/769",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776#issuecomment-1968012329:149,test,test,149,,https://github.com/google/deepvariant/issues/776#issuecomment-1968012329,1,['test'],['test']
Testability,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836:286,log,logs,286,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836,2,['log'],['logs']
Testability,"@Han-Cao , I ran on your BAM. My command is something like:. ```bash; docker run \; -v /home/pichuan/wgs-case-study/input/data:/input \; -v /home/pichuan/wgs-case-study/output:/output \; google/deepvariant:1.6.1 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/chm13v2.0_noY.fa.gz \; --reads=/input/HG00438.chr1_1000000_2000000.bam \; --output_vcf=/output/deepvariant.output.vcf.gz \; --output_gvcf=/output/deepvariant.output.g.vcf.gz \; --num_shards 64 \; --logging_dir=/output/logs \; --intermediate_results_dir /output/intermediate_results_dir \; --make_examples_extra_args ""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --report_title test-githubissue \; --regions chr1:1000000-2000000 \; --runtime_report; ```. Then, I checked how many variants there are:. ```; $ zcat deepvariant.output.vcf.gz | grep -v '^#' | wc -l; 4748; ```. A few thoughts and observations:. 1. Looking at IGV, it looks reasonable. (@AndrewCarroll also took a look and agreed); 2. The number of variants in this region didn't seem too unreasonable. ; 3. Your BAM is higher coverage, which could also be why more candidates have been made.; 4. I have not use CHM13 a lot myself. It is possible that CHM13 has regions that can take DeepVariant make_examples longer time to complete. If you notice specific regions being really slow (which you can use https://github.com/google/deepvariant/blob/r1.6.1/docs/runtime-by-region.md to check), please feel free to report those regions and we can take a closer look from there!; 5. For the question of whether you're running Giraffe correctly, I don't think I'm able to help answer that question. I'd encourage you to run your Giraffe+DV setting through something where you have truths for, so you can check hap.py output and see if that's expected. And, to save your time, I recommend running DeepVariant on just one chr (e.g., chr20) to check. Hope this helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847#issuecomment-2243995767:506,log,logs,506,,https://github.com/google/deepvariant/issues/847#issuecomment-2243995767,2,"['log', 'test']","['logs', 'test-githubissue']"
Testability,"@IndyHouseGuy , . Can you please do a simpler test? According to [this](https://stackoverflow.com/questions/50317119/docker-container-creating-directories-owned-by-root-i-need-them-owned-by-10001), there can be a number of things that might cause this behavior, including implicitly running docker as root in the system. In my local test, I have this behavior; ```; # Command 1; time docker run -it -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker. # Command 2; docker run -it -u `id -u`:`id -g` -v /data:/data \; google/deepvariant:0.9.0 \; mkdir /data/kishwar/test_ubuntu_docker_u; ```; Output:; ```; root root 4.0K Aug 5 14:55 test_ubuntu_docker/; shafin primarygroup 4.0K Aug 5 14:55 test_ubuntu_docker_u/; ```; Can you please run this and see if you get the same behavior?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255:46,test,test,46,,https://github.com/google/deepvariant/issues/550#issuecomment-1206552255,4,['test'],['test']
Testability,"@JakeHagen . I have one other question, do you know what the median insert size is (e.g. from the logging information of BWA)? One other possibility is that the insert sizes for this sample are different and this is interacting with the input channel for insert length. . If this is the case, then you would expect that DeepVariant 1.3 (which does not include this channel) would have less of that bimodal distribution. If you do check this and see a difference in GQ distribution, it would be good for us to know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1320638764:98,log,logging,98,,https://github.com/google/deepvariant/issues/586#issuecomment-1320638764,1,['log'],['logging']
Testability,@LogCrab Thank you for providing the files. We've tried to reproduce this in various ways without success. Could you provide the specs of the machine you ran this on? Thank you for your patience!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2077707012:1,Log,LogCrab,1,,https://github.com/google/deepvariant/issues/804#issuecomment-2077707012,1,['Log'],['LogCrab']
Testability,"@MariaNattestad thanks for the tag. Yes, we had seen this error before and @pichuan correctly identified it. If you have too many AUX tags then this error pops up. One way to test would be skip `PEPPER-Margin` entirely and run DeepVariant directly on the unphased bam and you'll see the same error. Unless WhatsHap is removing auxiliary tags, it should happen with that pipeline too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490#issuecomment-948982282:175,test,test,175,,https://github.com/google/deepvariant/issues/490#issuecomment-948982282,1,['test'],['test']
Testability,"@Npaffen Based on the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) the model has also been trained for duo-calling. In the paper, you will notice coverage results with improvements in the lower-coverage scenarios benchmarked against GIAB -- though the lowest I see is 15x. Regarding having combined results, there could be an issues regarding chromosome X for the non-PAR regions as shown in the paper, but maybe the model might already take into account GIAB labels for X and Y. To get around combined effects, you should be able to split your BAM [files by sample](https://gatk.broadinstitute.org/hc/en-us/articles/360037270511-SplitReads), and then use [samtools merge](http://www.htslib.org/doc/samtools-merge.html) to combine them together organizing them into BAM files by child, mother and father. In any case, you can see from the paper that the input channels are stacked between parent and child, which can hint at how mixing samples might impact the genotype detection from the model as described earlier.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729:255,benchmark,benchmarked,255,,https://github.com/google/deepvariant/issues/666#issuecomment-1614775729,1,['benchmark'],['benchmarked']
Testability,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253#issuecomment-567578821:353,log,logging,353,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821,1,['log'],['logging']
Testability,"@SHuang-Broad It could be disk limit issue, but I have a suspicion it might be memory related. Try to launching the free command in the background on some interval like 60 sec, before all the deepvariant ones:. free -s 60 > dv_mem_usage.txt. Then launch deepvariant in on the same machine from another terminal. Once you get the failure stop the logging from the command above. And look to see if memory has become exhausted. The above command can also be launched in parallel as part of the submitted command if that makes it easier. If that's the case, then just increase the memory requirements for the job. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960130177:346,log,logging,346,,https://github.com/google/deepvariant/issues/491#issuecomment-960130177,1,['log'],['logging']
Testability,"@Stikus Thanks for reporting the issue.; Let me confirm I understand the issue correctly-; You try to build on *Ubuntu 18.04*, but were having issue on numpy. Is that correct?. Currently, our setup was only tested on 16.04. When build-prereq.sh and run-prereq.sh was written, we did test it on 14 and 18. But over time, those settings were not regularly tested and maintained. We also didn't remove them from our scripts.; As you can see, https://github.com/google/deepvariant/blob/r1.1/Dockerfile was build on Ubuntu16.04. That said, we're also aware that [Ubuntu 16.04 will reach its end of standard support next April](https://wiki.ubuntu.com/Releases), so, we currently have an internal update that makes our standard build in Ubuntu 18.04 in future releases. I just didn't quite have time to make that the standard before v1.1. (And I also didn't test our script on Ubuntu 18.04. Sorry about that.). @Stikus Let me finish the internal testing of updating our scripts to 18.04, and I can share the new scripts with you so that you can build properly on Ubuntu18.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394#issuecomment-742111917:207,test,tested,207,,https://github.com/google/deepvariant/issues/394#issuecomment-742111917,5,['test'],"['test', 'tested', 'testing']"
Testability,@TuBieJun Try first resetting your credentials on your machine to be yours via either of the following commands:. `gcloud beta auth application-default login`. or. `gcloud auth login`,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/7#issuecomment-350527164:152,log,login,152,,https://github.com/google/deepvariant/issues/7#issuecomment-350527164,2,['log'],['login']
Testability,"@Zjianglin Can you check whether your singularity run can see the index file?. You said you have:. `singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1`. Can you try something like:. `singularity run /lustre/Data/toolsDB/deepvariant.sif ls $ref_idx` ?. By the way, can you `echo $ref_idx` to see what the value is? It should be something like `--ref=/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa`. Given that your name is `$ref_idx`, hopefully it's not the .fai file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1567721314:303,log,logx,303,,https://github.com/google/deepvariant/issues/653#issuecomment-1567721314,1,['log'],['logx']
Testability,"@akolesnikov do you have any suggestions for other commands to try? Roll back to previous version? Different parameters passed to the function? Other ways to run the Python code? I am not very familair with `Bazel` to be honest. Just confusing how the test pass, and *same exact* command works when running from pulled Docker container... . Happy to provide more details. Any ideas? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514348301:252,test,test,252,,https://github.com/google/deepvariant/issues/199#issuecomment-514348301,1,['test'],['test']
Testability,"@akolesnikov my command line for running make_examples was:. ```; python bin/make_examples.zip \; 	 --mode training \; 	 --ref ""project-retraining/testdata/sequence.fasta"" \; 	 --reads ""project-retraining/testdata/aligned_reads.bam"" \; 	 --examples ""project-retraining/training-examples/training_set.with_label.tfrecord.gz"" \; 	 --confident_regions ""project-retraining/testdata/variants.bed"" \; 	 --truth_variants ""project-retraining/testdata/variants.vcf.gz"" > ""project-retraining/logs/make_examples.log"" 2>&1; ```. and the header of the `aligned_reads.bam` file is:. ```; @HD	VN:1.3; @SQ	SN:NC_000913.3	LN:4639675	M5:05dc7a37701cdc6bcf154344a227983d; @RG	ID:343cd2783e	SM:c100278822550000001523007907041295	PU:m120131_103014_sidney_c100278822550000001523007907041295_s1_p0	PL:PacBio_RS	DT:2012-01-31T10:30:14; @PG	ID:/mnt/secondary/Smrtanalysis/opt/smrtanalysis/analysis/bin/SAMIO.py	VN:1.2.0.SF	CL:-b -o /mnt/secondary/Smrtanalysis/opt/smrtanalysis/common/jobs/034/034822/data/aligned_reads.sam -p /mnt/secondary/Smrtanalysis/opt/smrtanalysis/common/references/ecoli /mnt/secondary/Smrtanalysis/opt/smrtanalysis/common/jobs/034/034822/data/aligned_reads.cmp.h5; @CO	READS:39496; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-446206749:147,test,testdata,147,,https://github.com/google/deepvariant/issues/128#issuecomment-446206749,6,"['log', 'test']","['log', 'logs', 'testdata']"
Testability,"@anands-repo thanks for the pull request, we really appreciate community contributions! I added a comment about the approach used. . You're right that we cannot merge PRs on GitHub directly, but once the code is finalized, we can review/test internally. If everything looks good and you are ok with it, we would submit the patch first to the internal codebase. The changes would be pushed to GitHub in the next release, and we would credit you in the commit description and release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-716869195:237,test,test,237,,https://github.com/google/deepvariant/pull/365#issuecomment-716869195,1,['test'],['test']
Testability,@ardoli You will need to use Ubuntu 14 or 16 - see the following: . https://github.com/google/deepvariant/blob/59738f0ca91df3757d754e7ce6507f614816fd1c/docs/deepvariant-build-test.md,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372376833:175,test,test,175,,https://github.com/google/deepvariant/issues/6#issuecomment-372376833,1,['test'],['test']
Testability,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:179,test,test,179,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['test'],['test']
Testability,"@bcantarel Thank you for flagging this. I was able to reproduce this issue as well. While we investigate further, you can revert back to DeepVariant 1.4. I tested v1.4 and had no issues running the RNA-seq model. Note that we have only released one RNA-seq model. We'll continue to look into this and try to resolve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/766#issuecomment-1915376024:156,test,tested,156,,https://github.com/google/deepvariant/issues/766#issuecomment-1915376024,1,['test'],['tested']
Testability,"@bioinformaticaomicalabs , if you have a benchmarking bam file that you can share then I can try to run it on our default model and see if it matches your expectations. However, why training didn't work would require some more investigation on how exactly you are training and preparing your data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869#issuecomment-2315823650:41,benchmark,benchmarking,41,,https://github.com/google/deepvariant/issues/869#issuecomment-2315823650,1,['benchmark'],['benchmarking']
Testability,"@crazysummerW my intuition is that it would not make a big difference, but we have not empirically tested this. My guess is that It could make a difference in variants observed in regions with lower coverage. Here is a similar issue (although, in the context of DNA-seq): https://github.com/google/deepvariant/issues/384",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452:99,test,tested,99,,https://github.com/google/deepvariant/issues/652#issuecomment-1555394452,2,['test'],['tested']
Testability,"@dennishendriksen ,. It seems like you are using identical data for child and parent. That would create a lot of examples given all of the sites would be identical. Can you please try with HG002, HG003/HG004 duo data in your setup and see if you are still facing the same issue. You can download the data here:; ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata. curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam > input/HG002_R104_sup_merged.50x.chr20.bam; curl ${HTTPDIR}/HG002_R104_sup_merged.50x.chr20.bam.bai > input/HG002_R104_sup_merged.50x.chr20.bam.bai. curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam > input/HG003_R104_sup_merged.40x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.40x.chr20.bam.bai > input/HG003_R104_sup_merged.40x.chr20.bam.bai. curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam > input/HG004_R104_sup_merged.40x.chr20.bam; curl ${HTTPDIR}/HG004_R104_sup_merged.40x.chr20.bam.bai > input/HG004_R104_sup_merged.40x.chr20.bam.bai; ```. I would **strongly** suggest running it on a small chunk as runtime was one of the issues we faced with DeepTrio ONT mode.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724#issuecomment-1818057327:403,test,testdata,403,,https://github.com/google/deepvariant/issues/724#issuecomment-1818057327,1,['test'],['testdata']
Testability,"@depristo Thanks a lot for the clarification. ; In my case, I am trying to have a standalone version to test with, without GCP at this stage.; ; In fact, I did manually pull clif from github (commit `6c6d894a112d978bd5abfcab1052c60c5ee365a9`) before building, bypassing the chunk `note_build_stage Install CLIF binary` in `build-prereq.sh`, ; since I encountered the complaint of that OSS specific part with gsutil and clif, with this modified piece instead:; ```; export DV_PLATFORM=""ubuntu-16""; cd ..; git clone https://github.com/google/clif ; cd clif; ./INSTALL.sh; python setup.py install; sudo ldconfig # Reload shared libraries.; ```; To my understanding, the manually built clif should be of the same structure as the setup script intended. However I didnot check if this pre-built binary is actually functionally similar to my manually built one. Any chance to put this oss-prebuilt in the git repo, at least for testing ?. `OSS_CLIF_PKG=""oss_clif.${DV_PLATFORM}.latest.tgz""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351255520:104,test,test,104,,https://github.com/google/deepvariant/issues/12#issuecomment-351255520,2,['test'],"['test', 'testing']"
Testability,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351350738:486,test,tested,486,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738,2,"['log', 'test']","['logic', 'tested']"
Testability,"@depristo after rereading the 'deepvariant-build-test.md' file I am starting to think that 'gsutil' is being used to download the relevant files from GCP. Anyways, I was able to successfully build it and will start fiddling with it. Thank you for this awesome software!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-351582326:49,test,test,49,,https://github.com/google/deepvariant/issues/9#issuecomment-351582326,1,['test'],['test']
Testability,"@dkurt A quick update:. I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); I actually wonder if there's something weird with the threading code that you added to make the logging more smooth. (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected). I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-737626363:305,log,logging,305,,https://github.com/google/deepvariant/pull/363#issuecomment-737626363,1,['log'],['logging']
Testability,"@dkurt It does seem like the % of runtime reduction on WGS has been worse. 3 things have changed: ; 1. Previous number was evaluated on HG002; this time on HG003 (but the BAM is similar setting). ; 2. The second thing that has changed is your change to improve the logging.; 3. Third thing is - last time my two numbers were on the same GCE instance. This time, the baseline and the experimental numbers were from two different GCE instances (even though I did use [the same command](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get the same type). Empirically, on different GCE instances, even with the same code, I've been observing sometimes up ~10% runtime difference for call_variants. I suspect 3 is the main reason here. This can be verified if I can run the same thing with and without the use_openvino flag on the exactly same machine (sequentially). But I likely don't have time to do that again now...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735976955:265,log,logging,265,,https://github.com/google/deepvariant/pull/363#issuecomment-735976955,1,['log'],['logging']
Testability,"@dkurt Keeping them in the image is fine! I'm actually more curious about whether I can get rid of that big model.ckpt.data-00000-of-00001 file. :) . @dkurt One more question for you -- do you see any downside of enabling --use_openvino as default in our CPU run? Once this is built into our CPU docker image, it'll be nice to have it as default. I want to know if it might crash on non-Intel hardware or not. (I can also test it myself, but haven't got around to do that yet).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735612657:422,test,test,422,,https://github.com/google/deepvariant/pull/363#issuecomment-735612657,1,['test'],['test']
Testability,@drtamermansour ; Here is a simg file I just built:; https://drive.google.com/file/d/1P768MjXLa4xPmqj12UZqnmDqUZsUpbNh/view?usp=sharing; (which should be a file with the name `deepvariant.0.8.0.issue-178.simg`). I built this with the instruction here: https://github.com/google/deepvariant/issues/132#issuecomment-482430728; and tested with the steps here: https://github.com/google/deepvariant/issues/178#issuecomment-487218238. Please let me know if this works for you or not.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-503798268:329,test,tested,329,,https://github.com/google/deepvariant/issues/178#issuecomment-503798268,1,['test'],['tested']
Testability,"@drtamermansour I have not tried saving an image and running it somewhere I don't have root permission. I'll have to think about how to test that better. But before I do that, can I have you provide a few more information, such as:; What is the OS version of the super computer you're on, and what is the singularity version (`singularity --version`) on your AWS machine and your super computer?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-483498470:136,test,test,136,,https://github.com/google/deepvariant/issues/132#issuecomment-483498470,1,['test'],['test']
Testability,"@edg1983 one other possibility: was the run corresponding to `deepvar_0.9.0_oldrun.log` on different hardware than the other two runs? If you are running the other two runs on older hardware that does not support AVX2 or AVX512 instructions, runtime will be noticeably slower because the code will not be able to take advantage of TensorFlow MKL optimizations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-700196639:83,log,log,83,,https://github.com/google/deepvariant/issues/346#issuecomment-700196639,1,['log'],['log']
Testability,"@frapaport ; an update on Singularity - I've tested our latest setting (which will come out in the next release) by converting it in to a Singularity image. It seems to work fine for me. So, if you would be able to install singularity, that will be an easier way forward once our next release is out.; I'll still come back and revisit the usability of our bioconda installation. But might take a while.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480563989:45,test,tested,45,,https://github.com/google/deepvariant/issues/137#issuecomment-480563989,2,['test'],['tested']
Testability,"@githubtefo , you are providing the raw read inputs to DeepVariant? I requires reads aligned to the reference. Also, please see the logs after you run DeepVariant in the output directory.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2067311437:132,log,logs,132,,https://github.com/google/deepvariant/issues/810#issuecomment-2067311437,1,['log'],['logs']
Testability,"@gulkhan007 ,. Can you please paste the full command and the logs here so we can better understand what went wrong?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/754#issuecomment-1856840105:61,log,logs,61,,https://github.com/google/deepvariant/issues/754#issuecomment-1856840105,1,['log'],['logs']
Testability,"@gunjanbaid Thanks for your response!; I should have said that I had to fix the errors you listed in the first issue in order to run the script successfully, sorry about that. But thanks for pointing that out!. About the second issue, as you can see the BAM file is very big therefore it may take some time to generate a corrected file. Yet if this is what needs to be done I would love to get some help on how to remap the reference and generate the BAM with the `QUAL` string :). I also attach the output of running the command above in the `make_examples.log` file.; [make_examples.log](https://github.com/google/deepvariant/files/2752900/make_examples.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-453809513:558,log,log,558,,https://github.com/google/deepvariant/issues/138#issuecomment-453809513,3,['log'],['log']
Testability,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563:35,log,log,35,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563,2,['log'],['log']
Testability,"@helizabeth1103 @lucasbrambrink I'll clear up some confusion real quick here - the updated training script will only output checkpoints if tune performance outperforms existing performance. If you look closely in the log file you can see this line:. ```; I0401 03:09:48.932735 140045983049536 train.py:471] Skipping checkpoint with tune/f1_weighted=0.83932966 < previous best tune/f1_weighted=0.8400078; ```. Which states that checkpointing is being skipped because the performance was worse. So in general, if you aren't seeing checkpoints you likely need to adjust parameters or train for longer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746:217,log,log,217,,https://github.com/google/deepvariant/issues/797#issuecomment-2033443746,2,['log'],['log']
Testability,"@heznanda Glad to hear you got it working! Regarding the fix for this -- only if you are curious :) -- under the TensorFlow folder you should have a file called `.tf_configure.bazelrc`. That file should look like this (assuming your Python3 libs reside under `/usr/lib/python3/dist-packages`):. ```; build --action_env PYTHON_BIN_PATH=""/usr/local/bin/python3""; build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""; build --python_path=""/usr/local/bin/python3""; build:opt --copt=-Wno-sign-compare; build:opt --host_copt=-Wno-sign-compare; test --flaky_test_attempts=3; test --test_size_filters=small,medium; test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial; test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu; test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only; test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; ```. It will have these permissions (with your username instead of mine):. ```; -rw-rw-r-- 1 paul paul 581 Jun 7 13:49 .tf_configure.bazelrc; ```. Just add the contents above to the `.tf_configure.bazelrc` file and try building it again, assuming that for you the Python 3 `dist-packages` reside there as well. Let me know how it goes. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250:553,test,test,553,,https://github.com/google/deepvariant/issues/657#issuecomment-1584985250,10,"['benchmark', 'test']","['benchmark-test', 'test']"
Testability,"@huangl07 Does `./build-prereq.sh` complete successfully for you, as noted [here](https://github.com/google/deepvariant/blob/master/docs/deepvariant-build-test.md)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-350168328:155,test,test,155,,https://github.com/google/deepvariant/issues/6#issuecomment-350168328,1,['test'],['test']
Testability,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767302975:36,log,logs,36,,https://github.com/google/deepvariant/issues/412#issuecomment-767302975,1,['log'],['logs']
Testability,"@husamia technically yes, I believe you should be able to run via Docker on Windows, even with current version (but not with CUDA 11, as mentioned). However, this is not something we explicitly support/test against, so if you have issues running it feel free to let us know, but we may not be able to provide much support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/430#issuecomment-797712308:202,test,test,202,,https://github.com/google/deepvariant/issues/430#issuecomment-797712308,1,['test'],['test']
Testability,"@kirti141 from the log, I agree that it isn't quite clear. ; Can you tell us about your machine? How many CPU cores, RAM, what OS, etc.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925047566:19,log,log,19,,https://github.com/google/deepvariant/issues/482#issuecomment-925047566,2,['log'],['log']
Testability,"@kirti141 hm, this question (data sharing) turns out to be more complicated than I thought. I'll have to think about what's a best way for this. For now, I can try to run DV 1.2 on this large BAM file:; ```; $ gsutil ls -lh gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; 110.3 GiB 2019-02-26T18:04:41Z gs://deepvariant/case-study-testdata/HG002_NIST_150bp_50x.bam; ```; on a 64 cores CentOS machine, to see if I can reproduce the issue. I'll report back after I have a chance to run it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/482#issuecomment-925428637:252,test,testdata,252,,https://github.com/google/deepvariant/issues/482#issuecomment-925428637,2,['test'],['testdata']
Testability,"@kishwarshafin ,. What operating system are you working on?; Linux in TACC cluster (ls6). In the previous run, I was executing four separate runs on a single node, distributing them evenly across 32 threads. I did run the test data, it did finish and have generated the output files without error. Have attached the log file; [Deepvarint.txt](https://github.com/user-attachments/files/16364399/Deepvarint.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854#issuecomment-2248332964:222,test,test,222,,https://github.com/google/deepvariant/issues/854#issuecomment-2248332964,2,"['log', 'test']","['log', 'test']"
Testability,"@kishwarshafin . Sorry to let you know, but the researcher is not okay to share the BAM file due to confidentiality concerns. Quick question: where can I download the complete datasets (aligned BAM files) for HG002 and HG003 datasets for R10 ONT chemistry used for benchmarking in the study ""Local read haplotagging enables accurate long-read small variant calling""? Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854#issuecomment-2249065386:265,benchmark,benchmarking,265,,https://github.com/google/deepvariant/issues/854#issuecomment-2249065386,1,['benchmark'],['benchmarking']
Testability,"@kishwarshafin ; Thanks a lot for clarifying that, we were making a benchmark study on variant callers to optimize our pipeline. We also used PEPPER-DeepVariant and it worked with high precision. Again, thanks a lot! It was a helpful communication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619:68,benchmark,benchmark,68,,https://github.com/google/deepvariant/issues/814#issuecomment-2092651619,1,['benchmark'],['benchmark']
Testability,"@kishwarshafin,. Thank you for diving into this issue. The data was very likely derived from the [GIAB HG002 ONT Ultra-long UCSC sample](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/README_ONT-UL_UCSC_HG002.md) which base-called reads with guppy (version 3.2.5). We used minimap for alignment. I'll update our test to use different data for child and parent. Unfortunately I'm a bit pressed for time at the moment, but will do so on the first available occassion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724#issuecomment-1818289082:395,test,test,395,,https://github.com/google/deepvariant/issues/724#issuecomment-1818289082,1,['test'],['test']
Testability,"@ksw9 ; Hi, the current quickstart needs a small fix (I updated it internally; should come out in the next release):; `/home/${USER}` should be `${HOME}` to be more general.; And, note that in order for docker to access your file system, you do need the `-v` path.; So you probably want something like:. ```; OUTPUT_DIR=${HOME}/quickstart-output; mkdir -p ""${OUTPUT_DIR}""; REF=${HOME}/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta; BAM=${HOME}/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```. and ; `-v ${HOME}:${HOME} `; in the docker command",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-439585565:396,test,testdata,396,,https://github.com/google/deepvariant/issues/104#issuecomment-439585565,2,['test'],['testdata']
Testability,"@ksw9 can you post the full sequence of commands? I tried inside the docker, and it seems to be fine:. ```; [pichuan@pichuan-centos-test ~]$ sudo docker run -it gcr.io/deepvariant-docker/deepvariant:0.7.1; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:0.7.1' locally; Trying to pull repository gcr.io/deepvariant-docker/deepvariant ... ; 0.7.1: Pulling from gcr.io/deepvariant-docker/deepvariant; 18d680d61657: Pull complete ; 0addb6fece63: Pull complete ; 78e58219b215: Pull complete ; eb6959a66df2: Pull complete ; 54de1d38bbd7: Pull complete ; d17c3563217d: Pull complete ; ba1bdbdefce9: Pull complete ; 94eba53c4ad9: Pull complete ; 413f494b0501: Pull complete ; 4d89363e7fb4: Pull complete ; e9213d1ccf36: Pull complete ; fb6121657d6b: Pull complete ; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:0.7.1; root@e2fb03e85f9e:/# python -c ""import numpy""; root@e2fb03e85f9e:/# python -c ""import numpy as np""; root@e2fb03e85f9e:/# ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-438545095:132,test,test,132,,https://github.com/google/deepvariant/issues/104#issuecomment-438545095,1,['test'],['test']
Testability,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:193,test,test,193,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517,1,['test'],['test']
Testability,"@mattwood-codifiedgenomics To give you an update, I just tested with our latest code (still internal) and was able to run this successfully. We're hoping to make the next release not too long from now (~1month). . In my recent test that worked, I had to create a TPU version 2.5.0 (because that's the new TF version we're using). So, something like:. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.5.0 \; --zone=us-central1-c; ```. And then with that TPU, this worked:. ```; OUTPUT_GCS_BUCKET=<OURBUCKET>; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". time sudo docker run \; -v /home/${USER}:/home/${USER} \; deepvariant:latest \; /opt/deepvariant/bin/model_train \; --use_tpu \; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint=""""; ```. When the next release is out, it'll be great if you can try it and let us know if it works. Specifically, you'll need to specify flags like:; ```; --tpu_name=""pichuan-demo-tpu"" \; --tpu_zone=""us-central1-c"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-880959171:57,test,tested,57,,https://github.com/google/deepvariant/issues/469#issuecomment-880959171,2,['test'],"['test', 'tested']"
Testability,"@mccafj02,. Can you please send the command and logs for details? This issue is not related to GPU.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783#issuecomment-2010135314:48,log,logs,48,,https://github.com/google/deepvariant/issues/783#issuecomment-2010135314,1,['log'],['logs']
Testability,"@mclaugsf Thanks for the update! ; 1) In terms of failed jobs -- we also noticed that our current recommendation of case studies tend to mask the issues if a run failed in the middle, because we currently pipe all output to log files. We're making some changes so that if anything fails in the middle, it'll be more clear to the users later. I'm still fixing a few more things, hopefully it'll come out in the next release.; For now it's a good idea to just check the log files to make sure previous runs were successfully done before proceeding. 2) For call_variants, can you check your `call_variants.log` file and see what the lines look like?; In my run for the WGS casestudy, it converges to something like:. ```; I0815 18:49:08.438520 140611550078720 call_variants.py:359] Processed 113665 examples in 223 batches [0.222 sec per 100]; I0815 18:49:09.491303 140611550078720 call_variants.py:359] Processed 114177 examples in 224 batches [0.222 sec per 100]; I0815 18:49:10.535501 140611550078720 call_variants.py:359] Processed 114689 examples in 225 batches [0.221 sec per 100]; ```. In our case study, we recommend just running one `call_variants` per machine. `call_variants` itself does utilize multiple CPUs now, so if you use top or htop to check your run, you should see that it uses more than one CPU. In my previous experience, running multiple `call_variants` on the same machine tends to make the run slower. If you're running call_variants separately on each shard, and if you can do each of them on different machines, that's probably most ideal. But if you plan to try running multiple `call_variants` on the same machine, you might want to watch out the speed because it will likely not be linearly faster. (If you find otherwise, let me know. I haven't tried it myself for a while now)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430711053:224,log,log,224,,https://github.com/google/deepvariant/issues/105#issuecomment-430711053,6,['log'],['log']
Testability,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. ; DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:; ```; time sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; In my test run on a **t2.medium** instance, this took: ; ```; real0m23.790s; user0m0.032s; sys0m0.028s; ```; to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-866404518:1206,test,test,1206,,https://github.com/google/deepvariant/issues/462#issuecomment-866404518,1,['test'],['test']
Testability,@mvilsker The pileup_image python and C++ code here contains most of the logic for constructing the channels. https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image.py; https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image_native.h; https://github.com/google/deepvariant/blob/r1.1/deepvariant/pileup_image_native.cc. This code does the work of generating tensors for each example:. https://github.com/google/deepvariant/blob/r1.1/deepvariant/python/clif_converters.cc,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/449#issuecomment-828712929:73,log,logic,73,,https://github.com/google/deepvariant/issues/449#issuecomment-828712929,1,['log'],['logic']
Testability,@nilesh-iiita I can't immediately spot anything wrong with your command. I'd recommend looking at the `make_examples.log` file in the output directory to make sure that step completed successfully and candidates were generated. You can also set `--intermediate_results_dir=/output` to see if the `make_examples.tfrecord@8.gz` files are being created.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435#issuecomment-809953490:117,log,log,117,,https://github.com/google/deepvariant/issues/435#issuecomment-809953490,1,['log'],['log']
Testability,"@nurmians Still looking through the code - and looked at the [release notes](https://github.com/google/deepvariant/releases) - though still reasoning through it in case it might have been implemented via some alternate logic. In any case, here's a link to the script for converting genotypes of regions to haploid calls:. https://github.com/Ultimagen/VariantCalling/blob/master/ugvc/pipelines/convert_haploid_regions.py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096:219,log,logic,219,,https://github.com/google/deepvariant/issues/518#issuecomment-1691796096,1,['log'],['logic']
Testability,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:. ```Bash; root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; DeepVariant version 1.5.0; root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; root@0f98b9adcd2d:/# find /tmp; /tmp; /tmp/tmpb20xyssf; /tmp/__pycache__; /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; /tmp/tmp0y_1vxbg; root@0f98b9adcd2d:/# find | grep bazel; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; root@0f98b9adcd2d:/#; ```. I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457:779,test,testing,779,,https://github.com/google/deepvariant/issues/640#issuecomment-1550288457,4,['test'],['testing']
Testability,"@nvnieuwk Are you sure you are running inside the Docker image? I listed all the files regarding the `/tmp` folder inside the Docker container (which all must be the same for all users), and I cannot find the ones shown above:; > ; > ```shell; > root@0f98b9adcd2d:/# /opt/deepvariant/bin/run_deepvariant --version; > DeepVariant version 1.5.0; > root@0f98b9adcd2d:/# ls /tmp/Bazel.runfiles_gd__toh4/; > ls: cannot access '/tmp/Bazel.runfiles_gd__toh4/': No such file or directory; > root@0f98b9adcd2d:/# find /tmp; > /tmp; > /tmp/tmpb20xyssf; > /tmp/__pycache__; > /tmp/__pycache__/__autograph_generated_file7y1wd6da.cpython-38.pyc; > /tmp/__pycache__/__autograph_generated_fileilucivm5.cpython-38.pyc; > /tmp/tmp0y_1vxbg; > root@0f98b9adcd2d:/# find | grep bazel; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazelize_command.py; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazel_selected_py3.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/__pycache__/_bazelize_command.cpython-38.pyc; > ./usr/local/lib/python3.8/dist-packages/absl/testing/_bazel_selected_py3.py; > root@0f98b9adcd2d:/#; > ```; > ; > I remember seeing things like `/tmp/Bazel.runfiles_gd__toh4/` only when I compiled DeepVariant from scratch. It's a tricky task to get it compiled, so I just wanted to be sure you didn't mix up the session you might be running in. I've been running all my tests using a Nextflow pipeline. The command Nextflow uses to run the docker containers looks like this:; ```; docker run -i --cpu-shares 2048 --memory 3072m -u $(id -u) -e ""HOME=${HOME}"" -v /etc/passwd:/etc/passwd:ro -v /etc/shadow:/etc/shadow:ro -v /etc/group:/etc/group:ro -v $HOME:$HOME -v /tmp/tmppixeopws:/tmp/tmppixeopws -w ""$PWD"" --platform linux/x86_64 --name $NXF_BOXID docker.io/google/deepvariant:1.5.0 /bin/bash -ue /tmp/tmppixeopws/16/280c239bb924cbde08365e5fbf5788/.command.sh; ```. I've never had any problems that it would take some files from the current system wit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872:994,test,testing,994,,https://github.com/google/deepvariant/issues/640#issuecomment-1550787872,1,['test'],['testing']
Testability,"@olechnwin ,. Looking at the log, it is generating a lot of examples. Is the error-rate of the `scaffolds_FINAL.fasta` too high? Can you give a little more context on what type of genome you are running this on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578#issuecomment-1291218327:29,log,log,29,,https://github.com/google/deepvariant/issues/578#issuecomment-1291218327,1,['log'],['log']
Testability,@one-matrix details on building from source are here: https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md. Are you using Ubuntu 20.04?. Can you tell me a little bit more about what you are trying to do?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1863329822:125,test,test,125,,https://github.com/google/deepvariant/issues/756#issuecomment-1863329822,1,['test'],['test']
Testability,@oschwengers DeepVariant was included in [this bacterial variant calling benchmark](https://doi.org/10.1093%2Fgigascience%2Fgiaa007). tl;dr results were quite varied depending on the species and reference genome distance from the sample.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689:73,benchmark,benchmark,73,,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689,1,['benchmark'],['benchmark']
Testability,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:675,test,test,675,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['test'],['test']
Testability,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:643,test,test,643,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461,3,['test'],['test']
Testability,"@pichuan ; I tested the docker deepvariant:1.6 on a CPU-only machine.; And I changed tmp dir:; ```; mkdir -p output/intermediate_results_dir; mkdir -p output/tmp_dir; export TMPDIR=""$PWD/output/tmp_dir""; ```; Does this have any impact?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1801385588:13,test,tested,13,,https://github.com/google/deepvariant/issues/725#issuecomment-1801385588,1,['test'],['tested']
Testability,"@pichuan @amwenger ; Thank you very much for your response.; Perhaps I didn't express myself clearly. I performed two tests. One is HiFi reads, which is from the PacBio public revio data set. Another one is n1000.subreads.bam, which is from DeepConsensus. ; Both datasets have been tested successfully. Thank you for your help.; Have a great weekend!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780:118,test,tests,118,,https://github.com/google/deepvariant/issues/672#issuecomment-1621111780,4,['test'],"['tested', 'tests']"
Testability,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415936563:737,log,log,737,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563,2,['log'],['log']
Testability,@pichuan thanks for the work!; I can confirm that the log file is indeed much smaller now with v1.3.0.; And it's running faster.; Thank you!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-1004469728:54,log,log,54,,https://github.com/google/deepvariant/issues/491#issuecomment-1004469728,1,['log'],['log']
Testability,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736009433:73,benchmark,benchmarked,73,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433,2,"['benchmark', 'log']","['benchmarked', 'logging']"
Testability,"@pichuan, I think it's only about the speed, yes. I can benchmark it on my end and let you know if there is any benefit to use thread. That's good idea to switch to 18.04! We can also perform switch to the latest version of OpenVINO (which was not available for 16.04):. ```; sudo curl -o GPG-PUB-KEY-INTEL-OPENVINO-2021 https://apt.repos.intel.com/openvino/2021/GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo apt-key add GPG-PUB-KEY-INTEL-OPENVINO-2021; sudo echo ""deb https://apt.repos.intel.com/openvino/2021 all main"" | sudo tee - a /etc/apt/sources.list.d/intel-openvino-2021.list; sudo apt-get update; sudo apt-get install -y --no-install-recommends intel-openvino-dev-ubuntu18-2021.1.110; sudo ln -s /opt/intel/openvino_2021 /opt/intel/openvino; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393#issuecomment-742247654:56,benchmark,benchmark,56,,https://github.com/google/deepvariant/pull/393#issuecomment-742247654,1,['benchmark'],['benchmark']
Testability,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:95,benchmark,benchmark,95,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,2,"['benchmark', 'test']","['benchmark', 'testdata']"
Testability,"@pichuan, thank you for very detailed experiment! Looking forward to see whole genome results. > @dkurt One thing similar to what I observed before: call_variants with openvino seems to block at the beginning for quite a bit before it starts printing each of the logs:. Yes, this is expected due all the processing done at first iteration. It's not critical for performance but I can change it so it won't confuse users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735388012:263,log,logs,263,,https://github.com/google/deepvariant/pull/363#issuecomment-735388012,1,['log'],['logs']
Testability,"@pioneer-pi , the deepvariant manuscript from six years ago. Since then, the benchmarking methods, tools and data all has been updated. The latest version of bechmarking is GIAB v4.2.1 for HG002 that you can find [here](https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/). I would suggest reading the following manuscripts if you are interested to learn about these benchmarks:. 1) https://cell.com/cell-genomics/fulltext/S2666-979X(22)00058-1; 2) https://www.cell.com/cell-genomics/fulltext/S2666-979X-2200057-X; 3) https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1.abstract. The latest data can be found in: . https://storage.googleapis.com/brain-genomics-public/research/sequencing/fastq/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch37/. https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683:77,benchmark,benchmarking,77,,https://github.com/google/deepvariant/issues/852#issuecomment-2238167683,4,['benchmark'],"['benchmarking', 'benchmarks']"
Testability,"@pioneer-pi ,. I don't believe you can develop within the Docker environment. At least, I have not tried that successfully. You need to have a local setup where you can `modify -> build -> test` your code. I am unsure how can achieve all that within the docker container. The reason the github code is different because github is on `1.6.0` whereas you are using `1.5.0` for the docker.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1819460400:189,test,test,189,,https://github.com/google/deepvariant/issues/737#issuecomment-1819460400,1,['test'],['test']
Testability,"@pioneer-pi it looks like you are using python3 installed with miniconda (version 3.9). We currently only support Python 3.8, and we have not tested using a version of python supplied by miniconda. The `run-prereq.sh` script will install Python 3.8. I recommend trying to build without using a miniconda environment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/727#issuecomment-1802304427:142,test,tested,142,,https://github.com/google/deepvariant/issues/727#issuecomment-1802304427,1,['test'],['tested']
Testability,"@poddarharsh15, can you please check if the files were downloaded correctly and if their sizes look good. The case studies are designed in a way that you can simply copy-paste the commands and it should work. I just tested the case study and it worked on my end.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950:216,test,tested,216,,https://github.com/google/deepvariant/issues/853#issuecomment-2238236950,2,['test'],['tested']
Testability,"@rpoplin @depristo Is there a specific reason to exclude all of the super contigs in GRCh38 from deepvariant analysis? I agree with removing the HLA, EBV, M/MT and decoys, but removing the placed and unplaced contigs doesn't seem justified. Other callers do work on these regions so it doesn't seem like they are truly ""Common problematic contigs on GRCh38"" as stated in the release notes for v0.5.0. Does it affect training or calling if they were included? I'd suggest the user should be able to provide the list of regions to exclude or include as its taken a bit of work to sort out why deepvariant is not making calls on all contigs in the provided BAM files like a number of other callers we are testing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/37#issuecomment-529713917:702,test,testing,702,,https://github.com/google/deepvariant/issues/37#issuecomment-529713917,1,['test'],['testing']
Testability,@saliksyed the example on the Cloud runner page is actually a b37 genome:. ```; --ref gs://deepvariant/performance-testdata/hs37d5.fa.gz \; ```. See if that works for you? It should also already come with the corresponding indexed file and ready to go.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437426660:115,test,testdata,115,,https://github.com/google/deepvariant/issues/116#issuecomment-437426660,1,['test'],['testdata']
