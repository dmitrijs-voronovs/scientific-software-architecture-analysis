quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,Replace no-longer-available tutum/curl for CI curl test [CROM-6757],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6360:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6360,1,['avail'],['available']
Availability,Replaced use of Future.foreach with error handling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500,1,['error'],['error']
Availability,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1442:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442,1,['failure'],['failure']
Availability,Report causal exceptions during Google auth failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2090:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2090,1,['failure'],['failures']
Availability,Report causal exceptions during Google auth failures. Closes #2090,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2091:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2091,1,['failure'],['failures']
Availability,Report invalid string along with error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4161:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4161,1,['error'],['error']
Availability,Report-and-Retry Travis centaur errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:32,error,errors,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['errors']
Availability,"Reported by @asmirnov239. Cromwell 30.2 doesn't seem to short-circuit for logical operators so that the following fails to evaluate when `optional_int` is undefined:. ```wdl ; Integer? optional_int; if (defined(optional_int) && select_first([optional_int]) == 2 ) {; #expresssions to do if optional_int is defined and equal to 2; }; ```. expected behavior: if `optional_int` is undefined, if statement evaluates to false. ; actual behavior: cromwell fails and stops with the error: . >Evaluating defined(gc_low_high_filter_params) && select_first([gc_low_high_filter_params]) == 2 failed: select_first failed. All provided values were empty. While it hasn't been tested, I presume that cromwell doesn't short-circut for the `||` operator...but it should!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3384:475,error,error,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384,1,['error'],['error']
Availability,Reported by @fleharty - it appears that when call caching in symlink mode if the real file no long exists the caching will not fail. . Regardless of the mode we should be robust to cases where we're attempting to call cache but the underlying data is not there,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1671:171,robust,robust,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1671,1,['robust'],['robust']
Availability,Resolve downstream Dependencies 0.22 closes #1690,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1697:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1697,1,['down'],['downstream']
Availability,Resolve downstream Dependencies develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1698:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1698,1,['down'],['downstream']
Availability,Resolve downstream dependencies 0.21,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1696:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1696,1,['down'],['downstream']
Availability,"Resolves (or at least provides resolution to) one class of ""my workflow never completes"" problems. We will *hopefully* only ever see this error during development, but it seems like a useful backstop condition to enforce in case it ever does happen to live workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486:138,error,error,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486,1,['error'],['error']
Availability,Resolves one of the 2 major error modes I've been tracking,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/130:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/130,1,['error'],['error']
Availability,Restart / Recover should not kick off until services have initialized and Liquibase has run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196:10,Recover,Recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196,1,['Recover'],['Recover']
Availability,Restart/recover migration. Closes #1119,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340:8,recover,recover,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340,1,['recover'],['recover']
Availability,"Retry Call Caching, JES Error 404",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/517:24,Error,Error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/517,1,['Error'],['Error']
Availability,Retry HTTP 408 errors during DOS/DRS resolution BT-41,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6059:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6059,2,['error'],['errors']
Availability,Retry PAPI backend creation failures [CROM-6791],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6487:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487,1,['failure'],['failures']
Availability,Retry Travis downloads,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791:13,down,downloads,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791,1,['down'],['downloads']
Availability,Retry on PAPI v1 error code 10.14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4363:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4363,1,['error'],['error']
Availability,Retry on error code 13 if preemptible,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/478:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/478,1,['error'],['error']
Availability,Retry on failures to get access token,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4272:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4272,1,['failure'],['failures']
Availability,Returns 404 errors on JES API calls. Likely due to new API?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/591:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/591,1,['error'],['errors']
Availability,Revert changes introducing workflow pickup failures [BW-1128],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6697:43,failure,failures,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6697,1,['failure'],['failures']
Availability,Review note: ; ---. May be best to review the docs in their rendered form in case there are markdown errors that aren't obvious in the MD files:. * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/executionStore/; * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/valueStore/; * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6054:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6054,1,['error'],['errors']
Availability,Reword error message when failure is due to an invalid return code from the script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2740:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2740,2,"['error', 'failure']","['error', 'failure']"
Availability,Rewrite error message for type coersion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998,1,['error'],['error']
Availability,"Right now, only images for the `amd64` architecture are available on DockerHub. Given that Docker is well-supported on ASi/ARM chips, it would be helpful to have a native `arm64` image automatically built, rather than needing to run a non-native image unless I build it myself.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107:56,avail,available,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107,1,['avail'],['available']
Availability,Robust METADATA_VALUE embiggening. Closes #1607.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1609:0,Robust,Robust,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609,1,['Robust'],['Robust']
Availability,Robustify Centaur tests to a restarting Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2352:0,Robust,Robustify,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2352,1,['Robust'],['Robustify']
Availability,Robustify aborts to PRECONDITION_FAILED [BT-450] [CROM-6829],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6568:0,Robust,Robustify,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6568,1,['Robust'],['Robustify']
Availability,Robustity of workflowTiming diagrams improved,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/380:0,Robust,Robustity,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/380,1,['Robust'],['Robustity']
Availability,Rollback underscores [24_hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1902:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1902,1,['Rollback'],['Rollback']
Availability,Rollback workbench-util to 0.6-65bba14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6481:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6481,1,['Rollback'],['Rollback']
Availability,Root cause is absolutely our old friend `CromwellTestKitSpec`. Jenkins job # 1778; Jenkins job # 1838. Jenkins job # 2607 w/ the following .... ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3457535841 minutes. Last failure message: Submitted did not equal Succeeded.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdlAndAssertOutputs(CromwellTestKitSpec.scala:344); at cromwell.WorkflowOutputsSpec.$anonfun$new$4(WorkflowOutputsSpec.scala:38); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:314,failure,failure,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['failure'],['failure']
Availability,"Run @ruchim 's Travis test in a different mode, or branch, that shuts down and brings back up. Does a count right after to check that the right number of jobs are recovered (no duplicates). TO DO:; - [ ] Make sure you have as many JES jobs as you think you have; - [ ] If not, fix it!; - [ ] If so, yay!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,2,"['down', 'recover']","['down', 'recovered']"
Availability,Run in local with docker - error waiting for container read: operation timed out,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,1,['error'],['error']
Availability,Run mode: executing WDL from directory relative to current fails with import resolver error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4052:86,error,error,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4052,1,['error'],['error']
Availability,"Runnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshaki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:2041,error,error,2041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,1,['error'],['error']
Availability,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158:199,echo,echo,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158,4,['echo'],['echo']
Availability,"Running Centaur has several ""noop"" tasks. When passed to AWS Batch, we see failing tests and errors from the AWS SDK that the Command field cannot have empty strings. Assuming Cromwell expects no output/return code 0, this can be special-cased in AWSBatchJob.scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3737:93,error,errors,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737,1,['error'],['errors']
Availability,"Running a CWL workflow with local relative imports seems to fail. This is possibly related to #4308 but unsure. . Running with cromwell 36. . Process looks like the following; ```; $ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; $ cd dockstore-workflow-md5sum-unified; $ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; <snip>; Final process status is success; $ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; $ java -jar cromwell-36.jar run https://raw.githubusercontent.com/dockstore-testing/dockstore-workflow-md5sum-unified/develop/checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; <snip>; [2018-11-07 14:34:25,13] [info] Pre-Processing /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl; [2018-11-07 14:34:34,94] [error] WorkflowManagerActor Workflow a3cb6a14-3672-4132-8a24-2e0a4e66ff96 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.V",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366:514,down,download,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366,2,"['down', 'error']","['download', 'error']"
Availability,"Running in server mode, jobs that have localization error become immortal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['error'],['error']
Availability,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['error'],['error']
Availability,"Running the AWS backend with the latest develop Cromwell gives errors like this:. java.lang.Exception: Job 2de677d8-0842-4e17-ab26-288ffc3d8aaa failed for reason: unknown error: . {JobName: cromwell-job,JobId: 2de677d8-0842-4e17-ab26-288ffc3d8aaa,JobQueue: arn:aws:batch:us-east-1:369228243869:job-queue/mcovarr-queue-nouveau,Status: FAILED,StatusReason: **Container.image contains invalid characters.**,CreatedAt: 1488362254138,DependsOn: [],JobDefinition: arn:aws:batch:us-east-1:369228243869:job-definition/cromwell-job-definition:125,Parameters: {},Container: {**Image: library/python@sha256:d23845e4757f13266b42877c25b845e455127b85ec12e5d551bec5d8162e7cd4**,Vcpus: 1,Memory: 1907,Command: [/bin/sh, -c, /bin/bash /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/script > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stdout 2> /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stderr < /dev/null || echo -1 > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/rc],Volumes: [{Host: {SourcePath: /usr/share/iodir},Name: cromwell-volume}],Environment: [],MountPoints: [{ContainerPath: /usr/share/iodir,ReadOnly: false,SourceVolume: cromwell-volume}],Ulimits: [],}}. For this reason found in the ECS javadocs:. ```; Amazon ECS task definitions currently only support tags as image identifiers within a specified repository; (and not <code>sha256</code> digests); ```. Of course it would be preferable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2044:63,error,errors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2044,2,['error'],"['error', 'errors']"
Availability,"Running the `cd` command anywhere in the command block causes output files to not be found and for the workflow to throw an error:. [2018-01-08 15:05:21,44] [error] WorkflowManagerActor Workflow 6575132f-3c01-498f-ac7a-2a418568f002 failed (during ExecutingWorkflowState): Could not process output, file not found: /Users/jonn/Development/cromwell/cromwell-executions/Funcotator/6575132f-3c01-498f-ac7a-2a418568f002/call-MakeItFunky/execution/out.vcf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3116:124,error,error,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116,2,['error'],['error']
Availability,"Running this WDL (with a wider scatter count... start with 250) fails with a database persistence error like:. > The size of BLOB/TEXT data inserted in one transaction is greater than 10% of redo log size. Increase the redo log size using innodb_log_file_size. On Google, the redo size is fixed at 536,870,912 bytes, which means the maximum BLOB/TEXT size is ~53mb. ```; task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done; >>>; runtime {; docker: ""ubuntu:latest""; memory: ""3 GB""; cpu: ""1""; disks: ""local-disk 5 HDD""; preemptible: 3; }; output {; Array[File] outer = glob(""${sample_name}_${index}_*""); Array[File] inner = glob(""${sample_name}_${index}/*""); }; }. task MatrixRotation {; Array[Array[String]] input_matrix. command <<<; python <<CODE; import csv; import sys; with open('${write_tsv(input_matrix)}') as tsv_in:; input_matrix = [line.strip().split('\t') for line in tsv_in]; final_matrix = [["""" for x in range(len(input_matrix))] for y in range(len(input_matrix[0]))]; for x in range(len(input_matrix)):; for y in range(len(input_matrix[0])):; final_matrix[y][x] = input_matrix[x][y]; with open(""output.tsv"", ""w"") as tsv_out:; # lineterminator is a workaround for a cromwell bug that doesnt allow for '\r\n' line endings which this outputs by default; writer = csv.writer(tsv_out, delimiter='\t', lineterminator='\n'); writer.writerows(final_matrix); CODE; >>>. runtime {; docker: ""python:2.7""; memory: ""1 GB""; preemptible: 3; }. output {; File out = ""output.tsv""; Array[Array[String]] output_matrix = read_tsv(out); }; }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, 57, 58, 59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910,3,"['echo', 'error']","['echo', 'error']"
Availability,"Running this WDL (with a wider scatter count... start with 250) fails with a database persistence error like:. > The size of BLOB/TEXT data inserted in one transaction is greater than 10% of redo log size. Increase the redo log size using innodb_log_file_size. Which we can't do with CloudSQL on Google. task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done. > > > runtime {; > > > docker: ""ubuntu:latest""; > > > memory: ""3 GB""; > > > cpu: ""1""; > > > disks: ""local-disk 5 HDD""; > > > preemptible: 3; > > > }; > > > output {; > > > Array[File] outer = glob(""${sample_name}_${index}__""); > > > Array[File] inner = glob(""${sample_name}_${index}/_""); > > > }; > > > }. task MatrixRotation {; Array[Array[String]] input_matrix. ```; command <<<; python <<CODE; import csv; import sys; with open('${write_tsv(input_matrix)}') as tsv_in:; input_matrix = [line.strip().split('\t') for line in tsv_in]; final_matrix = [["""" for x in range(len(input_matrix))] for y in range(len(input_matrix[0]))]; for x in range(len(input_matrix)):; for y in range(len(input_matrix[0])):; final_matrix[y][x] = input_matrix[x][y]; with open(""output.tsv"", ""w"") as tsv_out:; # lineterminator is a workaround for a cromwell bug that doesnt allow for '\r\n' line endings which this outputs by default; writer = csv.writer(tsv_out, delimiter='\t', lineterminator='\n'); writer.writerows(final_matrix); CODE; >>>. runtime {; docker: ""python:2.7""; memory: ""1 GB""; preemptible: 3; }. output {; File out = ""output.tsv""; Array[Array[String]] output_matrix = read_tsv(out); }; ```. }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/911:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/911,3,"['echo', 'error']","['echo', 'error']"
Availability,"Running with release 24. As a minimal example, the following WDL:; ```; import ""foo.wdl"" as missing. workflow testMe {; 	call doNothing; }; task doNothing {; 	command {}; }; ```. and a zip file containing `baz.wdl` but not `foo.wdl`:; ```; [conradL@qimr13054 ~]$ unzip -l bar.zip ; Archive: bar.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 13:47 bar/; 0 02-07-2017 13:47 bar/baz.wdl; --------- -------; 0 2 files; ```. submit to server:; ```; [conradL@qimr13054 ~]$ curl http://localhost:8000/api/workflows/V1 -FwdlSource=@badImport.wdl -FwdlDependencies=@bar.zip; {; ""id"": ""b701aafd-445c-4f49-8ba6-452d56e69fd3"",; ""status"": ""Submitted""; }; ```. causes the server process to die with this in the logs:; ```; 2017-02-07 13:52:41,842 cromwell-system-akka.actor.default-dispatcher-33 ERROR - guardian failed, shutting down system; wdl4s.exception.ValidationException: Failed to import workflow foo.wdl.:; File not found /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$tryResolve$1(WdlNamespace.scala:198); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:810,ERROR,ERROR,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,Running workflows should heartbeat to workflow store,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3341:25,heartbeat,heartbeat,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341,1,['heartbeat'],['heartbeat']
Availability,S3 File object as task output error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6716:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6716,1,['error'],['error']
Availability,S3 filesystem retry download,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946,1,['down'],['download']
Availability,S3 permissions errors using Cromwell 37+,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,1,['error'],['errors']
Availability,S3 permissions errors using Cromwell 37+ BA-4740 (#4740),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088,1,['error'],['errors']
Availability,SFS job recovery. Closes #1162 Closes #666,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319,1,['recover'],['recovery']
Availability,"SGE defaults to csh, and at least in my environment, even with #!/bin/bash bash was not launched in the job. This could well just be a quirk of my environment, and I am happy if you decide to not take this change. However it seems like a more generally robust default for different environments.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3489:253,robust,robust,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3489,1,['robust'],['robust']
Availability,SGE resource 'cpu' binding error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['error'],['error']
Availability,"SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using the AWS CLI as follows, it works fine. ```; aws batch register-job-definition --cli-input-json file://event_history.json; {; ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator:207"",; ""jobDefinitionName"": ""PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator"",; ""revision"": 207; }; ```. Each run of this workflow creates many new revisions of this jo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:2028,Error,Error,2028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['Error'],['Error']
Availability,"Sam is now up and accessible. Complete the wiring process for CromIAM to be able to talk to a live Sam. @cjllanwarne Can you fill in the blanks on what this entails?. (EDIT below by cjllanwarne):. - Have a look at the `handleRequest` method in `SamActor`. It's called by the actor's `receive`.; - Right now, the placeholders complete `Promises` after a set time. You'll want to message SAM, wait for a response, and then complete them more appropriately.; - NB in case you're wondering, the `Promise/Future` stuff is because the CromIamApiService is not an actor and so cannot receive responses in the usual actor-y way. Feel free to make it better, I ended up doing this because the actor-y way was adding in enormous/unnecessary overheads in terms of boilerplate... YMMV.; - FWIW even if `CromIamApiService` isn't an actor, I still suspect `SamActor` will end up spinning up individual worker actors. It's just the interface back to the service which isn't messages.; - The SamActor is parameterized with the `userIdHeader`. SamActorRequests will both have a `userIdtoken` included (oops, `RegisterWorkflow` calls it `user` but it's the same thing). When you make your request to same, you'll need to add the appropriate header (called whatever the actor was parameterized with) with the user token's value.; - Success or Failure completions, everything else *should* be wired up correctly already. You might want to double check that (especially for SAM rejections). If the intrepid person picking up this ticket is not me (@cjllanwarne), I'm happy to make this make more sense in person, if required.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2469:1324,Failure,Failure,1324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2469,1,['Failure'],['Failure']
Availability,"Same symptom as https://github.com/broadinstitute/cromwell/pull/7385, different cause. This is @sjfleming's report in the ticket. After:; ```; INFO - WorkflowManagerActor: Workflow b23e299b-5fbd-4b12-8389-9aa73321fba1 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'break_with_stderr.load_data_csv' (reason 1 of 2): Evaluating select_first([stdout(), stderr()]) failed: stdout is not implemented at the workflow level, Failed to evaluate 'break_with_stderr.load_data_csv' (reason 2 of 2): Evaluating select_first([stdout(), stderr()]) failed: stderr is not implemented at the workflow level; INFO - WorkflowManagerActor: Workflow actor for b23e299b-5fbd-4b12-8389-9aa73321fba1 completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""b23e299b-5fbd-4b12-8389-9aa73321fba1""; }; ```. Before:; ```; ERROR - stdout is not implemented at the workflow level; java.lang.UnsupportedOperationException: stdout is not implemented at the workflow level; 	at cromwell.core.io.WorkflowCorePathFunctionSet.fail(CorePathFunctionSet.scala:12); 	at cromwell.core.io.WorkflowCorePathFunctionSet.stdout(CorePathFunctionSet.scala:20); 	at wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators$$anon$1.evaluateValue(EngineFunctionEvaluators.scala:54); 	at wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators$$anon$1.evaluateValue(EngineFunctionEvaluators.scala:48); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:75); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7386:946,ERROR,ERROR,946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7386,1,['ERROR'],['ERROR']
Availability,"Scattered workflow finished without error, but the output file is empty in the bucket",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006,1,['error'],['error']
Availability,See #2970 for the current error case which we now think should be treated as preemptible.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2978:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2978,1,['error'],['error']
Availability,"See [this forum post](http://gatkforums.broadinstitute.org/firecloud/discussion/9851/cryptic-failure-messages). >message: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@1fc7758d rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@39871354[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 1543924]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381:93,failure,failure-messages,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381,1,['failure'],['failure-messages']
Availability,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:254,ERROR,ERROR,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"See below for motivating use case WDL (now a test). The throwaways need OGINs to reference the `i`, but we weren't recording the OGINs that they were making. The upshot was that multiple usages of the same variable in a nested scope was producing multiple OGINs for the same value, and that was causing the ""duplicate FQN"" errors to trigger. ```; workflow nested_lookups {; Int i = 27; if(true) {; Int? throwaway = i # Make sure this 'i' OGIN doesn't duplicate the nested m1's 'i' OGIN; Int? throwaway2 = i # Make sure this 'i' OGIN doesn't duplicate the nested m1's 'i' OGIN; if(true) {; if(true) {; call mirror as m1 { input: i = i}; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2855:323,error,errors,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2855,1,['error'],['errors']
Availability,"See https://github.com/broadinstitute/cromwell/issues/1736. It looks like there's been a regression in the versioning. The last jar was named ""cromwell-24.jar"" (https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar) and according to #1736 that was intentional. The new jar is named ""cromwell-0.25.jar"" not ""cromwell-25.jar"" (https://github.com/broadinstitute/cromwell/releases/download/25/cromwell-0.25.jar). Is this a mistake? Shouldn't the file be named cromwell-25.jar?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2052:213,down,download,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2052,2,['down'],['download']
Availability,"See https://github.com/broadinstitute/cromwell/pull/4022 - originally closed because I wasn't sure it was the right thing, and I didn't see a way to test it. I'm now more certain it's worth trying. Even if this doesn't actually fix the deadlock and we need to use one of the other solutions, I think this may still be worth having because it will save the RDBMS a lot of work coordinating transactions. [`autocommit` documentation](https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html); [Slick transactions and pinned sessions](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions). Acceptance criteria:; - [ ] Not a performance regression - heartbeats are still efficiently written to the DB in batches; - [ ] Prevents the deadlock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249:497,rollback,rollback,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249,2,"['heartbeat', 'rollback']","['heartbeats', 'rollback']"
Availability,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3665:86,avail,available,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665,2,['avail'],['available']
Availability,"Seeing issues around OWL parsing like #4210, may need to retry more. . ```text; Error loading schemas: Problem parsing file:./cwl/src/test/resources/cwl/ontology/EDAM.owl Could not parse ontology. Either a suitable parser could not be found, or parsing failed. See parser logs below for explanation. The following parsers were tried: 1) org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser@35c1a1aa 2) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser@4e879e3e 3) org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser@6bd28ad6 4) org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser@5326aaf7 Detailed logs: -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser@35c1a1aa Stack trace: Encountered '<?xml version=""1.0""?>' at line 1 column 1. Expected either 'Ontology:' or 'Prefix:' (Line 1) org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser.parse(ManchesterOWLSyntaxOntologyParser.java:81) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.rdf.turtle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:80,Error,Error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['Error'],['Error']
Availability,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3827:35,robust,robustness,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827,2,"['failure', 'robust']","['failures', 'robustness']"
Availability,"Seen in [Jenkins build 577](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/) but also in a couple of my branches, where I seem to be able to reliably trigger it w/ some seemingly unrelated changes. . Sometimes they manifest as timeouts, in other cases the [wrong data is coming back](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/testReport/junit/cromwell.webservice/MetadataBuilderActorSpec/MetadataParser_should_support_nested_lists/). In my branches I'm reliably able to get `should build workflow scope tree from metadata events` fail by simply changing the package of `CromwellApiServiceSpec` (see branch `jg_hmm`). That error is not in this jenkins run, but I've seen those other failures in some of my other experiments (see branch `jg_refactor_reality` although that's just a series of me making strange edits to see what happens)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4288:173,reliab,reliably,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4288,4,"['error', 'failure', 'reliab']","['error', 'failures', 'reliably']"
Availability,"Seen on Cromwell 26-c14e64d. Reported to us as GAWB-1867. Here's a WDL:. ```; task HelloTask; 	{; 	File InFile; 	command; 		{; 		echo ""A file is here !"" ;; 		echo ""Here is the ls"" ;; 		ls -alh * ; 		ls -alht ${InFile} ; 		head ${InFile}; 		}; 	output; 		{; 		}; 	meta {author : ""Eddie Salinas""}; 	runtime { docker: ""eddiebroad/public_test_dsdeepb_2332"" }; 	}; 	; workflow HelloWorkflow; 	{; 	File InFile; 	call HelloTask; 		 {; 		input:InFile=InFile; 		}; 	}; ```. If I accidentally pass an empty string to the input, the job stays running indefinitely:. ```; {; ""workflowName"": ""HelloWorkflow"",; ""submittedFiles"": {; ""inputs"": ""{\""HelloWorkflow.InFile\"":\""\""}"",; ""workflow"": ""task HelloTask\n\t{\n\tFile InFile\n\tcommand\n\t\t{\n\t\techo \""A file is here !\"" ;\n\t\techo \""Here is the ls\"" ;\n\t\tls -alh * \n\t\tls -alht ${InFile} \n\t\thead ${InFile}\n\t\t}\n\toutput\n\t\t{\n\t\t}\n \tmeta {author : \""Eddie Salinas\""}\n\truntime { docker: \""eddiebroad/public_test_dsdeepb_2332\"" }\n\t}\n\t\nworkflow HelloWorkflow\n\t{\n\tFile InFile\n\tcall HelloTask\n\t\t {\n\t\tinput:InFile=InFile\n\t\t}\n\t}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c/workflow.logs\"",\n \""account_name\"": \""obamacloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c\"",\n \""read_from_cache\"": true\n}""; },; ""calls"": {; ""HelloWorkflow.HelloTask"": [{; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c/HelloWorkflow/f5c59dc3-fb98-42d9-9dbf-e9305d8e8793/call-HelloTask/HelloTask-stdout.log"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2278:129,echo,echo,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2278,2,['echo'],['echo']
Availability,Send next workflow heartbeat after it's been processed,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3275:19,heartbeat,heartbeat,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3275,1,['heartbeat'],['heartbeat']
Availability,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4055:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055,1,['failure'],['failures']
Availability,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4000:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000,1,['error'],['error']
Availability,Shard's are also in the path. Also the zero `0` should be included in the attempt and shard regexes. In case the number of attempts is higher than 9.; I found this error while running the dev version of cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5456:164,error,error,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456,1,['error'],['error']
Availability,Should address the cron failures by setting the maximum freeze scan interval to be less than the Carbonite-flavored Centaur's limit of patience.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5272:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5272,1,['failure'],['failures']
Availability,"Should help with debugging occasional failures like this in travis by replacing ""not found"" with ""this looks similar"":. ```; StatsDInstrumentationServiceActorSpec:; StatsDInstrumentationServiceActor; - should increment counters (1 second, 193 milliseconds); - should add count (1 second, 9 milliseconds); - should set gauges (1 second, 10 milliseconds); info- should set timings *** FAILED *** (3 seconds, 701 milliseconds); info Missing packet: prefix_value.cromwell.test_prefix.test.metric.bucket.timing.stddev:0.00|g (StatsDInstrumentationServiceActorSpec.scala:83); info org.scalatest.exceptions.TestFailedException:; info ...[0m[0m; info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4(StatsDInstrumentationServiceActorSpec.scala:83); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4$adapted(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.collection.immutable.Set$Set4.foreach(Set.scala:206); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$2(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); info at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); info at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); info ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4387:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4387,1,['failure'],['failures']
Availability,Shut down if any write to the Database fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4761:5,down,down,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4761,1,['down'],['down']
Availability,Shut down on heartbeat staleness [BT-589],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6696:5,down,down,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6696,2,"['down', 'heartbeat']","['down', 'heartbeat']"
Availability,Silence error messages on shutdown,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620,1,['error'],['error']
Availability,"Similar to #1820 -- Talk with FireCloud Analysis PO ( @abaumann ) to specifically see if we can gather workflow failure cases for Chet's FireCloud 1000 sample run which had several non-user failures. Incorporate these findings into the same meeting as #1820 . With those errors, attempt to quantify how often this happens in FireCloud by mining the FireCloud Cromwell DBs (e.g. how often does a job fail with JES error code X). Use good judgement to strike a balance of effort vs value here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1821:112,failure,failure,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1821,4,"['error', 'failure']","['error', 'errors', 'failure', 'failures']"
Availability,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4001:690,failure,failure,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001,1,['failure'],['failure']
Availability,Simplification of #5415 removing the `attempt-1` redirection. ~Starting out with Don't Look At Me because I anticipate there might be some test fixup before it's ready for prime time.~ Now available for looking at.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429:189,avail,available,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429,1,['avail'],['available']
Availability,"Since read_json and write_json are not implemented yet, I'm documenting for my team which types we can serialize and de-serialize using the available library functions. I wrote a pair of WDLs, one to write data to files and another to read data from files:. [test_read_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901057/test_read_data.wdl.txt); [test_write_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901056/test_write_data.wdl.txt). When I tested these I noticed that I can read and write a single Boolean value, and I can write complex types with Booleans, but reading Array[Boolean] didn't work and neither did reading any Map[Boolean,?] or Map[?,Boolean]. . **What is the best way to read in complex types that involve Boolean?**. The error I got when trying to read Array[Boolean] looks like this:. > Error: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. The errors I would get for the complex types involving Boolean look like this:. > Error: Failed to coerce one or more keys or values for creating a Map[String, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. > Error: Failed to coerce one or more keys or values for creating a Map[Int, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. Here are the files generated by test_write_data.wdl that I can read back in with test_read_data.wdl:. [a_false.txt](https://github.com/broadinstitute/cromwell/files/901058/a_false.txt); [a_float.txt](https://github.com/broadinstitute/cromwell/files/901061/a_float.txt); [a_string.txt](https://github.com/broadinstitute/cromwell/files/901063/a_string.txt); [a_true.txt](https://github.com/broadinstitute/cromwell/files/901062/a_true.txt); [a_zero.txt](https://github.com/broadinstitute/cromwell/files/901059/a_zero.t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152:140,avail,available,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152,4,"['Error', 'avail', 'error']","['Error', 'available', 'error', 'errors']"
Availability,"Since this is fairly backend-specific I've implemented as a `cromwell.backend.google.pipelines.common.api.RunStatus` instead of a `cromwell.core.ExecutionStatus`. It also reduces the scope of changes. Feedback welcome. Looks like this in metadata requests:; ```; ""calls"": {; ""sleepy_sleep.sleep"": [; {; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/sleepy_sleep/058bff35-4a55-4c0f-9113-0885f4119cd9/call-sleep/stdout"",; ""backendStatus"": ""AwaitingCloudQuota"",; ""compressedDockerSize"": 28566425,; ""commandLine"": ""sleep 180;\nls -la"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci"",; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""googleProject"": ""broad-dsde-cromwell-dev""; },; ```; The Terra UI workflow dashboard currently makes calls that look like; ```; https://rawls.dsde-dev.broadinstitute.org/api/workspaces/general-dev-billing-account/anichols-post-ppw/submissions/a7cfb487-9c30-4fd7-b10f-19d6f3c1d192/workflows/f52e4e4b-0299-44b7-a16f-904d2ee3f1e9; ?includeKey=end; &includeKey=executionStatus; &includeKey=failures; &includeKey=start; &includeKey=status; &includeKey=submittedFiles:workflow; &includeKey=workflowLog; &includeKey=workflowName; &includeKey=callCaching:result; &includeKey=callCaching:effectiveCallCachingMode; ```; so it would be easy to add a `&includeKey=backendStatus` and logic to evaluate something like; ```; if (executionStatus == ""Running"" && backendStatus == ""AwaitingCloudQuota"") {; msg = ""Trying to run but needs quota""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6655:1166,failure,failures,1166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655,1,['failure'],['failures']
Availability,Since version 32 Womtool validate give this error on wdl file without a workflow: ; `Namespace does not have a local workflow to run`. Is this required by spec? Would be better to still accept this for reusable tasks like this: https://github.com/biowdl/tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762,1,['error'],['error']
Availability,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1141:889,heartbeat,heartbeat,889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141,2,"['echo', 'heartbeat']","['echoing', 'heartbeat']"
Availability,Slurm squeue check-alive method is not robust to failures of communication with slurm controller,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5400:19,alive,alive,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400,3,"['alive', 'failure', 'robust']","['alive', 'failures', 'robust']"
Availability,"Small but annoying bug in WDL 1.0 support identified on the openWDL slack channel. The evaluation logic was all present and correct, only the type evaluators were not set up to determine the types of the expressions correctly, and returning errors inappropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5598:241,error,errors,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5598,1,['error'],['errors']
Availability,"So I've got a workflow with tasks A, B, C, D. The outputs of the workflow are the outputs of the tasks. I ran the workflow on a bunch of samples, and because of an error in one of my tools, the task D failed one some (but not all) samples: 6/8 workflows succeeded, 2/8 failed on task D. FireCloud writes workflow outputs back to its data model when the workflow completes. This means that for 2/8 of my samples, _none_ of the intermediates were written back to the data model. It would be nice if FireCloud could say ""this workflow failed, but this subset of workflow outputs were successfully computed, so I'll write those back for you."" However, FireCloud can't do this, because calling Cromwell's `/outputs` endpoint only provides the list of workflow outputs if the workflow succeeds. If the workflow fails or is still running, it returns `""outputs"" : {}`. Could Cromwell perhaps determine ""this workflow output has been computed to its final state, and thus I can tell you about it even though the workflow hasn't completed [successfully]""?. Note that the Swagger documentation for the `/outputs` endpoint says:. > Retrieve the outputs for the specified workflow. Cromwell will return any outputs which currently exist even if a workflow has not successfully completed. This does not seem to be the case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4139:164,error,error,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4139,1,['error'],['error']
Availability,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:489,avail,available,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,1,['avail'],['available']
Availability,"So given the following directory structure : ; ```; base; ├── imported_workflow; │   ├── wdl_tasks; │   │   ├── taska.wdl; │   │   └── taskb.wdl; │   └── workflow_b.wdl; ├── task_wdls; │   ├── task1.wdl; │   └── task2.wdl; └── workflow.wdl; ```. In `workflow.wdl` I have; ```WDL; import ""task_wdls/task1.wdl"" as task1; import ""task_wdls/task2.wdl"" as task2; import ""imported_workflow/workflow_b.wdl"" as workflow_b; ```. In `imported_workflow/worflow_b.wdl` I have; ```WDL; import ""wdl_tasks/taska.wdl"" as taska; import ""wdl_tasks/taskb.wdl"" as taskb; ```. If I run cromwell from the directory `base` and run `workflow.wdl`. I will stumble upon an error. It will try to import `wdl_tasks/taska.wdl` from the base directory instead of from the `imported_workflow` directory where `workflow_b.wdl` is located. ```; Failed to import workflow wdl_tasks/taska.wdl.:; File not found /home/ruben/test/base/wdl_tasks/taska.wdl; ```. This has also been discussed at: https://gatkforums.broadinstitute.org/wdl/discussion/11330/how-to-import-workflows-that-also-have-imports-themselves. Evaluating import statements for the directory where cromwell is executed is ""bad"" because; * Where you run cromwell matters. Even if using the same workflows. In terms of reproducibility this is not very nice.; * You cannot see from the WDL file what the base importing directory was meant to be. This follows from where cromwell is run, and this is not specified in the file itself.; * Because of this context dependence, sub workflows are not modular building blocks which can be moved around freely between workflows. This severely handicaps the usefulness of WDL. Import statements should be evaluated from the files which they are in because:; * It makes sub-workflows modular building blocks that can be moved around; * It makes it easier to review and understand the import statements.; * It follows the principle of least surprise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241:647,error,error,647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241,1,['error'],['error']
Availability,Solution for 503 errors in cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495,1,['error'],['errors']
Availability,"Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created. . Idea: occasionally run the `check-alive` command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2315:159,alive,alive,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2315,2,['alive'],['alive']
Availability,"Some light reading for @Horneth and @kshakir. This is largely Frankensteining of ""Olde Style"" code. Known missing or broken, I need to confirm that appropriate tickets exist for the restoration of the following:; - [x] #753 Abort; - [x] #751 Recover; - [x] #749 Preemptibility; - [x] #785 Persistence of any data (note this would not be a ticket to create a KV / metadata service, but to integrate this backend with such a service); - [x] #809 Hashing (prereq for #750); - [x] #750 Caching; - [x] #806 implement Firecloud style auth upload / deletion (the code is not present here); - [x] #808 Retries were removed from command script upload and JES run creation. Lessons learned:; - [x] #811 #812 Actor Factories should be responsible for sanity-checking configs .; - [x] #813 Initialization actors should have the ability to create workflow-level resources that can be shared by the other actors collaborating in the workflow execution. e.g. GCS Filesystems need only be created once per workflow, not for every call.; - [x] It's not clear how workflow logging (or logging in general) should work.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797:242,Recover,Recover,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797,1,['Recover'],['Recover']
Availability,Some refactoring: . * Separated the concepts of resolving and downloading.; * `DrsLocalizerMain#resolve` now chooses the `Downloader` implementation based on the content of the Martha response.; * The two downloader implementations support access and GCS URLs respectively.; * Tests for the downloader implementations have been separated from tests for the resolver.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6312:62,down,downloading,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312,4,"['Down', 'down']","['Downloader', 'downloader', 'downloading']"
Availability,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:319,ERROR,ERROR,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5182:276,echo,echo,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182,12,"['echo', 'failure']","['echo', 'failure']"
Availability,"Sometimes the timing diagram splits the results onto multiple lines, which then hides the lower part of the diagram because it's twice as long as expected. You end up with a mostly empty page with a small box that has as scroll wheel at the top of it. <img width=""1431"" alt=""screen shot 2018-04-05 at 4 06 59 pm"" src=""https://user-images.githubusercontent.com/4700332/38389168-956414ac-38eb-11e8-8459-dff52946091c.png"">. I noticed that the console for this diagram has the following errors:; ```; timing:20 Unable to add 'MarkDuplicates.SortSam's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'; addDataTableRow @ timing:20; timing:20 Unable to add 'MarkDuplicates.PreSort's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'; ```. I'm not sure if it's related to the display issue or not, but I don't see it on other workflows. ; See diagram here. https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing. It's the same job from issue #3483 so there might just be something off about the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3484:483,error,errors,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3484,1,['error'],['errors']
Availability,Sometimes users are interested in a particular workflow and would like to know when changes to some terminal state: failure or success. Allowing multiple users to register as interested in a workflow and emailing them when the workflow is reaches one of these states (maybe also registering the set of desired states to be notified about) would be useful.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678:116,failure,failure,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678,1,['failure'],['failure']
Availability,Space in checkpointFile name will make it fail,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:9,checkpoint,checkpointFile,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,1,['checkpoint'],['checkpointFile']
Availability,Spark backend script has syntax error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['error'],['error']
Availability,Spike in backpressure and 403 copy failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:35,failure,failures,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['failure'],['failures']
Availability,Spike: Investigate NPE causing workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772:40,failure,failure,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772,1,['failure'],['failure']
Availability,"Spoiler alert: The DB *does* have `WaitingForQueueSpace` statuses. Notes for reviewers:. * The metadata builder was throwing errors on old workflows which had previously generated `WaitingForQueueSpace` execution statuses (ie almost all of them).; * This PR fixes that by re-specifying that value as a valid status, even though nothing in the code _generates_ this status any more.; * The only ""live code"" changes are undoing the status deletion from https://github.com/broadinstitute/cromwell/pull/6047; * The test case is deliberately over-comprehensive (it makes sure the metadata builder can handle *ALL* current statuses). I'm trying to prevent future situations where statuses are removed from the list without realizing that that might break metadata builder logic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6131:125,error,errors,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6131,1,['error'],['errors']
Availability,Sporadic JES failure: error code 10 / message 15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233,2,"['error', 'failure']","['error', 'failure']"
Availability,Staging error for listing attribute in Directory input type,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,1,['error'],['error']
Availability,StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeAsync(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeAsync(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:502); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeOrRecover(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.aroundReceive(AwsAsyncJobExecutionActor.scala:23); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:25,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:3224,robust,robustExecuteOrRecover,3224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,1,['robust'],['robustExecuteOrRecover']
Availability,StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:356); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:320); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:70); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:1974,robust,robustPoll,1974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,1,['robust'],['robustPoll']
Availability,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1608,recover,recoverAsync,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Availability,Standardize PAPI detritus naming to match SFS for the benefit of the existing detritus filtering code. This also fixes the conflation of standard output and error from the user command with that of the whole exec script on PAPI that we've had since the beginning of time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3695:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3695,1,['error'],['error']
Availability,"Starting Cromwell on Windows and running any CWL workflow causes the server to crash and exit:; ```; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-50]: ; java.lang.UnsatisfiedLinkError: could not locate stub library in jar file. Tried [jni/x86_64-Windows/jffi-1.2.dll, /jni/x86_64-Windows/jffi-1.2.dll]; ```; What's worse, if one starts Cromwell back up it picks up that same workflow from the workflow store and immediately crashes. Notably, one can't even run WDLs at this point because the server does not stay up. The only remedy is to edit rows in the DB. I realize running on Windows is a self-inflicted wound in my case, hence ""low priority"". It does seem likely that not every researcher who wants to use Cromwell works somewhere that can afford to issue Macs. [jython_trace.txt](https://github.com/broadinstitute/cromwell/files/3048143/jython_trace.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802,1,['error'],['error']
Availability,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3243:485,down,downstream,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Starting up a new cromwell-44 server on CentOS7 with MySQL v5.7 (mysql Ver 14.14 Distrib 5.7.27, for Linux (x86_64) using EditLine wrapper). migrations complete without error:; ```; 2019-07-21 23:34:35,925 INFO - DROP INDEX METADATA_JOB_AND_KEY_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,935 INFO - Index METADATA_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:169,error,error,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['error'],['error']
Availability,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:2898,error,errors,2898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,1,['error'],['errors']
Availability,"Still driving on the Wash U workflow, this is a pregull checkpoint with some ~hacks~ work I did for blockers:. We round trip inputs YAML through Circe which uses its own ""Big"" number types that stringify with scientific notation. Our existing WomInteger / WomLong code didn't deal with that so the changes here add some flexibility. CWL explicitly allows filled optional to non-optional assignments with warnings, with an error at runtime if it turns out the optional wasn't actually filled. I expect some discussion on this 🙂",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3430:56,checkpoint,checkpoint,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Still looking at a Centaur test for this but I thought I'd let the seafowl have at it. Metadata looks like:. ```; {; ""workflowName"": ""three_step"",; ""submittedFiles"": {; ""workflow"": ""import \""3step-tasks.wdl\"" as tasks\nimport \""https://raw.githubusercontent.com/broadinstitute/centaur/591beaf8422af7c3faf51e437a91d94d13b76eba/src/main/resources/standardTestCases/aliased_subworkflows/subworkflow.wdl\"" as subworkflow\n\n\nworkflow three_step {\n call tasks.ps as ps\n call tasks.cgrep as cgrep {\n input: in_file = ps.procs\n }\n call tasks.wc as wc {\n input: in_file = ps.procs\n }\n output {\n cgrep.*\n wc.*\n }\n}\n"",; ""workflowType"": ""WDL"",; ""options"": ""{\n\n}"",; ""inputs"": ""{\""three_step.cgrep.pattern\"":\""mcovarr\""}"",; ""labels"": ""{}"",; ""imports"": {; ""https://raw.githubusercontent.com/broadinstitute/centaur/591beaf8422af7c3faf51e437a91d94d13b76eba/src/main/resources/standardTestCases/aliased_subworkflows/subworkflow.wdl"": ""task increment {\n Int i\n command {\n echo $(( ${i} + 1 ))\n }\n output {\n Int j = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\nworkflow subwf {\n Array[Int] is\n scatter (i in is) {\n call increment { input: i = i }\n }\n output {\n Array[Int] js = increment.j\n }\n}\n"",; ""3step-tasks.wdl"": ""task ps {\n command {\n ps\n }\n output {\n File procs = stdout()\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\ntask cgrep {\n String pattern\n File in_file\n command {\n grep '${pattern}' ${in_file} | wc -l\n }\n output {\n Int count = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\ntask wc {\n File in_file\n command {\n cat ${in_file} | wc -l\n }\n output {\n Int count = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\n""; }; },; .; .; .; ```. So the http imports come in under ""submittedFiles"" which is maybe a little weird. But other options would have http imports either not being next to the file imports and/or having to change the metadata schema.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2802:973,echo,echo,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2802,1,['echo'],['echo']
Availability,"Still need to performance-test on alpha. Problematic pairs:; * fetch <-> heartbeat (already coordinated); * fetch <-> abort (newly coordinated); * fetch <-> delete (newly coordinated). Example queries from MySQL deadlock printout in prod:. **Abort**; ```; update ; `WORKFLOW_STORE_ENTRY` ; set ; `WORKFLOW_STATE` = 'Aborting' ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '109a9d01-10b6-425d-8381-12a9d3a2c134'. ```; **Delete from workflow store**; ```; delete `WORKFLOW_STORE_ENTRY` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c4961523-321b-4172-abe8-e1e4eba94f43'. ```; **Fetch startable workflows**; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2020-09-18 05:08:18.823'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 30 for ; update; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:73,heartbeat,heartbeat,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,1,['heartbeat'],['heartbeat']
Availability,"Stop downloading, compiling, and uploading dead jes code [BT-84]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6311:5,down,downloading,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6311,1,['down'],['downloading']
Availability,"Stopped closing a scala `Future` over akka's `context.become`.; Flipped the default for `requestsAbortAndDiesImmediately` from `false` to `true`.; When killing a Standard backend job with rADI false, both the rc and the standard error are required.; Writing the stderr on abort for the SFS backend.; When rADI is true, sending a backend status of `""Aborted""`.; In the engine, set and store the `ExecutionStatus.Aborted`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,1,['error'],['error']
Availability,"Straight from the log: ""Something has gone horribly wrong!"". Jenkins build 2475 (2 errors); Jenkins build 2470; Jenkins build 2426",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4520:83,error,errors,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520,1,['error'],['errors']
Availability,"Strange ""Boxed Error"", probably authorization / config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:15,Error,Error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['Error'],['Error']
Availability,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:6748,ERROR,ERROR,6748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Submit docker error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['error'],['error']
Availability,Submit large N (perhaps 10K) workflows as fast as we can; ## Measure; * Observe any errors from submit endpoint; * How well is the summarizer keeping up with Metadata; * All workflows complete in < X Seconds; * CPU/memory usage of the Cromwell server ; * CPU/memory usage of the Database (directly from stackdriver) (TODO: Not sure we care?); ## Possible solutions; ### Mock-ify the Google backend; ## Goal; Submit workflows is successful and all return in < X seconds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4793:84,error,errors,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4793,1,['error'],['errors']
Availability,"Submitting a workflow with wrong credentials (eg invalid refresh token) triggers retries which are bound to fail and are significantly delaying the response from JesBackend, which in turn seems to lead to 502 proxy errors.; This might not be the only cause of this error but when submitting a few workflow with wrong token there was a flood of retries in cromwell logs which probably slows everything down.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/313:215,error,errors,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/313,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"Suggested by @vdauwera regarding read_tsv() import functionality:. When you have a TSV with a header, it would be great if you could set a parameter to `header=true`, in order to have WDL ignore the first line. . Even better, if the `read_tsv` function could use the header fields to read the content into a map instead of an array. So you could say `sample[inputBam]` instead of having to take `sample[1]`, which is more susceptible to errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1962:437,error,errors,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962,1,['error'],['errors']
Availability,"Support for namespaced inputs was dropped from 48 as mentioned in BA-6149. . The code was reworked in #5214 which disallowed namespaced inputs. There was one line of code that actively prohibited namespaced inputs. Removing this line allows namespaced inputs again. The namespaces are a bit different from the ones in 47, so I documented it in the changelog. Also I wrote some documentation on the inputs as I couldn't find any on the [latest development documentation](https://cromwell.readthedocs.io/develop/). . I like the newer namespaces much better than the old ones. A great job! This PR makes sure the fruit of this effort can be plucked by the pipeline developers. If womtool inputs needs to give a cleaner output, then maybe we can change a few other things. Changing it in the inputs parsing is not desirable IMO because that also affects cromwell. In an ideal world all the namespaced inputs are available for advanced pipeline users in cromwell, while not cluttering the womtool inputs output (unless a flag is set).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317:908,avail,available,908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317,1,['avail'],['available']
Availability,Surface TES System Logs to Cromwell when TES backend returns task error status,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6972:66,error,error,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6972,1,['error'],['error']
Availability,Sync centaur timeouts with heartbeats in test.inc.sh,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:27,heartbeat,heartbeats,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['heartbeat'],['heartbeats']
Availability,Synch this error message with SFS for failing centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1331:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1331,1,['error'],['error']
Availability,"Syntax that was accepted in draft-2:. ```; workflow w {. call empty {}. }. task empty {; command {}; output {}; }; ```. But when the exact workflow is run as version 1.0, it fails:. ```; [2018-12-18 11:01:42,27] [info] MaterializeWorkflowDescriptorActor [7ded9503]: Parsing workflow as WDL 1.0; [2018-12-18 11:01:42,46] [error] WorkflowManagerActor Workflow 7ded9503-e73c-4cca-ab37-5066c64780ef failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Unexpected symbol (line 4, col 15) when parsing 'call_body'. Expected input, got ""}"". call empty {}; ^. $call_body = :lbrace :input :colon $_gen18 :rbrace -> CallBody( inputs=$3 ); ; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:213). ```. and works again if `call empty {}` is changed to `call empty`. This shouldn't be a required change to update to v1.0, and would benefit from a fix!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501:321,error,error,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"System: mac; Cromwell: 36; Mode: Server; Backend: Local; Mysql: 5.5 or 5.7. ---. ### error info; ```; 2018-11-12 21:58:31,646 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-11-12 21:58:31,747 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where ((""HEARTBEAT_TIMESTAMP"" is null) or (""HEARTBEAT_TIM' at line 1; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2495); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1903); 	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1242); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at sl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"T INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34937,down,down,34937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['down'],['down']
Availability,TEST Integration errors: Chaos Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2113:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2113,1,['error'],['errors']
Availability,TEST Recovering Jobs: Turn Cromwell off/on,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:5,Recover,Recovering,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['Recover'],['Recovering']
Availability,"TL;DR: Boot disk size is increased automatically in Papi v2 if the (approximated) docker image size is > than what is requested (or the default one). Only works for Dockerhub and GCR at this moment because quay doesn't support yet manifest v2. - Refactor of the DockerHashActor: got rid of akka stream, replaced with fs2 streams and cats.effect.IO. Easier to read/write, modify and maintain; - Instead of just getting the digest from the manifest header response, parse the content and get the size of the layer. The sum of the sizes == compressed size of the image; - The size of the layers is only available in the version 2 of the manifest schema https://docs.docker.com/registry/spec/manifest-v2-2/; GCR and Dockerhub support it, quay.io not yet (soon according to their support); - Approximate the uncompressed size using a configurable compression factor. If the approximation is higher than the requested boot disk size, use that instead (only for Papi v2); - The refactoring allows for: no need for explicitly differentiation gcr zones, which means `gcr.io`, `us.gcr.io`, `eu.gcr.io` and `asia.gcr.io` all work; - The refactoring also isolate each registry from each other with their own thread pool. Which means if say dockerhub is having very long response time all of sudden, gcr and quay aren't impacted",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4472:600,avail,available,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4472,1,['avail'],['available']
Availability,"Tagged all database specific tests as `IntegrationTest`.; As mysql now has 5s to timeout, increased the testing timeout used to detect if mysql is available.; Removed unused code and optimized imports.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:147,avail,available,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['avail'],['available']
Availability,"Take the following simply WDL:; ```; version 1.0. workflow main {; call main {; input:; }; }. task main {; input {; String str; }. command <<<; >>>; }; ```. It will validate with Womtool:; ```; $ java -jar womtool-85.jar validate main.wdl ; Success!; ```. However, if you try to run it:; ```; $ java -jar cromwell-85.jar run main.wdl ; ...; Required workflow input 'main.main.str' not specified; ...; ```; It will immediately fail without being executed. Couldn't the same code in Cromwell that spots these errors be included in Womtool?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7139:507,error,errors,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7139,1,['error'],['errors']
Availability,Task.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(Valida,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:5054,Fault,FaultHandling,5054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,Tasks cannot call cache if their names are different (unless the tasks have no inputs!). The following tasks cannot call cache:. ```; task foo {; Int i; command { echo ${i} }; }; ```. ```; task bar {; Int i; command { echo ${i} }; }; ```. The call caching simpletons shouldn't include the call name in the input hash keys:. ```; +----------------------------+-----------------------------------------+----------------------------------+-----------------------+; | CALL_CACHING_HASH_ENTRY_ID | HASH_KEY | HASH_VALUE | CALL_CACHING_ENTRY_ID |; +----------------------------+-----------------------------------------+----------------------------------+-----------------------+; ....; | 137 | input: File foo.i | 778138a97b315ec95f374425255f8b9e | 12 |; | 138 | input: File bar.i | 778138a97b315ec95f374425255f8b9e | 13 |; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1989:163,echo,echo,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1989,2,['echo'],['echo']
Availability,Tasks with large files (>100GB) in input take a lot of time to start even if nothing (or almost nothing) happens in the command. I believe something should be optimized there because having a cache does not make sense when computing inputs hashes (or whatever slows thing down) is comparable or even slower to taking results out of cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213:272,down,down,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213,1,['down'],['down']
Availability,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:310,error,error,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,3,['error'],['error']
Availability,Test reliability: filesystem startup timeouts and docker health check errors [BA-6164],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5341:5,reliab,reliability,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5341,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"Tested by localizing a Blob file from Terra Prod, on a patched Cromwell in Terra Dev. This PR with foreign Blob URL:. ```; 2023-12-20 18:12:17.197 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 7,811,366,912 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/a7123170_f41bbba17a6f4409940127a60234695d-1/wd/wd/cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/inputs/lz304a1e79fd7359e5327eda.blob.core.windows.net/sc-705b830a-d699-478e-9da6-49661b326e77/inputs/Rocky-9.2-aarch64-dvd.iso; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:202,down,download,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['down'],['download']
Availability,"Tested locally with a long pause inserted into the metadata write actor. Shutting down and restarting after the subworkflow ""finished"" but before the final state metadata was written did not cause inconsistency in statuses written to the DB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6433:82,down,down,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6433,1,['down'],['down']
Availability,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,3,"['echo', 'error']","['echo', 'error']"
Availability,"Tested with Cromwell 52, 53. `memory-retry` does not work as expected. First of all it's activated only when `continueOnReturnCode` is set as `true` or list of some return codes.; I think this is intended as described in the documentation but why?. This is very weird. In most cases, return code of OOM is just 137.; Why don't we have something like `memoryRetryReturnCode`. I think it's too dangerous too set `continueOnReturnCode` as `true`.; Cromwell will pass any failure in all tasks.; So I set it as `[0, 137]` to catch `SIGKILL` due to OOM.; I also tried with `true` though. Here is my simple OOM tester WDL. I tested it with PAPIv2 beta based on Life Sciences API. ```wdl; version 1.0. workflow mem_retry {; call fail_oom; }. task fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:468,failure,failure,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['failure'],['failure']
Availability,Testing out disabling CWL in FC so I set the CWL language factory to `enabled=false`. The error message was very wrong:. ```; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; WDL draft 3 is not enabled; ```. Due to this [wonky bit](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/StandardLanguageFactoryConfig.scala#L10) of code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3921:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921,1,['error'],['error']
Availability,"Thanks to @mcovarr we *think* we figured out the cause of the memory leak.; The `WorkflowExecutionActor` was sending ""snapshots"" of its current internal data (including execution store and output store) to the `EJEA`and `SWEA` so they could pass it on to the JobPreparationActor (resp. `SubWorkflowPreparationActor`) so they could evaluate inputs using the OutputStore.; This PR rewires things so that the WEA data does not escape the WEA. Instead actors needing the OutputStore for input evaluation request it at the right time and it gets sent across but never stored in the downstream actors, therefore not holding references and allowing for proper GC.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2542:577,down,downstream,577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2542,1,['down'],['downstream']
Availability,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,1,['error'],['error']
Availability,The (edit by Chris: ~~Workflow Actor~~ WorkflowExecutionActor) should take action based on the kind of failure returned by the BE and retry if so indicated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/655:103,failure,failure,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/655,1,['failure'],['failure']
Availability,"The CWL [File](http://www.commonwl.org/v1.0/CommandLineTool.html#File) type requires a number of additional attributes to pass conformance tests. [Previously](https://travis-ci.org/broadinstitute/cromwell/jobs/298402505) Cromwell was returning:. ```json; {; ""ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014.output_file"": ""/home/travis/build/broadinstitute/cromwell/cromwell-executions/ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014/ac159f70-2fcf-469f-a23f-e53b894533b1/call-ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014/execution/error.txt""; }; ```. When the tests are expecting output like:. ```json; {; ""output_file"": {; ""checksum"": ""sha1$f1d2d2f924e986ac86fdf7b36c94bcdf32beec15"",; ""class"": ""File"",; ""location"": ""error.txt"",; ""size"": 4; }; }; ```. [Currently](https://travis-ci.org/broadinstitute/cromwell/jobs/302086244) there does not seem to be any outputs generated. After discussions, the `WomSingleFile` will be updated to support the new File attributes. TBD: CWL conformance tests must pass, but metadata responses must be backwards compatible for WDL run in Firecloud. Acceptance criteria; - [ ] CWL conformance test # 10 passes; - [ ] Metadata responses for Firecloud are still backwards compatible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2899:554,error,error,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2899,2,['error'],['error']
Availability,"The CopyWorkflowActor regularly gets timeouts when trying to copy the gigabytes of data that are typically associated with production workflows. Also this duplicates the amount of disk space used for a workflow. . This remedies that problem by hardlinking the files. It is much much faster, and the cromwell-executions folder can be safely removed afterward. This is very beneficial for people who run cromwell on a cluster backend in `run` mode. Ping @illusional . I have included a test case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6672:447,Ping,Ping,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672,1,['Ping'],['Ping']
Availability,"The Cromwell 59 JAR download from GitHub is broken. Also, how does one access the JIRA board? I created an account and tried to access it, but got a permission denied message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6500:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6500,1,['down'],['download']
Availability,"The GCP Batch PR has seen a number of seemingly unrelated failures building the DRS localizer [(Github actions link)](https://github.com/broadinstitute/cromwell/actions/runs/5580938822/jobs/10198518044?pr=7177) and I saw them locally too, both on `develop` and the Batch branch [(Slack link)](https://broadinstitute.slack.com/archives/G01D73CM63S/p1689702982488299?thread_ts=1689702892.491819&cid=G01D73CM63S). ```; W: GPG error: http://security.ubuntu.com/ubuntu jammy-security InRelease: At least one invalid signature was encountered.; E: The repository 'http://security.ubuntu.com/ubuntu jammy-security InRelease' is not signed. process ""/bin/sh -c apt-get -y update"" did not complete successfully; ```. It seems that specifying the OS explicitly instead of implicitly helps work around the problem. I confirmed that yesterday's build of the localizer uses the same base so this is not a radical change. [Nightly:](https://hub.docker.com/layers/broadinstitute/cromwell-drs-localizer/86-af9660e/images/sha256-ee39681ef7287e904fdde01874b5dfa80b045aed28b9cc4b017554bdb40015f1?context=repo); ```; docker inspect broadinstitute/cromwell-drs-localizer:86-af9660e; ""Labels"": {; ""org.opencontainers.image.ref.name"": ""ubuntu"",; ""org.opencontainers.image.version"": ""22.04""; }; ```; Local:; ```; docker inspect broadinstitute/cromwell-drs-localizer:86-813fc98-SNAP; ""Labels"": {; ""org.opencontainers.image.ref.name"": ""ubuntu"",; ""org.opencontainers.image.version"": ""22.04""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7179:58,failure,failures,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179,2,"['error', 'failure']","['error', 'failures']"
Availability,The HPC tutorial config causes the server to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3488:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488,1,['error'],['error']
Availability,"The Horicromtal Deadlock test started failing the morning of Monday, 2/26 with Docker pull failures:; ```; Head ""https://registry-1.docker.io/v2/dockercloud/haproxy/manifests/latest"":; error parsing HTTP 429 response body:; invalid character 'S' looking for beginning of value: ; ""Server capacity exceeded.\n""; ```; I was able to pull the [`dockercloud/haproxy` image](https://hub.docker.com/r/dockercloud/haproxy) locally but found that it was last updated 6 years ago. My suspicion is that ancient images are stored in a much less hot level of cache in the bowels of Docker Hub and may be more susceptible to capacity issues and timeouts. In order to adopt a current, official HAProxy image, I had to make a very basic config and we were off to the races. This is because the `dockercloud` image was a bit customized with special sauce to automatically configure itself by detecting running Docker containers. As a bonus, the new Alpine-based image is actually smaller than the ancient one, albeit only 25 MB vs. 43 MB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7376:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7376,2,"['error', 'failure']","['error', 'failures']"
Availability,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3515:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515,3,['error'],['error']
Availability,"The Methods Cromwell recently experienced an outage nearly identical to the Terra one, where requests to GCS and the database timed out. Like on Terra, a server restart fixed it. Their server is running version `54-97597a4` that [definitely has](https://github.com/broadinstitute/cromwell/commits/54_hotfix) the [PR](https://github.com/broadinstitute/cromwell/pull/5994) we did to handle null messages. In this PR I fixed another possible source of NPEs in `cromwell.engine.io.gcs.GcsBatchFlow#recoverCommand`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:45,outage,outage,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,2,"['outage', 'recover']","['outage', 'recoverCommand']"
Availability,"The OGE back end shell script that is submitted to the task scheduler on SGE/OGE back ends is potentially missing a sync statement. . `; (; cd /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution; sync; ). mv /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc.tmp /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc; `. We've had cases where job completed successfully, OGE reports the job as exiting normally, but the rc file is written with a '79' (undocumented behavior until we located it in the source code). And yet on examination of the logs, there were no errors, and the rc file was moved from rc.tmp (which is no longer there). Our assumption is that the mv command is not getting flushed to our storage system, and if cromwell polls for zombie tasks before that happens we will see it as a job failure. . See https://github.com/broadinstitute/cromwell/blob/68949364c7ffab8450116baa8e2a4e276bb16d70/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L465. which is not followed by SCRIPT_EPILOGUE that would add the required sync command after the mv.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988:663,error,errors,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988,2,"['error', 'failure']","['errors', 'failure']"
Availability,The PAPIv2 version of `interpretOperationStatus` calls `ErrorReporter#toUnsuccessfulRunStatus` instead of `RunStatus.UnsuccessfulRunStatus#apply`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5129:56,Error,ErrorReporter,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5129,1,['Error'],['ErrorReporter']
Availability,"The ServiceRegistry requires all services sitting behind it to at least be aware of the graceful shutdown infrastructure, e.g. handling a `ShutdownCommand` even if the service doesn't need to be graceful about its shutting downing. It'd be nicer if this could be made to not be the case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2575:223,down,downing,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2575,1,['down'],['downing']
Availability,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1383:183,echo,echo,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383,3,['echo'],['echo']
Availability,"The SingleWorkflow mode, via `java -jar cromwell.jar run …`, does not always wait for metadata to be completely processed. Thus when the actor exits, it may write out incomplete metadata json. This mainly seen during intermittent test failures that are looking for expected metadata from the output file:. ```scala; [info] SingleWorkflowRunnerActorWithMetadataSpec:; [info] A SingleWorkflowRunnerActor should ; [info] - successfully run a workflow outputting metadata *** FAILED *** (6 seconds, 143 milliseconds); [info] java.util.NoSuchElementException: None.get; [info] at scala.None$.get(Option.scala:347); [info] at scala.None$.get(Option.scala:345); [info] at cromwell.engine.workflow.SingleWorkflowRunnerActorSpec$OptionJsValueEnhancer$.toStringValue$extension(SingleWorkflowRunnerActorSpec.scala:44); [info] at cromwell.engine.workflow.SingleWorkflowRunnerActorSpec$OptionJsValueEnhancer$.toOffsetDateTime$extension(SingleWorkflowRunnerActorSpec.scala:43); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1932:235,failure,failures,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1932,1,['failure'],['failures']
Availability,"The `DrsCloudNioFileSystemProvider` was wrapping the retries of the `DrsPathResolver` with another set of `CloudNioRetry` retries. The product of these two retries at the previous configuration values would wait around 35 minutes (~:20 + 10 x ~3:30) to fail for each doomed attempt. That combined with a fairly wide scatter and a typo'd DRS path for `file` in code like . ```; task size {; input {; File file; }. Int file_size = ceil(size(file)); ...; }; ```; would completely block all 10 of the `IoActor`s [NIO threads](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L108). These changes remove the nested retries in the engine and dial back the patience for retries. If we want the retries to be more patient we'll probably have to make other code that is competing for `IoActor` threads more patient as well. Utility files for reproducing this error can be cherry picked from commit `ff7bddc8830802f7a606177d0eaf19c8f47ca865`. I don't know how to programatically link Google accounts to NIH accounts in Bond to be able to include this Centaur test in CI, though maybe we don't need to be linked to make sure this negative case errors within a reasonable timeout?. Workflow`9635fbf0-00b1-4635-b482-5a782cda5cd5` induced this problem in production, its metadata shows multiple `HaplotypeCaller` shards erroring out with ; ```; Failed to evaluate input 'disk_size' (reason 1 of 1): [Attempted 1 time(s)] - RuntimeException: Unexpected response during DRS resolution: RuntimeException: Could not access object 'drs://dg4.DFC/...'. Status: 500, reason: 'Internal Server Error', Martha location: 'https://.../martha_v3', message: 'Received error while resolving DRS URL. getaddrinfo ENOTFOUND dg4.dfc'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6439:919,error,error,919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6439,5,"['Error', 'error']","['Error', 'error', 'erroring', 'errors']"
Availability,"The `evaluateFiles` method is used to figure out what files are going to be produced or are referenced by an expression, before the task has been run.; In this context it doesn't make sense to do the work of evaluating `glob` which will fail since the task hasn't run.; Instead, fail immediately. When that happens, we fallback to a second algorithm to evaluate the files ([draft-2](https://github.com/broadinstitute/cromwell/blob/541636734705b7d93321a31ac5817f96b275eb0f/wdl/model/draft2/src/main/scala/wdl/draft2/model/expression/FileEvaluator.scala#L52), [draft-3](https://github.com/broadinstitute/cromwell/blob/541636734705b7d93321a31ac5817f96b275eb0f/wdl/model/draft3/src/main/scala/wdl/model/draft3/graph/expression/FileEvaluator.scala#L25)). This should fix the `dontglobinputs` transient centaur failures. With the caveat that the real underlying issue is this: https://github.com/broadinstitute/cromwell/issues/4209",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208:805,failure,failures,805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208,1,['failure'],['failures']
Availability,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5126:93,error,error,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126,2,"['Error', 'error']","['Error', 'error']"
Availability,"The `validateWomNamespace` method was using `NoIoFunctionSet` instead of the available ioFunctions, causing the workflow in the centaur test to fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3715:77,avail,available,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715,1,['avail'],['available']
Availability,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761:12,heartbeat,heartbeat,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761,1,['heartbeat'],['heartbeat']
Availability,"The centaur test `custom_mount_point` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n custom_mount_point""; ```. The error:. ```; - should successfully run custom_mount_point *** FAILED *** (2 minutes, 1 second); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/949:205,error,error,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/949,1,['error'],['error']
Availability,"The centaur test `jesexercises` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n jesexercises""; ```. The error:. ```; - should successfully run jesexercises *** FAILED *** (4 minutes, 40 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/951:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/951,1,['error'],['error']
Availability,"The centaur test `lots_of_inputs` is failing on JES. Sad! Make it great again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n lots_of_inputs""; ```. The error:. ```; - should successfully run lots_of_inputs *** FAILED *** (53 minutes, 12 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```. WARNING! It's also taking almost an hour to run...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/956:186,error,error,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/956,1,['error'],['error']
Availability,"The centaur test `passingfiles` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n passingfiles""; ```. The error:. ```; - should successfully run passingfiles *** FAILED *** (3 minutes, 30 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/950:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/950,1,['error'],['error']
Availability,"The centaur test `sizeenginefunction` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n sizeenginefunction""; ```. The error:. ```; - should successfully run sizeenginefunction *** FAILED *** (1 minute, 30 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/952:205,error,error,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/952,1,['error'],['error']
Availability,"The centaur test `workspaceenginefunctions` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n workspaceenginefunctions""; ```. The error:. ```; - should successfully run workspaceenginefunctions *** FAILED *** (10 seconds, 11 milliseconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/953:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/953,1,['error'],['error']
Availability,"The centaur test `write_lines` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n write_lines""; ```. The error:. ```; - should fail during execution write_lines *** FAILED *** (6 minutes, 0 seconds); java.lang.Exception: Unexpected terminal status Succeeded but was waiting for Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955:191,error,error,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955,1,['error'],['error']
Availability,"The centaur test `write_tsv` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n write_tsv""; ```. The error:. ```; - should successfully run write_tsv *** FAILED *** (2 minutes); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/954:187,error,error,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/954,1,['error'],['error']
Availability,"The combination of a separate metadata database and -DLOG_LEVEL=DEBUG is causing an out of memory error when the log output exceeds the max size of a byte array. We are executing Cromwell locally in run mode with a long running workflow (about 24 hrs) that includes several scatter-gather blocks. We switched to a separate metadata database to address the Java heap problem that occurs with the in-memory database. We enabled debug logging to try and troubleshoot an unrelated problem. Most of the log output at the increased level is appears to be from HSQL. After running for about 8 hrs, the following error appears in the output and Cromwell hangs:; ```; Exception in thread ""Exec Stream Pumper"" java.lang.OutOfMemoryError: Required array length 2147483639 + 39 is too large; 	at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); 	at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); 	at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); 	at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132); 	at org.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:92); 	at org.apache.commons.io.output.TeeOutputStream.write(TeeOutputStream.java:68); 	at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:108); 	at java.base/java.lang.Thread.run(Thread.java:1623); ```. We are running Cromwell using Dockstore as a wrapper using the following command:; ```; dockstore workflow launch --local-entry BiobankScrubWorkflow.wdl --json inputs.json > dockstore.log 2>&1 &; ```. At the time the OOME occurs, the size of the dockstore.log file is approx 2147485425 bytes. Based on the ""Saving copy of Cromwell stdout to..."" messages at the end of a successful Cromwell run, it would appear that Cromwell is internally buffering the stdout and stderr streams to save at the end of the run. So when the size of the stdout or stderr exceeds the Java buffer max size, the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7217:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217,2,['error'],['error']
Availability,The concurrent-job-limit now available to all backends.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2547:29,avail,available,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2547,1,['avail'],['available']
Availability,The coursier plugin is a drop-in replacement for resolution and download of ivy artifacts. Resolution and downloading are done in parallel. https://github.com/coursier/coursier,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3015:64,down,download,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015,2,['down'],"['download', 'downloading']"
Availability,The current call caching fails to correctly ignore whitespace. The following command blocks will not call cache together:. ```; command {; echo hello world; }; ```. ```; command { echo hello world }; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1988:139,echo,echo,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1988,2,['echo'],['echo']
Availability,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3493:12,heartbeat,heartbeat-scanning,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493,3,"['error', 'heartbeat']","['error', 'heartbeat-scanning', 'heartbeats']"
Availability,"The description of the problem can be found here; https://broadworkbench.atlassian.net/browse/BA-5881; Unfortunately, there is no direct way to reproduce `500 Internal Server Error Backend Error`. Therefore, it is unclear how to show that this PR solves the problem.; However, since this error causes IOException, we think that reproducing some other IOException with this file should reproduce the problem close enough. If you are interested, we have an [experimental branch](https://github.com/EpamLifeSciencesTeam/cromwell/pull/9) for reproducing IOException. In that branch, Cromwell stops during execution, giving us time to delete the `rc` file if we want to. It allows us to see that this fix actually works and Cromwell indeed tries to read the file multiple times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113:175,Error,Error,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113,3,"['Error', 'error']","['Error', 'error']"
Availability,"The docker attribute specified in the ""default_runtime_attributes"" in cromwell.conf does not get picked up. My workflow fails with error: . ```; 2019-12-19 03:04:56,497 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowManagerActor Workflow 759c0000-c343-4466-a26b-aa627785; 89b0 failed (during InitializingWorkflowState): Task gcpp has an invalid runtime attribute docker = !! NOT FOUND !!; ```; The relevant portion of cromwell.conf; ```; computecfg_00 {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""xxxxxx""; auth = ""default""; default-runtime-attributes {; queueArn = ""xxxxxx""; docker = ""ubuntu:latest""; }; ...; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5329:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5329,1,['error'],['error']
Availability,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:49,alive,alive,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,6,['alive'],['alive']
Availability,"The documentation has a nice section on [how to use Service Accounts with Cromwell](https://cromwell.readthedocs.io/en/develop/backends/Google/), with the Google Cloud backend. However, what it doesn't do is explain the roles/permissions that such an account needs. It would be appreciated if we had a list of permissions we could apply to our Service Accounts to know that we had the absolute minimum required for Cromwell to control jobs (probably separate lists for filesystem access and job management). Currently, the roles I've applied to my Service Account are:. * Compute Instance Admin (v1); * Genomics Pipelines Runner; * Service Account User; * Storage Object Admin. This works, but I know that these roles are quite permissive. Ideally I'd be able to lock it down to permissions that stop it from deleting buckets etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304:771,down,down,771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304,1,['down'],['down']
Availability,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1691:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691,3,['error'],"['error', 'errors']"
Availability,"The failure rate was getting unreasonable so I figured someone should do something. It might take a bit longer, but at least it'll more likely return green. ---. Once build activity died down after everyone went home, I was able to isolate the imapct of conformance PapiV2 by running it on its own: virtually none (my run started at 7:10). <img width=""898"" alt=""screen shot 2018-10-12 at 7 39 48 pm"" src=""https://user-images.githubusercontent.com/1087943/46898326-a37a2300-ce56-11e8-9c0a-ff5f33ade931.png"">. It's still possible that other jobs suck up quota which in turn affects conformance PapiV2 even if it uses very little itself. I asked for more quota again, because my first request didn't give us enough breathing room (screenshot _after_ first increase):; <img width=""976"" alt=""screen shot 2018-10-12 at 6 24 21 pm"" src=""https://user-images.githubusercontent.com/1087943/46898377-15526c80-ce57-11e8-8af6-f7844e84b843.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4248:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4248,2,"['down', 'failure']","['down', 'failure']"
Availability,"The file it was trying to read is gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e8-84ee4e7ef074/call-PreBqsrCheckContamination/PreBqsrCheckContamination-stdout.log. The task uses `read_float (stdout)` in its output block. . ```; 2016-06-01 09:47:17,888 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(9a7a405c):PreBqsrCheckContamination]: Failing call: Error reading gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e... at position 0; cromwell.util.AggregatedException: Error reading gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e... at position 0;   at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell.jar:0.19];   at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:130) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:625) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:664) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/928:355,ERROR,ERROR,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/928,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"The following WDL fails to parse because of the expressions in the array accesses. ```; workflow foo {; Array[String] inputs = [ ""A0"", ""A1"", ""B0"", ""B1"", ""C0"", ""C1"" ]. scatter(i in range(length(inputs) / 2)) {; String item0 = inputs[i * 2]; String item1 = inputs[i * 2 + 1]; call bar { input: item0 = item0, item1 = item1 }; }; }. task bar {; String item0; String item1; command { echo ""<< item0: ${item0}, item1: ${item1} >>"" }; output {; String combined = read_string(stdout()); }; }; ```. The error given is:; ```; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Unexpected symbol (line 5, col 29) when parsing 'e'. Expected rsquare, got *. String item0 = inputs[i * 2]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ). ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2599:380,echo,echo,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2599,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"The following WDL has issues on JES, AWS, and Local backends. The expectation would be that all occurrences of `file` and `maybe_file` would interpolate as relativized file paths. ```; task files {; File file ; File? maybe_file. command {; echo file: ${file} maybe_file: ${maybe_file} ${""file with concatenation: "" + file} ${""maybe_file with concatenation: "" + maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call files { input: file = file, maybe_file = file }; }; ```. On JES with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c, the ""maybe_file with concatenation"" leaves a residual `gs://` path; all other interpolations are correctly relativized. On AWS with Cromwell 9341a4dac6145233f2a33b092a8fc443c18744ea, both concatenations leave residual `s3://` paths. On Local with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c and an input file under my home directory, this throws an exception with the following trace:. ```; 2017-02-02 11:55:36,701 cromwell-system-akka.dispatchers.backend-dispatcher-44 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(5fdb357a)w.files:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instantiat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:240,echo,echo,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['echo'],['echo']
Availability,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:487,ERROR,ERROR,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['ERROR'],['ERROR']
Availability,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3190:252,echo,echo,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190,5,"['echo', 'failure']","['echo', 'failures']"
Availability,"The following stack trace appears:. ```; 2017-01-27 11:43:10,369 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 268771d3-0303-45b1-ba0f-9c93c27b6784 failed (during ExecutingWorkflowState): JobStore write failure: Invalid position in SerialClob object set; java.lang.Exception: JobStore write failure: Invalid position in SerialClob object set; 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor$$anonfun$11.applyOrElse(EngineJobExecutionActor.scala:239); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor$$anonfun$11.applyOrElse(EngineJobExecutionActor.scala:235); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:29); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerTh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1919:119,ERROR,ERROR,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1919,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"The following task delocalizes the output file: . ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf).tbi""; }; }; ```. In V25 the output file delocalizes correctly, but if you try to use this output file as input to a future task, it doesn't get the full google bucket path, it just has the string interpolation of the path that was on the VM in the IndexVCF task. . Instead the file shouldn't delocalize and a helpful error message should be provided that indicates that a File input shouldn't be interpreted as a String in the output section because it's using that local VM's relative path at that point (rather than the full google bucket path).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2236:721,error,error,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2236,1,['error'],['error']
Availability,"The following wdl validates and starts ""running"" even though nothing can happen:. ```; workflow CromwellSimpleCircularGraphBug {; 	; 		call Echo as A {input: string = item1}; 		String item2=A.out; ; 		call Echo as B{input: string = item2}; 		String item1=B.out; }. task Echo {; 	String string; 	command {; 		echo ${string}; 	}; 	runtime {; docker: ""ubuntu:latest""; 	}; 	output {; 		String out=read_string(stdout()); 	}; }; ```. This version is a bit trickier, perhaps. It also is ""running"":; ```; workflow CromwellCircularGraphWithScatterBug {; 	; scatter(item1 in list1) {; 		call Echo as A {input: string = item1}; 		String list2=A.out; }. scatter(item2 in list2) {; 		call Echo as B{input: string = item2}; 		String list1=B.out; } ; }. task Echo {; 	String string; 	command {; 		echo ${string}; 	}; 	runtime {; docker: ""ubuntu:latest""; 	}; 	output {; 		String out=read_string(stdout()); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2279:140,Echo,Echo,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2279,8,"['Echo', 'echo']","['Echo', 'echo']"
Availability,"The following workflow always fails on JES if call-caching is enabled. File paths are getting horribly mangled and the getHash fails. ```; task mirror {; File x; command {; echo noop; }; output {; File y = x; }; }. workflow call_caching {; File x_in; Array[Int] blahs = [1,2,3,4,5,6,7,8,9,10]. call mirror { input: x = x_in }. scatter ( blah in blahs ) {; call mirror as cached_mirror1 { input: x = mirror.y }; call mirror as cached_mirror2 { input: x = cached_mirror1.y }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/531:173,echo,echo,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/531,1,['echo'],['echo']
Availability,"The following workflow and subworkflow wdls do not work in cromwell v29 hot fix (3be5b8c0537ef051dece9fbad63a154b72ad510d): . ```; import ""subworkflow.wdl"" as sub. workflow wf {; 	Array[Float] arr = [1.0,2.0,3.0]. 	call sub.subwf {; 		input:; 			input_array = arr; 	}; }; ```. ```; workflow subwf {; 	Array[Float] input_array. 	call SumFloats {; 		input:; 			sizes = input_array,; 			preemptible_tries = 0; 	}. 	String size = if SumFloats.total_size < 2.0 then ""Small"" else ""Big"". 	call print {; 		input: ; 			s = size; 	}; }. task SumFloats {; 	Array[Float] sizes; 	Int preemptible_tries. 	command <<<; 	python -c ""print ${sep=""+"" sizes}""; 	>>>; 	output {; 		Float total_size = read_float(stdout()); 	}; 	runtime {; 		docker: ""python:2.7""; 		preemptible: preemptible_tries; 	}; }. task print {; 	String s. 	command {; 		echo ${s}; 	}. 	runtime {; 		docker: ""python:2.7""; 	}; }; ```. I get errors such as the following:. ```; message: ""Could not find size in input section of call wf.subwf""; message: ""Could not find SumFloats in input section of call wf.subwf""; message: ""No declaration named SumFloats for call wf.subwf""; message: ""Input evaluation for Call wf.subwf failed.""; message: ""Couldn't resolve all inputs for wf.subwf at index None.""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2753:821,echo,echo,821,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2753,2,"['echo', 'error']","['echo', 'errors']"
Availability,"The following workflow failed in cromwell (3da6e34e-a2a1-49fb-8a84-4d62aada9b3d) on production. . ```; {; ""preemptible"": true,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15-stdout.log"",; ""backendStatus"": ""Success"",; ""shardIndex"": 15,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""3"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 200 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""3.5 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""recalibration_report"": ""GatherBqsrReports.output_bqsr_report"",; ""disk_size"": ""agg_medium_disk"",; ""output_bam_basename"": ""recalibrated_bam_basename"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup""; },; ""returnCode"": 0,; ""failures"": [{; ""failure"": ""Read timed out"",; ""timestamp"": ""2016-04-23T18:29:53.764Z""; }],; ""jobId"": ""operations/EK2_jqLEKhiCmbzn296gnv4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-23T18:29:54.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15.log""; },; ""start"": ""2016-04-23T17:56:02.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738:1146,failure,failures,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738,2,['failure'],"['failure', 'failures']"
Availability,"The following workflow failed in cromwell (4ef40f07-ff52-426b-9610-3c9dc66ec67e) on production. Looking metadata we have no logs for the step that failed. ```; {; ""executionStatus"": ""Failed"",; ""shardIndex"": 5,; ""outputs"": {. },; ""runtimeAttributes"": {. },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""disk_size"": ""agg_medium_disk"",; ""dbSNP_vcf"": ""dbSNP_vcf"",; ""known_snps_sites_vcf"": ""known_snps_sites_vcf"",; ""dbSNP_vcf_index"": ""dbSNP_vcf_index"",; ""known_indels_sites_vcf_index"": ""known_indels_sites_vcf_index"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""recalibration_report_filename"": ""sample_name + \"".recal_data.csv\"""",; ""known_snps_sites_vcf_index"": ""known_snps_sites_vcf_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup"",; ""known_indels_sites_vcf"": ""known_indels_sites_vcf""; },; ""failures"": [{; ""failure"": ""Call failed to initialize: Could not persist runtime attributes: Timeout after 5059ms of waiting for a connection."",; ""timestamp"": ""2016-04-23T09:14:54.651Z""; }],; ""backend"": ""JES"",; ""end"": ""2016-04-23T09:14:56.000Z"",; ""attempt"": 1,; ""executionEvents"": [],; ""start"": ""2016-04-23T09:14:45.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:953,failure,failures,953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,2,['failure'],"['failure', 'failures']"
Availability,"The following workflow fails:. This workflow fails:. ```; task MakeMeAFile {; command <<<; echo FILECONTENT > output.txt; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; String out = read_string(""output.txt""); }; }. task ReadMeAFile {; File infile. command <<<; cat ${infile}; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File out = read_string(stdout()); }; }. workflow FileMakeAndRead {; call MakeMeAFile; call ReadMeAFile { input: infile = MakeMeAFile.out }; }; ```. Because:. ```; ""failures"": [; ""Could not find suitable filesystem to parse output.txt""; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/891:91,echo,echo,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/891,2,"['echo', 'failure']","['echo', 'failures']"
Availability,"The following workflow should produce an output array containing a single string. Instead it produces an empty array. This problem occurs when using the JES backend but not the local backend. ```; task x {; command {; echo hello > hello; }; output {; Array[String] a = glob(""hello""); }; runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call x; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/828:218,echo,echo,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/828,1,['echo'],['echo']
Availability,"The goal here is to gather reliability metrics on Cromwell. The theme here is basic.. the data is the most important artifact. Create something (e.g. a bash or python script) that ; ; - launches N (e.g. 50) workflows; - polls for workflow completion; - upon workflow completion, log workflow id + status (success/failure) and launch a new workflow. If a workflow fails, perhaps also grab the metadata for the workflow and write it out (although we could also query Cromwell for this later). This should run until we kill it. . Then start it up and let it run. Each day investigate workflow failures and generate a findings document which will feed into reliability ticket creation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1822:27,reliab,reliability,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1822,4,"['failure', 'reliab']","['failure', 'failures', 'reliability']"
Availability,"The idea being, sending an abort message does not get an `AbortSuccess` or `AbortFailed` response directly. Instead, the actor being aborted will cause its operation to fail and the parent should expect an ""Aborted"" response instead of a Success or Failure (although a Success or Failure may still be returned anyway). In this way, it's acceptable for a backend implementor to simply do nothing when `abort` is received. The parent will just continue waiting for a result of some sort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:249,Failure,Failure,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,2,['Failure'],['Failure']
Availability,"The labels PATCH endpoint will always return a 500 status code, no matter what exactly the error is. For example, using fake id “abc” returns status code 500, and using an unrecognized workflow id “774eeeac-aaaa-bbbb-8bf7-061b87ad19da” will also return 500. This may not be what we expected. . The only way to get a 400 code is to make a request with invalid json input, such as {“”:”test_label”}. But we couldn't get a 404 code as we expected, could someone get this fixed?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2961:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2961,1,['error'],['error']
Availability,"The limit of 300 is unsufficient for most tools, which log some startup messages before starting the actual work. This exceeds 300 characters easily. It results in not being able to see the actual error that crashed the job. This is quite annoying. Especially when the job fails on a CI job and you can't look into the logs on the remote CI server. 3000 is just another arbitrary limit, but hopefully better. EDIT: Example: https://github.com/biowdl/germline-DNA/pull/78/checks?check_run_id=1549055042",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6132:197,error,error,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6132,1,['error'],['error']
Availability,"The mint team ran a group of ~200 10x workflows where 18 failed due to PAPI error code 10.14. Many of the tasks were stuck waiting for quota. For example, the following non-preemptible task ran for 3h 51m before failing with this error:. `CellRanger.cellranger_count:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 14: VM ggp-16203705785274897556 stopped unexpectedly.` . It would be useful if the `max_retries` runtime parameter included retrying tasks that fail with the above error, so that the workflow does not need to be manually re-run after failing. **Configuration info**:; Cromwell version: Cromwell-as-a-Service (caas-prod) version `35-5f86a05-SNAP`; Backend: PAPI v1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4363:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4363,4,['error'],['error']
Availability,The motivating cases for this are the metadata-fetching performance improvements under development. We would not only like to fetch metadata but time how long it takes to retrieve. However the perf VM is currently shut down after the workflow completes and its metadata is extracted so this is unpossible without at least hacking the perf script a little.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4251:219,down,down,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4251,1,['down'],['down']
Availability,"The outputs of an optional scattered task that does **not** run should be undefined. Instead, Cromwell seems to think it is defined, and it has a length. This can cause all sorts of issues, such as breaking downstream tasks that are only supposed to run if the optional upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:207,down,downstream,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,4,"['down', 'error']","['downstream', 'error', 'errorcode']"
Availability,"The parameters available to config scripts are mostly undocumented. A list can be reverse engineered using the cromwell.examples.conf file, and by forcing the parser to crash - which prints the generated submit wdl script to the log. I could then build this config:. ```; submit-docker = """"""; /usr/bin/env ${job_shell} ${script}; echo ${job_name}; echo ${cwd}; echo ${out}; echo ${err}; echo ${script}; echo ${job_shell}; echo ${docker_cid}; echo ${docker_cwd}; """"""; submit = """"""; /usr/bin/env ${job_shell} ${script}; echo ${job_name}; echo ${cwd}; echo ${out}; echo ${err}; echo ${script}; echo ${job_shell}; """"""; ```. (note while String job_id is available in the submit task, using it causes a crash-loop). . The values of cwd and docker_cwd make sense, cwd (in both scripts) refers to the 'shared file system' path to the cromwell created directory, and docker_cwd strips everything before 'cromwell-executions'. I do not see the necessity for docker_cwd, but fair enough. However 'out', 'err' and 'script' do not follow the pattern. In 'submit' they are sfs paths, but in submit-docker they are docker paths. This kind of inconsistency, combined with lack of documentation makes cromwell extremely frustrating to use.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4212:15,avail,available,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4212,16,"['avail', 'echo']","['available', 'echo']"
Availability,The performance of method `processRunnableTaskCallInputExpression` first appeared on our radar in https://github.com/broadinstitute/cromwell/pull/5048. It came up again yesterday when we had a high CPU event on all three runners. I took a thread dump during the event and there were 44 threads busy with; ```; at wom.graph.CommandCallNode.toString(CallNode.scala:68); at java.lang.String.valueOf(String.java:2994); at java.lang.StringBuilder.append(StringBuilder.java:131); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:549); ```. I think we are making the mistake of calculating a potentially very large `toString` by using the default case class implementation. I do have the full stack dump available in case anyone is curious. ![Screen Shot 2019-09-11 at 5 00 45 PM](https://user-images.githubusercontent.com/1087943/64735012-b60d3a00-d4b5-11e9-8f60-e9a55fd20f07.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5172:787,avail,available,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5172,1,['avail'],['available']
Availability,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3650:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650,1,['error'],['error']
Availability,"The report in CROM-6808 includes two cases, these changes would only address the second. I'm not sure what would be causing the first case (an `Exception` with message text containing the text `Please try again.` that wasn't retried), so this PR may not be the final word on PAPI initialization failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6572:295,failure,failures,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6572,1,['failure'],['failures']
Availability,"The resolution of #2035 exposed this issue. A centaur issue was mitigated by https://github.com/broadinstitute/centaur/pull/165. A proper fix in the [StandardBackend](https://github.com/broadinstitute/cromwell/blob/335e9838abce1405f0d26fe18c5dba2dfa7ad0f5/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L173) will enable a centaur test to run a task with command:. ```; command {; echo ""hello"" > tmp; }; ```. This regression centaur test should run on all backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2047:417,echo,echo,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2047,1,['echo'],['echo']
Availability,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908:350,error,error,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,The source code used for localizing and executing the Cromwell exec script on BCS is embedded within tar.gz file in the cromwell code base. To make this code easier to update the code should be moved out of the tar and live as individual files within the source code. - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - The source code for the BCS worker is available in the cromwell source tree ; - Running `sbt assembly` still creates a an embedded resource `worker.tar.gz` that the BCS backend can upload to OSS.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3521:445,avail,available,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3521,1,['avail'],['available']
Availability,"The spec does say Array[?] is allowed so this should work. Tested on: d9b9262 and release 35. Test wdl:; ```wdl; version 1.0. workflow Test {; input {; }. # This fails currently; Array[SomeStruct] array = read_json(""test.json""). # This does work; SomeStructs array2 = read_json(""test2.json""). output {; }; }. struct SomeStruct {; String string; }. struct SomeStructs {; Array[SomeStruct] array; }; ```. test.json; ```json; [{""string"": ""bla""}]; ```. test2.json; ```json; {""array"": [{""string"": ""bla""}]}; ```. error:; ```; [2018-10-09 10:16:43,95] [error] WorkflowManagerActor Workflow 04cdb52a-6e98-4674-b313-f8f4d406ffb2 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to process workflow definition 'Test' (reason 1 of 1): Failed to process declaration 'Array[SomeStruct] array = read_json(""test.json"")' (reason 1 of 1): Cannot coerce expression of type 'Object' to 'Array[WomCompositeType {; string -> String ; }]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4219:507,error,error,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4219,2,['error'],['error']
Availability,"The specification according to the documentation is `${true=""--enabled"", false=""--disabled"" boolean_var}`, and the false tag is optional. However, I get this error when attempting this syntax:. ```. cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Both 'true' and 'false' attributes must be specified if either is specified:. --tumorBam=${tumor_bam} --normalBam=${normal_bam} ${true='--targeted' targeted} \; ^. at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:186); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:156); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:151); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:663); at akka.actor.FSM.processEvent$(FSM.scala:660); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.LoggingFSM.processEvent(FSM.scala:799); at akka.actor.LoggingFSM.processEvent$(FSM.scala:781); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor.aroundReceive(Actor.scala:513); at akka.actor.Actor.aroundReceive$(Actor.scala:511); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.ActorCell.r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2594:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2594,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811:141,echo,echo,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"The way this works for now is:; In JES configuration you need to specify an `authenticationMode` parameter which can take one of those 2 values: `service_account` or `refresh_token`; `service_account` works exactly the same way as it did before; `refresh_token` will require 2 fields to be passed as workflow options:; `account_name` and `refresh_token`. Depending on what is available, the JES backend will create, only if necessary and at workflow initialization time, a json file (gcloudauth.json) and upload it to the root directory of the workflow.; Currently this json file can contain 2 types of information:; - Docker credentials: These are optional and can be specified in the `jes` section in application.conf. They look like this:. ```; dockerAccount = ""my.docker@account.com""; dockerToken = ""mydockertoken""; ```. If they are specified a ""docker"" value will be added to gcloudauth.json containing those values.; This will allow using a private docker image that would not be accessible otherwise.; This functionality was mostly already there from Kristian PR, I just moved the credential location from gcs to conf.; - User credentials: These need to be added as workflow options : . ```; {; ""account_name"": ""myaccount@broadinstitute.org"",; ""refresh_token"": ""refresh_token""; }; ```. Again if in RefreshToken mode, they need to be there for every workflow call or an exception will be thrown at initialization time.; If they are there they will be added to the gcloudauth.json in the same way docker crednetials are, under a ""gcloud"" value. You should then get permission to localize input files that are only accessible to this user. So depending on what is provided (docker info, user info), you can get a gcloudauth.json with both credentials, only one, or no file at all. In any case the JES backend will try to delete this file when the workflow reaches a terminal state.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/190:376,avail,available,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/190,1,['avail'],['available']
Availability,"The wdl below doesn't pass input processing. It's a valid bash command. I suspect it has to do with the @ sign? . This wdl (also copied below) reproduces the issue:; /humgen/gsa-hpprojects/dev/tsato/m2-evaluation/wdl/test/hello.wdl; /humgen/gsa-hpprojects/dev/tsato/m2-evaluation/wdl/test/hello.json. #### wdl ####. task hello {; String addressee; ; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${array[@]}; do; echo $i; done; >>>. runtime {; docker: ""ubuntu:latest""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input. call hello {input: addressee = hello_and_goodbye_input }. output {; String hello_output = hello.salutation; }; }. #### json ####; {; ""hello_and_goodbye.hello_and_goodbye_input"": ""Takuto""; }; #### error message ####; ""failures"": [{; ""message"": ""Workflow input processing failed:\nUnable to load namespace from workflow: Unrecognized token on line 7, column 22:\n\n for i in ${array[@]}; do\n ^""; }]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819:363,echo,echo,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819,4,"['echo', 'error', 'failure']","['echo', 'error', 'failures']"
Availability,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3790:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790,1,['error'],['error']
Availability,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1457:859,down,down,859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457,1,['down'],['down']
Availability,"There are two general cases where the `WorkflowExecutionActor` switches the workflow to a failed state:; - [`handleRetryableFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L292-L306); - [`handleExecutionFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L240-L255). In _both_ cases, the method:; - is triggered by the failure of an individual job, but; - fails the entire workflow, and; - doesn't send signals to other jobs to stop. Thus other jobs stay running, while the workflow gets deregistered from the system. Because the workflow manager can no longer delegate aborting the running jobs, this issue may also be related to #1414 and #1504.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029:613,failure,failure,613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029,1,['failure'],['failure']
Availability,"There have been many requests over time to provide data on CPU, memory, and disk usage. . In an ideal world this would be a rich time series of data, in a less ideal world this would be max with a couple of periodic data points, and in a much more practical world it'd just be max usage. The last one is the definition of done here but if one wants to get ambitious ....... The PAPI backend already allows for a custom script to be attached, we should be putting in something on our own for any backend which runs things via unix command lines (ie spark jobs and such likely don't make sense). At least one group is using the following in production, this is likely a good start if not completely AOK. ``````#!/bin/bash; echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); echo ; echo --- Runtime Information ---. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(top -bn 2 -d 0.01 | grep '^%Cpu' | tail -n 1 | awk '{print $2}')%; echo \* Memory usage: $(free -m | grep Mem | awk '{ OFMT=""%.0f""; print ($3/$2)*100; }')%; echo \* Disk usage: $(df | grep cromwell_root | awk '{ print $5 }'); }. while true; do runtimeInfo; sleep 300; done```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507:721,echo,echo,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507,13,['echo'],['echo']
Availability,"There is a bug in the owl library where ontology parsing can sometimes fail, causing tests to fail.; See https://github.com/protegeproject/webprotege/issues/298. For now retry parsing a few times. Should fix the `cwl_format***` transient centaur failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4210:246,failure,failures,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4210,1,['failure'],['failures']
Availability,"There is a claim of 5–10% faster compiler performance since 2.12.8, which seems worthwhile. Not super scientific, but a build test before and after (with warmed JVM) went down from about 106 to 97 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5106:171,down,down,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5106,1,['down'],['down']
Availability,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4673:349,down,download,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673,2,['down'],"['download', 'downloading']"
Availability,"There is a potential race condition (which seems to have occurred at least once in our integration test) where Cromwell shuts down after submitting a job to PAPI but before writing the operation ID to the DB.; In this case, upon restart, the job will be submitted a second time, using the same bucket, causing all sorts of issues as 2 jobs are now concurrently reading from and writing to the same files.; [Example bucket](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/dontglobinputs/c10b823f-7f7c-4442-b0d1-a90f0f4e39a7/call-globtask/?project=broad-dsde-cromwell-dev&organizationId=548622027621)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4209:126,down,down,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4209,1,['down'],['down']
Availability,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1643:167,failure,failures,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643,1,['failure'],['failures']
Availability,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:765,ERROR,ERROR,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['ERROR'],['ERROR']
Availability,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1272:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272,4,['failure'],"['failure', 'failures']"
Availability,"There seems to be a failure mode where a workflow fails during centaur JES testing because the JES job can't find the auth JSON:. ```; java.lang.Exception: Task readFromCache.find:NA:1 failed. JES error code 3. Message: unable to load extra config: download from ""gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json"" failed: exit status 1: Copying gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json...; / [0 files][ 0.0 B/ 133.0 B] ; NotFoundException: 404 gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json does not exist.; ```. This happened on the readFromCacheFalse JES centaur test. logs are at `gs://cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/61e52cbe-50f5-4e45-b8fb-a61c1f902426/call-centaur/cromwell_root/logs/`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2405:20,failure,failure,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2405,3,"['down', 'error', 'failure']","['download', 'error', 'failure']"
Availability,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3901:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901,2,['down'],['down']
Availability,"There's an Akka Streams AbruptTerminationException being thrown by some process being shutdown unexpectedly. It's not causing any actual issues for users, however it's presence in the log is distracting. We should make sure that whatever process is throwing this error should be shut down more gracefully. ```[2017-06-05 16:56:14,16] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-unknown-operation#-1334744097]] terminated abruptly; [ERROR] [06/05/2017 16:56:14.156] [cromwell-system-akka.actor.default-dispatcher-37] [akka.actor.ActorSystemImpl(cromwell-system)] Outgoing request stream error (akka.stream.AbruptTerminationException)```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340:263,error,error,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340,6,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"There’s a new response from JES that should be retryable: ; JES error code 2. Message: Instance failed to start due to preemption.” . THE CATCH: Henry chatted with Google, and it sounds like JES error code 2 isn’t *always* about preemption. (But in the past few weeks we’ve had a handful of cloud workflows failing each day from this response, always with the message about preemption). So to be safe, we want to make this one retryable based on the combination of the code + the message. . Example: ; UUID: 7da0394b-371f-4fdb-ae70-737833c4fbfa; OPERATION_ID: operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. Relevant workflow metadata: ; ""failures""; : [; {; ""causedBy"": [],; ""message"": ""Task PairedEndSingleSampleWorkflow.CollectUnsortedReadgroupBamQualityMetrics:2:1 failed. JES error code 2. Message: Instance failed to start due to preemption.""; }; ],",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,4,"['error', 'failure']","['error', 'failures']"
Availability,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4951:237,failure,failure,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951,2,"['error', 'failure']","['errors', 'failure']"
Availability,"These changes get us incrementally closer to GHA migration by trimming down to what is absolutely necessary. I did the [87 release](https://github.com/broadinstitute/cromwell/releases/tag/87) with the exact version of the WDL in this PR, so I know it works. - Drop support for minor releases, unused under our modern release cadence; - Drop the Homebrew section of the WDL, they do it automatically for us and our instructions have said ""ignore this"" for several years now; - Remove references to `firecloud-develop` and Jenkins 🚀; - Document side effects for each task in release WDL",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7422:71,down,down,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7422,1,['down'],['down']
Availability,These workflows all failed with the stacktrace error in the comments section. These were run on gotc-prod. 836783df-339e-4631-a7cd-038072899edb - operations/EIfzx8m1Khjt87q2wPLY8CAgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU. 9925f072-debf-4e4c-98aa-01db0a34a62f - operations/EOqx-cy1KhioppLVjZKx1XQgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU. f7c465bb-e5e5-4cb8-b493-82f072a40040 - operations/EIL4ks21Khi1zfCMm4HHpO8BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:47,error,error,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['error'],['error']
Availability,"Things Changed; - added a cromwell-master, which refreshes status metadata. There can be only one.; - created a pool of cromwell_norefresh, which has status refresh turned off, these can scale; - found race condition when multiple cromwells try to create the liquibase lock table at once, configured to have master go first; - updated scripts/compose to handle the above two kinds of cromwell; - increased to 100 batches of 10 workflow each; - changed timeout script to show number of completed workflows and break when done; - delete database at start of run, so the above works; - ran heartbeats in auto-commit mode rather than in a single transaction; - dump out logs at end of run for debugging. Things I'd like to share; - Lock Ordering in SELECT...FOR UPDATE no es bueno, there are great feature in MYSQL v8 (SKIP LOCKED) but we can't use those yet; - how to configure mysql for query logging, and what it shows; - heartbeat batches were never a batched update, just a big transaction; - slick terminology can give give the wrong intuition; - impact of cleaning db before each run; - No deadlocks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508:587,heartbeat,heartbeats,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4110:281,error,error,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110,3,['error'],"['error', 'errors']"
Availability,"This PR addresses two bugs from WX-1260:; 1. An `echo` command that included asterisks wasn't wrapped in double quotes, causing an issue in some environments (the user's) but not others (the developer's). Added double quotes so this issue occurs in no environments. 2. The Script Preamble that exports the SAS token environment variable was running in a bash subshell, which means that the environment variable it populated wasn't available in the user's parent shell that is actually executing the task.; - To fix this, I added the option for script preambles to be executed in a bash subshell, or not. My thinking:; - It's generally good hygiene for scripts to run in their own subshell, and I didn't want to change anything about the GCP behavior, so I left that functionality as is.; - I didn't want to write a sas token to file in the subshell for the parent shell to read. Writing tokens to file seems not great for security.; - In order for the environment variable to be visible to the user command, I allowed the TES script to run in the parent shell. This bug revealed a gap in my testing: I had been confirming that my script could acquire the token successfully and correctly, but I hadn't actually tried to use that token inside the user command block. The concept of subshells eluded me at the time. After making this change, I've confirmed that the environment variable is indeed useable in the command block.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7326:49,echo,echo,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7326,2,"['avail', 'echo']","['available', 'echo']"
Availability,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028:875,failure,failure,875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028,1,['failure'],['failure']
Availability,"This PR adds the ability to launch a workflow as part of the Jenkins job. To do this you need to:. - create a workflow script which can download the necessary workflow files (source, input, options,..) and make `curl POST` request to Cromwell; - put the script inside cromwell -> scripts -> perf -> vm_scripts -> workflow_scripts folder; - pass the name of this script through `WORKFLOW_SCRIPT` param while building the Jenkins job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4090:136,down,download,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4090,1,['down'],['download']
Availability,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4675:290,failure,failures,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,"This PR documents how to use the singularity cache in a way that every image only gets pulled once. It also documents why the `--containall` flag should be used with singularity at all times. Pinging @illusional @TMiguelT and @vsoch as heavy singularity promotors. What do you think of this?. EDIT: I cancelled the tests, as this is documentation and should not influence the Cromwell code. EDIT2: Fixes #5063",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515:192,Ping,Pinging,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515,1,['Ping'],['Pinging']
Availability,"This PR fixes [this](https://github.com/broadinstitute/cromwell/issues/4086) bug. It now handles the cases when empty input file or invalid json (but valid yaml) inputs is passed to Cromwell, and returns better error messages for them. Closes #4086",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4375:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4375,1,['error'],['error']
Availability,"This PR includes the majority of the work required to implement the [Struct Literal Syntax of WDL 1.1 ](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#struct-literals). At a high level, we can now parse the new syntax, and turn that into a value that can be validated & used. This PR _does not_ include the strict type checking defined in the spec. Since that will require some small tweaks to the linking step (and for the sake of the reviewers), I figure it's better to do that in a subsequent PR. In practice, this means that errors aren't thrown in certain situations where you would expect them to be thrown. Examples:. ```; struct MyStructType {; Int myIntMember; String? myStringMember; }. // the new syntax; MyStructType a = MyStructType{myIntMember: 4, myStringMember: ""Hi!""}. // still works just like any old struct value; MyStructType b = a. // note that omission of optional members is allowed by the spec; MyStructType c = MyStructType{myIntMember: 3}. // Literals are values; Int someInt = (MyStructType{myIntMember: 5, myStringType: ""Bye!""}).myIntMember. // As of this PR, this will not throw an error (but it technically should); Boolean illegalBool = (MyStructType{myIntMember: 5, myStringType: ""Bye!"", iMadeUpThisBool: false}).iMadeUpThisBool; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7391:544,error,errors,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7391,2,['error'],"['error', 'errors']"
Availability,"This PR is a lot less scary than it looks. Most of the changed lines spring from two changes (and the changes are actually make everything a lot simpler!):. - `TaskDefinition` is split into two: `CallableTaskDefinition` which can be called, and `ExecutableTaskDefinition` which can be executed.; - `Callable` now doesn't have a `graph: ErrorOr[Graph]`. Instead the new, more specific `ExecutableCallable` has a `graph: Graph` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2793:336,Error,ErrorOr,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2793,1,['Error'],['ErrorOr']
Availability,This PR replaces the Java APIs with `gsutil` commands to download a GCS file during DRS localization. Closes https://broadworkbench.atlassian.net/browse/WA-168,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5552:57,down,download,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5552,1,['down'],['download']
Availability,"This PR updates the fetchWorkflowsToStart method to first find a hog group with lowest actively running workflows and then fetch workflows to start from that hog group. . Note to reviewers: I eventually had to separate the big select query into 2 parts - first find the hog group with lowest actively running workflows and then fetch workflows for that hog group because the outer select query had `forUpdate` and inner select query with `groupBy` clause which is not allowed in Postgres DB and throws the below error.; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; org.postgresql.util.PSQLException: ERROR: FOR UPDATE is not allowed with GROUP BY clause; ```. SQL generated from Slick:; Query to find hog group with lowest count of actively running workflows; ```; select ; x2.x3 ; from ; (; select ; `HOG_GROUP` as x3, ; count(1) as x4 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x2, ; (; select ; `HOG_GROUP` as x5, ; count(1) as x6 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < {ts '2022-02-16 10:26:31.39' }; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x7 ; where ; x2.x3 = x7.x5 ; order by ; (x2.x4 - x7.x6), ; x2.x3 ; limit ; 1; ```. Query to select startable workflows for above hog group; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6680:512,error,error,512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6680,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"This WDL validates and even executes ""successfully"" but it shouldn't:. ```; task hello {; command {; #nothing; }; output {; String out = ""out""; }; }. task bye {; String instring = ""bye""; command {; echo ${instring}; }; output {; String out = read_string(stdout()); }; }. workflow w {; call hello; call bye { input: instring = hello }; }; ```. `call bye { input: instring = hello }` is invalid",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2394:198,echo,echo,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2394,1,['echo'],['echo']
Availability,"This WDL:. ```; task nofiles {; command {; echo ignored; }. output {; Array[File] nofiles = glob(""*.no.such.files""); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call nofiles. output {; Array[File] out = nofiles.nofiles; }; }; ```. Causes this error:. ```; 2017-01-23 15:14:06,797 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - JesAsyncBackendJobExecutionActor [UUID(7e4afc46)w.nofiles:NA:1]: JesAsyncBackendJobExecutionActor [UUID(7e4afc46):w.nofiles:NA:1] Status change from Running to Failed; 2017-01-23 15:14:07,862 cromwell-system-akka.dispatchers.engine-dispatcher-8 ERROR - WorkflowManagerActor Workflow 7e4afc46-a1ff-424f-aecf-4ce742ed5fa8 failed (during ExecutingWorkflowState): It appears that some of the expected output files for task w.nofiles:NA:1 did not exist when the command exited.; A few things to try; 1) Check that the output section in your WDL is correct. Remember that all output files declared in a task must exist when the command exits.; 2) Check that the return code is available and is valid with respect to your command expected exit code; 3) Look into the stderr (gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/nofiles-stderr.log) file for evidence that some of the output files the command is expected to create were not created.; Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* -> gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/ (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/, command failed: CommandException: No URLs matched: /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/*\nCommandException: 1 file/object could not be transferred.\n); Check the content of stderr for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1896:43,echo,echo,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1896,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,This affects to HtCondor backend. Given:; Following WDL task. ``` ; task Example {; command {; echo foo;; mkdir testFolder; }; } ; ```. When:; Runs workflow using HtCondor backend. Then:; It creates testFolder in docker host machine. Expected result:; Whole command executed within the container.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1719:95,echo,echo,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1719,1,['echo'],['echo']
Availability,"This allows this workflow to run:. ```; task t {; Array[String] s; command {echo ${sep=',' s}}; output {String o = read_string(stdout())}; }. workflow w {; Array[String] x = read_lines(""gs://sfrazer-dev/array.txt""); call t {input: s=x}; }. ```. So this is the minimum implementation I could come up with... looking for design feedback too, this could be simple or become full-blown pluggable filesystems",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/824:76,echo,echo,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/824,1,['echo'],['echo']
Availability,"This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ""gred-cumulus-sb-01-991a49c4"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:3488,down,downloading,3488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['down'],['downloading']
Availability,"This bug was caused by ""userland"" (WDL) code throwing an exception where we did not expect it, causing issues for the ""kernel"" (Cromwell). Now we encapsulate possible exceptions in an `ErrorOr` so they can be safely evaluated in the `case Left(f) =>`. As a stylistic point, I changed `WomMap()` to `WomMap.apply()` to make it more obvious that we're really doing a lot of custom stuff and it's not just a regular old constructor. As a bonus, the new code detects all previous stuck-aborting workflows and fails them. After:; ```; INFO - WorkflowManagerActor: Workflow 1432d67e-3e95-40c8-acbd-d42f75040f1b failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'example2' (reason 1 of 1): Evaluating { ""second"": test, ""lowerLayer"": example1 } failed: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; INFO - WorkflowManagerActor: Workflow actor for 1432d67e-3e95-40c8-acbd-d42f75040f1b completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""1432d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:185,Error,ErrorOr,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Error'],['ErrorOr']
Availability,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:229,heartbeat,heartbeats,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"This change enables blob filesystems to be opened that do not belong to the workspace, including public containers, and containers the requesting user has access to via WSM. This involves frequent 'refreshing' of the stored open filesystem in the underlying NIO implementation. To minimize the number of redundant requests to WSM, the SAS token for each open filesystem is stored and checked for expiration before attempting to reopen a previously open filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140:304,redundant,redundant,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140,1,['redundant'],['redundant']
Availability,"This change makes workflows fail slow instead of fail fast. I'm uncomfortable committing this as-is because I don't fully understand when startRunnableCalls is used - and is it possible to miss the ""are we done (including failures)"" check. This _does_ work when a single call fails as part of a longer workflow. No other cases have been tested. Can anyone give me a quick tour of the control flow in WorkflowActor to make sure I haven't missed anything?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/436:222,failure,failures,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/436,1,['failure'],['failures']
Availability,This changes makes these error messages visible in the separate workflow log file we make available to users.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7220:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7220,2,"['avail', 'error']","['available', 'error']"
Availability,"This compiler flags allows the compiler to take types of kind `F[_,_]` and infers `F[_]` by parameterizing the right most argument and fixing the other ones. This allows us to omit type args when traversing to `Either` and `ErrorOr`, because scala can now see them as shape `F[_]` instead of ""you gave me a type w/ 2 type args and I was expecting one"". **I did a find/replace on all our traverses, this may be controversial**, chime in if you disagree. [Longer explanation](https://gist.github.com/djspiewak/7a81a395c461fd3a09a6941d4cd040f2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3580:224,Error,ErrorOr,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3580,1,['Error'],['ErrorOr']
Availability,"This currently only throws an exception when it tries to call the task. Instead, it should not be able to generate a validated WOM graph in the first place:; ```wdl; workflow oops {; call oopsie; }. task oopsie {; String str; command { echo ${str} }; runtime { docker: docker_image }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3326:236,echo,echo,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326,1,['echo'],['echo']
Availability,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3040:591,failure,failures,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040,1,['failure'],['failures']
Availability,This does assert at least that the value is valid Json. We may consider going to a coproduct in the future but this will get us past parsing failure in the conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2987:141,failure,failure,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2987,1,['failure'],['failure']
Availability,"This enables tests on a lot of Slick code that's not actually used yet in the New Worlde (nothing writes to the core engine tables since only Recover needs that and Recover hasn't been implemented). So these changes are valuable iff the New Worlde ultimately uses an engine Slick API that looks a lot like that in the Olde Worlde. My guess is that will end up being true, but at this point that's only a guess. Some things here will certainly be nixed (ExecutionEvents) or are likely to be heavily modified (anything caching-related).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/942:142,Recover,Recover,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/942,2,['Recover'],['Recover']
Availability,"This fails at runtime but we shouldn't even allow it to validate:. ```wdl; version 1.0; struct Foo {; Int foo_int; }. [...]. task bar {; input {; Foo f; }; command {; echo ~{f} # Bad interpolation, should be ~{f.foo_int}; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3917:167,echo,echo,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917,1,['echo'],['echo']
Availability,"This fixes the docker deadlock by doing 2 things:. - makes sure that all `HttpResponse`s are either consumed or discarded to avoid https://github.com/akka/akka/issues/19538; - removes the decoupling between the `tokenFlow` and the `manifestFlow`; This is believed to be the main cause of the problem. Decoupling the token request from the manifest request can create a situation where all the connections are being used for token requests, and no connection is available to make a manifest request which makes the stream freeze.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2287:461,avail,available,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2287,1,['avail'],['available']
Availability,This fixes the response error codes. It will now return `404 Not Found` for an unrecognized workflow ID and `400 Bad Request` for a malformed or invalid workflow ID.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3918:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918,1,['error'],['error']
Availability,"This happened after we turned off call caching to work around DSDEEPB-2938 for now and this error popped up in what seems to be post success processing of the task. Logs are in the comment. This was run on gotc-prod against JES-staging. ``` scala; 34108:2016-03-10 22:53:11,258 cromwell-system-akka.actor.default-dispatcher-8 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from - to Initializing; 50265:2016-03-10 22:55:07,880 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Initializing to Running; 64201:2016-03-10 22:57:00,293 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Running to Success. 2016-03-10 22:57:05,450 cromwell-system-akka.actor.default-dispatcher-27 ERROR - CallActor [UUID(7721686a):StripBamExtension:8]: Failing call: 500 Internal Server Error; Internal Error; cromwell.util.AggregatedException: 500 Internal Server Error; Internal Error; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112) ~[cromwell.jar:0.19]; at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:623) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:662) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:657) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureIn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:92,error,error,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"This implements the specification for unspecified task inputs https://github.com/openwdl/wdl/pull/359 . . It has been backported to wdl 1.0 because it enables useful functionality (I.e. allowing nested optional inputs to be defined if this is specified in the meta section of the workflow). . It may break some workflows that have their required inputs nested deeply inside subworkflows and tasks, but Broad's own WDL linter for IntelliJ has been warning against this behavior for a while now. . To me it always was a bit weird that Cromwell allowed `""my_workflow.my_subworkflow.my_required_input"": 5` in the input because it was required, but not `""my_workflow.my_subworkflow.my_optional_input"": 5` because it was optional. Now both are not allowed by default. And the optional nested input is only available after explicitly enabling it in the top-level workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5523:800,avail,available,800,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523,1,['avail'],['available']
Availability,"This is a bug report that seems to be caused by the `None` type introduced in version 1.1 not being handled properly when assigned to fields in custom structs. Consider the following minimal example:; ```; version development. struct OptionalFiles {; 	File required; 	File? optional; }. workflow TestNone {; 	input {; 		Array[File] letters = [""a.txt"", ""b.txt"", ""c.txt""]; 	}. 	scatter (letter in letters) {; 		OptionalFiles opt = {""required"": letter, ""optional"": None}; 	}. 	output {; 		Array[OptionalFiles] out = opt; 	}; }; ```; This is run in a directory containing empty files `a.txt`, `b.txt`, and `c.txt`. Assigning `None` to the `optional` field in type `OptionalFiles` seems to cause the following unhandled stacktrace:. ```; [2023-11-07 14:51:14,22] [info] MaterializeWorkflowDescriptorActor [4e522458]: Call-to-Backend assignments:; [2023-11-07 14:51:17,38] [error] Cannot construct WomMapType(WomStringType,WomOptionalType(WomAnyType)) with mixed types in map values: [WomOptionalValue(WomSingleFileType,Some(WomSingleFile(c.txt))), WomOptionalValue(WomAnyType,None)]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomOptionalType(WomAnyType)) with mixed types in map values: [WomOptionalValue(WomSingleFileType,Some(WomSingleFile(c.txt))), WomOptionalValue(WomAnyType,None)]; 	at wom.values.WomMap.<init>(WomMap.scala:65); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wom.values.WomMap$.coerceMap(WomMap.scala:30); 	at wom.values.WomMap$.apply(WomMap.scala:46); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:89); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:86); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:74); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateV",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:868,error,error,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['error'],['error']
Availability,"This is a general form of the workflow that I had this issue with:. ```; workflow wf {; Boolean condition; Array[String] files. scatter (row in files) {; if(condition) {; call A; }; if(!condition) {; call B; }; }; }; ```. The tasks themselves finish, however I get the following error:. `Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlFileType)) with this value: List(WdlSingleFile(/dsde/working/asmirnov/evaluations/CovariateCorrection/scripts/test_wdl/cromwell-executions/cnv_coverage_workflow/c71f4899-6031-4a21-b9b4-1dafd659cd05/call-CalculateTargetCoverage/shard-0/execution/TCGA-02-2483-10A-01D-1494-08.coverage.tsv), WdlSingleFile(/dsde/working/asmirnov/evaluations/CovariateCorrection/scripts/test_wdl/cromwell-executions/cnv_coverage_workflow/c71f4899-6031-4a21-b9b4-1dafd659cd05/call-CalculateTargetCoverage/shard-1/execution/TCGA-02-2485-10A-01D-1494-08.coverage.tsv))`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1936:279,error,error,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1936,1,['error'],['error']
Availability,"This is a port of the original pull [request](https://github.com/broadinstitute/wdl4s/pull/256) to the Cromwell tree. I hope this was done correctly, while addressing the review comments. Thanks for pointing out how to use `Try` and `should equal` in the testing framework. There are two outstanding comments, which I am answering here, since I am assuming you want to dismantle the old wdl4s git repository. . 1) TypeEvaluator.scala; *Q: Isn't the caller perhaps trying to ascertain if the expression actually can be coerced to the declared type?* ; A: That could be added as an assert. . 2) WdlSubworkflowWomSpec.scala; *Q: The magic conversion of a single Thing to an Array[Thing] was a feature someone explicitly asked for way back when. I never liked this feature but I'm sure there's WDL out there that expects this to work.*; A: I can see why a user might want that. A side effect is that Array[T] can automatically be coerced to Array[Array[T]]. I ran into a case where Array[String] was coerced into Array[Array[File]]. For example, the workflow below executes under Cromwell v0.29, involving the coercion of `Array[Int]` to `Array[Array[Int?]]`.; ; ```; # Trying out file copy operations; task Num {; Array[Array[Int?]] numbers; command {; }; output {; Array[Array[Int?]] result = numbers; }; }. workflow w {; Array[Int] primes = [2, 3, 5, 7, 11]; call Num { input: numbers = primes }; output {; Num.result; }; }; ```. This raises two issues in my mind: ; 1) There are many ways to convert a single dimensional array into a two dimensional array. Which one does the WDL language specify? ; 2) A user can mistakenly pass an incorrectly typed argument to a task without getting a compiler error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807:1697,error,error,1697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807,1,['error'],['error']
Availability,"This is a proposal on how to solve the ""insufficient data written"" error that is due to the batch request doing PAPI job creation / polling being too large. The actual limit number is unknown yet (ticket is open with google).; Depending on their answer the solution might look different but this would be one way to fix it.; I tested it with @ruchim's WDL that reproduces this problem and didn't see it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2554:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2554,1,['error'],['error']
Availability,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:51,recover,recoverAsync,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Availability,"This is against 29 but could also be an issue with the planned structure for 30. For comparison, a somewhat similar issue with subworkflow declaration evaluation [in 29](https://github.com/broadinstitute/wdl4s/pull/257) will inform the way subworkflow declaration evaluation should work in 30. An uninitialized optional declaration in a subworkflow kills the SubworkflowExecutionActor whether it is referenced or not. The workflow [here](https://github.com/broadinstitute/centaur/pull/242/files) is able to run against 29 hotfix with the patches in the [wdl4s](https://github.com/broadinstitute/wdl4s/pull/257) and associated cromwell PRs. But if the uninitialized optional declaration on [this line](https://github.com/broadinstitute/centaur/pull/242/files#diff-cc04c14d68a6a1a6d8d8366fc0c2f88cR48) is uncommented, the workflow fails. (Deliberately not quoting since lines don't wrap and anyway the thumbs downs are apropos). 2017-11-14 18:00:05,062 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-11-14 18:00:05,093 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - MaterializeWorkflowDescriptorActor [UUID(4b725606)]: Call-to-Backend assignments: decls.sub_decls.second_task -> Local, decls.sub_decls.first_task -> Local; 2017-11-14 18:00:06,129 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd [UUID(4b725606)]: Starting calls: SubWorkflow-sudecls:-1:1; 2017-11-14 18:00:06,130 cromwell-system-akka.actor.default-dispatcher-50 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreRegisterSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#-388497585] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/SubWorkflowExecutionActor-SubWorkflow-sudecls",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2902:907,down,downs,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2902,1,['down'],['downs']
Availability,"This is an attempt to make the separation between (JES / Local / SGE) and File systems (GCS, SharedFilesystem) more obvious. It enables implementation of more Backend / File systems combinations.; For example this wdl runs, on a Local cromwell instance :; Wdl: . ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String fileoutput = read_string(stdout()); String salutation = read_string(""gs://tjeandet/wdl_input_public.txt""); }; }. workflow hello {; File infile; String instring = read_string(infile); call hello {input: addressee = instring}; }; ```. Inputs:. ```; {; ""hello.infile"": ""gs://tjeandet/wdl_input_public.txt""; }; ```. It also works with user-defined authentication for GCS (refresh token).; More tests / cleaning is needed before it's reviewable but I wanted to put it out there before Thanksgiving.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305:311,echo,echo,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305,1,['echo'],['echo']
Availability,This is like #957 but for Failure Events. Failure Events are published as metadata but no longer rate their own separate table.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/964:26,Failure,Failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/964,2,['Failure'],['Failure']
Availability,"This is more of an epic and not a right now thing, but I wanted to capture the thought for future consideration. In the author's not-so-humble opinion we log way too much stuff, to the point that logs aren't particularly useful unless you know exactly what you're looking for. We also know that Cromwell's logs are blowing apart loggly and stuff like that. We also manage to log some stuff (e.g. stacktraces) multiple times for one incident. We also have many issues where logs would be super helpful yet all we see is something like ""an error occurred"" without any context. We should sit down as a group and with focus groups of downstream clients (e.g. firecloud, gotc) and go through what we're logging and look for ways to both massively debulk our logs as well as making sure that we're logging the most useful stuff.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778:538,error,error,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778,3,"['down', 'error']","['down', 'downstream', 'error']"
Availability,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4963:498,echo,echo,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963,3,"['Error', 'echo', 'error']","['Error', 'echo', 'error']"
Availability,"This is on PAPI side and has already been reported and acknowledged by Aaron. ```; pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4258:243,error,error,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4258,2,['error'],['error']
Availability,"This is related (at least) to https://github.com/broadinstitute/cromwell/issues/2446. Examine the workflow `wf` below.; ```wdl; workflow wf {; String? who. call hello { input:; who = who; }; }. task hello {; String? who = ""world"". command {; echo ""Hello, ${who}""; }; output {; String result = read_string(stdout()); }; }; ```. When running `wf` with Cromwell v32 the result is `hello.result = ""Hello, world""`. However, since we are calling the task with an empty value, it should override `who`, and the the result should be (I think) `Hello,""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3819:242,echo,echo,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3819,1,['echo'],['echo']
Availability,"This is sent out by the `EmptySubWorkflowStoreActor` but but not waited for by the `SubWorkflowExecutionActor` (which indeed ignores it if it ever receives one). This is awkward because it means we sometimes get ""Message not delivered"" error logs. We should either remove this message entirely or, if it's used by tests, we should only send one out if the receiver is actually waiting for it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2185:236,error,error,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2185,1,['error'],['error']
Availability,"This issue was prompted by the following thread on the forum: https://gatkforums.broadinstitute.org/wdl/discussion/10347/localization-via-hard-link-has-failed. By default, cromwell does not specify a user when running docker, which leads to two issues:. Firstly, it will run each tasks as whatever user the docker image specifies, typically 'root' as this is the default. This means that depending on how the docker image was build, a completely random user is suddenly the owner of your output files. For example, if the user inside the docker container has UID 1042, this could map to a completely unrelated user on the host system, who is suddenly the owner of your output files. Related to this issue is the fact that all output files have to be world-readable by default, or else the cromwell process will not have read access to the files it has just created (which are now owned by UID 1042). This is not desireable when cromwell is run on a system with many users, some of which should not have read access to eachothers data (for example when working with patient data). As a solution, I propose to change the default docker invocation to run the analysis as $EUID by default. This works even when the EUID is not mapped to a valid user within the docker image, and ensures that the cromwell user is the owner of all the files generated by docker. From man bash: ""EUID Expands to the effective user ID of the current user, initialized at shell startup. This variable is readonly."" , so it should be available on every system. This solution makes cromwell act more secure by default, and will also solve the issue of copying over data files as discussed on the forum and in #2620",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2658:1508,avail,available,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2658,1,['avail'],['available']
Availability,"This may be a known issue, in which case feel free to link this to that. If not... I ran a workflow on the DSDE-Methods server, which is running on P.API, and it failed when a file didn't exist. I was running a CNV pipeline to create a Panel of Normals, and a few of the bams (and their bai's) in the list did not exist. From @LeeTL1220: Cromwell can't localize a file because the file does not exist. As a result, instead of saying it, it does not create workflow root directory, doesn't make any usable log files.; The error message is just the name of the file, rather than to say that this file is missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2774:521,error,error,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2774,1,['error'],['error']
Availability,"This may be a possible dupe. Below is a stack trace of a `java.nio.Path` operation that threw an IOException the first time, but would have likely succeeded if tried again. Ideally the read/write operation, wdl task, whole workflow, etc. should be retried at some later asynchronous time. In the particular case below, the JES initialization actor was attempting to write the authentication file when it received a common 503 error:. ```; 2017-01-23 05:37:49,143 cromwell-system-akka.dispatchers.engine-dispatcher-45 ERROR - WorkflowManagerActor Workflow b08cdbf3-08ea-4bfa-a612-c4452f120c84 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:61); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:58); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile(JesInitializationActor.scala:58); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:52); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:51); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:156); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:155); 	at scala.concurrent.Future$$anonfun$flat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:426,error,error,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This might be the only remaining type of transient failure which hasn't been patched, but it does pop up fairly frequently. It looks like the usual pattern of retries would work here. ```; cromwell.core.CromwellFatalException: java.util.NoSuchElementException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.NoSuchElementException; 	at java.util.ArrayList$Itr.next(ArrayList.java:854); 	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43); 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:51,failure,failure,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,3,"['failure', 'recover']","['failure', 'recoverWith']"
Availability,"This one's a bit odd. Submitting workflows with imports to release 24 server intermittently gives errors in the logs. A simple workflow: ; `cat goodImport.wdl`:. ```; import ""bar.wdl"" as doIt. workflow testMe {; 	call doIt.doIt; }; ```; And a bunch of wdl tasks in a folder, only one of which is the actual dependency (`bar.wdl`); ```; conradL@qimr13054 ~]$ unzip -l foo.zip ; Archive: foo.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 14:46 foo/; 99 02-07-2017 14:45 foo/bar7.wdl; 98 02-07-2017 14:00 foo/bar.wdl; 99 02-07-2017 14:46 foo/bar8.wdl; 99 02-07-2017 14:45 foo/bar2.wdl; 100 02-07-2017 14:45 foo/bar10.wdl; 99 02-07-2017 14:46 foo/bar9.wdl; 99 02-07-2017 14:45 foo/bar1.wdl; 99 02-07-2017 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:98,error,errors,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['error'],['errors']
Availability,"This potential bug was encountered while writing a CWL task that required a pair of inputs. Namely, Cromwell is requiring a parameter that is both 1) not required per CWL spec and 2) not even used. Working and failing examples given below. . I am on a Linux Mint (19) machine, using `cromwell-36.jar` (downloaded October 29, 2018) with java10:; ```; $ java -version; openjdk version ""10.0.1"" 2018-04-17; OpenJDK Runtime Environment (build 10.0.1+10-Ubuntu-3ubuntu1); OpenJDK 64-Bit Server VM (build 10.0.1+10-Ubuntu-3ubuntu1, mixed mode); ```. For both the working and failing CWL files, I use the following input:; ```; {; ""paired_parameters"": {; ""itemA"": ""one"",; ""itemB"": ""two""; }; }; ```; Below is the **successful** CWL file. In particular, note the `inputs.type.name` which is set to a garbage value.; ```; {; ""cwlVersion"": ""v1.0"",; ""class"": ""CommandLineTool"",; ""inputs"": [; {; ""id"": ""paired_parameters"",; ""type"": {; ""type"": ""record"",; ""name"": ""SOME JUNK VALUE"",; ""fields"": [; {; ""name"": ""itemA"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-A="",; ""separate"": false; }; },; {; ""name"": ""itemB"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-B="",; ""separate"": false; }; }; ]; }; }; ],; ""outputs"": {; ""example_out"": {; ""type"": ""stdout""; }; },; ""stdout"": ""output.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:302,down,downloaded,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['down'],['downloaded']
Availability,"This removes several classes of confusing `ERROR` log messages that we're receiving under healthy conditions. For example, when calling `checkAccess` in the filesystem provider, which is intended to be used to detect whether a path exists, the library would log an error containing the path string. This resulted in several `ERROR` level log messages every time we created a directory structure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6982:43,ERROR,ERROR,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6982,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This requirement boils down to declaring multiple sources of an input and providing them to a `valueFrom` expression to determine the actual value of the input. We should pass another valueFrom-related test (\#63) when ExpressionTool support is merged, which is imminent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3204:23,down,down,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3204,1,['down'],['down']
Availability,This resolves the below error (which comes up when you run CromIAM):. `error while starting up loggers akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [java.lang.ClassNotFoundException: akka.event.slf4j.Slf4jLogger]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4254:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4254,2,['error'],['error']
Availability,"This seems like a bug. In my inputs.json, I have an input file that is a URL. Cromwell (v84) fails to pull it when I'm not using ""Local"" as a backend. The only difference between a working case and failed case is the name of the backend in the cromwell config. If you just change the name from ""Local"" to ""MyLocal"", it will fail. For example,. this works; ```; default = ""Local""; providers; {; Local; ```. And this fails; ```; default = ""MyLocal""; providers; {; MyLocal; ```. Command to reproduce error. export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf \; -Dbackend.providers.Local.config.dockerRoot=$(pwd)/cromwell-executions \; -Dbackend.providers.Local.config.root=$(pwd)/cromwell-executions \; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json. Error:; java.lang.IllegalArgumentException: Could not build the path ""https://portal.nersc.gov/cfs/m342/jaws/test_data/sample.fastq"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Failures: . Required files; [test-files.zip](https://github.com/broadinstitute/cromwell/files/10387814/test-files.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977:497,error,error,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977,3,"['Error', 'Failure', 'error']","['Error', 'Failures', 'error']"
Availability,"This seems too simple to be correct, would appreciate a quick looking-over. `1st-workflow.cwl` is an expected failure case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285:110,failure,failure,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285,1,['failure'],['failure']
Availability,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3318:407,error,error,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318,4,"['down', 'error']","['downsample', 'error']"
Availability,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366:1629,robust,robustness,1629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366,1,['robust'],['robustness']
Availability,This should also fix transient failures of the `abort a workflow mid run and restart immediately abort.restart_abort_tes` centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,1,['failure'],['failures']
Availability,This should help with error messages when Cromwell shuts down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620,2,"['down', 'error']","['down', 'error']"
Availability,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3496:491,Avail,Available,491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496,2,"['Avail', 'down']","['Available', 'down']"
Availability,"This sucked up a few hours of time with the debugger, we need a better error message/handling. If you misconfigure Cromwell (in my case using a service account) whereby the auth specified in JES.filesystems.gcs.auth is unable to write to the bucket specified in JES.config.root. Specifically, I found that the uploadCommandScript was dying silently in the JABJEA and my workflow just stopped running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1669:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669,1,['error'],['error']
Availability,"This task failed after a restart of Cromwell with the following stack trace. We have seen similar errors before - #588. This was only after one restart though. ```; 2016-05-24 16:59:43,266 cromwell-system-akka.actor.default-dispatcher-6 INFO - JesBackend [UUID(fd62961b):ApplyBQSR:11]: Starting call with pre-emptible VM. 2016-05-24 16:59:43,431 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(fd62961b)]: persisting status of ApplyBQSR:11 to Running. 2016-05-24 16:59:43,526 cromwell-system-akka.actor.default-dispatcher-6 INFO - JES Run [UUID(fd62961b):ApplyBQSR:11]: Status change from - to Success. 2016-05-24 16:59:44,173 cromwell-system-akka.actor.default-dispatcher-13 ERROR - WorkflowActor [UUID(fd62961b)]: Completion work failed for call ApplyBQSR:11.; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '741-PairedEndSingleSampleWorkflow.ApplyBQSR-recalibrated_bam-11-' for key 'UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:973) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3847) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3783) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2594) ~[cromwell.jar:0.19]; at com.mysql.jdbc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/880:98,error,errors,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/880,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1311:636,recover,recoverWith,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311,2,['recover'],['recoverWith']
Availability,"This test was introduced in 2019 to verify an improved error message when localization fails ([issue](https://github.com/broadinstitute/cromwell/issues/4603), [PR](https://github.com/broadinstitute/cromwell/pull/4718)). It works by localizing an exactly 1 GB file onto a 1 GB disk, that presumably has other stuff on it, and verifying that it fails in the expected way. Unfortunately, the out-of-disk condition seems to trigger behavior in PAPI that causes the task to hang for half an hour!. Since PAPI is on its way out and this error message still seems to be checked in [a different test case](https://github.com/broadinstitute/cromwell/pull/4718/files#diff-a348345a036a680f8e29070f0972f61b09f3f640f26b188504d69d5a7f71b554), I am recommending we just delete the test instead of spending any more time on this. ```; > gcloud beta lifesciences operations describe projects/1005074806481/locations/us-central1/operations/8650136336352694244 --format=json; {; ""done"": true,; ""error"": {; ""code"": 9,; ""message"": ""Execution failed: generic::failed_precondition: while running \""-c /bin/bash /cromwell_root/gcs_localization.sh\"": unexpected exit status 1 was not ignored""; },; ""metadata"": {; ""@type"": ""type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata"",; ""createTime"": ""2023-12-04T20:36:45.056562Z"",; ""endTime"": ""2023-12-04T21:10:43.697318162Z"" # <- WTF!!; }; [...]; ```. ```; Long duration; Warning: arning] Using a password on the command line interface can be insecure.; +--------------------------------------+-----------------+----------------------------+----------------------------+; | name | RUNTIME_MINUTES | start | end |; +--------------------------------------+-----------------+----------------------------+----------------------------+; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; | draft3_call_cache_capoeira | 27 | 2023-12-05 01:03:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7330:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7330,3,['error'],['error']
Availability,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441:483,recover,recoverable,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441,1,['recover'],['recoverable']
Availability,"This updates the 2 caching tests that we have been seeing cache hit failure with so that they do not attempt to read from the cache on their first run. For commentary, see https://broadworkbench.atlassian.net/browse/CROM-6807?focusedCommentId=52973. Proof that the different options are being respected:. 2021-11-04 18:54:05,200 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 3a536714:cacheBetweenWF.getAverage:-1:1 is eligible for call caching with **read = false** and write = true. 2021-11-04 18:54:55,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - BT-322 ce702d83:cacheBetweenWF.getAverage:-1:1 is eligible for call caching with **read = true** and write = true",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557,1,['failure'],['failure']
Availability,This was requested by @patmagee - I agree that it's a good idea. Find a way to detect if a liquibase migration is pending if Cromwell starts. Add a config option (defaulting to a safe mode) such that if this option is enabled and a liquibase migration is required that the process will exit with an error message stating:. - That a migration is necessary; - Encouragement to the user to backup their database and/or do further testing if in a production environment; - Describe how to override (including via command line) the setting to allow Cromwell to start properly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429,1,['error'],['error']
Availability,"This wdl throws an error:. ```; scatter (unmapped_bam in flowcell_unmapped_bams) {; String base_name = sub(sub(unmapped_bam, ""gs://.*/"",""""), "".unmapped.bam$"", """"); call CollectQualityYieldMetrics {; input:; input_bam = unmapped_bam,; metrics_filename = base_name + "".unmapped.quality_yield_metrics"",; disk_size = flowcell_small_disk; }; }; ```. ```; ""Workflow has invalid declarations: Invalid parameters for engine function sub: (Failure(java.lang.IllegalArgumentException: Invalid parameters for engine function sub: (Failure(wdl4s.WdlExpressionException: Could not find a value for unmapped_bam),Success(WdlString(gs://.*/)),Success(WdlString())).),Success(WdlString(.unmapped.bam$)),Success(WdlString())).""; ```. But if used inside the task call it is successful:. ```; scatter (unmapped_bam in flowcell_unmapped_bams) {; call CollectQualityYieldMetrics {; input:; input_bam = unmapped_bam,; metrics_filename = sub(sub(unmapped_bam, ""gs://.*/"",""""), "".unmapped.bam$"", """") + "".unmapped.quality_yield_metrics"",; disk_size = flowcell_small_disk; }; }; ```. This just makes it very inconvenient if there are multiple tasks that need the value from the sub function as an input.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/687:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/687,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"This will add to the instrumentation metric path the http return code in case of failure, giving better insight as to the reason of the failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3506:81,failure,failure,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3506,2,['failure'],"['failure', 'failures']"
Availability,"This will always mean that Cromwell doesn't believe the workflow exists, yet the error message is that there's no collction for the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224:81,error,error,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224,1,['error'],['error']
Availability,"This workflow `20d61ca9-0cc8-4dc1-9c20-85dbf9146de4`, running in the mint-dev Cromwell instance (version 35-fe8c4bf-SNAP) had a failed task where there is no start time in the metadata, but there is an end time. Was this task technically not started yet? If that's the case, then why would there be an end time?. ```; ""Adapter10xCount.inputs_for_submit.other_inputs"": [; {; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""shardIndex"": -1,; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Failed to evaluate 'Adapter10xCount.inputs_for_submit.other_inputs' (reason 1 of 1): Evaluating [{\""name\"": \""sample_id\"", \""value\"": GetInputs.sample_id}, {\""name\"": \""reference_name\"", \""value\"": reference_name}, {\""name\"": \""transcriptome_tar_gz\"", \""value\"": transcriptome_tar_gz}, {\""name\"": \""expect_cells\"", \""value\"": expect_cells}] failed: :\nFailed to coerce one or more keys or values for creating a Map[String, Int?]:\nList(java.lang.NumberFormatException: For input string: \""expect_cells\""\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:301)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:301)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat wom.types.WomIntegerType$$anonfun$coercion$1.applyOrElse(WomIntegerType.scala:20)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)\n\tat wom.types.WomType.$anonfun$coerceRawValue$2(WomType.scala:37)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat wom.types.WomType.coerceRawValue(WomType.scala:37)\n\tat wom.types.WomType.coerceRawValue$(WomType.scala:27)\n\tat wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9)\n\tat wom.values.WomOptionalValue.coerceAndSetNestingLevel(WomOptionalValue.scala:127)\n\tat wom.types.WomOptionalType$$anonfun$co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4128:452,failure,failures,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128,1,['failure'],['failures']
Availability,"This workflow run on JES will cause a JES failure with `the local copy message must have path set`. This is because the value for `a` is passed to JES as an input but because it's empty the submission fails. We should catch this before it gets to JES and fail with a better error message. ```; task t {; File a; command {; cat ${a}; }; output {; String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call t; }; ```. ```; {; ""w.t.a"": """"; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2367:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2367,2,"['error', 'failure']","['error', 'failure']"
Availability,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:131,failure,failures,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['failure'],['failures']
Availability,"This would greatly reduce my need to modify WDL scripts to start where I have data already processed. For example, if a script goes BAM-->coverage-->CNVs, if I have already collected coverage on my BAMs, I would like to be able to provide `coverage` to the same script and have Cromwell skip the tasks involving the BAM and run the remaining steps in the workflow, e.g. coverage-->CNVs. . I run WDLs using gcloud, within a VM and locally. I don't use FireCloud so my runs do not use call-caching. I want to take the boilerplate WDL scripts the GATK4 repo makes available to run processes. I am specifically looking at the latest somatic CNV workflow. If I have alreaded padded my intervals and/or collected counts on the BAMs, I'd like to still use the rest of the steps in the workflow by specifying in the INPUTS JSON an intermediate file. If the script is thus:; ```; call CNVTasks.PreprocessIntervals {; input:; intervals = intervals,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }. if (select_first([do_explicit_gc_correction, false])) {; call CNVTasks.AnnotateIntervals {; input:; intervals = PreprocessIntervals.preprocessed_intervals,; ref_fasta = ref_fasta,; ref_fasta_fai = ref_fasta_fai,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }; ```. In the inputs, instead of defining:; ```; ""CNVSomaticPanelWorkflow.intervals"": ""File"",; ```; I would like to be able to instead provide:; ```; ""CNVSomaticPanelWorkflow.PreprocessIntervals.preprocessed_intervals"": ""File"",; ```. And not have the run error due to the lack of the `CNVSomaticPanelWorkflow.intervals` file. . I would really appreciate such a feature as it saves me the time of having to rewrite WDL scripts for each tweaked subset workflow. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2949:561,avail,available,561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2949,2,"['avail', 'error']","['available', 'error']"
Availability,This would work if the failure mode is passed through workflow options but not if it's set in the config.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2239:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2239,1,['failure'],['failure']
Availability,"Thought I would check out the new GCPBATCH support in the latest cromwell. Used the [example config](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/GCPBATCH.conf) for the newly supported GCPBATCH with my projects settings added and it errored because of multiple zones were configured:. ```; default-runtime-attributes {; ...; zones: [""us-central1-a"", ""us-central1-b""]; }; ```; I changed it to just `[""us-central1-a""]` and worked fine and ran until completion so I guess it is how the parser is combining these zones into whatever underlying batch command needs to be executed. (I also tried the value ""us-central1-a us-central1-b"" as a string instead of an array cause I thought I read somewhere that was supported, but that didn't work either). Here is the error thrown by cromwell: . ![Screenshot 2023-10-04 at 13 11 01](https://github.com/broadinstitute/cromwell/assets/40811287/9c9cc8d8-8e08-44a3-826d-a0300fc95278)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7232:270,error,errored,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232,2,['error'],"['error', 'errored']"
Availability,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15565,down,down,15565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,13,['down'],['down']
Availability,Timeout persisting runtime attributes causes workflow failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:54,failure,failure,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['failure'],['failure']
Availability,"Timothy DeFreitas <notifications@github.com. > wrote:; > ; > @eddiebroad https://github.com/eddiebroad But those are all quoted; > strings, and don't look the same as a WDL comment. From an implementation; > perspective, doesn't cromwell pipe the command bock to /bin/bash anyway?; > And following bash rules unquoted # characters start a comment, so maybe; > WDL just has to follow the same comment parsing rules as bash?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200510863. ## . Edward A. Salinas; Senior Software Engineer - Getz Lab; Broad Institute of MIT and Harvard; 75 Ames Street, Rm 4013; Cambridge, MA 02142; Ph: 617-714-7905; Cell: 210-274-3172. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200514792). I'm hesitant to try to follow the same rules as Bash for parsing a command because I worry that it'll be error prone and difficult to pin down the details. It feels like we'd essentially reimplement a Bash parser in WDL. It'd be difficult and hard to get the nuances of something like this:. ```; command {; python <<CODE; print(""#octothorps!!#""); CODE; }; ```. Maybe something as simple as syntax highlighting could help too... if `#`s are not highlighted differently in the command section then that could also be a hint. Granted, I know syntax highlighters won't always be used. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200525363). If it's clear to WDL users how # is interpreted (or not interpreted) in the; command-block maybe that would be the best thing? Would a sentence/blurb; in the docs address this? Possibly as mentioned earlier good error; messages?. -eddie. On Wed, Mar 23, 2016 at 3:43 PM, Scott Frazer notifications@github.com; wrote:. > I'm hesitant to try to follow the same rules as Bas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:6866,error,error,6866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,2,"['down', 'error']","['down', 'error']"
Availability,"To fix the ""unmatched case"" error in #4651 :. * Make states a strict ADT rather than string-based; * Hopefully made the `pollStatus` logic a little easier to follow. Closes #4651",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654,1,['error'],['error']
Availability,"To prevent deadlocks, we have to acquire locks in the same order in the heartbeat writing batches and the workflow starting. Workflows currently start by ascending submission_time, so we need to order the updates in the same order. . Unfortunately, at the time we are making the sequence of DBIO actions we don't have that information so it needs to be propagated along to that point and then use that information to sort that collection of actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240:72,heartbeat,heartbeat,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240,1,['heartbeat'],['heartbeat']
Availability,"To wire up the File output, either by running the file delocalisation in the execution script, or by copying the required files to a known mounted volume. Success criteria: we can run a WDL such as:. ```; task fileOut {; command { echo ""hello, amazon"" > myFile }; output { File outFile = ""myFile"" }; }. workflow {; call fileOut; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1569:231,echo,echo,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1569,1,['echo'],['echo']
Availability,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4046:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046,1,['failure'],['failures']
Availability,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4771:757,error,error,757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771,1,['error'],['error']
Availability,"Today, during Job preparation (prior to localization), when Cromwell calls Martha to get information on a dos url, there is a schema (defined in the Cromwell config) which statically extracts an element from an array, which is no longer guaranteed to be a GS url. Ideally, when Martha returns URL info, Cromwell sorts through the array of urls to *extract the first gcs url*, and when there is no GCS url, the job fails with an error message explaining how the DOS url couldn't be resolved into a GCS url. IN addition, Cromwell can also the print the dos url + actual urls associated to the dos url for a user -- so they can easily take both pieces of info for further debugging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4118:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4118,1,['error'],['error']
Availability,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4691:313,avail,available,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691,4,"['avail', 'error']","['available', 'error']"
Availability,Tolerate optional output files not being present.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3456:0,Toler,Tolerate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3456,1,['Toler'],['Tolerate']
Availability,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:188,error,error,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,3,"['error', 'failure']","['error', 'failure']"
Availability,"Tracking production issue that occurred on May 30th at 6pm. Cromwell had 65K files open, reboot resolved the issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716:89,reboot,reboot,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716,1,['reboot'],['reboot']
Availability,"Transient ""failure"" in metadata during Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['failure'],['failure']
Availability,Transient error occasionally causes cromwell to have an error even though the workflow completed successfully,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,Transient failures uploading auth file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['failure'],['failures']
Availability,"Tried to pass string to wdl input of type ""File"", got no error and workflow failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1561:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1561,1,['error'],['error']
Availability,"Tried to pass string to wdl input of type ""File"", got no error and workflow failed. . I ran this in firecloud, and my expression for ""id"" evaluated as a string. Here's my wdl:. ```; task get_mutations_across_intervals {; File id; File orignal_maf; Array[File] mafs. command {; python /opt/make_intervals_v2.py ${id} ${orignal_maf} ${sep="" "" mafs}; }. runtime {; docker: ""gcr.io/broad-firecloud-itools/pysam""; }. output {; File interval_maf = ""${id}.forecallready.maf""; }; }. workflow get_mutations_across_intervals_wkfl {; call get_mutations_across_intervals; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1561:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1561,1,['error'],['error']
Availability,"Tried to re-run the 10K JG workflow with CC on, the workflow failed almost immediately with multiple errors like; ```; [ERROR] [04/18/2017 21:11:44.685] [cromwell-system-akka.dispatchers.service-dispatcher-86] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService/metadata-summary-actor] Failed to summarize metadata; java.util.concurrent.RejectedExecutionException: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@64919660 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@1dce40e4[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 800]; 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047); 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823); 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369); 	at slick.util.AsyncExecutor$$anon$2$$anon$3.execute(AsyncExecutor.scala:120); 	at slick.basic.BasicBackend$DatabaseDef$class.runSynchronousDatabaseAction(BasicBackend.scala:233); 	at slick.jdbc.JdbcBackend$DatabaseDef.runSynchronousDatabaseAction(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:210); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run$1(BasicBackend.scala:153); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:157); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:179); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInternal(BasicBackend.scala:78); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInternal(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run(BasicBackend.scala:75); 	at slick.jdbc.JdbcBackend$DatabaseDef.run(JdbcBackend.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Trying to follow example here: http://gatkforums.broadinstitute.org/wdl/discussion/7221/2-howto-write-a-simple-multi-step-workflow. I editted the json file (simpleVariantSelection_inputs.json) with the full paths on my local machine and replacing the ""File"" and ""String"" placeholders. However when I run I am getting the following error:. [2017-05-25 12:18:24,85] [info] WorkflowManagerActor Successfully started WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c; [2017-05-25 12:18:24,85] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-05-25 12:18:24,93] [error] WorkflowManagerActor Workflow e52409b4-c85a-4285-9453-f47c6b0ae86c failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Finished parsing without consuming all tokens. {; ^. cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Finished parsing without consuming all tokens. {; ^. 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:137); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:129); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:331,error,error,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Trying to print stdout and stderr to files so that I can see what my command is doing. I broke it down to the simple example shown in the docs. . ```task echo {; command <<< ; echo ""hello world"" ; echo ""another world""; >&2 echo ""hello world""; >>>; output {; File message = stdout(); File message2 = stderr(); }; }; ```. Both stdout and stderr return empty. My goal is to capture all stdout and stderr from whatever is in the command section. . On real workflows, I also get a bunch of nonsense in the stdout stderr logs. If my run fails I want to know what caused it. Using cromwell version 72. Any help?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686:98,down,down,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686,5,"['down', 'echo']","['down', 'echo']"
Availability,"Trying to read an object that has the same name as a variable causes Cromwell to abort not just the workflow, but the entire Cromwell instance; WDL file:; ```; task TestTask {; 	; 	command {; 		echo ""Hello World!"" > hello_world.txt; 	}. 	output {; 		File exists = ""hello_world.txt""; 	}. 	runtime {; 		docker: will_fail.docker; 		memory: will_fail.memory; 		disks: ""local-disk "" + will_fail.small_disk + "" HDD""; 	}; }. workflow KillsCromwell {; 	String test_string. # This here kills Cromwell; 	Object runtime_params = read_object(runtime_params). 	call TestTask ; }; ```. Inputs File:; ```; {; 	""KillsCromwell.test_string"": ""This is a string"",; 	""KillsCromwell.runtime_params"": {; 		""genomes_cloud_image"": ""broadinstitute/genomes-in-the-cloud:2.2.4-1469632282"",; 		""small_disk"": 100,; 		""medium_disk"": 200,; 		""large_disk"": 300,; 		""x_large_disk"": 400,; 		""preemptible_tries"": 3; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1946:194,echo,echo,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946,1,['echo'],['echo']
Availability,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4880:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880,1,['recover'],['recover']
Availability,"TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_bed' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf' not specified.""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. But once I filled these out in my inputs json I then got this error. ```; [; {; causedBy: [; {; causedBy: [ ],; message: ""Missing inputs for subworkflow call SomaticRoot.TumorAlignment at index None: read_length, ref_fasta, agg_preemptible_tries, ref_dict, haplotype_database_file, ref_alt, ref_ann, known_indels_sites_indices, dbSNP_vcf, ref_sa, dbSNP_vcf_index, unmapped_bam_suffix, ref_amb, contamination_sites_ud, contamination_sites_bed, ref_bwt, ref_fasta_index, increase_disk_size, fingerprint_genotypes_file, preemptible_tries, known_indels_sites_VCFs, contamination_sites_mu, wgs_coverage_interval_list, ref_pac.""; }; ],; message: ""Couldn't resolve all inputs for SomaticRoot.TumorAlignment at index None.""; }; ],; ```. I think I'm passing in everything correctly but it could be an error on my part as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:3077,error,error,3077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,2,['error'],['error']
Availability,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Two new centaur tests (test types actually):. * Simulates the transition from PAPI v1 to PAPI v2. Start with an FC-like config where PAPI v1 is the default `Papi` backend and PAPI v2 is available in a `Papiv2` backend. Launch a workflow into this system with two sequential calls. Shut down Cromwell once the second call starts. Tweak the Cromwell config to add `name-for-call-caching-purposes = ""Papi""` to the `Papiv2` backend. Bring Cromwell back up and let the previously-running workflow finish. Then launch a second workflow identical to the first except that its options request `{ ""backend"": ""Papiv2""}`. Let this workflow finish and confirm it call cached both calls from the first workflow despite running on different versions of PAPI. * Simulates the new steady-state after the v2 transition when new workflows are submitted with `{""backend"": ""Papiv2""}`. Run a workflow, shut down Cromwell in the middle of its execution, then bring Cromwell back up. Confirm this workflow completes successfully. Launch a second copy of the workflow also requesting `{""backend"": ""Papiv2""}` and confirm it completes successfully and caches to all the calls of the first run and that both executed on PAPI v2.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4490:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4490,3,"['avail', 'down']","['available', 'down']"
Availability,"URL:; https://github.com/broadinstitute/cromwell/releases/download/28/cromwell-28.jar. Expected SHA-256: 25c6c30fe062fb4a8384ef82aa5313ad5a5ee05eae62a2c831219d7c6c756347. Actual SHA-256:; c9ce762df236588ded042ceaf099848c0c1685d34788442ee251615ff13b5190. The expected checksum is what what we had in Homebrew for the SHA-256 when the formula was upgraded to version 28 on Fri Jun 30 12:44:57 2017 -0700. See https://github.com/Homebrew/homebrew-core/pull/15166. But currently downloading the file gives a different checksum. I wanted to make sure you weren't hacked, and ask what the reason(s) for the changes were.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463:58,down,download,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463,2,['down'],"['download', 'downloading']"
Availability,Unable to complete PAPI request due to system or connection error (Unknown Error.),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,2,"['Error', 'error']","['Error', 'error']"
Availability,Unable to recover running jobs in AWS backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,Unable to run multiple cromwell jobs in run mode due to connection not being available,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:77,avail,available,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['avail'],['available']
Availability,Unclear error when dealing with File evaluates to empty string,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,1,['error'],['error']
Availability,Unhelpful Error Message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:10,Error,Error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,1,['Error'],['Error']
Availability,Unify RP error check so it also applies to singleton files [BA-6235],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406,1,['error'],['error']
Availability,"Unless I configured something improperly, all output from stdout() is doubled when running with SLURM. Example pipeline:. ```; version 1.0. # WORKFLOW DEFINITION; workflow WholeGenomeGermlineSingleSample {; call SumFloats; output {; Float out = SumFloats.total_size; }; }. task SumFloats {; input {; Array[Float] sizes = [1,2,3,4,5.0]; Int preemptible_tries=3; }. command <<<; python -c ""print ~{sep=""+"" sizes}""; >>>; output {; Float total_size = read_float(stdout()); }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/python:2.7""; preemptible: preemptible_tries; }; }; ```. The error raised with cromwell-53 is:; Failed to read_float(""/data/og/ted/cromwell-executions/WholeGenomeGermlineSingleSample/00090ef9-5211-4f18-9de9-daf3de791408/call-SumFloats/execution/stdout"") (reason 1 of 1): For input string: ""15.0; 15.0""; The stdout file truly contains this. Running with local backend returns no error.; Contents of conf file:. ```; backend {; default = ""SLURM""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); concurrent-job-limit = 30; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_cwd}/execution/script""; """""". kill = ""scancel ${job_id}""; check-alive = ""sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932:578,error,error,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932,2,['error'],['error']
Availability,"Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedRe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2621,echo,echo,2621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['echo'],['echo']
Availability,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3125:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125,1,['error'],['error']
Availability,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1338:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338,3,"['down', 'failure', 'recover']","['down', 'failures', 'recoverSpec']"
Availability,Unzip CWL Dependencies and make available to pre-processing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2777:32,avail,available,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2777,2,['avail'],['available']
Availability,Update requester-pays error detection [BT-574],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689,1,['error'],['error']
Availability,Updated GCR integration test to check for additional errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/366:53,error,errors,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/366,1,['error'],['errors']
Availability,"Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from 1.2.11 to 1.4.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7026:945,down,down,945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7026,1,['down'],['down']
Availability,"Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from 0.40.10 to 0.40.12.; [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.12) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.12). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7031:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7031,1,['down'],['down']
Availability,"Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from 0.40.2 to 0.40.10.; [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.10) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.2...v0.40.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6849:1354,down,down,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6849,1,['down'],['down']
Availability,"Updates ; * [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client); * [com.google.api-client:google-api-client-java6](https://github.com/googleapis/google-api-java-client). from 1.33.2 to 1.33.4.; [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v1.33.4) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v1.33.2...v1.33.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.api-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6853:1154,down,down,1154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6853,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.akka:akka-http](https://github.com/akka/akka-http); * [com.typesafe.akka:akka-http-spray-json](https://github.com/akka/akka-http); * [com.typesafe.akka:akka-http-testkit](https://github.com/akka/akka-http). from 10.1.15 to 10.2.9.; [GitHub Release Notes](https://github.com/akka/akka-http/releases/tag/v10.2.9) - [Version Diff](https://github.com/akka/akka-http/compare/v10.1.15...v10.2.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Applied Scalafix Migrations</summary>. * com.typesafe.akka:akka-http.*:10.2.0 (created no change); * dependency:MigrateToServerBuilder@com.typesafe.akka:akka-http-scalafix-rules:10.2.0; * Documentation: https://doc.akka.io/docs/akka-http/10.2/migration-guide/migration-guide-10.2.x.html#akka-http-10-1-x-10-2-0; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.akka"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.akka"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, scalafix-migrations, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6869:1468,down,down,1468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6869,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.slick:slick](https://github.com/slick/slick); * [com.typesafe.slick:slick-hikaricp](https://github.com/slick/slick). from 3.4.0-M1 to 3.4.0.; [GitHub Release Notes](https://github.com/slick/slick/releases/tag/v3.4.0) - [Version Diff](https://github.com/slick/slick/compare/v3.4.0-M1...v3.4.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.slick"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.slick"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6872:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6872,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.slick:slick](https://github.com/slick/slick); * [com.typesafe.slick:slick-hikaricp](https://github.com/slick/slick). from 3.4.0-M1 to 3.4.1.; [GitHub Release Notes](https://github.com/slick/slick/releases/tag/v3.4.1) - [Version Diff](https://github.com/slick/slick/compare/v3.4.0-M1...v3.4.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.slick"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe.slick"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, version-scheme:pvp, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7045:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7045,1,['down'],['down']
Availability,"Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6874:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6874,1,['down'],['down']
Availability,"Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.14.1 to 0.14.4.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.4) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7048:1239,down,down,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7048,1,['down'],['down']
Availability,"Updates ; * [io.github.jbwheatley:pact4s-circe](https://github.com/jbwheatley/pact4s); * [io.github.jbwheatley:pact4s-scalatest](https://github.com/jbwheatley/pact4s). from 0.7.0 to 0.8.0.; [GitHub Release Notes](https://github.com/jbwheatley/pact4s/releases/tag/v0.8.0) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.7.0...v0.8.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7051:1061,down,down,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7051,1,['down'],['down']
Availability,"Updates ; * [org.http4s:http4s-blaze-client](https://github.com/http4s/http4s); * [org.http4s:http4s-circe](https://github.com/http4s/http4s); * [org.http4s:http4s-dsl](https://github.com/http4s/http4s). from 0.21.31 to 0.21.33.; [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.33) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.33). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6885:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6885,1,['down'],['down']
Availability,"Updates ; * [org.http4s:http4s-blaze-client](https://github.com/http4s/http4s); * [org.http4s:http4s-circe](https://github.com/http4s/http4s); * [org.http4s:http4s-dsl](https://github.com/http4s/http4s). from 0.21.31 to 0.21.34.; [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7067:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7067,1,['down'],['down']
Availability,"Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from 2.7.0 to 2.8.0.; [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.8.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.8.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6897:1319,down,down,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6897,1,['down'],['down']
Availability,"Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from 2.7.0 to 2.9.0.; [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.9.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.9.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7077:1319,down,down,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7077,1,['down'],['down']
Availability,"Updates ; * ch.qos.logback:logback-access; * ch.qos.logback:logback-classic; * ch.qos.logback:logback-core. from 1.2.10 to 1.2.11. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6842:835,down,down,835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6842,1,['down'],['down']
Availability,"Updates ; * com.fasterxml.jackson.core:jackson-annotations; * [com.fasterxml.jackson.core:jackson-core](https://github.com/FasterXML/jackson-core); * com.fasterxml.jackson.core:jackson-databind. from 2.13.3 to 2.13.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7033:934,down,down,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7033,1,['down'],['down']
Availability,"Updates ; * org.slf4j:jcl-over-slf4j; * org.slf4j:jul-to-slf4j; * org.slf4j:log4j-over-slf4j; * org.slf4j:slf4j-api. from 1.7.32 to 1.7.36. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.slf4j"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.slf4j"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6896:839,down,down,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6896,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:batch; * software.amazon.awssdk:cloudwatchlogs; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.194 to 2.17.265. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6901:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6901,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:batch; * software.amazon.awssdk:cloudwatchlogs; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.265 to 2.17.295. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7082:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7082,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.152 to 2.17.172. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/e5600937e537c0b06266c2860dfad5605a4de5ef/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6734:839,down,down,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6734,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.152 to 2.17.173. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5ba079e5e6b075ded705306e58f3111e16796466/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6738:808,down,down,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6738,1,['down'],['down']
Availability,"Updates Cromwell with the latest changes from wdl4s; I was able to unignore half of the unit tests that were previously ignored, and re-enable 60 centaur tests (on local and TES).; I haven't looked at the JES failures yet, so this is not be merged until they're fixed, but I though I would make a PR a bit early as this re-wires a decent amount of stuff that was broken due to missing FQNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680:209,failure,failures,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680,1,['failure'],['failures']
Availability,"Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from 2.17.0 to 2.18.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6847:879,down,down,879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6847,1,['down'],['down']
Availability,"Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from 2.18.0 to 2.24.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7029:879,down,down,879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7029,1,['down'],['down']
Availability,"Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from 1.7.0 to 1.7.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6843:845,down,down,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6843,1,['down'],['down']
Availability,"Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from 1.7.1 to 1.10.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7027:1095,down,down,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7027,1,['down'],['down']
Availability,"Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from 1.4.2 to 1.4.6. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6844:831,down,down,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6844,1,['down'],['down']
Availability,"Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from 1.4.6 to 1.8.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7028:831,down,down,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7028,1,['down'],['down']
Availability,"Updates [com.azure:azure-security-keyvault-secrets](https://github.com/Azure/azure-sdk-for-java) from 4.3.7 to 4.3.8. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-security-keyvault-secrets"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-security-keyvault-secrets"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6845:865,down,down,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6845,1,['down'],['down']
Availability,"Updates [com.azure:azure-storage-blob-nio](https://github.com/Azure/azure-sdk-for-java) from 12.0.0-beta.18 to 12.0.0-beta.19. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob-nio"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6846:865,down,down,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6846,1,['down'],['down']
Availability,"Updates [com.chuusai:shapeless](https://github.com/milessabin/shapeless) from 2.3.7 to 2.3.9.; [GitHub Release Notes](https://github.com/milessabin/shapeless/releases/tag/v2.3.9) - [Version Diff](https://github.com/milessabin/shapeless/compare/v2.3.7...v2.3.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.chuusai"", artifactId = ""shapeless"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.chuusai"", artifactId = ""shapeless"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6848:989,down,down,989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6848,1,['down'],['down']
Availability,"Updates [com.chuusai:shapeless](https://github.com/milessabin/shapeless) from 2.3.9 to 2.3.10.; [GitHub Release Notes](https://github.com/milessabin/shapeless/releases/tag/v2.3.10) - [Version Diff](https://github.com/milessabin/shapeless/compare/v2.3.9...v2.3.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.chuusai"", artifactId = ""shapeless"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.chuusai"", artifactId = ""shapeless"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, version-scheme:pvp, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7030:992,down,down,992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7030,1,['down'],['down']
Availability,"Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from 1.1.1 to 1.2.0.; [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v1.2.0) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v1.2.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7032:1292,down,down,1292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7032,1,['down'],['down']
Availability,"Updates [com.github.pathikrit:better-files](https://x-access-token@github.com/pathikrit/better-files) from 3.9.1 to 3.9.2.; [GitHub Release Notes](https://x-access-token@github.com/pathikrit/better-files/releases/tag/v3.9.2) - [Changelog](https://x-access-token@github.com/pathikrit/better-files/blob/master/CHANGES.md) - [Version Diff](https://x-access-token@github.com/pathikrit/better-files/compare/v3.9.1...v3.9.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.pathikrit"", artifactId = ""better-files"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.pathikrit"", artifactId = ""better-files"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7034:1159,down,down,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7034,1,['down'],['down']
Availability,"Updates [com.github.sbt:sbt-git](https://github.com/sbt/sbt-git) from 2.0.0 to 2.0.1.; [GitHub Release Notes](https://github.com/sbt/sbt-git/releases/tag/v2.0.1) - [Version Diff](https://github.com/sbt/sbt-git/compare/v2.0.0...v2.0.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.0.0).; You might want to review and update them manually.; ```; CromwellRefdiskManifestCreator/pom.xml; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""sbt-git"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""sbt-git"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7035:1253,down,down,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7035,1,['down'],['down']
Availability,"Updates [com.github.scopt:scopt](https://github.com/scopt/scopt) from 4.0.1 to 4.1.0.; [GitHub Release Notes](https://github.com/scopt/scopt/releases/tag/v4.1.0) - [Version Diff](https://github.com/scopt/scopt/compare/v4.0.1...v4.1.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.scopt"", artifactId = ""scopt"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.github.scopt"", artifactId = ""scopt"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6851:964,down,down,964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6851,1,['down'],['down']
Availability,"Updates [com.google.api:gax-grpc](https://github.com/googleapis/gax-java) from 2.12.2 to 2.19.0.; [GitHub Release Notes](https://github.com/googleapis/gax-java/releases/tag/v2.19.0) - [Version Diff](https://github.com/googleapis/gax-java/compare/v2.12.2...v2.19.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6852:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6852,1,['down'],['down']
Availability,"Updates [com.google.api:gax-grpc](https://github.com/googleapis/gax-java) from 2.19.0 to 2.19.6.; [GitHub Release Notes](https://github.com/googleapis/gax-java/releases/tag/v2.19.6) - [Version Diff](https://github.com/googleapis/gax-java/compare/v2.19.0...v2.19.6). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7036:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7036,1,['down'],['down']
Availability,"Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from 1.5.3 to 1.10.0.; [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.10.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.10.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.5.3).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6857:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6857,1,['down'],['down']
Availability,"Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from 1.5.3 to 1.16.0.; [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.16.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.16.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7040:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7040,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-monitoring](https://github.com/googleapis/java-monitoring) from 3.2.5 to 3.2.10.; [GitHub Release Notes](https://github.com/googleapis/java-monitoring/releases/tag/v3.2.10) - [Version Diff](https://github.com/googleapis/java-monitoring/compare/v3.2.5...v3.2.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7041:1048,down,down,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7041,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-nio](https://github.com/googleapis/java-storage-nio) from 0.124.8 to 0.124.14.; [GitHub Release Notes](https://github.com/googleapis/java-storage-nio/releases/tag/v0.124.14) - [Version Diff](https://github.com/googleapis/java-storage-nio/compare/v0.124.8...v0.124.14). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.124.8).; You might want to review and update them manually.; ```; CHANGELOG.md; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6860:1284,down,down,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6860,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-nio](https://github.com/googleapis/java-storage-nio) from 0.124.8 to 0.124.21.; [GitHub Release Notes](https://github.com/googleapis/java-storage-nio/releases/tag/v0.124.21) - [Version Diff](https://github.com/googleapis/java-storage-nio/compare/v0.124.8...v0.124.21). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.124.8).; You might want to review and update them manually.; ```; CHANGELOG.md; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7042:1284,down,down,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7042,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.2.5 to 1.2.11.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.2.11) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.2.5...v1.2.11). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.2.5).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6861:1340,down,down,1340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6861,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.2.5 to 1.2.12.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.2.12) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.2.5...v1.2.12). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7043:1073,down,down,1073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7043,1,['down'],['down']
Availability,"Updates [com.google.guava:guava](https://github.com/google/guava) from 31.0.1-jre to 31.1-jre. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.guava"", artifactId = ""guava"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.guava"", artifactId = ""guava"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6863:823,down,down,823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6863,1,['down'],['down']
Availability,"Updates [com.google.oauth-client:google-oauth-client](https://github.com/googleapis/google-oauth-java-client) from 1.33.1 to 1.33.3.; [GitHub Release Notes](https://github.com/googleapis/google-oauth-java-client/releases/tag/v1.33.3) - [Version Diff](https://github.com/googleapis/google-oauth-java-client/compare/v1.33.1...v1.33.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.oauth-client"", artifactId = ""google-oauth-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.oauth-client"", artifactId = ""google-oauth-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6864:1083,down,down,1083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6864,1,['down'],['down']
Availability,"Updates [com.iheart:ficus](https://github.com/iheartradio/ficus) from 1.5.1 to 1.5.2.; [GitHub Release Notes](https://github.com/iheartradio/ficus/releases/tag/v1.5.2) - [Version Diff](https://github.com/iheartradio/ficus/compare/v1.5.1...v1.5.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.5.1).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.iheart"", artifactId = ""ficus"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.iheart"", artifactId = ""ficus"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6865:1237,down,down,1237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6865,1,['down'],['down']
Availability,"Updates [com.lihaoyi:pprint](https://github.com/com-lihaoyi/PPrint) from 0.7.1 to 0.7.3.; [GitHub Release Notes](https://github.com/com-lihaoyi/PPrint/releases/tag/0.7.3) - [Version Diff](https://github.com/com-lihaoyi/PPrint/compare/0.7.1...0.7.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.lihaoyi"", artifactId = ""pprint"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.lihaoyi"", artifactId = ""pprint"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6866:974,down,down,974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6866,1,['down'],['down']
Availability,"Updates [com.lihaoyi:pprint](https://github.com/com-lihaoyi/PPrint) from 0.7.3 to 0.8.1.; [GitHub Release Notes](https://github.com/com-lihaoyi/PPrint/releases/tag/0.8.1) - [Version Diff](https://github.com/com-lihaoyi/PPrint/compare/0.7.3...0.8.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.lihaoyi"", artifactId = ""pprint"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.lihaoyi"", artifactId = ""pprint"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7044:974,down,down,974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7044,1,['down'],['down']
Availability,"Updates [com.storm-enroute:scalameter](http://scalameter.github.io/) from 0.19 to 0.21. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.19).; You might want to review and update them manually.; ```; CHANGELOG.md; database/migration/src/main/resources/changesets/restart_and_recover_migration.xml; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.storm-enroute"", artifactId = ""scalameter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.storm-enroute"", artifactId = ""scalameter"" }; }]; ```; </details>. labels: library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6867:1444,down,down,1444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6867,1,['down'],['down']
Availability,"Updates [com.typesafe.scala-logging:scala-logging](https://github.com/lightbend/scala-logging) from 3.9.4 to 3.9.5.; [GitHub Release Notes](https://github.com/lightbend/scala-logging/releases/tag/v3.9.5) - [Version Diff](https://github.com/lightbend/scala-logging/compare/v3.9.4...v3.9.5). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.scala-logging"", artifactId = ""scala-logging"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.scala-logging"", artifactId = ""scala-logging"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6871:1036,down,down,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6871,1,['down'],['down']
Availability,"Updates [com.typesafe:config](https://github.com/lightbend/config) from 1.4.1 to 1.4.2.; [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.2) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.1...v1.4.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6868:973,down,down,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6868,1,['down'],['down']
Availability,"Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from 3.8.0 to 3.9.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7046:832,down,down,832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7046,1,['down'],['down']
Availability,"Updates [eu.timepit:refined](https://github.com/fthomas/refined) from 0.9.28 to 0.9.29.; [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.9.29) - [Version Diff](https://github.com/fthomas/refined/compare/v0.9.28...v0.9.29). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6873:973,down,down,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6873,1,['down'],['down']
Availability,"Updates [io.circe:circe-config](https://github.com/circe/circe-config) from 0.8.0 to 0.10.0.; [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7047:905,down,down,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7047,1,['down'],['down']
Availability,"Updates [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras) from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe-generic-extras/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe-generic-extras/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b38f25673d5484107dba15f3ace47a65d20c9952/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6908:1278,down,down,1278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6908,1,['down'],['down']
Availability,"Updates [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras) from 0.14.1 to 0.14.3.; [GitHub Release Notes](https://github.com/circe/circe-generic-extras/releases/tag/v0.14.3) - [Version Diff](https://github.com/circe/circe-generic-extras/compare/v0.14.1...v0.14.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7049:1278,down,down,1278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7049,1,['down'],['down']
Availability,"Updates [io.circe:circe-yaml](https://github.com/circe/circe-yaml) from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe-yaml/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe-yaml/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-yaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-yaml"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7050:1228,down,down,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7050,1,['down'],['down']
Availability,"Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from 1.45.0 to 1.45.1.; [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.45.1) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.45.0...v1.45.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6876:968,down,down,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6876,1,['down'],['down']
Availability,"Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from 1.45.1 to 1.45.4.; [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.45.4) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.45.1...v1.45.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7052:968,down,down,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7052,1,['down'],['down']
Availability,"Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from 5.2.4 to 5.7.4.; [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/5.7.4) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.2.4...5.7.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6877:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6877,1,['down'],['down']
Availability,"Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from 5.7.4 to 6.14.0.; [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/6.14.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...6.14.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7053:998,down,down,998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7053,1,['down'],['down']
Availability,"Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from 1.0.56 to 1.0.61.; [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.61) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.61). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6878:1020,down,down,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6878,1,['down'],['down']
Availability,"Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from 1.0.56 to 1.0.64.; [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.64) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.64). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7054:1020,down,down,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7054,1,['down'],['down']
Availability,"Updates [jakarta.annotation:jakarta.annotation-api](https://projects.eclipse.org/projects/ee4j.ca) from 1.3.5 to 2.1.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.annotation"", artifactId = ""jakarta.annotation-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.annotation"", artifactId = ""jakarta.annotation-api"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7056:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7056,1,['down'],['down']
Availability,"Updates [mysql:mysql-connector-java](https://github.com/mysql/mysql-connector-j) from 8.0.28 to 8.0.30.; [GitHub Release Notes](https://github.com/mysql/mysql-connector-j/releases/tag/8.0.30) - [Version Diff](https://github.com/mysql/mysql-connector-j/compare/8.0.28...8.0.30). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6879:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6879,1,['down'],['down']
Availability,"Updates [net.sourceforge.owlapi:owlapi-distribution](https://github.com/owlcs/owlapi) from 5.1.19 to 5.1.20. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.sourceforge.owlapi"", artifactId = ""owlapi-distribution"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""net.sourceforge.owlapi"", artifactId = ""owlapi-distribution"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6880:857,down,down,857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6880,1,['down'],['down']
Availability,"Updates [nl.grons:metrics4-scala](https://github.com/erikvanoosten/metrics-scala) from 4.2.8 to 4.2.9.; [GitHub Release Notes](https://github.com/erikvanoosten/metrics-scala/releases/tag/v4.2.9) - [Changelog](https://github.com/erikvanoosten/metrics-scala/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/erikvanoosten/metrics-scala/compare/v4.2.8...v4.2.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""nl.grons"", artifactId = ""metrics4-scala"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""nl.grons"", artifactId = ""metrics4-scala"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6881:1101,down,down,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6881,1,['down'],['down']
Availability,"Updates [org.apache.commons:commons-csv](https://gitbox.apache.org/repos/asf?p=commons-csv.git) from 1.9.0 to 1.10.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/plugins.sbt; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-csv"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-csv"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7059:1096,down,down,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7059,1,['down'],['down']
Availability,"Updates [org.apache.tika:tika-core](https://tika.apache.org/) from 2.3.0 to 2.4.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6882:1359,down,down,1359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6882,1,['down'],['down']
Availability,"Updates [org.apache.tika:tika-core](https://tika.apache.org/) from 2.3.0 to 2.7.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7061:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7061,1,['down'],['down']
Availability,"Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from 3.1.6 to 3.1.7.; [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.7) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.6...v3.1.7) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.6...3.1.7). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.1.6).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6883:1354,down,down,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6883,1,['down'],['down']
Availability,"Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from 3.1.7 to 3.1.9.; [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.9) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7063:1005,down,down,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7063,1,['down'],['down']
Availability,"Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from 2.32 to 2.39.; [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.39) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.39). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. labels: library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7064:1500,down,down,1500,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7064,1,['down'],['down']
Availability,"Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from 4.1.1 to 4.3.0.; [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.3.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7065:907,down,down,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7065,1,['down'],['down']
Availability,"Updates [org.hsqldb:hsqldb](http://hsqldb.org) from 2.6.1 to 2.7.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7066:1040,down,down,1040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7066,1,['down'],['down']
Availability,"Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from 2.7.4 to 2.7.6.; [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.6) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.6). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6887:1172,down,down,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6887,1,['down'],['down']
Availability,"Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from 2.7.4 to 2.7.8.; [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.8) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.8). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7069:1172,down,down,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7069,1,['down'],['down']
Availability,"Updates [org.mock-server:mockserver-netty](https://www.mock-server.com) from 5.11.2 to 5.14.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6888:833,down,down,833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6888,1,['down'],['down']
Availability,"Updates [org.mock-server:mockserver-netty](https://www.mock-server.com) from 5.14.0 to 5.15.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7070:833,down,down,833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7070,1,['down'],['down']
Availability,"Updates [org.mockftpserver:MockFtpServer](https://mockftpserver.org) from 3.0.0 to 3.1.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.0.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.options; database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7071:1198,down,down,1198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7071,1,['down'],['down']
Availability,"Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from 3.12.4 to 5.1.1.; [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.1.1) - [Version Diff](https://github.com/mockito/mockito/compare/v3.12.4...v5.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7072:982,down,down,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7072,1,['down'],['down']
Availability,"Updates [org.mozilla:rhino](https://mozilla.github.io/rhino/) from 1.7.13 to 1.7.14. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mozilla"", artifactId = ""rhino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mozilla"", artifactId = ""rhino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6890:808,down,down,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6890,1,['down'],['down']
Availability,"Updates [org.postgresql:postgresql](https://github.com/pgjdbc/pgjdbc) from 42.3.3 to 42.3.6.; [Changelog](https://github.com/pgjdbc/pgjdbc/blob/master/CHANGELOG.md). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.postgresql"", artifactId = ""postgresql"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.postgresql"", artifactId = ""postgresql"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6891:897,down,down,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6891,1,['down'],['down']
Availability,"Updates [org.postgresql:postgresql](https://github.com/pgjdbc/pgjdbc) from 42.4.1 to 42.4.3.; [Changelog](https://github.com/pgjdbc/pgjdbc/blob/master/CHANGELOG.md). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.postgresql"", artifactId = ""postgresql"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.postgresql"", artifactId = ""postgresql"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7073:897,down,down,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7073,1,['down'],['down']
Availability,"Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from 1.13.1 to 1.13.5.; [GitHub Release Notes](https://github.com/scala-graph/scala-graph/releases/tag/1.13.5). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6892:925,down,down,925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6892,1,['down'],['down']
Availability,"Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from 2.13.9 to 2.13.10.; [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.10) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7074:984,down,down,984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7074,1,['down'],['down']
Availability,"Updates [org.scalactic:scalactic](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.13.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.13) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.13). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.10).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalactic"", artifactId = ""scalactic"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scalactic"", artifactId = ""scalactic"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6893:1312,down,down,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6893,1,['down'],['down']
Availability,"Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.13.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.13) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.13). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.10).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6894:1312,down,down,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6894,1,['down'],['down']
Availability,"Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.15.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.15) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.15). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7075:1016,down,down,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7075,1,['down'],['down']
Availability,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 1.9.3 to 2.0.2.; [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.2) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v1.9.3...v2.0.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6895:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6895,1,['down'],['down']
Availability,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 2.0.4 to 2.0.7.; [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.7) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.7). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7076:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7076,1,['down'],['down']
Availability,"Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from 2.3.2 to 3.0.0.; [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.0.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.0.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7078:1229,down,down,1229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7078,1,['down'],['down']
Availability,"Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from 1.0.10 to 1.0.11.; [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.0.11) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.10...v1.0.11). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6898:975,down,down,975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6898,1,['down'],['down']
Availability,"Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from 1.0.11 to 1.2.1.; [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.1) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7079:1205,down,down,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7079,1,['down'],['down']
Availability,"Updates bio.terra:workspace-manager-client from 0.254.452-SNAPSHOT to 0.254.586-SNAPSHOT. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7025:1092,down,down,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7025,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20220819-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6854:855,down,down,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6854,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20230127-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7037:855,down,down,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7037,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220328-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6855:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6855,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220913-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7038:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7038,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220401-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6856:871,down,down,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6856,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220916-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7039:871,down,down,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7039,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-bigquery from 2.10.0 to 2.10.10. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6858:815,down,down,815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6858,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-monitoring from 3.2.5 to 3.2.9. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.5).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6859:1083,down,down,1083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6859,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-storage from 2.9.2 to 2.9.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6862:810,down,down,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6862,1,['down'],['down']
Availability,"Updates com.typesafe.sbt:sbt-git from 1.0.2 to 2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.2).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.wdl; src/ci/bin/test.inc.sh; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.sbt"", artifactId = ""sbt-git"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.sbt"", artifactId = ""sbt-git"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6870:1106,down,down,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6870,1,['down'],['down']
Availability,"Updates io.github.swagger2markup:swagger2markup from 1.3.3 to 1.3.4. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.3.3).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.swagger2markup"", artifactId = ""swagger2markup"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.github.swagger2markup"", artifactId = ""swagger2markup"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6875:1081,down,down,1081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6875,1,['down'],['down']
Availability,"Updates jakarta.activation:jakarta.activation-api from 1.2.1 to 1.2.2. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.2.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; project/GenerateRestApiDocs.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.activation"", artifactId = ""jakarta.activation-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.activation"", artifactId = ""jakarta.activation-api"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7055:1102,down,down,1102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7055,1,['down'],['down']
Availability,"Updates jakarta.xml.bind:jakarta.xml.bind-api from 2.3.2 to 2.3.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.xml.bind"", artifactId = ""jakarta.xml.bind-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.xml.bind"", artifactId = ""jakarta.xml.bind-api"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7057:1059,down,down,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7057,1,['down'],['down']
Availability,"Updates mysql:mysql-connector-java from 8.0.28 to 8.0.32. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7058:790,down,down,790,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7058,1,['down'],['down']
Availability,"Updates org.apache.httpcomponents:httpclient from 4.5.13 to 4.5.14. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.httpcomponents"", artifactId = ""httpclient"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.httpcomponents"", artifactId = ""httpclient"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7060:810,down,down,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7060,1,['down'],['down']
Availability,"Updates org.broadinstitute.dsde.workbench:workbench-google from 0.21-5c9c4f6 to 0.23-629b08c. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7062:850,down,down,850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7062,1,['down'],['down']
Availability,"Updates org.hsqldb:hsqldb from 2.6.1 to 2.7.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6884:770,down,down,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6884,1,['down'],['down']
Availability,"Updates org.liquibase:liquibase-core from 4.8.0 to 4.15.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6886:793,down,down,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6886,1,['down'],['down']
Availability,"Updates org.liquibase:liquibase-core from 4.8.0 to 4.19.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7068:793,down,down,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7068,1,['down'],['down']
Availability,"Updates org.mockito:mockito-core from 3.11.2 to 3.12.4. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6889:786,down,down,786,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6889,1,['down'],['down']
Availability,"Updates org.webjars:swagger-ui from 4.5.0 to 4.5.2. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6899:780,down,down,780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6899,1,['down'],['down']
Availability,"Updates org.webjars:swagger-ui from 4.5.2 to 4.15.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7080:781,down,down,781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7080,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:batch from 2.17.152 to 2.17.179. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/a73dbf8f0f456857ba8dd425f017fa09baa6c7e4/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6746:798,down,down,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6746,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:batch from 2.17.152 to 2.17.189. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/0a4d1a09b47760b51c89870db3ba25430aae8ad2/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6759:798,down,down,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6759,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:cloudwatchlogs from 2.17.152 to 2.17.180. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/a73dbf8f0f456857ba8dd425f017fa09baa6c7e4/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6750:816,down,down,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6750,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:cloudwatchlogs from 2.17.152 to 2.17.190. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/c2be23ff17cde28ee3c44ff6474bb4621d41dbbd/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6760:816,down,down,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6760,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:core from 2.17.152 to 2.17.181. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/ba26ba03fb0bdd22cfa55b5595686b14ad894174/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6751:796,down,down,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6751,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:core from 2.17.152 to 2.17.191. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/c2be23ff17cde28ee3c44ff6474bb4621d41dbbd/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6761:796,down,down,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6761,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:s3 from 2.17.152 to 2.17.182. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9599889c173a765f84ef9138c01ded7a1a8d561c/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6753:792,down,down,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6753,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:s3 from 2.17.152 to 2.17.193. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2504ff1bee3ef0eeb3c13a8d1917caf18e1aef5f/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6764:792,down,down,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6764,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.174. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5ba079e5e6b075ded705306e58f3111e16796466/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6740:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6740,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.184. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/4e12175cb40eccba16c6030fae30ac4fab719481/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6754:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6754,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.194. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/ac359164689493f9b5c048982a362475b9a4129b/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6765:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6765,1,['down'],['down']
Availability,Updates to fix coercion errors. DSDEEPB-727,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/84:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/84,1,['error'],['errors']
Availability,"Use ""read committed"" transaction isolation levels for workflow starting and heartbeat writing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4238:76,heartbeat,heartbeat,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4238,1,['heartbeat'],['heartbeat']
Availability,"Use of `wdlSource` is meant to be deprecated, not a hard error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2497:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2497,1,['error'],['error']
Availability,Use reference files from mounted disk instead of downloading them from the `gcp-public-data--broad-references` bucket [BA-6482],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5746:49,down,downloading,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746,1,['down'],['downloading']
Availability,Use the right failureMode value,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2239:14,failure,failureMode,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2239,1,['failure'],['failureMode']
Availability,"User @gavvvr reports:. If i run cromwell with -Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible"" it does not work. Also using -Dconfig.file=application.conf does not work. Only workflow_failure_mode option in JSON config works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1380:74,failure,failure-mode,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380,1,['failure'],['failure-mode']
Availability,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4031:330,error,error,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031,2,['error'],['error']
Availability,"Using `better.files` for metadata writing, and removed heaviest duplicated `FileUtil` similarity.; Refactored `Main` to avoid test issues with `DelayedInit`, `System.exit`, consistent error generation, etc.; Removed blocking code from `SingleWorkflowRunnerActor`, and passing `Shutdown` as message now, instead of killing system directly.; Reused / refactored some of the test methods for actors, and removed bitrotted `DefaultWorkflowManagerActor`.; Left comments (warnings?) about possible bugs / improvements to workflow `Actor` messaging.; Replaced use of Java `SystemProperties` with Scala `sys.props`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265,1,['error'],['error']
Availability,"Using cromwell v36, not all log output appears to be captured even if log level set to DEBUG. For example, using the HaplotypeCallerGvcf_GATK4 reference on GitHub, if an non-existent path is provided for `input_bam` in the inputs file, this line will cause its workflow to fail:; ```; Int disk_size = ceil(((size(input_bam, ""GB"") + 30) / hc_scatter) + ref_size) + 20; ```. The resulting 'file not found' error is printed (50x, one per shard) on cromwell's stdout, but not captured anywhere in the logs written to disk. The workflow execution directory in `cromwell-executions/HaplotypeCallerGvcf_GATK4/{guid}` is just an empty folder and its workflow log (if preserved) indicates the workflow was started but no errors:; ```; ...; 2018-10-24 13:29:31,409 INFO - WorkflowExecutionActor-28a80dbf-a2ae-4fc8-96e6-0279c32a0e13 [UUID(28a80dbf)]: Starting HaplotypeCallerGvcf_GATK4.HaplotypeCaller (50 shards); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310,2,['error'],"['error', 'errors']"
Availability,"Using release 0.21.; There seems to be a discrepancy between [what the docs say should happen](https://github.com/broadinstitute/cromwell#error-handling) if you submit badly-formed WDL inputs to REST API, and what actually happens: which is that the inputs are accepted (get back a response with ""Submitted"" status), but later fail. Or am I misunderstanding the docs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515:138,error,error-handling,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515,1,['error'],['error-handling']
Availability,"Using the AWS backend with Cromwell 85, if the S3 key of the input contains spaces, the generated commands for localizing the inputs seem to be incorrect, with the command line argument to the the `aws s3 cp` command not being quoted. ### Steps to reproduce. Run a workflow using the AWS backend where the S3 uri of an input contains the space character. Here is a minimal WDL for reproducing; basically any task that uses a File input will do.; ```; version 1.0; task repro_task {; input {; File in; }; command <<<; cat '~{in}'; >>>; output {; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""debian:bullseye-slim""; }; }; ```. And here is an example inputs JSON:; ```; {; ""in"": ""s3://bucket-name/test/somewhere containing spaces/filename.txt""; }; ```. ### Observed result. Here is a line from the script that is generated, a bit below the line that reads `echo '*** LOCALIZING INPUTS ***'`. ```; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://bucket-name/test/somewhere containing spaces/filename.txt /tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. This leads to `aws s3 cp` to fail with an error message like this:; ```; Unknown options: spaces/filename.txt,/tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. ### Expected result. I think that in the script the argument to `aws s3 cp` should be quoted, for example like this:. ```; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress 's3://bucket-name/test/somewhere containing spaces/filename.txt' /tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. ### Additional information. The location in the code where the command line is created would seem to be at [AwsBatchJob.scala:132](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L132).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7102:873,echo,echo,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7102,2,"['echo', 'error']","['echo', 'error']"
Availability,"Using this wdl, call caching takes a very long time for each task in the scatter to go from success to done. This is labeled ""cromwell final overhead"" in the timing diagram. This task scatters 100 wide and outputs 2 arrays of 901 elements each. It is also more error prone with call caching on than off. ```; task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done; >>>; runtime {; docker: ""ubuntu:latest""; memory: ""3 GB""; cpu: ""1""; disks: ""local-disk 5 HDD""; }; output {; Array[File] outer = glob(""${sample_name}_${index}_*""); Array[File] inner = glob(""${sample_name}_${index}/*""); }; }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,; 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,; 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,; 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,; 91, 92, 93, 94, 95, 96, 97, 98, 99, 100 ]. scatter (idx in indexing_list) {; call FileSpam { input: index = idx }; }; output {; None; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901:261,error,error,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901,3,"['echo', 'error']","['echo', 'error']"
Availability,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3381:442,echo,echo,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381,1,['echo'],['echo']
Availability,"VE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment varia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3678,Down,Downloading,3678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Down'],['Downloading']
Availability,"VSCode uses the bloop build server, and with the current version of SCoverage, there is an anticipated binary conflict for the scala-xml library due to the major version difference, which does not seem to be causing an issue. This change forces this to be ignored rather than throwing an error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7124:288,error,error,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7124,1,['error'],['error']
Availability,Vague error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4032:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4032,1,['error'],['error']
Availability,Validate checksum for AWS S3 signed URL downloads during localization [BT-257],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6496:40,down,downloads,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6496,1,['down'],['downloads']
Availability,"Validates `cpuMin`, `cpuMax`, `ramMin` and `ramMax` as valid cpu / memory runtime attributes; Maps `cpuMin` and `ramMin` to `cpu` and `memory` (respectively); All the resource requirements are then available in the runtime attributes map for the backend to play with if it wants to / can.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3077:198,avail,available,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3077,1,['avail'],['available']
Availability,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5217:85,error,errors,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217,1,['error'],['errors']
Availability,"Variables in string interpolations do not undergo existence checks, so the following will validate despite `t` not existing:. ```; task foo {; # Shouldn't validate, no 't' defined:; String s = ""I like to drink ${t}""; command {; echo ${s}; }; output {; String out = read_string(stdout()); }; }; ```. EDIT: Simplified the example",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2209:228,echo,echo,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2209,1,['echo'],['echo']
Availability,"Version 1.67 is vulnerable and versions 1.69+ are safe. Upgraded to latest version 1.70. `sbt assembly` succeeds. The line; ```; addDependencyTreePlugin; ```; enables a handy new command; ```; whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; ```; shows what chain of artifacts uses a leaf-node dependency – an inversion of the traditional dependency tree:; ```; [info] org.bouncycastle:bcprov-jdk15on:1.67; [info] +-org.bouncycastle:bcpkix-jdk15on:1.67; [info] +-io.grpc:grpc-xds:1.46.0; [info] +-io.grpc:grpc-googleapis:1.46.0; [info] +-com.google.api:gax-grpc:2.18.1; [info] +-com.google.cloud:google-cloud-resourcemanager:1.4.0; [info] +-org.broadinstitute:cloud-nio-spi_2.13:80-d24645a-SNAP [S]; [info] +-org.broadinstitute:cloud-nio-util_2.13:80-d24645a-SNAP [S]; ```. With this PR's fix in place, the output is a gratifying ""this isn't used anywhere"" error:; ```; root(aen_bw_1227)> | 80> whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] Expected 'org.broadinstitute'; [error] Expected '1.70'; [error] whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6775:860,error,error,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6775,5,['error'],['error']
Availability,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772:257,failure,failures,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772,3,"['error', 'failure']","['error', 'failures']"
Availability,WAAS will have different `/status` requirements than the reader or writer Cromwells. This could involve (depending on time available) either:; * Generalizing the current status implementation to allow callers of `/status` to opt-in/opt-out of status information they do or don't care about; * Creating a new specialist health implementation which knows what WAAS would care about.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4737:123,avail,available,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4737,1,['avail'],['available']
Availability,WDL syntax error in comment still reported as an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,2,['error'],['error']
Availability,"WDL tasks that specify an output directory of ""."" (sometimes indirectly) will cause failures in downstream tasks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844:84,failure,failures,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844,2,"['down', 'failure']","['downstream', 'failures']"
Availability,WM-2428: Include full error context when failing to abort TES jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7354:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7354,1,['error'],['error']
Availability,WOMtool should specify what WDL (or subWDL) has the error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3055:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055,1,['error'],['error']
Availability,WOMtool validate returns unhelpful fatal error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['error'],['error']
Availability,"WX-1126 Upgrade to modern Python, 3.8 not available in package repo anymore",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7164:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7164,1,['avail'],['available']
Availability,WX-1225 Print TES error messages to job logger,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7220:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7220,1,['error'],['error']
Availability,WX-1595 Refactor the preemption error handling from GCP Batch backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,1,['error'],['error']
Availability,WX-1611 Fix GHA ordering error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7424:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7424,1,['error'],['error']
Availability,"WX-1625 Better handling of code 9 ""no available zones"" error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7431:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7431,2,"['avail', 'error']","['available', 'error']"
Availability,WX-1672 Fix Docker-related CI failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,1,['failure'],['failures']
Availability,WX-757 Fix workflow stuck in aborting after WDL type error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['error'],['error']
Availability,WX-843 Workflow failure reason should accurately indicate issues opening blob filesystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965:16,failure,failure,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965,1,['failure'],['failure']
Availability,WX-876 Surface TES System Logs to Cromwell when TES backend returns task error status,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6979:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6979,2,['error'],['error']
Availability,"WX-927 GCPBATCH retry with more memory and error message fixes, Centaur tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,1,['error'],['error']
Availability,WX-927 Proper GCP Batch failure on noAddress,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7542:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7542,1,['failure'],['failure']
Availability,"Warn on async failures, we're not logging debug and this is a bad state.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/376:14,failure,failures,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/376,1,['failure'],['failures']
Availability,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1155:575,down,down,575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155,1,['down'],['down']
Availability,"We are investigating ways we can store additional information with a cromwell run that is useful for our downstream applications. The two ways to pass associated metadata with a workflow are the Labels attribute through the runtimeOptions, or the `meta` object within cromwell itself. Exposing either of these as parsable json in the metadata objet would achieve what we are looking for; ; ## Meta; If I Have a workflow with a meta object:. ```; workflow a {; call b. meta {; some_key: ""some_value""; }; }; ```; The only way to retrieve the meta values from the /metadata endpoint would be to re-parse the workflow string contained in metadata object ; ```; ""workflow"": "" .....""; ```; Since they are simple KV pairs it would be nice to have an output that looks like the following, embedded in the `metadata` object:; ```; {; ""meta"": {; ""key"":""value"",; ""key2"":""value2""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421:105,down,downstream,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421,1,['down'],['downstream']
Availability,"We are running Cromwell with the Google Genomics (aka Google Pipelines API, aka Google Life Sciences API) backend plugin. . There's a known issue on Google's side (unfortunately no public link) that causes ssh-server to fail to start up (`tcp4 0.0.0.0:22: bind: address already in use`). This causes the entire workflow to fail. A change in [SSHAccessAction](https://github.com/broadinstitute/cromwell/blob/a69d12ec71bd453bf50be563f0666e7fb65c6874/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/SSHAccessAction.scala#L24) will allow the worker to ignore the error:; ```scala; setIgnoreExitStatus(true); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771:613,error,error,613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771,1,['error'],['error']
Availability,"We are running a wld pipeline on GCP. Requested 16 CPSs. we have; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; set in the .conf file. Nevertheless, the task was given an e2-standard-16 machine, and took 9 hours to turn a bam into a cram, and another hour to upload the 51 gb ram file. I ran the code again, only change was I used an n2-standard-16; The entire task, including the upload, finished in less than 30 minutes. How do we make it so we never, ever, get an ""e"" machine? I downloaded the cromwell code and looked through it, trying to figure out where it is you pick the machine type on GCP, but never found anything that looked like it would pick an e machine. What am I missing?. The cromwell that assigned the e2-standard-16 was cromwell-87.jar, if that matters. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474:485,down,downloaded,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474,1,['down'],['downloaded']
Availability,"We are seeing infrequent situations where JES says a call is done but cromwell thinks it is still running. I see the call starting in the logs:; ```; 2017-02-08 18:55:58,500 cromwell-system-akka.dispatchers.engine-dispatcher-963 INFO - WorkflowExecutionActor-fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 [UUID(fa7e25a2)]: Starting calls: pon_gatk_workflow.PadTargets:NA:1; ```. Then the only suspicious things I see later in the logs are these messages (which could be completely unrelated however they do start to appear 2.5 minutes after the job completes on JES):; ```; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-424 INFO - The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$Enn#762671444] unexpectedly terminated while conducting 100 polls. Making a new one...; java.lang.NullPointerException: null; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-246 ERROR - null; ```. For future reference by a FireCloud admin the operations id is ELSl1PihKxjdhp-6gvr3weYBILma7PWMHyoPcHJvZHVjdGlvblF1ZXVl and the workflow id is fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 and the workflow was aborted 3PM Feb 8.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965:938,ERROR,ERROR,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965,1,['ERROR'],['ERROR']
Availability,"We are trying to abort a job in FireCloud using cromwell .21. The workflow appears stuck in submitted state. It appears to have no calls. Is there are race condition that is we abort too soon the job is stuck?. ```; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/status; {; ""status"": ""Submitted"",; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d""; }(dvoet@wm163-585) 14:55; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/metadata; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d"",; ""inputs"": {. },; ""submission"": ""2017-01-20T16:37:31.589Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:864,echo,echo,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,1,['echo'],['echo']
Availability,"We are trying to create a reference disk manifest as mentioned here:; https://github.com/broadinstitute/cromwell/blob/928ad9616bb81cd8948077a1d0319ade6127e521/src/ci/resources/papi_v2_reference_image_manifest.conf. While running the create_image.sh with an input.tsv reflecting our storage (; https://github.com/broadinstitute/cromwell/blob/develop/scripts/reference_disks/create_images.sh ); I get this error:; ```; Checking for existing resources...; Creating images...; DRY RUN to manifest: refdata-disk-image-public-2023-12-19.manifest.conf; {; imageIdentifier: ""projects/xxxxxxx/global/images/refdata-disk-image-public-2023-12-19""; diskSizeGb: 1310; files: [; curl: (22) The requested URL returned error: 401 Unauthorized; create_images.sh: line 128: 16#: invalid integer constant (error token is ""16#""); ```. I am wondering if I can get any help to resolve this. We are trying to get PAPIv2 working, and our workflows expect that a data directory get passed as input to the workflows and bound to the docker image so that the tools have access to them. . Example: ; `rqcfilter2.sh -Xmx${default=""60G"" memory} -da threads=${jvm_threads} ${chastityfilter} jni=t in=${input_files} path=filtered rna=f trimfragadapter=t qtrim=r trimq=0 maxns=3 maq=3 minlen=51 mlf=0.33 phix=t removehuman=t removedog=t removecat=t removemouse=t khist=t removemicrobes=t sketch kapa=t clumpify=t tmpdir= barcodefilter=f trimpolyg=5 usejni=f rqcfilterdata=${database}/RQCFilterData > >(tee -a ${filename_outlog}) 2> >(tee -a ${filename_errlog} >&2)`. Where ${database} would be the base directory containing all the reference data.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7348:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7348,3,['error'],['error']
Availability,"We are trying to expose workflow failure messages to the user in Firecloud. However, the failures field seems to have an inconsistent format:. Compare the failures sections for the following:. ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""{...},; ""calls"": {; ""echo_strings.echo_files"": [{; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""machineType"": ""us-central1-c/n1-standard-1"",; ""googleProject"": ""broad-dsde-dev"",; ""executionBucket"": ""gs://cromwell-dev/cromwell-executions"",; ""zone"": ""us-central1-c"",; ""instanceName"": ""ggp-3462354720519617596""; },; ""runtimeAttributes"": {...},; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""jobId"": ""operations/EJiq_oWfKxi8-N-X4qiwhjAgw7vetLsXKg9wcm9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:33,failure,failure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,3,['failure'],"['failure', 'failures']"
Availability,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333:553,down,downstream,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333,3,"['down', 'error']","['downstream', 'error']"
Availability,"We can not get memory retry to work. Have not not found anywhere a complete example showing it working, including what should be int eh .conf file. If such an example exists, please point us to it,. Command:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; comp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:561,echo,echo,561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,['echo'],['echo']
Availability,"We do this for preemptible VMs, when a job fails with certain error codes (13/14 I believe) which indicate that a VM shut down unexpectedly, we retry a certain number of times which is user specified via the WDL task definition. @dvoet has requested this same feature for non-preemptible VMs for FireCloud",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1042:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042,2,"['down', 'error']","['down', 'error']"
Availability,"We get noisy messages in our logs that seem to come from [this line of code](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/core/azure-core/src/main/java/com/azure/core/util/serializer/SerializerEncoding.java#L62) in the Azure SDK.; ```; 2023-02-17 16:30:13 reactor-http-nio-2 WARN - 'Content-Type' not found. Returning default encoding: JSON; ```; Searching for the error, I found [an issue](https://github.com/Azure/azure-sdk-for-java/issues/32250) and [fix PR](https://github.com/Azure/azure-sdk-for-java/pull/32833) mentioning it (in rhyming but not identical circumstances). I confirmed that going from `1.7.1` to `1.10.2` brings in the improved handling by examining the copy of `HttpResponseBodyDecoder.java` that SBT downloads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7088:378,error,error,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7088,2,"['down', 'error']","['downloads', 'error']"
Availability,"We had a situation where develop was broken (grab commit 0ff86c409d2e5dac4b766fceb89f47ba3c304f99). If you run ""sbt test"" it fails with a compilation error about HtCondorInitializationActorSpec.scala. However, Travis for this is green. Travis is running:. sbt -Dbackend.providers.Local.config.filesystems.local.localization.0=copy clean coverage nointegration:test coverageReport && sbt coverageAggregate && sbt assembly. Which if you run it locally yields a successful test. First thought was that it was because this test was flagged as ""nointegration"" but that's not even the case. However even if it were, we should be checking that things compile even if we don't run a certain class of tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/888:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/888,1,['error'],['error']
Availability,"We had two workflows fail with the following message:. ```; ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; { ; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }""; ```. JES indicated that the rate of unavailability is below what they consider a problem, and suggested retries. I showed the stacktrace below to Miguel, who says there is a retry around this call, but that it is fairly short. ```; {;   ""code"" : 503,;   ""errors"" : [ {;     ""domain"" : ""global"",;     ""message"" : ""Backend Error"",;     ""reason"" : ""backendError"";   } ],;   ""message"" : ""Backend Error""; };   at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19];   at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:173,error,errors,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,6,"['Error', 'error']","['Error', 'errors']"
Availability,"We have a cromwell task that failed with the unhelpful error message: `Unexpected failure in EJEA (root cause not captured).` After investigation we found that the task was being killed by the google cloud compute system:; ```; ""error"": {; ""code"": 1,; ""message"": ""Operation canceled at 2017-07-23T01:01:39-07:00 because it is older than 6 days""; },; ```; It would be better if cromwell was able to report this error directly, so we don't have to look at the gcloud operations properties. Looking at the cromwell logs I see this sequence:; ```; 2017-07-23 08:09:42 [cromwell-system-akka.actor.default-dispatcher-3449] WARN c.e.w.l.e.WorkflowExecutionActor - WorkflowExecutionActor-f15009ab-b5bc-47ac-b832-3d0ae2f15888 [UUID(f15009ab)]: WorkflowExecutionActor [UUID(f15009ab)] received an unhandled message: AbortedResponse(PairedEndSingleSampleWorkflow.MarkDuplicates:-1:1) in state: WorkflowExecutionInProgressState; 2017-07-23 08:09:47 [cromwell-system-akka.actor.default-dispatcher-3375] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f15009ab-b5bc-47ac-b832-3d0ae2f15888 failed (during ExecutingWorkflowState): Unexpected failure in EJEA (root cause not captured).; ```. In this case the google operations ID is `EOjNtPvUKxiOgeqc763--TEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU`. I can provide more operations IDs if necessary.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,6,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"We have a workflow that scatters 95 wide as its first step. When running with `read_from_cache=false` the workflow chugs along normally. When we allow the same workflow to read from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.google",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:423,error,errors,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,4,"['Error', 'error']","['Error', 'errors']"
Availability,"We need to re-figure out our hashing strategy for docker images, especially in GCR. In the meantime, any GCR image that does not have a `v2+json` hash available will run, but will always fail call caching. We currently request a ""v2+json"" Docker-Content-Digest for all GCR images. Recently, GCR [""correctly""](https://enterprise.google.com/supportcenter/managecases#Case/0016000000MNGDy/U-13919143) returns 404 when it doesn't actually have ""v2+json"" for the images. There does appear to be an alternative ""v1+prettyjws"" Docker-Content-Digest available for GCR images. (FYI there are a [number of search hits](https://www.google.com/search?q=Docker-Content-Digest) re: ""Docker-Content-Digest"", especially v1 vs. v2, etc.) It's likely that Google's ""correction"" mentioned above was to fix the server from returning the wrong ""v1"" hash when ""v2"" was requested-but-not-available. While cromwell only uses the headers via HTTP HEAD, the hashes in the headers and the contents of an HTTP GET _are_ different between v1 and v2. Google tech support have also mentioned that clients may request either hash type, submitting both `Accept` headers, and that one can read the `Content-Type` from the server to determine which `Docker-Content-Digest` was returned. Bodies reformatted by jq for readability:; ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v1+prettyjws' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v1+prettyjws; Content-Length: 2647; Docker-Content-Digest: sha256:781290a693dc805993b19b7b4c5be40f7688f595312646b926abe2baae2fa9ff; Date: Mon, 06 Nov 2017 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:151,avail,available,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,3,['avail'],['available']
Availability,"We noticed this in FC alpha -- no status field. Rebooting Cromwell (well over an hour after submission) fixed it. ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-07b4785f-2cc2-4147-bd6f-67cf8a4049ba/fe75135f-3637-4ead-9bc7-26573cfc50cc/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-07b4785f-2cc2-4147-bd6f-67cf8a4049ba/fe75135f-3637-4ead-9bc7-26573cfc50cc\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""bea0383d-6ca8-4392-b048-03914e87444d"",; ""inputs"": {. },; ""submission"": ""2017-02-11T01:27:49.935Z""; }; ```. Cromwell version: 24-489f66b; hostname: gce-cromwell-alpha102; workflow id: 3d01da76-98f9-4751-a3c0-efc61ef67030",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978:48,Reboot,Rebooting,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978,2,"['Reboot', 'echo']","['Rebooting', 'echo']"
Availability,"We randomly receive PKIX and RPC deadline errors on a small subset of GCP LS API and Batch runs, which always crash the main Cromwell java process. We have added local certs to our java install launching cromwell - this has reduced, but not eliminated PKIX errors. We have no remedy for these issues at the moment besides submitting again. Is there anything we can do as users to make cromwell more fault-tolerant of cloud-related issues like these?. PKIX error example:. Failed to query job status (projects/XXXXX/jobs/job-7638b0fb-XXXX-XXXX-81ca-9f609da4c664) from GCP; com.google.api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:112); at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); at io.grpc.PartialForw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,4,"['error', 'fault']","['error', 'errors', 'fault-tolerant']"
Availability,"We run custom WDL genomics pipelines and also parts of the BioWDL somaticcalling pipeline (https://biowdl.github.io/) using the broadinstitute/cromwell:64 cromwell container with a gridengine backend. Occasionally we see errors like the below:. `; cromwell_1 | 2023-03-20 14:56:42,590 cromwell-system-akka.dispatchers.engine-dispatcher-193 INFO - WorkflowManagerActor: Workflow 3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; cromwell_1 | Bad output 'scatterList.scatters': Failed to read_lines(""/home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout"") (reason 1 of 1): [Attempted 1 time(s)] - IOException: Could not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.imp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:221,error,errors,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['error'],['errors']
Availability,We should be able to grab the principal ID from the HTTP header via the apache proxy. Get this info from DevOps and demonstrate capability to grab this in the HTTP code. I suspect the easiest path here would be to talk to some folks in firecloud on how to handle this for testing and such when the proxy is not available,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2132:311,avail,available,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2132,1,['avail'],['available']
Availability,"We track collection via a label injection. If a workflow dies early on, eg MWDA, the label doesn't appear to be persisted and thus the user can't look up errors for their WF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3225:154,error,errors,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3225,1,['error'],['errors']
Availability,"We typically have workflow inputs that fall into different categories based on how often they vary.; For example there are inputs that vary according to:; Sample (every time); Genome build (almost never); Plexity (rarely, but sometimes). It would be nice if we could pass multiple workflow jsons to the cromwell server. That would allow us to maintain a static, per-genome build inputs file as well as more dynamic per-run inputs files. The former points to public data and could be distributed among our collaborators when we share our method. We get around the absence of this feature by munging multiple jsons together on every submission. But this makes things harder to share with others. It's also error prone. Priority is low. This is a convenience and community-adoption feature- not a blocker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/982:704,error,error,704,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/982,1,['error'],['error']
Availability,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. First try will be with using Travis and probably moving to Jenkins if it can't be done. This test could be run nightly/weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2336:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2336,1,['down'],['down']
Availability,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. The easiest first step would be to make this a part of the Tyburn daily test we already have on Jenkins. This test should probably run weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337,1,['down'],['down']
Availability,"We were recently told that FireCloud has updated it's Cromwell version to 0.24. For our local testing, we had previously been using Cromwell 0.21, so I was trying to upgrade our build system to use 0.24 as well. However, v0.24 seems to have some regressions with respect to our WDLs. In addition after encountering the error cromwell is unkillable without `kill -9`. To reproduce (Note: I am using OSX 10.10.5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:319,error,error,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,3,"['Down', 'down', 'error']","['Download', 'download', 'error']"
Availability,"We would like to handle certain JES VM errors in a manner which allows for some automatic retries prior to failing the job. There will be two categories, each handled slightly differently from the other:. - Preemption for preemptible VMs (i.e. Error 14); - JES error codes other than preemption which we believe to be transient (e.g. Error 13). For both situations, the following statements will be true:. - When one of these events happens we will determine if the job is retryable (see below for criteria); - If the job is not retryable it will be failed; - If the job is retryable; -- An event will be sent to the metadata service; -- The retry will be logged; -- The attempt number shall be incremented ; -- The job will be attempted again. In terms of visibility to the end user, preemption shall be handled exactly as it is now. For the other error codes we retry up to 2 times, and there will be a server level configuration option to toggle this behavior on and off. There will be a subtle implementation change for these compared to how preemption is handled now. In the JES backend if a job fails for one of these two reasons and we decide the job is retryable (either because it was preempted and the remaining preemption count is > 0 or non-preemption and the master retry count is > 0) a signal will be sent back to the engine that the job failed but is to be retried. The difference here from the current behavior is that the JES backend is the one tracking the retry counts and merely telling the backend to retry the job instead of the engine tracking the retry count(s).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1910:39,error,errors,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1910,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"We would like to handle certain JES VM errors in a manner which allows for some automatic retries prior to failing the job. There will be two categories, each handled slightly differently from the other:. Preemption for preemptible VMs (i.e. Error 14); JES error codes other than preemption which we believe to be transient (e.g. Error 13); For both situations, the following statements will be true:. When one of these events happens we will determine if the job is retryable (see below for criteria); If the job is not retryable it will be failed; If the job is retryable; -- An event will be sent to the metadata service; -- The retry will be logged; -- The attempt number shall be incremented; -- The job will be attempted again; In terms of visibility to the end user, preemption shall be handled exactly as it is now. For the other error codes we retry up to 2 times, and there will be a server level configuration option to toggle this behavior on and off. There will be a subtle implementation change for these compared to how preemption is handled now. In the JES backend if a job fails for one of these two reasons and we decide the job is retryable (either because it was preempted and the remaining preemption count is > 0 or non-preemption and the master retry count is > 0) a signal will be sent back to the engine that the job failed but is to be retried. The difference here from the current behavior is that the JES backend is the one tracking the retry counts and merely telling the backend to retry the job instead of the engine tracking the retry count(s).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1912:39,error,errors,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1912,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"We're beginning to get warning during our build because both wdl4s and cromwell are specifying different lenthall versions:. ```scala; [warn] There may be incompatibilities among your library dependencies.; [warn] Here are some of the libraries that were evicted:; [warn] 	* org.broadinstitute:lenthall_2.11:0.20 -> 0.21-e1b7822-SNAP; [warn] Run 'evicted' to see detailed eviction warnings; ```. Options:; 1. wdl4s leaves the lenthall dependency as a compile dependency, the default. cromwell removes its explicit dependency on lenthall, and receives lethall as a transitive dependency through wdl4s. Anytime cromwell wants to update lenthall, wdl4s will need to also be updated and regression tested.; 2. wdl4s marks the lenthall dependency as provided. It must be included in cromwell, wdltool, rawls(?), and other downstream build.sbt's. This option may not catch lenthall version incompatibilities between cromwell and wdl4s until runtime, but will not cause version conflicts in build.sbt.; 3. We ignore the problem, and continue to have sbt output warnings whenever cromwell and wdl4s disagee on a lenthall version. Has similar potential runtime errors as option 2.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1955:817,down,downstream,817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1955,2,"['down', 'error']","['downstream', 'errors']"
Availability,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,2,['error'],['error']
Availability,We've seen in centaur local testing that we fail at times because cromwell is reading an output file before all of its contents has been written and we have no way to detect this (except via the artificial expectation management of centaur). There was some handwaving about filesystems and putting in sleeps but both of these seem like the wrong path. @kcibul this seems like this could go under the reliability umbrella?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1868:400,reliab,reliability,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868,1,['reliab'],['reliability']
Availability,"We've seen several workflows fail with this error:. 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Completion work failed for call CollectUnsortedReadgroupBamQualityMetrics:10.; java.net.SocketTimeoutException: Read timed out;   at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72];   at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72];   at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72];   at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72];   at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72];   at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72];   at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72];   at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72];   at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72];   at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72];   at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72];   at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72];   at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72];   at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72];   at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72];   at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72];   at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72];   at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72];   at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19];   at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"What I started to get from time to time is that some jobs keep running forever while no files are created in its executions folder (incl. technical files like script and script.submit). If happens (althoug not deterministically, I cannot get 100% reproducibility) in the latest versions (like cromwell-31-d84be2e-SNAP.jar) of cromwell when I have folders as File inputs for both workflow and tasks.; When I looked into cromwell logs I noticed:; ```; [ERROR] [03/09/2018 00:55:25.108] [cromwell-system-akka.dispatchers.engine-dispatcher-27] [akka://cromwell-system/user/cromwell-service/WorkflowManager; Mar 09 00:55:25 web start-cromwell.sh[110916]: java.io.IOException: Is a directory; Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read0(Native Method); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.IOUtil.read(IOUtil.java:197); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); Mar 09 00:55:25 web start-cromwell.sh[110916]: at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:794); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); Mar 09 00:55:25 web start-cromwell.sh[110916]: at org.apache.commons.code",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:451,ERROR,ERROR,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['ERROR'],['ERROR']
Availability,"What are the restrictions on the WDL that can appear in the `runtime-attributes` section of a provider definition? Issue #4685 indicates that `Array` types cannot be specified. I have also learnt that you cannot reference symbols that have already been defined. For example:. ```; runtime-attributes = """"""; Int foo?; String mode = if defined(foo) then ""foo"" else ""bar""; """"""; ```. This gives an error on Cromwell's warm up as follows:. ```; [2019-03-04 16:26:34,77] [error] No identifiers should be looked up: foo; ```. The above is just a toy example, but this functionality would be *really* useful. It's not clear, much like the `Array` restriction, why this isn't allowed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4700:394,error,error,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4700,2,['error'],['error']
Availability,"What should be a transient Google credential error is fatal, not transient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436,1,['error'],['error']
Availability,What's not there yet:; - Private dockerhub; - ~~GPU support~~. Everything else should work ~~once the resilience fix for dockerhub is deployed in PAPI.~~. 95% of the relevant code of this PR is in the `cromwell.backend.google.pipelines.v2alpha1` package. The 5% left is due to refactoring to better isolate what is V1 logic and V2 logic.; The rest is noise due to renaming JES stuff to PAPI stuff.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3583:102,resilien,resilience,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3583,1,['resilien'],['resilience']
Availability,"When Cromiam tries to create a Sam resource that already exists, Sam will return a `HTTP 409 Conflict`. This ""error"" is expected, and should not be forwarded back to the client. Instead, the 409 response should be followed up with a request from Cromiam to Sam to add the user to the existing Sam resource. This functionality was previously implemented, but in #4624 **all** Sam errors were passed on, even the expected `409`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4885:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4885,2,['error'],"['error', 'errors']"
Availability,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4756:210,Failure,Failures,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756,4,"['Failure', 'error']","['Failures', 'error', 'errors']"
Availability,"When I created a struct and then use Array[MyStruct] it works, but when I go to Array[Array[Mystruct]] if fails with the latest (dev) version of Cromwell, for both development and 1.0 versions of wdl. I enclose the zip with workflows (there I have Array[Array[Gentrome]] that crashes); [quant_index_batch.zip](https://github.com/broadinstitute/cromwell/files/4117802/quant_index_batch.zip). the error is:; ```; Workflow input processing failed. WorkflowFailure(Failed to evaluate input 'references' (reason 1 of 1): No coercion defined from '{""genome"":""/data/ensembl/99/species/acanthochromis_polyacanthus/Acanthochromis_polyacanthus.ASM210954v1.dna.toplevel.fa"",""species"":""acanthochromis_polyacanthus"",""subversion"":""ensembl_99"",""transcriptome"":""/data/ensembl/99/species/acanthochromis_polyacanthus/Acanthochromis_polyacanthus.ASM210954v1.cdna.all.fa"",""version"":""ASM210954v1""}' of type 'spray.json.JsObject' to 'Array[WomCompositeType { species -> String subversion -> String? version -> String genome -> File transcriptome -> File }]'.,List()); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5387:395,error,error,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5387,1,['error'],['error']
Availability,"When I run a workflow and a task fails because the exit code is non-zero, I get an error message (good!) followed by a Cromwell runtime exception (bad!). I don't really care where in the cromwell code the exit code if zero is handled ;). If anything, I would like to see the stderr of the task that failed... but really just don't display that stacktrace. E.g. [2017-01-06 07:21:10,22] [error] WorkflowManagerActor Workflow 122adad6-115e-4c3a-8e83-0782145cf3e3 failed (during ExecutingWorkflowState): Call BamToUnmappedBams.SortSam:0:1: return code was 1; java.lang.RuntimeException: Call BamToUnmappedBams.SortSam:0:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:418); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:356); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:320); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,2,['error'],['error']
Availability,"When I see the below message, I think it is a failure, when in reality it just means that it could not find the docker image on docker hub, but found it when it checked the local docker instance. ```; [2017-08-10 20:51:05,54] [warn] BackendPreparationActor_for_bb616a1f:CNVSomaticPanelWorkflow.AnnotateTargets:-1:1 [bb616a1f]: Docker lookup failed:. java.lang.Exception: Docker image broadinstitute/gatk:2c8bf87cf523f25443f6eb7ddbe83efb0e03a69c not found; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538:46,failure,failure,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538,1,['failure'],['failure']
Availability,"When I submit jobs to AWS Batch through cromwell, if my batch compute instance uses the latest ECS agent (ver 1.25) then all my AWS Batch jobs fail with the error: ""CannotStartContainerError: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused ""exec: \""gzipdata\"": executable file not found in $PATH"": unknown"".; Jobs run fine if I directly submit them to batch instead of submitting thro cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604,2,"['Error', 'error']","['Error', 'error']"
Availability,"When I try to refer condition[0] in the following example I get weird ; ```; Can't index Success(WdlString(GSM1696299)) with index Success(WdlInteger(0)); ```; error. As an input here I give a row from tsv file, like; ```tsv; Transgenic_Control_L4	GSM1696283	GSM1696284; ```; I use if(sample != condition[0]) check because I cannot get array.tail in wdl (I opened a https://github.com/broadinstitute/cromwell/issues/2414 issue on this) and have to skip the first element somehow. Looks like it does not evaluate condition[0] as array here.; Here is the code sample that raises this error. It is one of subworkflows.; ```wdl; workflow cleanup {. Array[String] condition. scatter (sample in condition) {; #first sample is not a sample but name of the condition; if(sample != condition[0]) {; call get_sample {input: condition = sample[0], sample = sample }; }; }. output {; String out = ""WORKS!""; }. }. task get_sample {. String condition; String sample. # read the following explanations for parameters; # https://edwards.sdsu.edu/research/fastq-dump/. #command {; # fastq-dump --skip-technical --gzip --readids --read-filter pass --dumpbase --split-files --clip ${file}; #}. #quay.io/comp-bio-aging/geoparse --location /data --filetype sra --keep_sra true GSM1696283 GSM1696284; command {; --filetype fastq --keep_sra false ${sample}; }. runtime {; docker: ""quay.io/comp-bio-aging/geoparse@sha256:bdf03cda576e24985060a795bede5b50eca89c9053a0fe179b8f9bc282e4db00""; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2413:160,error,error,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2413,2,['error'],['error']
Availability,"When I try to use globs, I get the following error:. ```; Evaluating glob(file_pattern) failed: glob(path, pattern) not implemented yet; ```. I've created a minimal reproducible example:. ## File `glob.wdl`; ```; task hello {; File in. command {; egrep 'CROMWELL' '${in}'; }. output {; Array[String] matches = read_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunner",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,1,['error'],['error']
Availability,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,7,"['Echo', 'Error', 'echo', 'error']","['Echo', 'Error', 'echo', 'error']"
Availability,When I use cromwell with mysql database I often get the following error:; ```; Caused by: java.sql.SQLException: Can't get stat of './cromwell/DATABASECHANGELOGLOCK.TRG' (Errcode: 13 - Permission denied); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:963); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3966); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3902); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2526); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2673); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2545); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2503); 	at com.mysql.jdbc.StatementImpl.executeInternal(StatementImpl.java:839); 	at com.mysql.jdbc.StatementImpl.execute(StatementImpl.java:739); 	at com.zaxxer.hikari.proxy.StatementProxy.execute(StatementProxy.java:83); ```. What is interesting is that ./cromwell does not exist at all. In my case I use official latest docker mysql container for mysql (where I manually create cromwell database),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2222:66,error,error,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2222,1,['error'],['error']
Availability,"When I was debugging my issue with maps ( https://github.com/broadinstitute/cromwell/issues/4663 ) I noticed (multiple times); ![screenshot_2019-02-15 screenshot](https://user-images.githubusercontent.com/842436/52890479-9e730a80-318d-11e9-9858-e6961d935d62.png); that even though all the tasks succeeded, the latest task (salmon) did not cache when a runtime error was thrown with workflow output type. I think as the task itelf succeeded it should be cached, so users will not loose several hours when only a small adjustment in the output type is needed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4664:360,error,error,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4664,1,['error'],['error']
Availability,"When I was running Cromwell on HPC, I found that analysis slowed down significantly when I set call-caching.; I used slurm and singularity as the backend.; The attachment is my configuration file, please help to see what is wrong, thank you!; [slow.cromwell.config.txt](https://github.com/broadinstitute/cromwell/files/10112324/slow.cromwell.config.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6958:65,down,down,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6958,1,['down'],['down']
Availability,"When Sam is no longer recording the workflow ID but the collection ID, for non-submission workflow based reqests we'll need to first look up the collection id from cromwell prior to pinging Sam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2852:182,ping,pinging,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2852,1,['ping'],['pinging']
Availability,"When a step in a workflow fails, you can go read the `stderr` file for logs which are very informative. Cromwell also logs useful information to stdout. But checking the `/metadata` API endpoint always gives the same message: ; ```; ""executionStatus"": ""Failed"",; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Job rna.kallisto:0:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ```. It only tells you that the return code was not registered, not a clue into what might have caused the error. Could this be improved?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4856:264,failure,failures,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856,2,"['error', 'failure']","['error', 'failures']"
Availability,"When cromwell needs to recover many jobs at once, or check many caches at once it seems to only do one at a time. I found this with the following wdl, scattered by about 300 ways, run on broad filesystem with the SGE backend. I'd be happy to provide the json and list of inputs if needed. ```; task Decapitate {; 	File inputFile; 	String outputFilename. 	command {; 		tail -n+2 ${inputFile}; 	}; 	runtime {; 		memory: ""4 GB""; 	}; 	output {; 		Array[Array[String]] outputMatrix=read_tsv(stdout()); 	}; }. task BreakUpRow {; 	String PROJECT; 	String SAMPLE; 	String DATA_TYPE; 	String COMPARE_SAMPLE ; 	String COMPARE_PROJECT; 	String CLEAN_SAMPLE; 	String CLEAN_COMPARE_SAMPLE. 	command {; 		#do nothing; 	}; 	runtime {; 		memory: ""1 GB""; 	}; 	output {; 		Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bam""]; 		Array[File] indexes = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bai"", ""/seq/picard_aggregation/${COMPARE_PROJECT}/${CLEAN_COMPARE_SAMPLE}/current/${CLEAN_COMPARE_SAMPLE}.bai""]; 		File genotypes = ""/seq/references/reference_genotypes/non-hapmap/${PROJECT}/Homo_sapiens_assembly19/${CLEAN_SAMPLE}.vcf""; 		String PROJECT_out = ""${PROJECT}""; 		String SAMPLE_out = ""${SAMPLE}""; 		String DATA_TYPE_out = ""${DATA_TYPE}""; 		String COMPARE_SAMPLE_out = ""${COMPARE_SAMPLE}""; 		String COMPARE_PROJECT_out = ""${COMPARE_PROJECT}""; 		String CLEAN_SAMPLE_out = ""${CLEAN_SAMPLE}""; 		String CLEAN_COMPARE_SAMPLE_out = ""${CLEAN_COMPARE_SAMPLE}""; 	}. }. task Fingerprint {; String PICARD; File input_bam; File input_bam_index; File haplotype_database_file; File genotypes; String vcf_sample; String output_name; ; command {; java -Dsamjdk.buffer_size=131072 -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -Xmx1024m \; -jar ${PICARD} \; CheckFingerprint \; INPUT=${input_bam} \; OUTPUT=${output_name} \; GENOTYPES=${genotypes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1844:23,recover,recover,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1844,1,['recover'],['recover']
Availability,"When download input from S3 to instance, it may fail. But the workflow will continue to run.; Retry and exit after 5 times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4749:5,down,download,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4749,1,['down'],['download']
Availability,"When executing a workflow with import statements, Cromwell 36 fails when the WDL version is set to `1.0`, but succeeds when the version is set to `draft-2`. The error I get is:. ```; [2018-10-18 14:44:50,45] [error] WorkflowManagerActor Workflow bb8cc90a-75a5-4fca-a38a-13043700dbaf failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 1, column 1:. import ""gatk3-data-processing/processing-for-variant-discovery-gatk3.wdl"" as processing; ^; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4273:161,error,error,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4273,2,['error'],['error']
Availability,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1185:147,error,error,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185,6,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error', 'errors']"
Availability,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4111:383,error,error,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111,9,"['Echo', 'echo', 'error']","['Echo', 'echo', 'error']"
Availability,"When running [this workflow step](https://github.com/genome/cancer-genomics-workflow/blob/c5eabd41d5cba375bec79eaffed97c80ab24babb/umi_alignment/extract_umis.cwl#L22), the constructed command-line was different than expected for the `--read-structure` parameter. This short CWL demonstrates:. `test.cwl`:; ```cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```; `test.yml`:; ```yml; bonus: [""first"", ""second""]; ```. Running with `cwltool test.cwl test.yml` yields:; `hello.txt`:; ```; --bonus first second; ```; Running with `/usr/bin/java -jar cromwell-32-0c557ab-SNAP.jar run -t cwl -i test.yml test.cwl` yields:; `cromwell-executions/test.cwl/6817868a-76e6-48b6-8702-4fe456b23277/call-test.cwl/execution/hello.txt`:; ```; --bonus --bonus first --bonus second; ```. The result from `cwltool` is what I would expect based on the `filesA` example in [this user guide](http://www.commonwl.org/user_guide/09-array-inputs/).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3589:403,echo,echo,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3589,1,['echo'],['echo']
Availability,When running `sbt assembly` I get a merge conflict on netty-resolver. Likely this is coming from the AWS SDK dependency. . ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/org/asynchttpclient/netty-resolver/2.0.35/netty-resolver-2.0.35.jar:io/netty/resolver/NoopAddressResolverGroup.class; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/io/netty/netty-resolver/4.1.22.Final/netty-resolver-4.1.22.Final.jar:io/netty/resolver/NoopAddressResolverGroup.class; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514:129,error,error,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514,3,['error'],['error']
Availability,"When running a docker image through Cromwell, it assumes that you are the root user for the docker container. I was trying to run a docker image which has to be run as a non-root user, so they don't have access to the root user home folder (/root). The problem with this is that Cromwell will place any files you pass to the container in the /root directory, so you need to be the root user or else you will get a permission error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/472:425,error,error,425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/472,1,['error'],['error']
Availability,"When running a sufficiently nested workflow, I get the error `File name too long`, because WDL is generating a temporary file named `temp-s3-6826696772619511254cromwell-execution_best_practise_3bf9c436-31f7-4d86-ba87-cf54248e05bc_call-ConvertPairedFastQsToUnmappedBamWf_unmap.ConvertPairedFastQsToUnmappedBamWf_e10704e8-7ff4-4e4c-89ad-48270734ba25_call-CreateFoFN_write_lines_238fd975f9e3d166dbec07b20ad88c51.tmp`, which is greater than 256 characters and thus too large for any Linux system. I'm running Cromwell 36 on with the AWS Batch backend. The traceback is:. ```; 2018-10-19 01:29:11,310 cromwell-system-akka.dispatchers.backend-dispatcher-8681 ERROR - Failed command instantiation; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:565); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:500); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand(AwsBatchAsyncBackendJobExecutionActor.scala:74) ; at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:313); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132) ; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBacken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"When running firecloud in a box, the services start with an empty database and initialize them with liquibase. Sometimes the cromwell service can't connect to port 8000 right away and when this happens the service shuts down and restarts. Typically, the shutdown happens during the liquibase initialization and this leave a lock in the DB. On restart, cromwell can connect to port 8000, but can't complete the DB initialization because of the lock left during shutdown. The cromwell system should either complete the DB initialization before shutting down if it can't connect to the port or it should handle an interrupted DB initialization on restart. [cromwell-app.rtf.txt](https://github.com/broadinstitute/cromwell/files/1166281/cromwell-app.rtf.txt); [cromwell-proxy.rtf.txt](https://github.com/broadinstitute/cromwell/files/1166282/cromwell-proxy.rtf.txt); [cromwell-mysql.rtf.txt](https://github.com/broadinstitute/cromwell/files/1166280/cromwell-mysql.rtf.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2476:220,down,down,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2476,2,['down'],['down']
Availability,"When running jobs on backends with job runtime limits such as LSF or SLURM, jobs reaching the runtime limits are killed by the backend. [Cromwell never detects that this occurs](http://gatkforums.broadinstitute.org/wdl/discussion/9542/does-cromwell-detect-task-failures-based-on-check-alive), and will wait forever for a job that is already dead. It would be helpful to configure periodic checks for whether tasks are still alive, and enter failure modes for non-zero return codes when unfinished tasks are no longer alive.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2281:261,failure,failures-based-on-check-alive,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281,4,"['alive', 'failure']","['alive', 'failure', 'failures-based-on-check-alive']"
Availability,"When running our joint genotyping workflow, after when starting workflows, we run into an array out of bounds index exception that references a histogram. This causes the machine to become locked up because all cpu resources are being used and then weirdness ensues. ![image](https://cloud.githubusercontent.com/assets/13023616/14790046/781a034a-0add-11e6-90b4-05df4ff61433.png). ```; 2016-04-22 04:01:20,697 cromwell-system-akka.actor.default-dispatcher-36 INFO - WorkflowActor [UUID(1f5b729c)]: starting calls: JointGenotyping.TabixAndGenotypeGVCF; 2016-04-22 04:01:20,697 cromwell-system-akka.actor.default-dispatcher-36 INFO - WorkflowActor [UUID(1f5b729c)]: persisting status of TabixAndGenotypeGVCF:2596 to Starting.; 2016-04-22 04:01:20,703 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-41]; java.lang.ArrayIndexOutOfBoundsException: value outside of histogram covered range. Caused by: java.lang.IndexOutOfBoundsException: index 4737; at org.HdrHistogram.AbstractHistogram.handleRecordException(AbstractHistogram.java:441) ~[cromwell.jar:0.19]; at org.HdrHistogram.AbstractHistogram.recordSingleValue(AbstractHistogram.java:433) ~[cromwell.jar:0.19]; at org.HdrHistogram.AbstractHistogram.recordValue(AbstractHistogram.java:346) ~[cromwell.jar:0.19]; at kamon.metric.instrument.HdrHistogram.record(Histogram.scala:115) ~[cromwell.jar:0.19]; at akka.kamon.instrumentation.ActorCellInstrumentation.aroundBehaviourInvoke(ActorCellInstrumentation.scala:69) ~[cromwell.jar:0.19]; at akka.actor.ActorCell.invoke(ActorCell.scala:483) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) [cromwell.jar:0.19]; at akka.dispatch.Mailbox.run(Mailbox.scala:220) [cromwell.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19];",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/735:797,ERROR,ERROR,797,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/735,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1690:660,echo,echo,660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690,2,['echo'],['echo']
Availability,"When running the official Broad workflows (for example [this one](https://github.com/gatk-workflows/gatk3-data-processing)), on Cromwell 36 setup on AWS, I get the following error:. ```; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk' or '/mount/point' but got: 'local-disk 100 HDD'; at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:174,error,error,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['error'],['error']
Availability,"When running the workflow in #820 , the output arrays should each have 901 elements. It seems that randomly the array outputs for some of the shards have less than 901 elements even though the output glob buckets have 901 elements in them. This causes tasks further down in the workflow to fail. This is not repeatable in terms of the same shards running into the issue but at this scale it seems to affect at least one shard per workflow. 351dd737-fe83-4e86-bc5b-19d4c7d2fbf7 - gotc dev",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/843:266,down,down,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/843,1,['down'],['down']
Availability,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079:83,Echo,Echo,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079,5,"['Echo', 'echo']","['Echo', 'echo']"
Availability,"When starting up the cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.Services",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:530,ERROR,ERROR,530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"When submitted to a local Cromwell built from latest `develop`, the workflow; ```; version 1.0. workflow short_circuit {; input {; Int c = if true then 1 else 1 / 0; Int d = if false then 1 / 0 else 1; }; if (false && 1 / 0 == 0) {; Int a = 1; }; if (true || 1 / 0 == 0) {; Int b = 1; }. }; ```; fails with error; ```; Failed to evaluate 'if_condition' (reason 1 of 1):; Evaluating (true || ((1 / 0) == 0)) failed: Divide by zero error: 1 / WomInteger(0); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3765:307,error,error,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3765,2,['error'],['error']
Availability,"When submitting the workflow to cromwell use the collection name in the workflow labels under the key `cromwell_collection_name`. . Further, users are **not** allowed to have used this label on their own, attempts to do so are an error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2838:230,error,error,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2838,1,['error'],['error']
Availability,"When testing out a change to a CWL workflow I ran into a RuntimeException:; ```; java.lang.RuntimeException: Unhandled CwlExpressionCommandPart value 'WomCoproductValue(WomCoproductType(NonEmptyList(WomStringType, WomIntegerType)),WomString(42))' of type Coproduct[String, Int]; ```; This simple CWL file will reproduce the error:; ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - int; - ""null""; inputBinding:; position: 1; outputs:; response:; type: stdout; ```; with an appropriate input YAML, e.g.:; ```yaml; value: 42; ```. The first commit in this PR was enough to get past that error; however, there was another issue if one of the possible types was instead a `File`. The file did not appear to be getting localized. So, a second example:. ```cwl; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""/bin/echo""]; arguments: [""I got this value:""]; stdout: response.txt; inputs:; value:; type:; - string; - File; - ""null""; secondaryFiles: [.fai, .ann, .index]; inputBinding:; position: 1; outputs:; response:; type: stdout; ```. The second commit to this PR addresses that issue. (Without this commit Cromwell did not crash when run locally without Docker, but did crash with an `IllegalArgumentException` from calling `subpath` with an invalid range [here](https://github.com/broadinstitute/cromwell/blob/384f0b8f22399342705dc43ab9dac2d6b16bbf3b/backend/src/main/scala/cromwell/backend/io/JobPathsWithDocker.scala#L57) with Docker.). These changes were enough to get my workflow running, but I'm not sure if there are other changes that should be made to properly handle `WomCoproductValue`. (For instance, I have not tried it in GCP yet.)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143:324,error,error,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143,4,"['echo', 'error']","['echo', 'error']"
Availability,"When testing/adapting production WDLs for public use, I often run into the problem that some of the input files are in private buckets. Because there are many input files, it's tedious to check all of them. So I tend to miss one or two that eventually make my workflows fail, often in late stages of the workflow. This can also happen if you get something wrong in a file path (typo, forgot a subdirectory etc). . It would be incredibly helpful if there was an option to have Cromwell check that all the external input files (specified with full paths to eg a gs:// bucket) are available and reachable (meaning we have permission to read them) before kicking off the actual workflow, and fail with an informative error if it's not the case. This would save a fair amount of iteration on large workflows with many external inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163:578,avail,available,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163,2,"['avail', 'error']","['available', 'error']"
Availability,"When the workflow logs are copied over from a bucket to an RP-enabled bucket, Cromwell fails to do the operation today and returns the error:; <img width=""1261"" alt=""Screen Shot 2019-03-25 at 12 57 13 PM"" src=""https://user-images.githubusercontent.com/14941133/54967379-ceac8680-4f4d-11e9-9f2a-3c2b0edab7f6.png"">. AC: Correct and incorporate the workflow log copying behavior to retry such failures with the defined google project.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4770:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4770,2,"['error', 'failure']","['error', 'failures']"
Availability,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1444:95,error,errors,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444,2,['error'],"['error', 'errors']"
Availability,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:92,error,error,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,2,"['echo', 'error']","['echo', 'error']"
Availability,"When trying to run a workflow locally with docker containers, I found that sometime (not always, and depends on the context, and not always in the same part of the pipeline) there is an error about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:186,error,error,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,6,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"When using PAPI v2, the execution events for a single job are broken down into a lot of minute actions, and those actions are represented in the timing diagram. The issue is that the level of granularity provided by Cromwell's execution events is too zoomed in, and its not easy to digest. Since the operation metadata itself will hold most of this granularity, it should be useful for Cromwell to group certain actions into a higher level grouping concept based on its understanding of the job. For example, here's the execution events from a single run: ; ```; ""executionEvents"": [; {; ""startTime"": ""2018-08-14T16:16:40.069663Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:42.510303Z""; },; {; ""startTime"": ""2018-08-14T16:16:43.002063Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:69,down,down,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['down'],['down']
Availability,"When using Pipelines API v2 on Cromwell v38 -- it seems like if localization fails, Cromwell tries to delocalize files and then prints an error message (printed below) that is essentially impossible to debug. AC:; 1. Upon localization failure, Cromwell shouldn't run any other actions.; 2. Upon localization failure, the error message returned should print the *file* that couldn't be localized and the error message it failed with. Along with a link to the log for this job. ```Task depthOfCoverageTest.depthOfCov:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: while running ""/bin/sh -c python -c 'import base64; print(base64.b64decode(\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIHJtIC1mICRIT01FLy5jb25maWcvZ2Nsb3VkL2djZSAmJiBnc3V0aWwgIGNwIGdzOi8vZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgL2Nyb213ZWxsX3Jvb3QvZmMtZjExMDcxOTktNGQwMi00Y2E0LWJlZGYtOTA3MDkyZDUxMzMyL2NnYWV4dC9zdTJjL3VrX2djdC9fRUdBUjAwMDAxMjg4NjM0X0VHQVMwMDAwMTAwMTA4NF9MNjc1Ml9nYXRrMi5iYWkgPiBnc3V0aWxfb3V0cHV0LnR4dCAyPiYxCiMgUmVjb3JkIHRoZSBleGl0IGNvZGUgb2YgdGhlIGdzdXRpbCBjb21tYW5kIHdpdGhvdXQgcHJvamVjdCBmbGFnClJDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIHJtXCAtZlwgXCRIT01FLy5jb25maWcvZ2Nsb3VkL2djZVwgXCZcJlwgZ3N1dGlsXCBcIGNwXCBnczovL2ZjLWYxMTA3MTk5LTRkMDItNGNhNC1iZWRmLTkwNzA5MmQ1MTMzMi9jZ2FleHQvc3UyYy91a19nY3QvX0VHQVIwMDAwMTI4ODYzNF9FR0FTMDAwMDEwMDEwODRfTDY3NTJfZ2F0azIuYmFpXCAvY3JvbXdlbGxfcm9vdC9mYy1mMTEwNzE5OS00ZDAyLTRjYTQtYmVkZi05MDcwOTJkNTEzMzIvY2dhZXh0L3N1MmMvdWtfZ2N0L19FR0FSMDAwMDEyODg2MzRfRUdBUzAwMDAxMDAxMDg0X0w2NzUyX2dhdGsyLmJhaVwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CgogICMgQ2hlY2sgaWYgaXQgbWF0Y2hlcyB0aGUgQnVja2V0SXNSZXF1ZXN0ZXJQYXlzRXJyb3JNZXNzYWdlCiAgaWYgZ3JlcCAtcSAiQnVja2V0IGlzIHJlcXVlc3RlciBwYXlzIGJ1Y2tldC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4809:138,error,error,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4809,6,"['error', 'failure']","['error', 'failure']"
Availability,"When using a docker hash in V25 in order to get call caching I get the following error: . ```; 2017-05-01 18:06:39 [cromwell-system-akka.actor.default-dispatcher-3670] ERROR c.e.w.l.e.EngineJobExecutionActor - Failed copying cache results for job GenotypeGVCFsComparison.IndexVCF:-1:1, invalidating cache entry.; java.io.IOException: Failed to copy gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz to gs://broad-dsde-methods/cromwell-execution-25/GenotypeGVCFsComparison/c5bc4f99-d969-49ca-9e2e-a3dac7a4b12a/call-IndexVCF/dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz; 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:49); 	at cromwell.core.path.PathCopier$$anonfun$copy$2.applyOrElse(PathCopier.scala:48); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.core.path.PathCopier$.copy(PathCopier.scala:48); 	at cromwell.backend.standard.StandardCacheHitCopyingActor.duplicate(StandardCacheHitCopyingActor.scala:36); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:66); 	at cromwell.backend.callcaching.CacheHitDuplicating$$anonfun$copySimpletons$1.apply(CacheHitDuplicating.scala:62); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:81,error,error,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,4,"['ERROR', 'Failure', 'error', 'recover']","['ERROR', 'Failure', 'error', 'recoverWith']"
Availability,"When using the Google Backend (PAPIv2) with versions of Cromwell v36+, there is a concern around running the ""delocalize cwl.output.json"" action for WDL workflows, as it only **required** for CWL workflows). The pain points of this behavior (in order of severity) are:; 1. Disallows a user from using the `noAddress` runtime attribute, because now it forces all WDL tasks to have access to DockerHub in order to succeed.; 2. Introduces scope for transient failures, meaning the WDL job fails if the pull for the jq docker image fails. ; 3. Slows down WDL jobs as it unnecessarily pulls the jq image from dockerhub. AC: Don't run the `delocalize cwl.output.json` event for WDL workflows at all.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4426:456,failure,failures,456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4426,2,"['down', 'failure']","['down', 'failures']"
Availability,"When using the type `string[]` along with the key `prefix`, instead of the prefix being added at the head of string array, the prefix is being applied to each element, along with the head of the list of elements. Example CWL:; ```; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: ['/bin/echo']; stdout: ""hello.txt""; inputs:; bonus:; type: string[]; inputBinding:; prefix: ""--bonus""; outputs:; hello:; type: stdout; ```. Example Inputs:; ```; bonus: [""first"", ""second""]; ```. Actual Response:; ```; [2018-05-02 15:19:13,62] [info] BackgroundConfigAsyncJobExecutionActor [93064d21test.cwl:NA:1]: '/bin/echo' '--bonus' '--bonus' 'first' '--bonus' 'second'; ```. Expected Response: http://www.commonwl.org/user_guide/09-array-inputs/; ```; '/bin/echo' '--bonus' 'first' 'second'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3588:321,echo,echo,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3588,3,['echo'],['echo']
Availability,"When viewing example [conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/PAPIv2.conf) and API [documentation](https://cloud.google.com/life-sciences/docs/concepts/locations), I see that we can use any location for compute itself, but pipeline metadata is stored in one of several locations. How would one change the configuration to run pipelines in e.g. us-east1, while accessing API at us-central1?; I tried some options and I either got improper argument error, or umatched zone/region error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6193:495,error,error,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6193,2,['error'],['error']
Availability,"When we interact with Google APIs, including JES and GCS sometimes things go awry for transient reasons. The belief is that retrying a bit would make it go green. We want to handle this automatically for the user. We shall:. - Retry up to N times via an exponential backoff. N is TBD (10?); - Upon each retry, emit a log message; - If N is reached the job will fail w/ the last error event as the failure reason. TBD: Design discussion on how to implement this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1914:378,error,error,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1914,2,"['error', 'failure']","['error', 'failure']"
Availability,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1414:504,down,down,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414,2,"['down', 'failure']","['down', 'failure']"
Availability,When we're determining call cache hits we have an issue where we need to pull hashes and compare them to potential hits in our database. Many of these lookups are likely to be failures. It seems like a bloom filter could be a good optimization here (provided the memory tradeoff is worth the time benefit) by filtering out obvious negative results prior to hitting the database.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248:176,failure,failures,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248,1,['failure'],['failures']
Availability,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814:288,Echo,Echo,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814,6,"['Echo', 'echo']","['Echo', 'echo', 'echoOut']"
Availability,"Whenever I run workflow with mistake a see some weird errors in cromwell-server akka-logs while nothing is written to cromwell-workflow-logs; Here is a simple example:; ```; workflow worms {. File samplesFile. #Name \t File; Array[Array[File]] samples = read_tsv(samplesFile). scatter (sample in samples) {; call stats {; input:; fileName = sample(0), #simple mistake with ""()"" instead of ""[]""; file = sample(1); }; }; }. task stats {. String fileName; File file. command {; /opt/sratoolkit/sra-stat ${file} > stats.txt; }. runtime {; docker: ""itsjeffreyy/sratoolkit""; }. output {; File stats = ""stats.txt""; }; }; ```; Here I make quite typical mistake by using () instead of []. But when I send it to the server for execution it I get an empty cromwell-workflow-logs folder and errors are displayed only in stdout of the cromwell server. What I expect is to see them in cromwell-workflow-logs (if my expectations are wrong, it would be nice to have in documentation a description of which log folder are for which type of errors); By the way, according to the error I get, you use runtime reflection to search for ""sample"" function (that is perceived as a function instead of array due to round brackets). Not the safest, way to identify functions, IMHO.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2210:54,error,errors,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2210,4,['error'],"['error', 'errors']"
Availability,"Whenever SAM returns a successful response the content is JSON. When there's an error, say a `HTTP 500` the SAM response is HTML. However in all cases CromIAM tries to deserialize the response into JSON. The effect is that the original error message is lost producing a stack trace similar to the one below. A/C: SAM errors are reported and/or logged with original/redacted information, and NOT logged with the error below. . ```java; [INFO] [05/14/2018 18:08:50.471] [default-akka.actor.default-dispatcher-5853] [CromIamServer$(akka://default)] Requesting authorization for view access for user [REDACTED] on a request to view for collection [REDACTED]; [ERROR] [05/14/2018 18:08:50.507] [CromIamServer-akka.actor.default-dispatcher-443] [CromIamServer$(akka://default)] Request failed java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; java.lang.RuntimeException: Unable to look up collections for user [REDACTED]: Unsupported Content-Type, supported: application/json; 	at cromiam.webservice.QuerySupport.$anonfun$preprocessQuery$3(QuerySupport.scala:67); 	at akka.http.scaladsl.server.Directive$SingleValueModifiers.$anonfun$flatMap$1(Directive.scala:141); 	at akka.http.scaladsl.server.Directive.$anonfun$tflatMap$2(Directive.scala:69); 	at akka.http.scaladsl.server.directives.FutureDirectives.$anonfun$onComplete$3(FutureDirectives.scala:37); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$2(FastFuture.scala:37); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.$anonfun$transformWith$3(FastFuture.scala:52); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3622:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622,5,"['ERROR', 'error']","['ERROR', 'error', 'errors']"
Availability,"While benchmarking some performance enhancements I've been playing with I kept noticing that no matter how fast I could get things eventually performance would drop back down to baseline levels from develop. I traced it down to the `WriteMetadataActor`, specifically it appears that writing boatloads of individual events (not atypical under load) can cause a lot of problems (not particularly surprising, but ...). It seems like some sort of batching/work pulling scheme could work wonders here although presumably it'd then come at the cost of memory (to buffer the unwritten values). That's just one thought, not a prescription",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810:170,down,down,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810,2,['down'],['down']
Availability,"While debugging an issue with @helgridly we noticed that when Cromwell comes up & it tries to start up the DB (currently via the EngineMetadataServiceActor) it will eventually call SchemaManager.updateSchema. . If this call fails for some reason Cromwell continues to run even though any call to the metadata service (He noticed it via metadata requests) will fail. Recommendation: In any subsystem which a) we require to be running and b) it requires a DB, we should fail to initialize Cromwell if there's a failure in that space",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1128:509,failure,failure,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128,1,['failure'],['failure']
Availability,"While investigating the Dockerhub outage [0][1] and the Quay outage [2] we found that Cromwell can get into a bad state that interferes with Docker hash lookups. Rebooting Cromwell fixes the problem. On Friday 6/12 I was demoing the repro scenario I've been working on and I mentioned that I can't easily go from a ""broken Quay"" to a ""working Quay"" scenario because I have to restart Cromwell to pick up the DNS change... _restart Cromwell to pick up the DNS change_. _restart Cromwell to pick up the DNS change?_. _restart Cromwell to pick up the DNS change!!!_. Yesterday 6/15 I discovered that the JVM has a DNS cache with an infinite TTL [3]. Lowering this to a reasonable value allowed my test scenario to work without restarting the JVM, which it didn't before. Here is my theory:; 1. A Docker repo starts taking on water and engineers scramble to fix it; 2. They make infrastructure changes that change DNS records.; 3. Cromwell's retained DNS record breaks the ability to communicate with the repo indefinitely, even after the outage is resolved.; 4. Restarting Cromwell clears the cache. [0] https://broadworkbench.atlassian.net/browse/PROD-272; [1] https://broadworkbench.atlassian.net/browse/BA-6070; [2] https://broadworkbench.atlassian.net/browse/BA-6454; [3] https://www.ibm.com/support/pages/gis-dns-cache-never-updates-ip-after-dns-name-ip-changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5546:34,outage,outage,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5546,4,"['Reboot', 'outage']","['Rebooting', 'outage']"
Availability,"While running a WDL on Google cloud with a very large task array I received the weirdest error:; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task xxx.xxx:2292:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [\""docker\"" \""pull\"" \""us.gcr.io/mccarroll-mocha/bcftools@sha256:ef8a606bfa1c5cef455dc2b97812f0a7f16d89e2962eee7654457a371462194b\""]: exit status 1 (standard error: \""error pulling image configuration: download failed after attempts=1: error parsing HTTP 403 response body: invalid character '<' looking for beginning of value: \\\""<?xml version='1.0' encoding='UTF-8'?><Error><Code>AccessDenied</Code><Message>Access denied.</Message><Details>We're sorry, but this service is not available in your location</Details></Error>\\\""\\n\"")""; }; ],; ```; And the shard-2292 directory only contained the following files (no `stderr` file):; ```; gcs_delocalization.sh; gcs_localization.sh; gcs_transfer.sh; script; ```; The task array is very large (0-2437) but only three different tasks had this error (1124, 1510, 2292). It seems as if Cromwell tried to pull the docker (`us.gcr.io/mccarroll-mocha/bcftools:1.16-20221221` in this case) from GCR but received a 403 response for some reasons and did not know how to interpret it. When rerunning the pipeline with CallCaching, it continued without issues. My suggestion is that this is a rare failure mode that might only occur for large task arrays and Cromwell needs to interpret the 403 error and retry to pull the docker after a few seconds rather than failing the whole pipeline.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132,13,"['Error', 'avail', 'down', 'error', 'failure']","['Error', 'available', 'download', 'error', 'failure', 'failures']"
Availability,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117:54,failure,failure,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117,2,"['error', 'failure']","['error', 'failure']"
Availability,"While testing cromwell-36 with AWS batch I was able to reproduce this error:. ```; 2019-02-25 09:38:52,508 cromwell-system-akka.dispatchers.engine-dispatcher-24 ERROR - WorkflowManagerActor Workflow b6b9322c-3929-4b72-9598-45d97dfb858d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"While trying to figure out where to put quota retry, I made some changes to `StandardAsyncExecutionActor` to help wrap my head around it. We probably ultimately want quota retried in `PipelinesApiAsyncBackendJobExecutionActor`, but I think the refactor is worth checking in so the next person has an easier time. I also included a preparatory refactor that makes `isQuotaMessage` available at the appropriate scope for use in the next PR, so it'll be smaller.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7432:380,avail,available,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7432,1,['avail'],['available']
Availability,"While trying to pull a docker with Cromwell 58, I get the following error:. `""message"": ""Task xxx.xxx:0:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: pulling image: docker pull: running [\""docker\"" \""pull\"" \""us.gcr.io/xxx/xxx@sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""]: exit status 1 (standard error: \""error pulling image configuration: error parsing HTTP 400 response body: invalid character '<' looking for beginning of value: \\\""<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectMissing</Code><Message>Bucket is a requester pays bucket but no user project provided.</Message><Details>Bucket is Requester Pays bucket but no billing project id provided for non-owner.</Details></Error>\\\""\\n\"")"",`. I understand that the issue is that the Google bucket where the docker is located is requester pays and Cromwell does not know what to do in this case, but it is not immediately clear what I should do to fix it. It would be a great improvement if Cromwell could interpret this response and provide a more informative error message so that the user could immediately know what needs to be addressed. In particular, I am not fully sure what I should be doing. These are excerpts from my configuration file:; ```; ...; engine {; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; }; }; }; ...; services {; MetadataService {; ...; config {; carbonite-metadata-service {; filesystems {; gcs {; auth = ""service-account""; }; }; ...; }; }; }; }; ...; backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; project = ""xxx""; ...; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; ...; }; }; ...; }; }; }; }; ...; ```; Where should the configuration for telling Cromwell which project to use when pulling dockers be?. I also do not understand why this issue arises a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235:68,error,error,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235,7,"['Error', 'error']","['Error', 'error']"
Availability,"While trying to write call caching tests. Note the `rCopy` is just a recycled `float` input. ```; {; ""radius"": 1.0; }; ```. ```; cwlVersion: v1.0; class: Workflow; # Workflow-level DockerRequirement; requirements:; DockerRequirement:; dockerPull: ""ubuntu:latest""; inputs:; - id: radius; type: float; outputs:; - id: area; outputSource: ""#one/rSquared""; type: float; steps:; - id: one; in:; - id: radius; source: ""#radius""; out:; - id: rSquared; - id: rCopy; run:; inputs:; - id: radius; type: float; outputs:; - id: rSquared; outputBinding:; glob: stdout.txt; loadContents: true; outputEval: $(parseFloat(self[0].contents)); type: float; - id: rCopy; outputBinding:; outputEval: $(inputs.radius); type: float; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; arguments:; - valueFrom: echo; - valueFrom: ""`expr ""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""*""; shellQuote: false; - valueFrom: $(inputs.radius); - valueFrom: ""`""; shellQuote: false; stdout: stdout.txt; ```. ```; name: cwl_caching; testFormat: workflowsuccess; backendsMode: ""only""; backends: [Local]; tags: [localdockertest]. files {; wdl: cwl_caching/cwl_caching.cwl; inputs: cwl_caching/cwl_caching.json; }. metadata {; status: Succeeded; }. workflowType: CWL; workflowTypeVersion: v1.0; ```. ```; cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'rCopy': No coercion defined from '1' of type 'Int' to 'Float'.; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:688); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3480:808,echo,echo,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3480,1,['echo'],['echo']
Availability,"While using Jes + call caching enabled, if you call on a task with a new name, then when Jes will copy the outputs from the previously performed task but will not rename the files to the new name. For example, my WDL has a task two, and in the workflow I ""call two as four"". ; - When I check within the google bucket for call four, all the files are named as ""two-stdout.log"", ""call two-stderr.log"". ; - When I check within the google bucket for call two, there are a group of files named call two and a group of files named call four. It appears that although the appropriate files for call four are created, they aren't moved into the call four directory in google. So when I run the WF, I get this error: ; ""failure"": ""Item not found: cloud-cromwell-dev/cromwell-executions/cacheWithinWF/c972cace-b423-4c1c-87e4-01356163757f/call-four/four-stdout.log"". Because although the file four-stdout.log actually exists, it exists under the directory call-two instead of call-four.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/781:701,error,error,701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/781,2,"['error', 'failure']","['error', 'failure']"
Availability,"Who:. Reported by rhoades. When:. During Wed Oct 17 office hours. Where:. Bucket: https://console.cloud.google.com/storage/browser/fc-60afd89c-b660-4b1a-b963-d5053469bea8/4006fb72-486c-4b2c-b9b4-cc585c95909b/GenerateDuplexConsensusBams/a4f4286e-72eb-4567-9e2f-dd5b29f4ebde/call-DuplexConsensusMergeBamAlignments. (workspace was shared with FC's support group). What:. The `/script` file in the above `a4f4` gcs dir references a *different* wf path `0e6a`. When this old/bad `/script` file runs, it of course errors out with file not found, as the files were localized from `a4f4` not `0e6a`. Why?. Guessing that this job (or an upstream job?) started to call cache, copied the old `/script` but then realized that the `.bam` was missing. For some reason a new `/script` is not present, and the old one is run. Notes:. This was repeatable. When rhoades re-ran the sample (w/ call caching) the workflow mostly call cached, but then failed on this call, again trying to run a bad `/script`. He's currently attempting to re-run without call caching to see if the sample's workflow completes successfully. A/C:. Even if the upstream bam-input *was* deleted, the call cache failure should fall back to running the job successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4269:508,error,errors,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4269,2,"['error', 'failure']","['errors', 'failure']"
Availability,"Will need https://github.com/broadinstitute/wdl4s/pull/52 merged. Enables WDL like . ```; task t {; String i; command {; echo ""lol""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = ""hello""}; String a = t.o # This currently would not work; call t as u {input: i = a }; String b = u.o; ; output {; String wo = b; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1699:121,echo,echo,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1699,1,['echo'],['echo']
Availability,"With Cromwell 30.2:. Attempting to abort a workflow mostly running on SGE triggers lots of these errors:. 2018-02-09 11:52:46,500 cromwell-system-akka.dispatchers.engine-dispatcher-96 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Queued and running SGE jobs continue as if nothing happened.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259:97,error,errors,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259,1,['error'],['errors']
Availability,"With Cromwell 41, when submitting an imports.zip containing workflow imports, if the zipfile has a subdirectory, I get these errors:. 2019-05-14 11:05:38,560 cromwell-system-akka.dispatchers.engine-dispatcher-101 ERROR - WorkflowManagerActor Workflow a3fb73f1-2976-456\; 3-a691-a356e2744dda failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.Materiali\; zeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /tmp/imports_workflow_a3fb73f1-2976-4563-a691-a356e2744dda_9012613107684347004.zip7783627211874195701/tasks/tasks_assembly.wdl; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$ma\; terialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWor\; kflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processE\; vent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescr\; iptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969:125,error,errors,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"With a clean database, run this with the JES backend (no inputs):. ```; task x {; String s; command {echo ${s}}; output {String t = read_string(stdout())}; runtime {docker: ""ubuntu:latest""}; }. workflow w {; call x {input: s=""foo""}; call x as y {input: s=x.t}; }; ```. call x runs fine. call y _should_ get a cache hit to call x, but instead:. ```; [2016-03-22 15:20:59,921] [info] WorkflowActor [54108c5b]: starting calls: w.y; [2016-03-22 15:20:59,921] [info] WorkflowActor [54108c5b]: persisting status of y to Starting.; [2016-03-22 15:21:00,344] [info] WorkflowActor [54108c5b]: Call Caching: Cache hit. Using 54108c5b:x as results for 54108c5b:y; [2016-03-22 15:21:00,351] [info] WorkflowActor [54108c5b]: inputs for call 'y':; s -> WdlString(foo); [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: created call actor for y.; [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: persisting status of y to Running.; [2016-03-22 15:21:05,567] [error] CallActor [54108c5b:y]: Failing call: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; cromwell.util.AggregatedException: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112); at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124); at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:640); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:424); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:418); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/603:101,echo,echo,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/603,1,['echo'],['echo']
Availability,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1291:415,failure,failure,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291,1,['failure'],['failure']
Availability,"With cromwell command mode `java -jar cromwell-53.jar run WF.wdl -i input.json`, UTF-8 characters are supported well. With the server mode however, UTF-8 char shown as ???? . ```; task check_output{; String p; String c; command {; echo ${p} ; echo ${c} > out.txt; }; output {; File C = ""out.txt""; }; }; ```. in an example like this, the script file in cromwell fodler is :. ```; (; cd /disk/gvc/wdlrunner-example/JOB/job_control/cromwell-executions/WF/0c1d3e8f-8ba7-4ad8-80de-b6919cd4fd07/call-check_output/execution. echo ??????; echo ?? > out.txt; exit 1; ) > ""$out0c1d3e8f"" 2> ""$err0c1d3e8f""; ```. the content of out.txt is ; ```; ??; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5867:231,echo,echo,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5867,4,['echo'],['echo']
Availability,"With your current version of Cromwell, the workflow does not terminate even if the underlying task is killed by the HPC scheduler due to out of memory error. This should be generalized to batch schedulers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107:151,error,error,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107,1,['error'],['error']
Availability,Wom: Reinstate stable error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2904:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2904,1,['error'],['error']
Availability,Wom: Reinstate workflow failure modes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2865:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2865,1,['failure'],['failure']
Availability,Wom: Renicen error message for missing output,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871,1,['error'],['error']
Availability,"Womtool does not give error if the file does not exist, so; ```; java -jar ~/Soft/womtool-37.jar validate I_do_not_exist.wdl; ```; Returns me ; ```; /pipelines/sources/rna-seq/pipelines/quantification/I_do_not_exist.wdl; ```; so I think that everything is ok as no erorrs is given while in fact I have a typo in the filename somewhere",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4665:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665,1,['error'],['error']
Availability,Womtool endpoint: separate field for input errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824:43,error,errors,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824,1,['error'],['errors']
Availability,Womtool returns vauge error when using output keyword as variable name,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4031:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031,1,['error'],['error']
Availability,Won't go green until https://github.com/broadinstitute/wdl4s/pull/67. Includes better error messages and halting failure for RuntimeExceptions during expression evaluation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1863:86,error,error,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1863,2,"['error', 'failure']","['error', 'failure']"
Availability,"Workaround for ""The job exceeded the maximum log length, and has been terminated"" Travis error [BA-6480]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5553:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5553,1,['error'],['error']
Availability,"WorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:17193,error,errors,17193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,Workflow Actor - Call Recover appropriately,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664:22,Recover,Recover,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664,1,['Recover'],['Recover']
Availability,Workflow copy final outputs none.get failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:37,failure,failure,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['failure'],['failure']
Availability,Workflow fails with slick error with large scale call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182,1,['error'],['error']
Availability,Workflow fails without error possibly due to declaring variables in a scatter,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826,1,['error'],['error']
Availability,Workflow failure mode tests. Closes #1034,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1216:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1216,1,['failure'],['failure']
Availability,Workflow failure mode. Closes #1005,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1203:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1203,1,['failure'],['failure']
Availability,"Workflow failure: ""Workflow is making no progress but has the following unstarted job keys""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6238:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6238,1,['failure'],['failure']
Availability,Workflow option to retry any task failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,1,['failure'],['failure']
Availability,Workflow pickup on heartbeat TTL expiration should be highly visible,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4451:19,heartbeat,heartbeat,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4451,1,['heartbeat'],['heartbeat']
Availability,WorkflowActor should abort workflow's execution before escalating failure to WorkflowManagerActor when one of it's child actors crashed [BA-6071],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5253:66,failure,failure,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5253,1,['failure'],['failure']
Availability,WorkflowDockerLookupActor robustification. [BA-6056],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5230:26,robust,robustification,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230,1,['robust'],['robustification']
Availability,WorkflowExecutionActor bogs down with needless Scope equals,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1457:28,down,down,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457,1,['down'],['down']
Availability,WorkflowStore failures: Make sure WorkflowStore entry is removed before JobStore entry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247:14,failure,failures,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247,1,['failure'],['failures']
Availability,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:9529,down,down,9529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,6,['down'],['down']
Availability,"Would it be possible to make cromwell available as a maven dependency?. There are a lot of very useful utilities with regards to `wdl` parsing etc. It would be amazing to build homegrown tools around this ecosystem, and also to contribute back missing parts / useful additions to workflow management in general. I think in particular taking into consideration the complexity of parsing the wdl language, this would be extremely helpful!. Thanks anyways for this amazing tool and wish you all a great weekend!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6543:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6543,1,['avail'],['available']
Availability,Wrap ask errors from WSCWA to avoid ClassCastException.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3826:9,error,errors,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3826,2,['error'],['errors']
Availability,"Write heartbeats in a batch, but not in a transaction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249:6,heartbeat,heartbeats,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249,1,['heartbeat'],['heartbeats']
Availability,Write heartbeats in autocommit mode.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4444:6,heartbeat,heartbeats,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4444,1,['heartbeat'],['heartbeats']
Availability,Write workflow heartbeats.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3493:15,heartbeat,heartbeats,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493,1,['heartbeat'],['heartbeats']
Availability,"Y_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qsub \; -terse \; -b n \; -N cromwell_45d03417_say_hello \; -wd cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; -o cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stdout \; -e cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stderr \; \; -l m_mem_free=$(expr 4096 / 1)m \; -l h_rt=3600 \; -l s_rt=3600 \; \; \; \; -V; 2020-10-08 16:09:02,038 cromwell-system-akka.dispat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:8153,echo,echo,8153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,2,['echo'],['echo']
Availability,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3683:302,recover,recover,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683,1,['recover'],['recover']
Availability,"You can use select_first() to coerce an optional variable into a not-optional variable. However, this copy error I made is definitely not valid:. ```; # variant_caller.errorcode has type Array[String?]; # pass has type String; if(defined(variant_caller.errorcode)) {; if(!(select_first([variant_caller.errorcode[0] == pass, ""silly bogus fallback""]) == pass)) {; # some code; } }; ```. The syntax error is much more egregious than #7194, but it also isn't getting flagged by womtool, instead leading to an error at runtime. > ""Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((select_first([(variant_caller.errorcode[0] == pass), ""silly bogus fallback""]) == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value."". You can make an argument that in a just world, #7194 is valid syntax due to it being in a codeblock that only runs if variant_caller.errorcode exists, and should cause neither a womtool nor runtime error. But an equality that isn't part of a quoted string probably never belongs in a select_first() array.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7195:107,error,error,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7195,9,['error'],"['error', 'errorcode']"
Availability,"[2022-12-15 21:14:50,21] [info] checkpointClose end; [2022-12-15 21:14:50,21] [info] Checkpoint end - txts: 101866; [2022-12-15 21:14:50,52] [info] Checkpoint start; [2022-12-15 21:14:50,52] [info] checkpointClose start; [2022-12-15 21:14:50,52] [info] checkpointClose synched; [2022-12-15 21:14:50,57] [info] checkpointClose script done; [2022-12-15 21:14:50,57] [info] dataFileCache commit start; [2022-12-15 21:14:50,57] [info] dataFileCache commit end; [2022-12-15 21:14:50,60] [info] checkpointClose end; [2022-12-15 21:14:50,60] [info] Checkpoint end - txts: 101868; [2022-12-15 21:14:50,61] [info] Checkpoint start; [2022-12-15 21:14:50,61] [info] checkpointClose start; [2022-12-15 21:14:50,61] [info] checkpointClose synched; [2022-12-15 21:14:50,69] [info] checkpointClose script done; [2022-12-15 21:14:50,69] [info] dataFileCache commit start; [2022-12-15 21:14:50,70] [info] dataFileCache commit end; [2022-12-15 21:14:50,73] [info] checkpointClose end; [2022-12-15 21:14:50,74] [info] Checkpoint end - txts: 101875; [2022-12-15 21:14:50,74] [info] Checkpoint start; [2022-12-15 21:14:50,74] [info] checkpointClose start; [2022-12-15 21:14:50,74] [info] checkpointClose synched; [2022-12-15 21:14:50,78] [info] checkpointClose script done; [2022-12-15 21:14:50,78] [info] dataFileCache commit start; [2022-12-15 21:14:50,78] [info] dataFileCache commit end; [2022-12-15 21:14:50,80] [info] checkpointClose end; [2022-12-15 21:14:50,81] [info] Checkpoint end - txts: 101877; [2022-12-15 21:14:50,81] [info] Checkpoint start; [2022-12-15 21:14:50,81] [info] checkpointClose start; [2022-12-15 21:14:50,81] [info] checkpointClose synched; [2022-12-15 21:14:50,85] [info] checkpointClose script done; [2022-12-15 21:14:50,85] [info] dataFileCache commit start; [2022-12-15 21:14:50,85] [info] dataFileCache commit end; [2022-12-15 21:14:50,87] [info] checkpointClose end; [2022-12-15 21:14:50,88] [info] Checkpoint end - txts: 101879; [2022-12-15 21:14:50,89] [info] Running with database db.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:7045,Checkpoint,Checkpoint,7045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,13,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpointClose']"
Availability,[41] PAPI v2 error 10 patch BA BA-5838,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5078:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5078,1,['error'],['error']
Availability,[42] PAPI v2 error 10 patch BA-5838,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5079:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5079,1,['error'],['error']
Availability,[43] PAPI v2 Error 10 patch BA-5838,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5080:13,Error,Error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5080,1,['Error'],['Error']
Availability,[45 hotfix edition] Error code 10 take 2 [BA-5952],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5138:20,Error,Error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5138,1,['Error'],['Error']
Availability,"[47 hotfix] Error 10, Take 3 [No JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5227:12,Error,Error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5227,1,['Error'],['Error']
Availability,[47 hotfix] WorkflowDockerLookupActor robustification. [BA-6056],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5231:38,robust,robustification,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5231,1,['robust'],['robustification']
Availability,[52-hotfix] Be resilient if jobs fail half-way through a very large scatter [BA-6537],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5695:15,resilien,resilient,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5695,1,['resilien'],['resilient']
Availability,[53 hotfix] Add error handling if Google returns null for file size or hash [BW-415],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6006:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6006,1,['error'],['error']
Availability,[53 hotfix] GcsBatchFlow should be resilient to exceptions with null error message happening on attempt to execute GCS batch request [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5998:35,resilien,resilient,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5998,2,"['error', 'resilien']","['error', 'resilient']"
Availability,[ERROR] Could not localize File? (optional),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6582:1,ERROR,ERROR,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6582,1,['ERROR'],['ERROR']
Availability,"[File] arr2 = select_first([t1.out, arr1]); }. task t1 {; Int i; command {; echo ${i} > out.txt; }; output {; File out = 'out.txt'; }; }; ```. This workflow worked fine with cromwell-34.; ```; $ java -jar ~/cromwell-34.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:20:58,35] [info] Running with database db.url = jdbc:hsqldb:mem:a975ddc6-f298-4393-b1f0-e93250d3cca8;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:07,82] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:21:07,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:21:07,95] [info] Running with database db.url = jdbc:hsqldb:mem:d98689d1-c87b-486c-aa55-626823fb3bb1;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:08,32] [info] Slf4jLogger started; [2018-10-25 21:21:08,56] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-fcf9c1d"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:21:08,59] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:21:08,63] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:08,64] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:21:08,64] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:09,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:21:09,81] [info] SingleWorkflowRunnerActor: Version 34; [2018-10-25 21:21:09,82] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:21:09,86] [info] Unspecified type (Unspecified version) workflow 0bb77c74-4c5c-4314-8463-072e7055ee7c submitted; [2018-10-25 21:21:09,90] [info] SingleWorkflowRunnerActor: Workflow submitted 0bb77c74-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:1279,heartbeat,heartbeat,1279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,[HtCondor] Add recovery functionality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1249:15,recover,recovery,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1249,1,['recover'],['recovery']
Availability,"[Jira issue](https://broadworkbench.atlassian.net/browse/BA-5943). Using Cromwell 44 and PAPI v2, occasionally machines in GCE are preempted but not handled as such. Metadata snippet:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task test_combine.combine:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation""; }; ],; ""jobId"": ""projects/finngen-refinery-dev/operations/18318369325465658337"",; ""backend"": ""PAPIv2"",; ""end"": ""2019-08-20T14:54:37.214Z"",; ```. Stackdriver log snippet:. ```; {; ""insertId"": ""15cmu2qg1chkm09"",; ""jsonPayload"": {; ""event_timestamp_us"": ""1566312261571808"",; ""actor"": {; ""user"": ""system""; },; ""resource"": {; ""name"": ""google-pipelines-worker-6eba778d59d69dcfe9189620b91117c5"",; ""type"": ""instance"",; ""zone"": ""europe-west1-b"",; ""id"": ""1966470788939888666""; },; ""trace_id"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""event_type"": ""GCE_OPERATION_DONE"",; ""operation"": {; ""id"": ""1400679280860576170"",; ""name"": ""systemevent-1566312254625-5908d7d8b55f5-68011c08-d6e13e66"",; ""type"": ""operation"",; ""zone"": ""europe-west1-b""; },; ""event_subtype"": ""compute.instances.preempted"",; ""info"": [; {; ""code"": ""STATUS_MESSAGE"",; ""detail_message"": ""Instance was preempted.""; }; ],; ""version"": ""1.2""; },; ```. Although this happens rarely, it causes large workflows to fail and we'd like to avoid rerunning such workflows because at scale other issues may arise with e.g. call caching timeouts, and things are more manageable without otherwise unnecessary reruns. Any help would be much appreciated!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5136:191,failure,failures,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136,2,"['error', 'failure']","['error', 'failures']"
Availability,[Test Reliability] Better message around StatsDInstrumentationSpec failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4387:6,Reliab,Reliability,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4387,2,"['Reliab', 'failure']","['Reliability', 'failures']"
Availability,[Test Reliability] More time for SubmissionSupportSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4385:6,Reliab,Reliability,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4385,1,['Reliab'],['Reliability']
Availability,"[The ticket](https://broadworkbench.atlassian.net/browse/DDO-2190) has a ton more info and I talked with @aednichols about this--the short version is:; - Whatever is doing the HTTP request to Cromwell (Akka?) does not set the Host header ([per MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host)) if it has been customized beforehand; - We've accidentally been doing that by copying the _request's_ headers that came into CromIAM, which obviously include the Host header correlating to CromIAM itself; - Thus CromIAM sends requests to Cromwell with an incorrect Host header; - This hasn't mattered before because Cromwell's Layer 4 load balancer doesn't care and Cromwell's Apache proxy didn't actually need to do host-based routing because it just forwards everything to one app, Cromwell; - This suddenly matters a lot now because BEEs use an Nginx controller for ingress instead of a GCP Layer 4 load balancer, and _it_ needs to use host-based routing; - TL;DR: CromIAM is proxying to Cromwell wrong-ish and it very much does not work in BEEs. Solution: strip out the Host header just like CromIAM already does for Timeout-Access, and everything is happy. The impact to live environments should be zero because they clearly didn't care about the header before. If this fails anywhere, Argo sees the failure immediately like it did for BEEs, and it sees it in a way that any deployment or promotion would be halted because CromIAM would fail to come online from Argo's perspective.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6803:1319,failure,failure,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6803,1,['failure'],['failure']
Availability,"[WX-1107](https://broadworkbench.atlassian.net/browse/WX-1107?atlOrigin=eyJpIjoiNTM5ZGY2MzMyYWM5NGE2MThjMzg3NDAzOTY0YjZmMzYiLCJwIjoiaiJ9). One of our Integration Tests was failing due to a python dependency issue described [here](https://github.com/docker/docker-py/issues/3113). Long story short, using an older version of `requests` is a workaround to an issue that exists in the python `docker` package. Until that package gets updated, our script will fail due to the docker library passing an invalid argument to the requests library. . This fix forces the github runner that is running the HoricromtalDeadlock test to downgrade its `requests` package, resolving the issue. This change will not affect anything other than that one test. . [WX-1107]: https://broadworkbench.atlassian.net/browse/WX-1107?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7131:624,down,downgrade,624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131,1,['down'],['downgrade']
Availability,[WX-1361] Remove Confusing Error Message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7449:27,Error,Error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7449,1,['Error'],['Error']
Availability,[WX-495] DRS Parallel Downloads,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7214:22,Down,Downloads,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7214,1,['Down'],['Downloads']
Availability,[WX-499] DRS Parallel Downloads Follow-up,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7229:22,Down,Downloads,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7229,1,['Down'],['Downloads']
Availability,"[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2905,error,error,2905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,271 cromwell-system-akka.actor.default-dispatcher-4 ERROR - BackendCallExecutionActor [UUID(8c7774be):BillyBob]: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; cromwell.util.AggregatedException: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:13535,Failure,Failures,13535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['Failure'],['Failures']
Availability,[develop edition] GcsBatchFlow should be resilient to exceptions with null error message happening on attempt to execute GCS batch request [BW-411],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5994:41,resilien,resilient,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5994,2,"['error', 'resilien']","['error', 'resilient']"
Availability,[develop] Add error handling if Google returns null for file size or hash [BW-415],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6007:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6007,1,['error'],['error']
Availability,"[error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:1762,error,error,1762,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability,[hotfix v41 edition] error 10 take 2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5139:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5139,1,['error'],['error']
Availability,"[unpredictable_time_example.json.txt](https://github.com/broadinstitute/cromwell/files/1380844/unpredictable_time_example.json.txt). In the attached metadata json file from a recent run (extension mangled with txt because github complains about json uploads) I'm seeing unpredictable datetime strings which, as a result, are difficult to parse. . 1. `2017-10-04T21:47:18.661720750Z` (the norm); 2. `2017-10-04T14:49:56.008-07:00` (`-07:00` suffix is also very common); 3. `2017-10-04T21:54Z` (`calls: count.align_reads_split: executionEvents: ""waiting for quota""). It doesn't seem to be restricted to start or end times. This metadata file was obtained from the swagger REST metadata endpoint from a local version of cromwell 29. I verified it in browser and also downloaded it directly from the endpoint with requests. request: `http://localhost:6361/api/workflows/v1/74e00c0a-ffb8-4e1a-94db-b0169ca7ed42/metadata`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2743:764,down,downloaded,764,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2743,1,['down'],['downloaded']
Availability,"[variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defined() check still is not enough like is the case for setting not_optional_error_code, then that should be caught before runtime. ## backends effected; The womtool valida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2355,error,errorcode,2355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,"\""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_penalty_weight\""\n },\n \""default\"": 1,\n \""id\"": \""#purple-2.44.cwl/somatic_penalty_weight\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-somatic_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/somatic_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-structural_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/structural_vcf\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-sv_recovery_vcf\""\n },\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#purple-2.44.cwl/sv_recovery_vcf\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-threads\""\n },\n \""default\"": 2,\n \""id\"": \""#purple-2.44.cwl/threads\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""Name of the tumor sample. This should correspond to the value used in AMBER and COBALT.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor\""\n },\n \""id\"": \""#purple-2.44.cwl/tumor\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Tumor only mode. Disables somatic fitting.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""-tumor_only\""\n },\n \""default\"": false,\n \""id\"": \""#purple-2.44.cwl/tumor_only\""\n }\n ],\n \""outputs\"": [\n {\n \""type\"": \""Directory\"",\n \""outputBinding\"": {\n \""glob\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:101937,recover,recovered,101937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,"\/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:45.288207Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.822335Z"",; ""description"": ""Stopped running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\""); fi '\"""",; ""endTime"": ""2018-08-14T16:16:58.180303Z""; },; {; ""startTime"": ""2018-08-14T16:16:56.948463Z"",; ""description"": ""Started running \""-c mkdir -p \/cromwell_root\/0c83f20c && cat \/cromwell_root\/cwl.output.json 2>\/dev\/null | jq -r '.. | .path? \/\/ .location? \/\/ empty | gsub(\\\""file:\/\/\\\""; \\\""\\\"")' > \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt\"""",; ""endTime"": ""2018-08-14T16:16:57.281247Z""; },; {; ""startTime"": ""2018-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:2759,echo,echo,2759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJob",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:2199,error,error,2199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability,"] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_task/inputs/download.rockylinux.org/pub/rocky/9/isos/aarch64/Rocky-9.3-aarch64-minimal.iso; 2023-12-20 18:42:08.431 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:42:08.432 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:42:08.433 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:42:08.436 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:05.5983839 Bandwidth: 264.71 MiB/s; 2023-12-20 18",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:2029,down,download,2029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['down'],['download']
Availability,"] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.; [2018-08-30 17:53:41,29] [info] Shutdown finished.; ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the class names in the above output are misleading. For example, in the line:; ```; CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; ```; a user may want to investigate the terms ""batch size"" and ""process rate"" through the documentation or forums, which will likely have meaning. CallCacheWriteActor, on the other hand, is likely an implementation detail that probably adds confusion rather than clarity. hello_world_0.wdl:; ```wdl; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:7508,echo,echo,7508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['echo'],['echo']
Availability,"] dataFileCache commit start; [2022-12-15 21:14:51,85] [info] dataFileCache commit end; [2022-12-15 21:14:51,88] [info] checkpointClose end; [2022-12-15 21:14:51,88] [info] Checkpoint end - txts: 102079; [2022-12-15 21:14:51,89] [info] Checkpoint start; [2022-12-15 21:14:51,89] [info] checkpointClose start; [2022-12-15 21:14:51,89] [info] checkpointClose synched; [2022-12-15 21:14:51,95] [info] checkpointClose script done; [2022-12-15 21:14:51,95] [info] dataFileCache commit start; [2022-12-15 21:14:51,96] [info] dataFileCache commit end; [2022-12-15 21:14:51,99] [info] checkpointClose end; [2022-12-15 21:14:51,99] [info] Checkpoint end - txts: 102086; [2022-12-15 21:14:51,99] [info] Checkpoint start; [2022-12-15 21:14:51,99] [info] checkpointClose start; [2022-12-15 21:14:51,99] [info] checkpointClose synched; [2022-12-15 21:14:52,03] [info] checkpointClose script done; [2022-12-15 21:14:52,03] [info] dataFileCache commit start; [2022-12-15 21:14:52,04] [info] dataFileCache commit end; [2022-12-15 21:14:52,42] [info] checkpointClose end; [2022-12-15 21:14:52,43] [info] Checkpoint end - txts: 102088; [2022-12-15 21:14:52,43] [info] Checkpoint start; [2022-12-15 21:14:52,43] [info] checkpointClose start; [2022-12-15 21:14:52,43] [info] checkpointClose synched; [2022-12-15 21:14:52,46] [info] checkpointClose script done; [2022-12-15 21:14:52,46] [info] dataFileCache commit start; [2022-12-15 21:14:52,46] [info] dataFileCache commit end; [2022-12-15 21:14:52,49] [info] checkpointClose end; [2022-12-15 21:14:52,50] [info] Checkpoint end - txts: 102090; [2022-12-15 21:14:52,81] [info] Slf4jLogger started; [2022-12-15 21:14:53,15] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b254006"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2022-12-15 21:14:53,38] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:11890,checkpoint,checkpointClose,11890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,11,"['Checkpoint', 'checkpoint', 'failure', 'heartbeat']","['Checkpoint', 'checkpointClose', 'failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:2890,down,download,2890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['down'],['download']
Availability,"_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; apptainer exec --cleanenv --bind ""${cwd}:${docker_cwd},<path>"" ""$IMAGE"" ""${job_shell}"" ""${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }. sge_docker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ""docker exec -v ${cwd}:${docker_cwd} -v <path> ${job_shell} ${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; } ; }; Local; {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config; {; #concurrent-job-limit = 5; run-in-background = true; # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? mountOption; """""". # Submit string when there is no ""docker"" runtime attribute.; submit = ""/usr/bin/env bash ${script}""; ; # if the apptainer .sif for the image is created this will automatically use it; # otherwise it will pull from dockerhub; # if not using on dori change the source path f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:5201,alive,alive,5201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['alive'],['alive']
Availability,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:35,error,error,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,5,['error'],"['error', 'errors']"
Availability,"_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set the resources used by Cromwell in local, what file I need to create/modify or how should I cange m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3152,echo,echo,3152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['echo'],['echo']
Availability,"_cnloh_dir"": ""splits/"",; ""case_gatk_acnv_workflow.seg_param_eta"": 0.05,; ""case_gatk_acnv_workflow.seg_param_trim"": 0.025,; ""case_gatk_acnv_workflow.plots_dir"": ""plots/"",; ""case_gatk_acnv_workflow.seg_param_pmethod"": ""HYBRID"",; ""case_gatk_acnv_workflow.seg_param_nperm"": 10000,; ""case_gatk_acnv_workflow.PoN"": ""/data/ice_rcs_eval.v1.pd250.spark.pon"",; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:82715,failure,failures,82715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['failure'],['failures']
Availability,"_index; Int? max_output; Array[String]? ignore; Int disk_size; Int preemptible_tries. command {; java -Xmx4000m -jar stuff.jar blah; }; runtime {; memory: ""7 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File report = ""${report_filename}""; }; }; ```. Call (note there is no value supplied for max_output):. ```; call ValidateSamFile as ValidateReadGroupSamFile {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:1443,Error,Error,1443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,1,['Error'],['Error']
Availability,"_lines(stdout()); }; }. workflow example {; String file_pattern = '*.sh' ; Array[File] files = glob(file_pattern); scatter(path in files) {; call hello {input: in=path}; }; }; ```. When run with all-default paramters as follows:. ```; java -jar cromwell-36.jar run glob.wdl; ```. The following output results:. ```; [2019-01-07 16:21:06,14] [info] Running with database db.url = jdbc:hsqldb:mem:094e8bf9-be0f-4d7c-854a-0cf1a15dc0d7;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:16,40] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-01-07 16:21:16,42] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-01-07 16:21:16,62] [info] Running with database db.url = jdbc:hsqldb:mem:2efc8123-f7e8-4fe3-abed-48d1bcf8eb97;shutdown=false;hsqldb.tx=mvcc; [2019-01-07 16:21:17,27] [info] Slf4jLogger started; [2019-01-07 16:21:17,78] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-231ef13"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-01-07 16:21:17,87] [info] Metadata summary refreshing every 2 seconds.; [2019-01-07 16:21:17,94] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,00] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-01-07 16:21:18,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-01-07 16:21:19,53] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-01-07 16:21:19,58] [info] SingleWorkflowRunnerActor: Version 36; [2019-01-07 16:21:19,61] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-01-07 16:21:19,69] [info] Unspecified type (Unspecified version) workflow 18de8166-5f29-4288-9fa4-6741565446fd submitted; [2019-01-07 16:21:19,74] [info] SingleWorkflowRunnerActor: Workflow submitted [38;5;2m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:1239,heartbeat,heartbeat,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.kinship_count' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.reported_sex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:23182,failure,failures,23182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"`CromwellServer` now exits with a 1 if the server does not start up. Still could use clean termination handling (via lenthall?).; Refactored the `WorkflowManagerSystem` passing into `Main`, including ensuring that during tests the internal actor system does get shut down, but not prematurely.; Made the `PromiseActor` more generic. TODO: Could move to lenthall, and possibly make it even easier to use, as right now it requires using `tell` instead of `!` to ensure the `sender` is set correctly.; `sys.exit()` is only called in the `object Main`, not conditionally in the `class Main`.; Moved `getAction()` from the `Main` class to the object.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/347:267,down,down,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/347,1,['down'],['down']
Availability,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1437:93,redundant,redundantly,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437,3,"['error', 'redundant']","['error', 'redundantly']"
Availability,"```; $ echo 'version development. workflow main {; call main { input: s1 = ""x"", s2 = ""y"" }; output { Array[File] f = main.f }; }. task main {; input {; String s1; String s2; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s1}""; touch ""d/~{s2}""; echo -e ""d/~{s1}\nd/~{s2}""; >>>. output {; Directory d = ""d""; Array[File] f = read_lines(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }' > main.wdl; ```. This workflow when run on Google Cloud using Cromwell 74:; ```; $ java -Dconfig.file=PAPIv2.conf -jar cromwell-74.jar run main.wdl; ```; will succeed. When run on Google Cloud using Cromwell 75:; ```; $ java -Dconfig.file=PAPIv2.conf -jar cromwell-75.jar run main.wdl; ```; the workflow will fail with message:; ```; GCS output file not found: gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d; ```; However, the directory is correctly delocalized:; ```; $ gsutil ls -l gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d; 0 2022-02-13T00:00:00Z gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d/x; 0 2022-02-13T00:00:00Z gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d/y; TOTAL: 2 objects, 0 bytes (0 B); ```. The delocalization script is aware that `d` is directory:; ```; $ gsutil cat gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/gcs_delocalization.sh; source '/cromwell_root/gcs_transfer.sh'. timestamped_message 'Delocalization script execution started...'. # xxx; delocalize_6c578056c74a8d9a80724855ddac131c=(; ""mccarroll-mocha"" # project; ""3"" # max attempts; ""150M"" # parallel composite upload threshold, will not be used for directory types; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/memory_retry_rc""; ""/cromwell_root/memory_retry_rc""; ""optional""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677:7,echo,echo,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677,2,['echo'],['echo']
Availability,"```; 2017-12-01 11:49:13,302 cromwell-system-akka.dispatchers.engine-dispatcher-63 ERROR - WorkflowManagerActor Workflow 994bc7a0-2340-4903-9feb-af685f3197d0 failed (during ExecutingWorkflowState): Cannot run program ""ps"": /[...]/cromwell-executions/wf_name/994bc7a0-2340-4903-9feb-af685f3197d0/call-task1/execution/stdout.check (No such file or directory); java.io.IOException: Cannot run program ""ps"": /[...]/cromwell-executions/wf_name/994bc7a0-2340-4903-9feb-af685f3197d0/call-task1/execution/stdout.check (No such file or directory); 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048); 	at cromwell.backend.sfs.ProcessRunner.run(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.Stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:83,ERROR,ERROR,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['ERROR'],['ERROR']
Availability,```; 2019-04-27 01:13:54 [cromwell-system-akka.actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.Ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:77,ERROR,ERROR,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,3,"['ERROR', 'avail', 'error']","['ERROR', 'available', 'error']"
Availability,"```; The code passed to eventually never returned normally. Attempted 14 times over 5.123310162 seconds. Last failure message: Vector(""hola"", ""hello"", ""bonjour"") was not equal to Vector(""hola"", ""hello"", ""bonjour"", ""aurevoir"").; ```. ```; tc: BatchingDbWriter should process again when previous processing finished and we're still over batch size (each time with same test case); ```. https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/161/testReport/cromwell.core.actor/BatchActorSpec/BatchingDbWriter_should_process_again_when_previous_processing_finished_and_we_re_still_over_batch_size/. If one needs more info, please talk to @ndbolliger",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4223:110,failure,failure,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4223,1,['failure'],['failure']
Availability,```; [ERROR] [01/27/2017 13:33:05.570] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 0fcd7ba4-f1e0-4e16-9b67-c83ad2378f44 failed (during ExecutingWorkflowState): Could not evaluate no_address.out = read_string(stdout()); java.lang.RuntimeException: Could not evaluate no_address.out = read_string(stdout()); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:182); 	at wdl4s.Task$$anonfun$11$$anonfun$4.applyOrElse(Task.scala:181); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:181); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:174); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.Task.evaluateOutputs(Task.scala:174); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.postProcess(JesAsyncBackendJobExecutionActor.scala:366); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:391); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionSuccess(JesAsyncBackendJobExecutionActor.scala:46); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:6,ERROR,ERROR,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,3,"['ERROR', 'Failure', 'recover']","['ERROR', 'Failure', 'recoverWith']"
Availability,"```; [INFO] [06/02/2016 19:49:46.376] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$ib/$a] $a: Call-to-Backend assignments: hello.hello -> local; [INFO] [06/02/2016 19:49:46.376] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$ib] $$ib transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [INFO] [06/02/2016 19:49:46.377] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$ib/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib/WorkflowInitializationActor-fd304efe-2bba-4859-9ffd-6ef7d4ae29d5] State is now terminal. Shutting down.; [WARN] [06/02/2016 19:49:46.381] [test-system-akka.actor.default-dispatcher-3] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$ib]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to Mater",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:579,down,down,579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,2,['down'],['down']
Availability,```; java -Dconfig.file=/root/cromwell-application.conf -jar /root/cromwell/cromwell-62.jar run hello.wdl; ```. ```; WorkflowManagerActor: Workflow 19f1873b-7e61-4683-9585-747bf842a261 failed (during ExecutingWorkflowState): java.lang.Exception: Job id job-0000000060963A8300007BD2000D7072 failed: 'cluster(job-0000000060963A8300007BD2000D7072_cromwell_0) error: InvalidArgument'; 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:281); 	at cromwell.backend.impl.bcs.BcsAsyncBackendJobExecutionActor.handleExecutionFailure(BcsAsyncBackendJobExecutionActor.scala:32); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1315); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1311); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352:356,error,error,356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352,2,"['error', 'recover']","['error', 'recoverWith']"
Availability,"```; task test {; String? s. command { ; echo ""Here's s:""; echo ""${s}""; }; }. workflow printString {; String? inputS; call test {input: s = inputS}; }; ```. Acceptance criteria: make the above work... or have it report an error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/575:41,echo,echo,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/575,3,"['echo', 'error']","['echo', 'error']"
Availability,"```; workflow test {; 	call t1 { input : flag1 = false, }; }. task t1 {; 	Boolean? flag1; 	command {; 		echo test1 > test1.txt; 		echo test2 > test2.txt; 	}; 	output {; 		File out_false = if false then glob('test1.txt')[0] else glob('test2.txt')[0] # try commenting this out; 		File out = if select_first([flag1,false]) then glob('test1.txt')[0] else glob('test2.txt')[0]; 	}; }; ```; This code works fine. But If you comment out a line `File out_false = if false then glob('test1.txt')[0] else glob('test2.txt')[0] # try commenting this out` in output {}, then I get an error. ```; $ java -jar ../cromwell-29.jar run test_select_first_in_if_in_output.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-01 20:00:53,03] [info] Running with database db.url = jdbc:hsqldb:mem:6ae82874-e5ea-4c15-9f1d-09f8d0406019;shutdown=false;hsqldb.tx=mvcc; [2017-12-01 20:01:00,60] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-01 20:01:00,61] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-01 20:01:01,07] [info] Slf4jLogger started; [2017-12-01 20:01:01,35] [info] Metadata summary refreshing every 2 seconds.; [2017-12-01 20:01:01,37] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-01 20:01:01,51] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-01 20:01:02,89] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-01 20:01:02,94] [info] Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 submitted.; [2017-12-01 20:01:02,94] [info] SingleWorkflowRunnerActor: Workflow submitted 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:104,echo,echo,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,3,"['echo', 'error']","['echo', 'error']"
Availability,```The code passed to eventually never returned normally. Attempted 51 times over 3.8950992149999997 seconds. Last failure message: HighLoad was not equal to NormalLoad.```. `https://broadinstitute.atlassian.net/browse/GAWB-3876`. ```tc: LoadControllerServiceActor should update global load level periodically (each time with same test case)```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4237:115,failure,failure,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4237,1,['failure'],['failure']
Availability,"```WDL; task SplitNCigarReads {; input {; File inputBam; File inputBamIndex; File referenceFasta; File referenceFastaDict; File referenceFastaFai; String outputBam; Array[File] intervals = []. Int memory = 4; Float memoryMultiplier = 4; String dockerImage = ""quay.io/biocontainers/gatk4:4.1.0.0--0""; }. command {; set -e; mkdir -p $(dirname ~{outputBam}); gatk --java-options -Xmx~{memory}G \; SplitNCigarReads \; -I ~{inputBam} \; -R ~{referenceFasta} \; -O ~{outputBam} \; ~{prefix('-L ', intervals)}; }. output {; File bam = outputBam; File bamIndex = sub(outputBam, ""\.bam$"", "".bai""); }. runtime {; docker: dockerImage; memory: ceil(memory * memoryMultiplier); }; }; ```; expected behavior: By default nothing happens as intervals is empty. So this should evaluate to an empty string. No intervals flag is passed to GATK.; Actual behaviour:; ```; [2019-07-29 08:49:39,27] [error] WorkflowManagerActor Workflow 3de3bd74-b387-4d35-a704-73a4054387e9 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.Exception: Failed command instantiation; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:47); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:877,error,error,877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,1,['error'],['error']
Availability,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990:49,echo,echo,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990,5,['echo'],['echo']
Availability,"```wdl; task two_input_task {; String inA; String inB; command { echo ${inA} and ${inB} }; runtime { docker: ""ubuntu"" }; }. workflow duplicate_conditional_input {; String my_input = ""input""; if (false) {; call two_input_task { input: inA = my_input, inB = my_input }; }; }; ```. Runs in 29 (d45f001). In develop (0fd1635) this crashes with:. ```java; [2017-12-02 16:30:57,99] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-02 16:30:59,03] [error] WorkflowManagerActor Workflow e7ef9b84-2a08-4295-a2ab-fea19ddcba76 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.my_input; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:7",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2973:65,echo,echo,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2973,2,"['echo', 'error']","['echo', 'error']"
Availability,`backendStatus` call metadata key not reliably generated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3998:38,reliab,reliably,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998,1,['reliab'],['reliably']
Availability,`commandLine` call metadata key not reliably generated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4590:36,reliab,reliably,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4590,1,['reliab'],['reliably']
Availability,`dockerImageUsed` call metadata key not reliably generated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4001:40,reliab,reliably,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001,1,['reliab'],['reliably']
Availability,"`java -Dconfig.file=/home/jgentry/jes.conf -jar ~/cromwelserver/target/scala-2.12/cromwell-35-49d15d2-SNAP.jar run https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/gcp/somatic-workflow/main-somatic.cwl -i main-somatic-samples.json -t CWL -o /home/jgentry/bootDisk.json`. Led to. `2018-09-26 15:24:34,35] [error] WorkflowManagerActor Workflow bd190770-2d7a-4a14-893a-71a7081512c4 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/bd190770-2d7a-4a14-893a-71a7081512c4.temp.8962849824592019273/bd190770-2d7a-4a14-893a-71a7081512c4.cwl failed with null; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211)`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4152:318,error,error,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4152,1,['error'],['error']
Availability,`womtool graph germline.wdl`; I got an error:; ![image](https://github.com/broadinstitute/cromwell/assets/61043072/03a9e638-6f63-45b7-ab74-a81d599d9de9). wdl and version:; germline.wdl(https://github.com/biowdl/germline-DNA/releases/download/v5.0.0/germline_v5.0.0.wdl); womtool:; ```shell; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; argcomplete 1.12.3 pyhd8ed1ab_0 conda-forge; atomicwrites 1.4.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; bcrypt 3.2.2 py310h5764c6d_1 conda-forge; brotli-python 1.0.9 py310hd8f1fbe_9 conda-forge; bullet-python 2.2.0 py310hff52083_6 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; coloredlogs 15.0.1 pyhd8ed1ab_3 conda-forge; cromwell 0.40 1 bioconda; cryptography 41.0.2 py310h75e40e8_0 conda-forge; docker-py 6.1.3 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.1.2 pyhd8ed1ab_0 conda-forge; expat 2.5.0 hcb278e6_1 conda-forge; findutils 4.6.0 h166bdaf_1001 conda-forge; font-ttf-dejavu-sans-mono 2.37 hab24e00_0 conda-forge; fontconfig 2.14.2 h14ed4e7_0 conda-forge; freetype 2.12.1 hca18f0e_1 conda-forge; gettext 0.21.1 h27087fc_0 conda-forge; humanfriendly 10.0 py310hff52083_4 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; importlib_resources 6.0.0 pyhd8ed1ab_1 conda-forge; iniconfig 2.0.0 pyhd8ed1ab_0 conda-forge; jinja2 3.1.2 pyhd8ed1ab_1 conda-forge; js2py 0.74 pyhd8ed1ab_0 conda-forge; jsonschema 4.18.4 pyhd8ed1ab_0 conda-forge; jsonschema-specifications 2023.7.1 pyhd8ed1ab_0 conda-forge; keyutils 1.6.1 h166bdaf_0 conda-forge; krb5 1.21.1 h659d440_0 conda-forge; lark 1.1.7 pyhd8ed1ab_0 conda-for,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7212:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7212,2,"['down', 'error']","['download', 'error']"
Availability,"a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1105,down,down,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['down'],['down']
Availability,"a.MetadataComponent$$anon$1.combine(MetadataComponent.scala:15); #011at cats.kernel.instances.MapMonoid.combine(map.scala:26); #011at cats.kernel.instances.MapMonoid.combine(map.scala:36); #011at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); #011at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); #011at scala.collection.immutable.Map$Map1.foreach(Map.scala:116); #011at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); #011at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); #011at cats.kernel.instances.MapMonoid$$anonfun$combine$2.apply(map.scala:36); #011at cats.kernel.instances.MapMonoid$$anonfun$combine$2.apply(map.scala:37); #011at cats.kernel.SemigroupFunctions.maybeCombine(Semigroup.scala:50); #011at cromwell.webservice.metadata.MetadataComponent$$anon$1.combine(MetadataComponent.scala:9); #011at cromwell.webservice.metadata.MetadataComponent$$anon$1.combine(MetadataComponent.scala:18); #011at scala.math.Ordering$$anon$9.max(Ordering.scala:199); #011at scala.math.Ordering$class.max(Ordering.scala:103); #011at scala.math.Ordering$$anon$9.gteq(Ordering.scala:204); #011at scala.math.Ordering$$anonfun$by$1.apply(Ordering.scala:219); #011at scala.math.Ordering$$anonfun$by$1.apply(Ordering.scala:219); #011at cromwell.webservice.metadata.MetadataPrimitive$$anonfun$2.apply(MetadataComponent.scala:42); #011at cromwell.webservice.metadata.MetadataPrimitive$$anonfun$2.apply(MetadataComponent.scala:43); #011at scala.Enumeration.withName(Enumeration.scala:124); java.util.NoSuchElementException: No value found for 'Preempted'. 2017-07-12 17:53:26,768 cromwell-system-akka.dispatchers.api-dispatcher-201 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/ce7b5164-8a0c-4386-a993-d4a522460df7/metadata,List(blah blah headers,Empty,HTTP/1.1); ```. This results in FC not displaying workflow details, which annoys users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2438:10598,ERROR,ERROR,10598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2438,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"a.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); at scala.util.Try$.apply(Try.scala:192); at scala.util.Success.map(Try.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121); at scala.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2896,error,error,2896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['error'],['error']
Availability,a.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1870,recover,recoverAsync,1870,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Availability,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3942:249,ERROR,ERROR,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942,1,['ERROR'],['ERROR']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi; 1608597279104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:92045,down,download,92045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi; 1608597040452,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:26592,down,download,26592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz; 1608597101844,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz; 1608597103907,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-73/cacheCopy/SR00c.HG03009.txt.gz.tbi; 1608597105908,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:44681,down,download,44681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz; 1608597144746,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-11/cacheCopy/SR00c.HG00375.txt.gz; 1608597146753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz.tbi; 1608597149087,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:56530,down,download,56530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz; 1608597208254,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi; 1608597210297,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:73333,down,download,73333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz; 1608597286657,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz; 1608597289714,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz; 1608597293653,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:95158,down,download,95158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz; 1608597169668,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz; 1608597174143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:63385,down,download,63385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz; 1608597232457,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz; 1608597234131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:79579,down,download,79579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz; 1608597132184,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-22/cacheCopy/SR00c.HG01112.txt.gz; 1608597133659,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:53413,down,download,53413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz; 1608597081079,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-79/cacheCopy/SR00c.HG03370.txt.gz; 1608597083236,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:39070,down,download,39070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz; 1608597029300,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz; 1608597031084,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:24725,down,download,24725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-42/cacheCopy/SR00c.HG01885.txt.gz; 1608597499938,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz; 1608597502968,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-101/cacheCopy/SR00c.HG04161.txt.gz; 1608597504024,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:155647,down,download,155647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz; 1608597374474,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-83/cacheCopy/SR00c.HG03476.txt.gz; 1608597376806,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-131/cacheCopy/SR00c.NA19443.txt.gz; 1608597378569,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:118221,down,download,118221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-51/cacheCopy/SR00c.HG02186.txt.gz; 1608597465182,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-77/cacheCopy/SR00c.HG03111.txt.gz; 1608597466639,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz.tbi; 1608597468522,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-19/cacheCopy/SR00c.HG00844.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:144437,down,download,144437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz; 1608596954593,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz; 1608596956029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi; 1608596958265,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:4144,down,download,4144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz; 1608597192326,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz; 1608597194778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-55/cacheCopy/SR00c.HG02275.txt.gz; 1608597196671,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-108/cacheCopy/SR00c.NA12872.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:69597,down,download,69597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz; 1608597232457,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-61/cacheCopy/SR00c.HG02490.txt.gz; 1608597234131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz.tbi; 1608597236161,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:80198,down,download,80198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz; 1608597154187,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz; 1608597156183,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi; 1608597157882,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-5/cacheCopy/SR00c.HG00187.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:59014,down,download,59014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz; 1608597343594,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz; 1608597345651,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi; 1608597348432,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:110110,down,download,110110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-74/cacheCopy/SR00c.HG03085.txt.gz; 1608597289714,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz; 1608597293653,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-149/cacheCopy/SR00c.NA20764.txt.gz; 1608597295559,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:95777,down,download,95777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz; 1608596979652,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-40/cacheCopy/SR00c.HG01874.txt.gz; 1608596981096,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz.tbi; 1608596983896,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-69/cacheCopy/SR00c.HG02658.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:11638,down,download,11638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz; 1608597068826,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz; 1608597070520,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi; 1608597072537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-4/cacheCopy/SR00c.HG00150.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:35326,down,download,35326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz; 1608597098791,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz; 1608597101844,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-17/cacheCopy/SR00c.HG00701.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:43441,down,download,43441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"a:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1272,error,error,1272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,"a:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6328,ERROR,ERROR,6328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['ERROR'],['ERROR']
Availability,"a@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1164,error,error,1164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['error'],['error']
Availability,"aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulskis ; > Director of Platform Engineering, Data Sciences Platform ; > Broad Institute of MIT and Harvard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:3777,failure,failures,3777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failures']
Availability,"ab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1686,failure,failure,1686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['failure'],['failure']
Availability,ableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:363); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.scala:356); 	at lenthall.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:17); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespaceWithImports(MaterializeWorkflowDescriptorActor.scala:356); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.validateNamespace(MaterializeWorkflowDescriptorActor.scala:372); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:172); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:132); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:2253,Error,ErrorOr,2253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,1,['Error'],['ErrorOr']
Availability,"ableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.foreach(List.scala:381) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$class.map(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.immutable.List.map(List.scala:285) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:522) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; 2016-05-24 16:59:44,174 cromwell-system-akka.actor.default-dispatcher-12 INFO - JesBackend [UUID(fd62961b):ApplyBQSRToUnmappedReads]: Starting call with pre-emptible VM; 2016-05-24 16:59:44,175 cromwell-system-akka.actor.default-dispatcher-22 INFO - WorkflowActor [UUID(fd62961b)]: persisting status of ApplyBQSR:11 to Failed.; 2016-05-24 16:59:44,180 cromwell-system-akka.actor.default-dispatcher-13 ERROR - WorkflowActor [UUID(fd62961b)]: Duplicate entry '741-PairedEndSingleSampleWorkflow.ApplyBQSR-recalibrated_bam-11-' for key 'UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO'. 2016-05-24 16:59:44,175 cromwell-system-akka.actor.default-dispatcher-22 INFO - WorkflowActor [UUID(fd62961b)]: persisting status of ApplyBQSR:11 to Failed. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/880:5305,ERROR,ERROR,5305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/880,1,['ERROR'],['ERROR']
Availability,ackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:130); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:4638,robust,robustExecuteOrRecover,4638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,2,['robust'],['robustExecuteOrRecover']
Availability,ackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.execute(SharedFileSystemAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:43); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:71); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:4361,robust,robustExecuteOrRecover,4361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['robust'],['robustExecuteOrRecover']
Availability,actor.ActorSystemImpl.systemActorOf(ActorSystem.scala:745) at akka.testkit.TestKitBase.$init$(TestKit.scala:170) at akka.testkit.TestKit.<init>(TestKit.scala:896) at akka.testkit.TestProbe.<init>(TestKit.scala:954) at akka.testkit.TestProbe.<init>(TestKit.scala:956) at akka.testkit.TestProbe$.apply(TestKit.scala:990) at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:109) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(E,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:1491,Robust,RobustClientHelperSpec,1491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,added server logging for error code 10 [BA-5703],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5030:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5030,1,['error'],['error']
Availability,adds AES256 encryption to s3 file uploads and downloads via the ecs-proxy.; important for any workflows that need to be HIPAA compliant.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4264:46,down,downloads,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4264,1,['down'],['downloads']
Availability,advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7136,Error,ErrorOr,7136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,age.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2$adapted(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsEnhancedRequest$.$anonfun$recoverFromProjectNotProvided$3(GcsEnhancedRequest.scala:18); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseExce,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:2183,recover,recoverFromProjectNotProvided,2183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['recover'],['recoverFromProjectNotProvided']
Availability,"ain.reported_sex:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:15:03,84] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:15:03,84] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:22:59,01] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.kinship_count:-1:1-20000000009 [9e4f5894main.kinship_count:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,03] [info] BT-322 9e4f5894:main.kinship_count:-1:1 cache hit copying success with aggregated hashes: initial = 40DB3965745EAB4613A3E2804F447EFE, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,03] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.kinship_count:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.reported_sex:-1:1-20000000001 [9e4f5894main.reported_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,12] [info] BT-322 9e4f5894:main.reported_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:21896,failure,failures,21896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: ""-l h_vmem="" + memory + ""G"": Cannot perform operation: -l h_vmem= + WomLong(4); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.writeTaskScript$(ConfigAsyncJobExecutionActor.scala:55); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeTaskScript(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs(ConfigAsyncJobExecutionActor.scala:43); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.processArgs$(ConfigAsyncJobExecutionActor.scala:39); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.processArgs$lzycompute(ConfigAsyncJobExecutionActor.scala:211); at cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:1708,Error,Error,1708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['Error'],['Error']
Availability,"akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: executing: module load proxy; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qsub \; -terse \; -b n \; -N cromwell_45d03417_say_hello \; -wd cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; -o cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stdout \; -e cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:7942,echo,echo,7942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['echo'],['echo']
Availability,"al upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actual",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1181,error,errorcode,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['error'],['errorcode']
Availability,al.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7230,Error,ErrorOr,7230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"al; [INFO] [06/02/2016 19:49:46.376] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$ib] $$ib transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [INFO] [06/02/2016 19:49:46.377] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$ib/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib/WorkflowInitializationActor-fd304efe-2bba-4859-9ffd-6ef7d4ae29d5] State is now terminal. Shutting down.; [WARN] [06/02/2016 19:49:46.381] [test-system-akka.actor.default-dispatcher-3] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$ib]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:1172,down,down,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"all-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:3:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> kidney), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456754), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456754), WomString(name) -> WomString(GSM2927750), WomString(gsm) -> WomString(GSM2927750), WomString(title) -> WomString(RNA_105_kidney_Control))), WomString(run) -> WomString(SRR6456754), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:7382,failure,failure,7382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,2,"['down', 'failure']","['download', 'failure']"
Availability,"allable.call(UnaryCallable.java:112); 		at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.$anonfun$submit$1(GcpBatchApiRequestHandler.scala:11); 		at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.withClient(GcpBatchApiRequestHandler.scala:29); 		at cromwell.backend.google.batch.api.GcpBatchApiRequestHandler.submit(GcpBatchApiRequestHandler.scala:9); 		at cromwell.backend.google.batch.actors.GcpBatchBackendSingletonActor$$anonfun$normalReceive$1.$anonfun$applyOrElse$1(GcpBatchBackendSingletonActor.scala:65); 		at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678); 		at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); 		at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 		at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 		at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 		at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 		at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 		at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. It seems that Cromwell only accepts public VPC network with names starting as `global/networks/...`, while my actual network name was automatically attached by prefix `projects/${projectId}/global/networks/` (as shown in Line 1 of the error message above). I just wonder if this is because I have something wrong in my conf file, or I missed some setup at GCP Batch side. Thanks!. I'm using Cromwell v87. And my conf file is. ```; ...; backend {; ...; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""application-default""; }; ...; }; }; ```. where `my-private-network` and `my-private-subnetwork` are GCP project labels.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500:4766,error,error,4766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500,1,['error'],['error']
Availability,"ally. When we allow the same workflow to read from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]. at com.googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:1125,error,errors,1125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,1,['error'],['errors']
Availability,"alueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1665,Error,ErrorOr,1665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"ameters"": {}, ; ""jobDefinition"": ""arn:aws:batch:us-east-1:260062248592:job-definition/hc_Haplotypecaller-hc_HC_GVCF:19527"", ; ""statusReason"": ""Essential container in task exited"", ; ""jobId"": ""7c2d29c2-f04e-4b3f-8579-915a6fbc9033"", ; ""attempts"": [{; ""startedAt"": 1558552881926, ""container"": {; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""containerInstanceArn"": ""arn:aws:ecs:us-east-1:260062248592:container-instance/3cfe8456-fd3e-420d-91bc-aa1d8d134194"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""exitCode"": 0}, ""stoppedAt"": 1558553539743, ""statusReason"": ""Essential container in task exited""}], ; ""jobQueue"": ""arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4"", ; ""dependsOn"": [], ; ""startedAt"": 1558552881926, ; ""jobName"": ""Haplotypecaller_HC_GVCF"", ; ""createdAt"": 1558552763368, ""stoppedAt"": 1558553539743}]}; ```. Clearly, the AWS Batch job parameters are referencing a completely different set of input files from the set described in the workflow log. In this particular case, the job described in the log was started via cromwell run using v36 on an isolated EC2 instance, while the workflow described by the job parameters json was submitted to a cromwell v36.1 server running on a completely separate EC2 instance. This would point to call caching NOT being the problem but a more fundamental issue with how Cromwell interfaces with the AWS Batch backend to submit jobs. We've also observed this result using Cromwell v40 and 41, the latter using a completely new stack created just for that version, in both run and server modes. If more information is needed, please reach out and we'll provide what we can; the transient nature of the Batch job parameters and the lack of a set of cases that reliably reproduce this error has made it difficult for us to investigate and we're hoping developer assistance can get this resolved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:16862,reliab,reliably,16862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,2,"['error', 'reliab']","['error', 'reliably']"
Availability,anch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:3286,Robust,RobustClientHelperSpec,3286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,"and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:2259,error,errors,2259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['error'],['errors']
Availability,andard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeAsync(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeAsync(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:502); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeOrRecover(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.aroundReceive(AwsAsyncJobExecutionActor.scala:23); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:3585,robust,robustExecuteOrRecover,3585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,1,['robust'],['robustExecuteOrRecover']
Availability,"another CTKS down, maybe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4543:13,down,down,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4543,1,['down'],['down']
Availability,ardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:70); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.Fork,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:2323,robust,robustPoll,2323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,1,['robust'],['robustPoll']
Availability,"artTime"": ""2018-08-14T16:14:32.007439Z"",; ""description"": ""Stopped pulling \""ubuntu@sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7\"""",; ""endTime"": ""2018-08-14T16:14:32.463087Z""; },; {; ""startTime"": ""2018-08-14T16:17:06.327716Z"",; ""description"": ""Worker released"",; ""endTime"": ""2018-08-14T16:18:14.462Z""; },; {; ""startTime"": ""2018-08-14T16:16:58.180303Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:17:00.575023Z""; },; {; ""startTime"": ""2018-08-14T16:13:13.678Z"",; ""description"": ""Pending"",; ""endTime"": ""2018-08-14T16:13:13.678Z""; },; {; ""startTime"": ""2018-08-14T16:16:42.510303Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:18173,echo,echo,18173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"as I could get without help:. + Migration scheme correctly implemented. All the necessary tables with all the correct constraints (foreign key, unique, primary key) are created on startup.; + Updated upstream liquibase in order to allow unique constraints to be defined properly.; + Made sure all the types were converted in SQLite types (TEXT, INTEGER, BLOB etc.); + Updated the testing to understand SQLite types properly. So far so good. Unfortunately the testing does not recognize the foreign key, primary key or unique constraints, even though they are defined (clearly visible in the sqlitebrowser). . Since the testing is just testing, I also decided to run cromwell with a workflow, but that does not work: ; ```; [ERROR] [07/20/2020 14:01:02.134] [cromwell-system-akka.dispatchers.engine-dispatcher-50] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreEngineActor] Error trying to fetch new workflows; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:1010); at org.sqlite.core.DB.newSQLException(DB.java:1022); at org.sqlite.core.DB.throwex(DB.java:987); at org.sqlite.core.NativeDB.prepare_utf8(Native Method); at org.sqlite.core.NativeDB.prepare(NativeDB.java:134); at org.sqlite.core.DB.prepare(DB.java:264); at org.sqlite.core.CorePreparedStatement.<init>(CorePreparedStatement.java:45); at org.sqlite.jdbc3.JDBC3PreparedStatement.<init>(JDBC3PreparedStatement.java:30); at org.sqlite.jdbc4.JDBC4PreparedStatement.<init>(JDBC4PreparedStatement.java:19); at org.sqlite.jdbc4.JDBC4Connection.prepareStatement(JDBC4Connection.java:35); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:241); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:205); at com.zaxxer.hikari.pool.ProxyConnection.prepareStatement(ProxyConnection.java:311); at com.zaxxer.hikari.pool.HikariProxyConnection.prepareStatement(HikariProxyConne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:1172,error,error,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,2,['error'],['error']
Availability,"ashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:3918,error,error,3918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['error'],['error']
Availability,"ask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.En",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6364,Fault,FaultHandling,6364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,askInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:980); at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1363); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1391); at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1375); at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:563); at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185); at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:320); ... 31 more; Caused by: java.io.EOFException: SSL peer shut down incorrectly; at sun.security.ssl.InputRecord.read(InputRecord.java:505); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:961); ... 43 more```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:5291,down,down,5291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,1,['down'],['down']
Availability,"at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException: Unsupported Content-Type, supported: application/json; 	at akka.http.scaladsl.unmarshalling.Unmarshaller$UnsupportedContentTypeException$.apply(Unmarshaller.scala:158); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$EnhancedFromEntityUnmarshaller$.$anonfun$forContentTypes$3(Unmarshaller.scala:114); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshaller.$anonfun$transform$3(Unmarshaller.scala:23); 	at akka.http.scaladsl.unmarshalling.Unmarshaller$$anon$1.apply(Unmarshaller.scala:58); 	at akka.http.scaladsl.unmarshalling.Unmarshal.to(Unmarshal.scala:25); 	at cromiam.sam.SamClient.$anonfun$collectionsForUser$1(SamClient.scala:52); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	... 12 more; ```. Unmarshalling section where the response may not always be `HTTP 200` and the entity could actually be an error HTML string: https://github.com/broadinstitute/cromwell/blob/4252c85d417b727d7745d67fdec03f6f175b6e58/CromIAM/src/main/scala/cromiam/sam/SamClient.scala#L50-L53",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3622:3923,error,error,3923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3622,1,['error'],['error']
Availability,at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2025,recover,recoverAsync,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Availability,"at has either one column or two and scatters a task over each row of the tsv. The task prints the second column if it is present. When the input `fake.tsv` has one column, everything is fine. However, when it has two columns eg; ```; 1</TAB>1; 2</TAB>2; ```; it fails with ""Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlIntegerType)) with this value: List(WdlInteger(1), WdlInteger(2))"". (*Side question: why is it trying to make a list out of values in two different scattered rows?*) Using `Int?` in the conditional instead of `Int` does not make a difference. Another bizarre twist: if instead of reading in from a file I hardcode the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:1222,echo,echo,1222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['echo'],['echo']
Availability,"at one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as possible (considering exponentially long waiting times in between retries) until a threshold amount of time has passed since the first try (10 minutes by default).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:1585,failure,failure,1585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,2,"['avail', 'failure']","['available', 'failure']"
Availability,"at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.PromiseActorRef$.$anonfun$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:5570,Error,ErrorHandling,5570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,1,['Error'],['ErrorHandling']
Availability,atSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:2796,Robust,RobustClientHelperSpec,2796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,"ata dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] /build/cos-tools",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:4021,Down,Downloading,4021,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,"ate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H8-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5H9-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HA-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HB-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/1641cccf-e6f1-42fd-9f17-9fd7d355ce3c/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-PK-A5HC-01A-11D-A30A-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```. I should also mention that this was in v21, when I attempted in v24, I got a more spectacular failure - `java.lang.UnsupportedOperationException: Could not evaluate expression: ""--maf2 "" + write_lines(maf2)`, and then what appeared to be an infinite loop of the exact same stack trace over and over, which after a standard `ctrl-c` kill became `Waiting for 1 workflows to abort...` over and over again, which finally necessitated a `kill -9`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1875:16024,failure,failure,16024,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1875,1,['failure'],['failure']
Availability,"ated to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:1439,failure,failure,1439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['failure'],['failure']
Availability,ats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstan,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7173,Error,ErrorOr,7173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"atter (file in read_lines(shard_fofn)){; 		call SelectVariants{; 			input:; 				vcf=file,; 				intervals=interval_list; 		}; 	}; ```. where `shard fofn` has 20k+ lines inside of it. When running this workflow different shards failed ; with one of two errors. 30-50 different shards would fail each retry. ```; {; failures: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [ ],; message: ""Data received in non-data state: 6""; }; ],; message: ""Connection has been shutdown: javax.net.ssl.SSLProtocolException: Data received in non-data state: 6""; }; ],; message: ""Connection has been shutdown: javax.net.ssl.SSLProtocolException: Data received in non-data state: 6""; }; ],; message: ""All reopens failed""; }; ],; message: ""vcf""; }; ],; message: ""Input evaluation for Call mergeAndGetSites.SelectVariants failed.""; }; ],; message: ""Couldn't resolve all inputs for mergeAndGetSites.SelectVariants at index Some(778).""; }; ],; attempt: 1,; shardIndex: 778; },; ```; or . ```; {; failures: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [ ],; message: ""Connection closed prematurely: bytesRead = 34066, Content-Length = 2974047""; }; ],; message: ""Connection closed prematurely: bytesRead = 34066, Content-Length = 2974047""; }; ],; message: ""All reopens failed""; }; ],; message: ""vcf""; }; ],; message: ""Input evaluation for Call mergeAndGetSites.SelectVariants failed.""; }; ],; message: ""Couldn't resolve all inputs for mergeAndGetSites.SelectVariants at index Some(19820).""; }; ],; attempt: 1,; shardIndex: 19820; }; ]; },; ```. When I take the `read_lines(shard_fofn)` and assign it to a variable and then pass that to the scatter everything works fine. Is this just bad luck or is there possibly a real issue here?. ```; 	Array[File] fofn_files = read_lines(shard_fofn). 	scatter (file in fofn_files){; 		call SelectVariants{; 			input:; 				vcf=file,; 				intervals=interval_list; 		}; 	}; ``",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2965:1145,failure,failures,1145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2965,1,['failure'],['failures']
Availability,"ava -version; java version ""1.8.0_73""; Java(TM) SE Runtime Environment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:1106,avail,available,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['avail'],['available']
Availability,"avassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:437) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:41) ~[cromwell.jar:0.19]; at slick.driver.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:38) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell.jar:0.19]; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_72]; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_72]; at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_72]; Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2926) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3344) ~[cromwell.jar:0.19]; ... 16 common frames omitted; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(143681e1)]: persisting status of GatherBqsrReports to Failed.; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 2016-04-26 14:06:37,741 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(143681e1)]: Beginning transition from Running to Failed.; 2016-04-26 14:06:39,456 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(143681e1)]: transitioning from Running to Failed.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:3318,ERROR,ERROR,3318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,aws cromwell run error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872,1,['error'],['error']
Availability,aws.config error https://cromwell.readthedocs.io/en/jg_add_http_doc/tutorials/AwsBatch101/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278,1,['error'],['error']
Availability,"ay/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-4/execution/out.txt""]; },; ""id"": ""0bb77c74-4c5c-4314-8463-072e7055ee7c""; }; ```. But I got the following error when I tried with 36.; ```; $ java -jar ~/cromwell-36.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:17:04,83] [info] Running with database db.url = jdbc:hsqldb:mem:bb200ed8-7db5-49a0-a250-ca46b3332697;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,03] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:17:12,04] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:17:12,13] [info] Running with database db.url = jdbc:hsqldb:mem:c7a7ec22-dec6-4fae-a53b-6c9933402fa9;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,59] [info] Slf4jLogger started; [2018-10-25 21:17:12,88] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-f5ccf1c"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:17:12,90] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:17:12,98] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:12,98] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:17:12,98] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:13,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:17:13,80] [info] SingleWorkflowRunnerActor: Version 36; [2018-10-25 21:17:13,81] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:17:13,84] [info] Unspecified type (Unspecified version) workflow e22c6324-5aec-4694-8750-f62160e2ca81 submitted; [2018-10-25 21:17:13,85] [info] SingleWorkflowRunnerActor: Workflow submitted e22c6324-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:10299,heartbeat,heartbeat,10299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"aybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch test",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:1902,error,error,1902,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,"b was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: pulling image: docker pull: running [\""docker\"" \""pull\"" \""us.gcr.io/xxx/xxx@sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""]: exit status 1 (standard error: \""error pulling image configuration: error parsing HTTP 400 response body: invalid character '<' looking for beginning of value: \\\""<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectMissing</Code><Message>Bucket is a requester pays bucket but no user project provided.</Message><Details>Bucket is Requester Pays bucket but no billing project id provided for non-owner.</Details></Error>\\\""\\n\"")"",`. I understand that the issue is that the Google bucket where the docker is located is requester pays and Cromwell does not know what to do in this case, but it is not immediately clear what I should do to fix it. It would be a great improvement if Cromwell could interpret this response and provide a more informative error message so that the user could immediately know what needs to be addressed. In particular, I am not fully sure what I should be doing. These are excerpts from my configuration file:; ```; ...; engine {; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; }; }; }; ...; services {; MetadataService {; ...; config {; carbonite-metadata-service {; filesystems {; gcs {; auth = ""service-account""; }; }; ...; }; }; }; }; ...; backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; project = ""xxx""; ...; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; ...; }; }; ...; }; }; }; }; ...; ```; Where should the configuration for telling Cromwell which project to use when pulling dockers be?. I also do not understand why this issue arises at all as the Google bucket with the dockers is a us multi-region bucket and the computation is in us-central1, so there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235:1133,error,error,1133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235,1,['error'],['error']
Availability,"b-cdef-0123-456789abcdef/call-main/gcs_delocalization.sh; source '/cromwell_root/gcs_transfer.sh'. timestamped_message 'Delocalization script execution started...'. # xxx; delocalize_6c578056c74a8d9a80724855ddac131c=(; ""mccarroll-mocha"" # project; ""3"" # max attempts; ""150M"" # parallel composite upload threshold, will not be used for directory types; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/memory_retry_rc""; ""/cromwell_root/memory_retry_rc""; ""optional""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/rc""; ""/cromwell_root/rc""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/monitoring.log""; ""/cromwell_root/monitoring.log""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stdout""; ""/cromwell_root/stdout""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stderr""; ""/cromwell_root/stderr""; ""required""; ""text/plain; charset=UTF-8""; ""directory""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d""; ""/cromwell_root/d""; ""required""; """"; ). delocalize ""${delocalize_6c578056c74a8d9a80724855ddac131c[@]}""; ; timestamped_message 'Delocalization script execution complete.'; ```. But somehow a new check was included in Cromwell 75 that wants `d` to be a file even if it is delocalized as a directory. This breaks the only [workaround](https://support.terra.bio/hc/en-us/community/posts/360071476431-Terra-fails-to-delocalize-files-listed-through-read-lines-) available in Cromwell to be able to delocalize a list of files not determined a priori before the start of the task. Notice that `glob()` is not an acceptable alternative as `glob()` does not provide control over the order of the output files.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677:3142,avail,available,3142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677,1,['avail'],['available']
Availability,"b-d96c52c32200] State is now terminal. Shutting down.; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.544] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$mb] $$mb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$mb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb/WorkflowInitializationActor-93102a71-3bff-432b-beb6-5506dcc13159] State is now terminal. Shutting down.; [info] WorkflowActorSpec:; [info] ShadowWorkflowActor; [info] - should move from WorkflowUnstartedState to MaterializingWorkflowDescriptorState *** FAILED ***; [info] WorkflowFailedState was not equal to MaterializingWorkflowDescriptorState (WorkflowActorSpec.scala:45); [info] - should transition to InitializingWorkflowState with correct call assignments given workflow options; [info] - should transition to InitializingWorkflowState with correct call assignments given runtime-attributes; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:4089,down,down,4089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,2,['down'],['down']
Availability,"b-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; ```. All the input files that were mentioned in the above command are on s3 and as you can see from the command line from above that cromwell correctly localized those. However, cromwell is failing to create a job with the following error:. ```; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.relativePathAndVolume(AwsBatchAsyncBackendJobExecutionActor.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356:1583,error,error,1583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356,1,['error'],['error']
Availability,"b.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/frankenstein.wdl. Input file: https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/map-variantcall-hg38.json. Possibly related to #4412 but not sure as I don't see the same error message. When submitting a workflow via the cromwell server we **consistently** see a failure to hash some items in S3 resulting in call caching being disabled for the run. We have seen this for a number of workflows, here we are including just one. . Call caching is a **hugely** important feature for us and if it is not available we may would have to reconsider using Cromwell. I think I have discussed with @ruchim the fact that all objects in S3 have a hash already computed (the ETag header) so there should not be timeouts in computing these hashes as they are available with a head request (you don't need to download the whole object). . Error message (extract from `/metadata` output):. ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Hashing request timed out for: s3://bucketname/cromwell-tests/Panel_BWA_GATK4_Samtools_Var_Annotate/162c863f-c22a-4b7c-bb37-f5195b329b36/call-ApplyBQSR/shard-0/smallTestData.hg38.recal.bam""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Config file:. ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:aws-database;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; {; name = ""assume-role-based-on-another""; scheme = ""assume_role""; base-auth = ""d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563:1019,Error,Error,1019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563,1,['Error'],['Error']
Availability,"b/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3998,down,downloading,3998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,2,['down'],"['download', 'downloading']"
Availability,"b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:1643,down,downstream,1643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['down'],['downstream']
Availability,"bWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:46); 	at com.google.cloud.BaseWriteChannel.close(BaseWriteChannel.java:149); 	at com.google.cloud.storage.contrib.nio.CloudStorageWriteChannel.close(CloudStorageWriteChannel.java:57); 	at java.nio.channels.Channels$1.close(Channels.java:178); 	at java.nio.file.Files.write(Files.java:3300); 	at cromwell.backend.impl.jes.io.package$PathEnhanced$.writeAsJson$extension(package.scala:13); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$5.apply(JesInitializationActor.scala:80); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$5.apply(JesInitializationActor.scala:80); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	... 6 more; Caused by: com.google.api.client.http.HttpResponseException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1070); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:518); 	... 20 more. [INFO] [01/27/2017 14:39:36.101] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor WorkflowActor-732474fd-88b0-4a5e-ad19-5ee5cd71d141 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:4022,error,error,4022,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,4,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101); 	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144); 	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174); 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3011); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.i.MetadataSummaryRefreshActor - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDir",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:3464,ERROR,ERROR,3464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['ERROR'],['ERROR']
Availability,"bc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:16:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:10:1]: Status change from - to WaitingForReturnCode; `. Can someone assist me with this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:4529,error,error,4529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,6,"['Error', 'error']","['Error', 'error']"
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz; 1608597615294,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz; 1608597617667,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz; 1608597619328,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:187042,down,download,187042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz; 1608597584919,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz; 1608597587281,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz; 1608597589990,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:179582,down,download,179582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-2/cacheCopy/SR00c.HG00129.txt.gz; 1608597587281,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz; 1608597589990,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz; 1608597592842,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:180201,down,download,180201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz; 1608597185060,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz; 1608597186282,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi; 1608597189769,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:67730,down,download,67730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-7/cacheCopy/SR00c.HG00277.txt.gz; 1608596992645,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz; 1608596994248,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-65/cacheCopy/SR00c.HG02611.txt.gz.tbi; 1608596996095,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-56/cacheCopy/SR00c.HG02299.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:15376,down,download,15376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz; 1608597239225,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-9/cacheCopy/SR00c.HG00337.txt.gz; 1608597241175,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz.tbi; 1608597243204,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:82063,down,download,82063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz; 1608597066051,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz; 1608597068826,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz; 1608597070520,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-75/cacheCopy/SR00c.HG03099.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:34705,down,download,34705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"be7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:45.309551Z""; },; {; ""startTime"": ""2018-08-14T16:16:39.695039Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:40.069663Z""; },; {; ""startTime"": ""2018-08-14T16:14:32.007439Z"",; ""description"": ""Stopped pulling \""ubuntu@sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7\"""",; ""endTime"": ""2018-08-14T16:14:32.463087Z""; },; {; ""startTime"": ""2018-08-14T16:17:06.327716Z"",; ""description"": ""Worker released"",; ""endTime"": ""2018-08-14T16:18:14.462Z""; },; {; ""startTime"": ""2018-08-14T16:16:58.180303Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:16836,echo,echo,16836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PrePro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1196,error,error,1196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['error'],['error']
Availability,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:24279,error,error,24279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"butes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""jobId"": ""28216"",; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:28.554Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:25.040Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:25.122Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.122Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:25.164Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.164Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23T13:53:25.291Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.291Z"",; ""description"": ""RunningJob"",; ""endT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:68651,failure,failures,68651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['failure'],['failures']
Availability,"c gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:45.288207Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.822335Z"",; ""description"": ""Stopped running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\""); fi '\"""",; ""endTime"": ""2018-08-14T16:16:58.180303Z""; },; {; ""startTime"": ""2018-08-14T16:16:56.948463Z"",; ""description"": ""Started running \""-c mkdir -p \/cromwell_root\/0c83f20c && cat \/cromwell_root\/cwl.output.json 2>\/dev\/null | jq -r '.. | .path? \/\/ .location? \/\/ empty | gsub(\\\""file:\/\/\\\""; \\\""\\\"")' > \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt\"""",; ""endTime"": ""2018-08-14T16:16:57.281247Z""; },; {; ""startTime"": ""2018-08-14T16:16:36.306767Z"",; ""description"": ""Started running \""\/bin\/bash \/cromwell_root\/script\"""",; ""endTime"": ""2018-08-14T16:16:37.108175Z""; },; {; ""startTime"": ""2018-08-14T16:14:03.091743Z"",; ""description"": ""Stopped pulling \""google\/cloud-sdk:alpine\"""",; ""endTime"": ""2018-08-14T16:14:03.127407Z""; },; {; ""startTime"": ""2018-08-14T16:13:25.071851Z"",; ""description"": ""waiting for quota"",; ""endTime"": ""2018-08-14T16:13:25.620872Z""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:2977,echo,echo,2977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,2,['echo'],['echo']
Availability,c(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:4976,Fault,FaultHandling,4976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi; 1608597091077,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-122/cacheCopy/SR00c.NA19001.txt.gz.tbi; 1608597093130,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz; 1608597095782,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:42203,down,download,42203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-18/cacheCopy/SR00c.HG00740.txt.gz.tbi; 1608597211840,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-135/cacheCopy/SR00c.NA19679.txt.gz.tbi; 1608597213995,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz.tbi; 1608597216321,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:75216,down,download,75216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi; 1608597532744,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-125/cacheCopy/SR00c.NA19102.txt.gz.tbi; 1608597533973,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz.tbi; 1608597536250,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:165241,down,download,165241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-26/cacheCopy/SR00c.HG01356.txt.gz.tbi; 1608597435583,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-148/cacheCopy/SR00c.NA20752.txt.gz.tbi; 1608597438407,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-115/cacheCopy/SR00c.NA18560.txt.gz; 1608597439856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:136340,down,download,136340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi; 1608597362768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi; 1608597364585,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz.tbi; 1608597367102,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:115106,down,download,115106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz.tbi; 1608597261055,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-145/cacheCopy/SR00c.NA20509.txt.gz.tbi; 1608597263910,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:88313,down,download,88313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz.tbi; 1608597538865,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-152/cacheCopy/SR00c.NA20869.txt.gz.tbi; 1608597540849,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz.tbi; 1608597542416,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:167126,down,download,167126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz.tbi; 1608596966063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz.tbi; 1608596968605,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz; 1608596971072,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:7900,down,download,7900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz.tbi; 1608597509827,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-113/cacheCopy/SR00c.NA18549.txt.gz.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:158770,down,download,158770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-89/cacheCopy/SR00c.HG03709.txt.gz.tbi; 1608597387962,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz.tbi; 1608597391284,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-156/cacheCopy/SR00c.NA21133.txt.gz; 1608597393258,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:122598,down,download,122598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:640); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:508); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```. A deadlock in the DB =>; ```; 2017-07-13 22:14:36,622 cromwell-system-akka.dispatchers.service-dispatcher-549 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Deadlock found when trying to get lock; try restarting transaction; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:951); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:6638,ERROR,ERROR,6638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['ERROR'],['ERROR']
Availability,"c.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: job id: bccmdfdd6o377kru9q6g; 2018-06-07 13:09:22,744 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from - to Running; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:6395,ERROR,ERROR,6395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['ERROR'],['ERROR']
Availability,"c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:39.695039Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.673071Z"",; ""description"": ""Started running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\""); fi '\"""",; ""endTime"": ""2018-08-14T16:16:57.822335Z""; },; {; ""startTime"": ""2018-08-14T16:14:33.759759Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:10851,echo,echo,10851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi; 1608597182124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz; 1608597185060,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz; 1608597186282,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:67113,down,download,67113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz; 1608597359185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz; 1608597361344,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz.tbi; 1608597362768,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:113850,down,download,113850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz; 1608597577251,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz; 1608597579845,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz; 1608597581880,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-138/cacheCopy/SR00c.NA19795.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:177725,down,download,177725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-119/cacheCopy/SR00c.NA18945.txt.gz; 1608597012199,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz; 1608597014237,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-78/cacheCopy/SR00c.HG03369.txt.gz.tbi; 1608597016404,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-80/cacheCopy/SR00c.HG03436.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:20372,down,download,20372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-120/cacheCopy/SR00c.NA18956.txt.gz; 1608597351053,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz; 1608597352822,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz.tbi; 1608597355348,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-102/cacheCopy/SR00c.HG04183.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:111981,down,download,111981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-121/cacheCopy/SR00c.NA18995.txt.gz; 1608597333778,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz; 1608597335401,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz.tbi; 1608597338143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:107626,down,download,107626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-129/cacheCopy/SR00c.NA19351.txt.gz; 1608597204085,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-147/cacheCopy/SR00c.NA20522.txt.gz; 1608597206512,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-13/cacheCopy/SR00c.HG00457.txt.gz; 1608597208254,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:72714,down,download,72714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz; 1608597600688,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz; 1608597602076,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz.tbi; 1608597604365,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-50/cacheCopy/SR00c.HG02085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:183308,down,download,183308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-134/cacheCopy/SR00c.NA19678.txt.gz; 1608597023863,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz; 1608597029300,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:23487,down,download,23487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-146/cacheCopy/SR00c.NA20510.txt.gz; 1608597635134,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-103/cacheCopy/SR00c.NA06984.txt.gz; 1608597637215,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz.tbi; 1608597639911,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:192649,down,download,192649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz; 1608597523198,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz; 1608597524955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:162108,down,download,162108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-53/cacheCopy/SR00c.HG02235.txt.gz; 1608597108506,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-105/cacheCopy/SR00c.NA11894.txt.gz; 1608597111201,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-140/cacheCopy/SR00c.NA19913.txt.gz; 1608597112463,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:46548,down,download,46548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-6/cacheCopy/SR00c.HG00239.txt.gz.tbi; 1608597610071,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-45/cacheCopy/SR00c.HG02002.txt.gz; 1608597612565,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz; 1608597615294,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:185806,down,download,185806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz.tbi; 1608597062290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-99/cacheCopy/SR00c.HG04118.txt.gz; 1608597066051,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-8/cacheCopy/SR00c.HG00288.txt.gz; 1608597068826,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-143/cacheCopy/SR00c.NA20321.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:34088,down,download,34088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-87/cacheCopy/SR00c.HG03684.txt.gz; 1608597573618,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz; 1608597577251,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-128/cacheCopy/SR00c.NA19350.txt.gz; 1608597579845,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-39/cacheCopy/SR00c.HG01861.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:177104,down,download,177104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,cActionComponent.scala:522) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]; > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74]; > at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74]; > Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.error.Error.error(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Constraint.getException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.index.IndexAVLMemory.insert(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.persist.RowStoreAVL.indexRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.TransactionManagerMVCC.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Table.insertSingleRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDML.insertRowSet(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementInsert.getResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDMQL.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.executeCompiledStatement(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > ..,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:4273,error,error,4273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['error'],['error']
Availability,cJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1754,recover,recover,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Availability,"cMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimCellBarcode/execution/script; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: job id: 39755; [2019-07-10 14:32:59,19] [info] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimCellBarcode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.stan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:9454,error,error,9454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['error'],['error']
Availability,"c_min_purity_spread_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Minimum number of somatic variants required to assist highly diploid fits. Default 300.\\n\"",\n \""id\"": \""#somatic_min_total_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""float\""\n ],\n \""doc\"": \""Proportion of somatic deviation to include in fitted purity score. Default 1.\\n\"",\n \""id\"": \""#somatic_penalty_weight_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of somatic variant vcf to assist fitting in highly-diploid samples.\\nSample name must match tumor parameter. GZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#somatic_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of structural variant vcf for more accurate segmentation.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#structural_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional location of failing structural variants that may be recovered.\\nGZ files supported.\\n\"",\n \""secondaryFiles\"": [\n \"".tbi\""\n ],\n \""id\"": \""#sv_recovery_vcf_purple\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads used for amber step\\n\"",\n \""id\"": \""#threads_amber\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to run cobalt command\\n\"",\n \""id\"": \""#threads_cobalt\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads to use - set to 8 by default\"",\n \""id\"": \""#threads_gridss\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Number of threads\\n\"",\n \""id\"": \""#threads_purple\""\n },\n {\n \""type\"": \""File\"",\n \""doc\"": \""tumour BAM file\\n\"",\n \""secondaryFiles\"": [\n \"".bai\""\n ],\n \""id\"": \""#tumor_bam\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""sample name of tumor. Must match the somatic snvvcf sample name. (Defa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:114090,recover,recovered,114090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['recover'],['recovered']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz; 1608597565330,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz; 1608597567063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:173991,down,download,173991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz; 1608597617667,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-52/cacheCopy/SR00c.HG02221.txt.gz; 1608597619328,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-24/cacheCopy/SR00c.HG01325.txt.gz.tbi; 1608597622374,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:187661,down,download,187661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-126/cacheCopy/SR00c.NA19143.txt.gz; 1608597026409,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-3/cacheCopy/SR00c.HG00140.txt.gz; 1608597029300,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz; 1608597031084,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:24104,down,download,24104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz; 1608597152166,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-63/cacheCopy/SR00c.HG02586.txt.gz; 1608597154187,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-82/cacheCopy/SR00c.HG03472.txt.gz; 1608597156183,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-116/cacheCopy/SR00c.NA18638.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:58395,down,download,58395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-20/cacheCopy/SR00c.HG01060.txt.gz; 1608597266985,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-95/cacheCopy/SR00c.HG03850.txt.gz; 1608597268460,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz.tbi; 1608597270319,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:89551,down,download,89551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz; 1608597592842,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz; 1608597594378,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi; 1608597597343,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-130/cacheCopy/SR00c.NA19377.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:181439,down,download,181439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-28/cacheCopy/SR00c.HG01393.txt.gz; 1608597624684,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz; 1608597626312,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz.tbi; 1608597627949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-34/cacheCopy/SR00c.HG01709.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:189526,down,download,189526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-31/cacheCopy/SR00c.HG01507.txt.gz; 1608597340856,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-67/cacheCopy/SR00c.HG02642.txt.gz; 1608597343594,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-92/cacheCopy/SR00c.HG03744.txt.gz; 1608597345651,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:109491,down,download,109491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz; 1608597565330,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz; 1608597567063,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz.tbi; 1608597569034,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:174610,down,download,174610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz; 1608597172290,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz; 1608597174143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz; 1608597175918,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:64004,down,download,64004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-49/cacheCopy/SR00c.HG02069.txt.gz; 1608597174143,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-30/cacheCopy/SR00c.HG01474.txt.gz; 1608597175918,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-37/cacheCopy/SR00c.HG01794.txt.gz.tbi; 1608597177960,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:64623,down,download,64623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-59/cacheCopy/SR00c.HG02374.txt.gz; 1608597560491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-10/cacheCopy/SR00c.HG00349.txt.gz; 1608597562850,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-38/cacheCopy/SR00c.HG01799.txt.gz; 1608597565330,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:173372,down,download,173372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz; 1608596952115,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-57/cacheCopy/SR00c.HG02332.txt.gz; 1608596954593,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-90/cacheCopy/SR00c.HG03722.txt.gz; 1608596956029,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-111/cacheCopy/SR00c.NA18530.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:3525,down,download,3525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"ca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-97/cacheCopy/SR00c.HG03872.txt.gz; 1608597589990,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-27/cacheCopy/SR00c.HG01384.txt.gz; 1608597592842,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz; 1608597594378,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-71/cacheCopy/SR00c.HG02953.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:180820,down,download,180820,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,caching of the last executed task does not work if workflow output throws error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4664:74,error,error,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4664,1,['error'],['error']
Availability,"cal hashes for the docker image and all inputs and outputs, I see a ""Cache Miss"" as the result, every time. . The call caching stanza in my metadata looks like this, for example. Am I missing something? ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""4B2AB7B9EA875BF5290210F27BB9654D"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:1659,failure,failure-mode,1659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['failure'],['failure-mode']
Availability,cala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.run(StatsDInstrumentationServiceActorSpec.scala:16) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4349:4890,Error,ErrorHandling,4890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4349,2,['Error'],['ErrorHandling']
Availability,cala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2136,recover,recoverAsync,2136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recoverAsync']
Availability,"cala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.fork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6814,recover,recoverWith,6814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Availability,cala:418); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:356); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:320); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:70); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:1794,robust,robustPoll,1794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,1,['robust'],['robustPoll']
Availability,"cala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-01-20 09:33:07,58] [info] WorkflowManagerActor WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46 is in a terminal state: WorkflowFailedState; [2017-01-20 09:33:24,62] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 transitioned to state Failed; ```. Failure is due to inability to find the files listed in the file created by write_lines():; ```; $ tail -n 10 stderr; Traceback (most recent call last):; File ""/src/Merge_MAFs.py"", line 182, in <module>; main(sys.argv[1:]); File ""/src/Merge_MAFs.py"", line 76, in main; concatenatedMafFilename = _handle_mafs(args); File ""/src/Merge_MAFs.py"", line 83, in _handle_mafs; mafPaths = _getMafPaths(args.mafpaths); File ""/src/Merge_MAFs.py"", line 98, in _getMafPaths; raise Exception(""MAF doesn't exist: %s"" % mafPath); Exception: MAF doesn't exist: /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J1-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```. File created by write_lines() - I believe the files listed within *should* be under the directory ```/root/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/D",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:15805,Failure,Failure,15805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['Failure'],['Failure']
Availability,"call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:2:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> liver), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456687), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456687), WomString(name) -> WomString(GSM2927683), WomString(gsm) -> WomString(GSM2927683), WomString(title) -> WomString(RNA_105_liver_Control))), WomString(run) -> WomString(SRR6456687), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95e-2040-4e38-a0d7-0b82c48bbca6/call-salmon/shard-0/execution/quant_SRR6456687), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:9096,failure,failure,9096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,2,"['down', 'failure']","['download', 'failure']"
Availability,"can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] /build/cos-tools: \nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz\ncurl: (16) Error in the HTTP2 framing layer\n\nreal\t0m3.705s\nuser\t0m0.573s\nsys\t0m1.834s\n[ERROR 2021-02-22 23:09:25 UTC] Could not download toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz, gi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:4688,Down,Downloading,4688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,"cattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528; [; {; ""Id"": ""cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528"",; ""Created"": ""2022-11-09T05:37:16.872195116Z"",; ""Path"": ""/bin/bash"",; ""Args"": [; ""/bark-bark/IS_THIS_TUBERCULOSIS/7570b5dc-4714-48a7-96b2-9c62245e3618/call-get_organism_names/shard-885/execution/script""; ],; ""State"": {; ""Status"": ""created"",; ""Running"": false,; ""Paused"": false,; ""Restarting"": false,; ""OOMKilled"": false,; ""Dead"": false,; ""Pid"": 0,; ""ExitCode"": 0,; ""Error"": """",; ""StartedAt"": ""0001-01-01T00:00:00Z"",; ""FinishedAt"": ""0001-01-01T00:00:00Z""; },; ""Image"": ""sha256:1c9072d6415cff6481014f64cf7486482dc61620bf09806bafffb1697bf344b1"",; ""ResolvConfPath"": """",; ""HostnamePath"": """",; ""HostsPath"": """",; ""LogPath"": ""/var/lib/docker/containers/cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528/cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528-json.log"",; ""Name"": ""/zen_almeida"",; ""RestartCount"": 0,; ""Driver"": ""overlay2"",; ""Platform"": ""linux"",; ""MountLabel"": """",; ""ProcessLabel"": """",; ""AppArmorProfile"": """",; ""ExecIDs"": null,; ""HostConfig"": {; ""Binds"": [; ""/private/var/folders/vp/327wktbj3wqb65q3v3n8qpxc0000gn/T/1667969838761-0/dockstore-is-cool/IS_THIS_TUBERCULOSIS/7570b5dc-4714-48a7-96b2-9c62245e3618/call-get_organism_names/shard-885:/bark-bark/IS_THIS_TUBERCULOSIS/7570b5dc-4714-48a7-96b2-9c62245e3618/call-get_organism_names/shard-885""; ],; ""ContainerIDFile"": """",; ""LogConfig"": {; ""Type"": ""json-file"",; ""Config"": {}; },; ""NetworkMode"": ""default"",; ""PortBindings"": {},",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:2473,Error,Error,2473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['Error'],['Error']
Availability,"cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # reso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:1892,error,error,1892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,1,['error'],['error']
Availability,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:1329,heartbeat,heartbeat,1329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,centaur tests blocked by this (note - some seem to be using the behavior foro ther purposes and might still fail):. ```; continue_on_return_code; exit; failures.terminal_status; default_runtime_attributes; globbingBehavior; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4813:152,failure,failures,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4813,1,['failure'],['failures']
Availability,"cessMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online doc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:8408,down,down,8408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['down'],['down']
Availability,ch(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionCompo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5655,Error,Error,5655,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['Error'],['Error']
Availability,"ch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19344,error,error,19344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"ch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-02-02 11:55:36,711 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 5fdb357a-3f1d-45b7-a85b-c22caa755c36 failed (during ExecutingWorkflowState): java.lang.IllegalArgumentException; cromwell.core.CromwellFatalException: java.lang.IllegalArgumentException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:6890,recover,recoverWith,6890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['recover'],['recoverWith']
Availability,"ch.files""); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call nofiles. output {; Array[File] out = nofiles.nofiles; }; }; ```. Causes this error:. ```; 2017-01-23 15:14:06,797 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - JesAsyncBackendJobExecutionActor [UUID(7e4afc46)w.nofiles:NA:1]: JesAsyncBackendJobExecutionActor [UUID(7e4afc46):w.nofiles:NA:1] Status change from Running to Failed; 2017-01-23 15:14:07,862 cromwell-system-akka.dispatchers.engine-dispatcher-8 ERROR - WorkflowManagerActor Workflow 7e4afc46-a1ff-424f-aecf-4ce742ed5fa8 failed (during ExecutingWorkflowState): It appears that some of the expected output files for task w.nofiles:NA:1 did not exist when the command exited.; A few things to try; 1) Check that the output section in your WDL is correct. Remember that all output files declared in a task must exist when the command exits.; 2) Check that the return code is available and is valid with respect to your command expected exit code; 3) Look into the stderr (gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/nofiles-stderr.log) file for evidence that some of the output files the command is expected to create were not created.; Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* -> gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/ (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/, command failed: CommandException: No URLs matched: /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/*\nCommandException: 1 file/object could not be transferred.\n); Check the content of stderr for potential additional information: gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1896:1028,avail,available,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1896,1,['avail'],['available']
Availability,check-alive polling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2281:6,alive,alive,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2281,1,['alive'],['alive']
Availability,"cho@broadinstitute.org""; }. }. workflow tool_gsea_mrnaseq_subtypes_workflow {; call tool_gsea_mrnaseq_subtypes; }; ```. Thanks,; Tim. ---. @geoffjentry commented on [Tue Mar 29 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-203042213). When talking to @knoblett she said that this appears to be something which previously was resolved - perhaps there's been a regression, or it's slightly different somehow. ---. @kbergin commented on [Wed Mar 30 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-203469521). I also observed something similar. One of my tasks was failing because I had an input designated as an input in the call, and in the task, but it actually wasn't an input for the workflow nor in the json. I feel like validate or some error handling should have caught that? Instead it just didn't make it through JES and said it 'failed to localize inputs'. But Brad pointed out that I think I used swagger to validate not wdltools, so this could be entirely my fault, as he said those may not be in sync / up to date wise? I wasn't aware. Anyways here was my situation in case it's at all helpful, the missing var is **File gender_mask_bed**:. ```; workflow GenomeStripBamWorkflow {; String sample_name; String bam_name; String analysis_directory; File bam; File ref_fasta; File ref_fasta_index; File ref_dict; File ref_genome_sizes; File ploidy_map; File copy_number_mask; File copy_number_mask_index; File read_depth_mask; File ref_profile; File genome_mask; File genome_mask_index; File configs. ##This is the call that had the issue, there are some before it that it depends on, ; ##but not for the missing input (gender_mask_bed); call CallSampleGender as CallSampleGender {; input:; analysis_directory = analysis_directory,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; genome_mask = genome_mask,; genome_mask_index = genome_mask_index,; ploidy_map = ploidy_map,; header_bam = ExtractBamSubset.header_bam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:3157,fault,fault,3157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,1,['fault'],['fault']
Availability,"chols. The workflow is a small test with everything in an S3 bucket:; ; https://github.com/bcbio/test_bcbio_cwl/tree/master/aws. I'm happy to report that I made good progress and have bcbio-vm using CloudFormation templates to setup the Cromwell batch ready AMI and AWS Batch requirements. I can then generate the right Cromwell AWS configuration and launch jobs to AWS batch. I see them get submitted, EC2 resources get spun up and jobs get queued and run. Awesome. When they're all ready and prepped to run, the instances fail with not finding the `cwl.inputs.json` file staged into the working directory:; ```; [2019-01-25 13:53:43,03] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:53:59,61] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Initializing to Running; [2019-01-25 13:58:25,58] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10alignment_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:39,11] [info] AwsBatchAsyncBackendJobExecutionActor [2c2e5a10prep_samples_to_rec:NA:1]: Status change from Running to Failed; [2019-01-25 13:58:40,06] [error] WorkflowManagerActor Workflow 2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 failed (during ExecutingWorkflowState): Job alignment_to_rec:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-alignment_to_rec/alignment_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586:1418,error,error,1418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586,1,['error'],['error']
Availability,"cific](https://github.com/broadinstitute/cromwell/blob/develop/filesystems/s3/src/main/scala/cromwell/filesystems/s3/batch/S3BatchIoCommand.scala) IoCommands but they are not effectively being treated in any specific way anywhere. They would need to be matched [here](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/IoActor.scala#L119) and processed subsequently in an S3 specific manner, like it's currently done for GCS, to be useful. Because this isn't the case now, the S3 code in `S3BatchIoCommand` is effectively never called.; - It works because there is an implementation of java nio for S3. The `S3BatchIoCommand` ends up in the [NioFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala) which uses the methods of the nio interface to execute the commands.; **A big issue is that the nio interface does not have a ""hash"" method**. To work around that, the `NioFlow` [streams down the content and md5s it](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L111), [unless told otherwise](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109). This is what's currently happening to S3 files. As a final twist, it turns out the [pattern match on GcsPath](https://github.com/broadinstitute/cromwell/blob/ec67d653c58c9c5b4b25609731a8082f3b540fe6/engine/src/main/scala/cromwell/engine/io/nio/NioFlow.scala#L109) in the hash method is actually just a fail safe but is not really needed. That is because some `IoCommand`s, including the `IoHashCommand`, are subclassed as `GcsBatchIoCommand`s and are processed through the [GcsBatchFlow](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/io/gcs/GcsBatchFlow.scala). The main goal of this ""flo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4463:1130,down,down,1130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4463,1,['down'],['down']
Availability,ckend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:200); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:13032,robust,robustExecuteOrRecover,13032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['robust'],['robustExecuteOrRecover']
Availability,ckend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:190); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:3094,robust,robustExecuteOrRecover,3094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['robust'],['robustExecuteOrRecover']
Availability,ckend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:2566,robust,robustExecuteOrRecover,2566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['robust'],['robustExecuteOrRecover']
Availability,"cker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; \""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\""; ""; """"""; kill-docker = ""scancel ${job_id}"". filesystems {; local {; localizat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:5676,echo,echo,5676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['echo'],['echo']
Availability,"ckgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3831,recover,recoverWith,3831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['recover'],['recoverWith']
Availability,"code byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in cas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674:1553,echo,echo,1553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674,1,['echo'],['echo']
Availability,"com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:2697,ERROR,ERROR,2697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['ERROR'],['ERROR']
Availability,compilation failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3791:12,failure,failure,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791,1,['failure'],['failure']
Availability,components-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:4197,down,downloading,4197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['down'],['downloading']
Availability,"configuration file used is as follows (edited to remove the main script):; ```; include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 300; runtime-attributes = """"""; Int cpu; Int memory_mb; String? lsf_queue; String? lsf_project; String? docker; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=fa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:1728,echo,echo,1728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['echo'],['echo']
Availability,"copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 601F8C709AA96517AA171B340CCA88BF, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,32] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.month_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.kinship_count' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.reported_sex' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.sex_aneuploidy' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.month_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:22:59,84] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.year_of_birth:-1:1-20000000028 [9e4f5894main.year_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:23829,failure,failures,23829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,cromwell ERROR: Finished parsing without consuming all tokens,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438:9,ERROR,ERROR,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438,1,['ERROR'],['ERROR']
Availability,cromwell fails to download url input file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977:18,down,download,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977,1,['down'],['download']
Availability,cromwell run on aws error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5861:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5861,1,['error'],['error']
Availability,"cromwell server fails with ""Error in bytecode""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:28,Error,Error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['Error'],['Error']
Availability,cromwell server start error with MySQL: Error searching for abort requests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,2,"['Error', 'error']","['Error', 'error']"
Availability,cromwell should retry importing subworkflow when there is temporary server error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3637:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3637,1,['error'],['error']
Availability,cromwell slows down in large scatters (against sge),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938:15,down,down,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938,1,['down'],['down']
Availability,"cromwell version: 29-675e865-SNAP. I have a `read_lines` call inside of a `scatter` definition like . ```; scatter (file in read_lines(shard_fofn)){; 		call SelectVariants{; 			input:; 				vcf=file,; 				intervals=interval_list; 		}; 	}; ```. where `shard fofn` has 20k+ lines inside of it. When running this workflow different shards failed ; with one of two errors. 30-50 different shards would fail each retry. ```; {; failures: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [ ],; message: ""Data received in non-data state: 6""; }; ],; message: ""Connection has been shutdown: javax.net.ssl.SSLProtocolException: Data received in non-data state: 6""; }; ],; message: ""Connection has been shutdown: javax.net.ssl.SSLProtocolException: Data received in non-data state: 6""; }; ],; message: ""All reopens failed""; }; ],; message: ""vcf""; }; ],; message: ""Input evaluation for Call mergeAndGetSites.SelectVariants failed.""; }; ],; message: ""Couldn't resolve all inputs for mergeAndGetSites.SelectVariants at index Some(778).""; }; ],; attempt: 1,; shardIndex: 778; },; ```; or . ```; {; failures: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [; {; causedBy: [ ],; message: ""Connection closed prematurely: bytesRead = 34066, Content-Length = 2974047""; }; ],; message: ""Connection closed prematurely: bytesRead = 34066, Content-Length = 2974047""; }; ],; message: ""All reopens failed""; }; ],; message: ""vcf""; }; ],; message: ""Input evaluation for Call mergeAndGetSites.SelectVariants failed.""; }; ],; message: ""Couldn't resolve all inputs for mergeAndGetSites.SelectVariants at index Some(19820).""; }; ],; attempt: 1,; shardIndex: 19820; }; ]; },; ```. When I take the `read_lines(shard_fofn)` and assign it to a variable and then pass that to the scatter everything works fine. Is this just bad luck or is there possibly a real issue here?. ```; 	Array[File] fofn_files = read_lines(shard_fofn). 	scatter (file",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2965:360,error,errors,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2965,2,"['error', 'failure']","['errors', 'failures']"
Availability,"cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; sync. ); mv /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: job id: bccmdfdd6o377kru9q6g; 2018-06-07 13:09:22,744 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:4897,echo,echo,4897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['echo'],['echo']
Availability,cromwell-system-akka.dispatchers.backend-dispatcher ERROR - Read timed out (java.net.SocketTimeoutException),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:52,ERROR,ERROR,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['ERROR'],['ERROR']
Availability,"cromwell-system-akka.dispatchers.service-dispatcher-19 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346:55,ERROR,ERROR,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Failed to summarize metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:54,ERROR,ERROR,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,1,['ERROR'],['ERROR']
Availability,"cs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:2527,down,down,2527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['down'],['down']
Availability,"cs/wf_options/Google.md) and [this doc](https://github.com/broadinstitute/cromwell/blob/develop/docs/backends/GCPBatch.md) ); My content of options.json is ; ```; {""google_labels"":{""workflow-run-execution-id"":""97fed6c4-6442-4efe-9e73-7b3592a33480""}} ; ```; and my command is `java -Dconfig.file=config -jar /app/cromwell.jar run wf.wdl -i input.json -o options.json`. The error I am getting:; <img width=""1409"" alt=""Screen Shot 2024-01-04 at 9 58 25 AM"" src=""https://github.com/broadinstitute/cromwell/assets/1992953/e559bccf-dd96-4dea-a48a-6649e448a26f"">. After adding string prefix to my own uuid as label value, the error was gone and my workflow ran smoothly. However, I do need to pass in the uuid so downstream analysis pipeline can still work. . Related code reporting error is [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L65) and error is produce [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) and [this is the regex definition](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L21). I am wondering can the [code](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) be updated to only check the key of a label so it aligns with GCP which does not have this restriction on label values? Or use another regex `""[a-z0-9]([-a-z0-9]*[a-z0-9])?""` for label value check?. Cromwell version: I built develop container, and the tag is `87-ee2b10f-SNAP`.; Backend: GCP Batch. Please let me know if this could be something y",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7351:1139,error,error,1139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7351,1,['error'],['error']
Availability,"ct2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-TumorCramToBam/1046545_23163_0_0.bam --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:1902,error,error,1902,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['error'],['error']
Availability,"ctor-e22c6324-5aec-4694-8750-f62160e2ca81; [2018-10-25 21:17:13,86] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:17:13,86] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-1/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12651,echo,echo,12651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"ctor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:07,54] [info] WorkflowManagerActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:5394,failure,failures,5394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['failure'],['failures']
Availability,"ctor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. To a novice user it is not clear whether that this can safely be ignored. This is a problem in particular if this first use actually fails for some other reason. The user will spend time trying to figure out if the problem is caused by a credential issue. I tried to see if I could easily suppress this by setting up ""backends"" to only include ""local"". I pulled down the [application.conf](https://github.com/broadinstitute/cromwell/blob/9f759a54a0b8873f1338f36391f985477d83475a/engine/src/main/resources/application.conf) which sets:. ```; backend {; // Either ""jes"", ""local"", or ""sge"" (case insensitive); defaultBackend = ""local""; // List of backends which this Cromwell supports. Be sure to include the defaultBackend!; backendsAllowed = [ ""local"" ]; ```. and then passed the file as:. `$ java -Dconfig.file=application.conf -jar cromwell.jar run hello.wdl hello.json; `; But the exception and warning were still raised. Should the ADC check be occurring even when backendsAllowed does not include JES?. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/705:5468,down,down,5468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/705,1,['down'],['down']
Availability,ctor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:2800,robust,robustExecuteOrRecover,2800,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,2,['robust'],['robustExecuteOrRecover']
Availability,"ctorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.jdbc.JdbcBackend$DatabaseFactoryDef@forConfig(String,Config,Driver):Database. database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://gdr-cromwell-uat-URL/uat_gdr_cromwell?rewriteBatchedStatements=true""; user = ""uat_gdr_cromwell""; password = ""<<cromwell_mysql_password>>""; connectionTimeout = 15000; }; }. system {; abort-jobs-on-terminate = false; max-concurrent-workflows = 1000; new-workflow-poll-rate = 2; max-workflow-launch-count = 50; }. # helpful links; # https://devhub.io/repos/broadinstitute-cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:2633,alive,alive,2633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['alive'],['alive']
Availability,ctorOf(ActorSystem.scala:745) at akka.testkit.TestKitBase.$init$(TestKit.scala:170) at akka.testkit.TestKit.<init>(TestKit.scala:896) at akka.testkit.TestProbe.<init>(TestKit.scala:954) at akka.testkit.TestProbe.<init>(TestKit.scala:956) at akka.testkit.TestProbe$.apply(TestKit.scala:990) at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:109) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:1526,Robust,RobustClientHelperSpec,1526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,"cutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:2103,echo,echo,2103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['echo'],['echo']
Availability,"d 1 workflows from the WorkflowStoreActor; 2018-01-17 20:38:36,970 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - MaterializeWorkflowDescriptorActor [UUID(53c058fc)]: Call-to-Backend assignments: SplitLargeRG.Alignment -> JES, SplitLargeRG.SamSplitter -> JES, SomaticPairedEndSingleSampleWorkflow.GetBwaVersion -> JES, SomaticPairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba -> JES, SplitLargeRG.GatherBamFiless -> JES, SplitLargeRG.SumSplitAlignedSizes -> JES; 2018-01-17 20:38:38,399 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-53c058fc-65db-4347-9433-cc0753614776 [UUID(53c058fc)]: Starting calls: SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1; 2018-01-17 20:38:41,234 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: `# not setting set -o pipefail here because /bwa has a rc=1 and we dont want to allow rc=1 to succeed because; # the sed may also fail with that error and that is something we actually want to fail on.; /usr/gitc/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //'`; 2018-01-17 20:38:49,163 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: job id: operations/ENrHrLeQLBiG_cWio8C8naYBIKngm4KOFSoPcHJvZHVjdGlvblF1ZXVl; 2018-01-17 20:39:01,789 cromwell-system-akka.dispatchers.backend-dispatcher-47 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from - to Running; 2018-01-17 20:41:59,809 cromwell-system-akka.dispatchers.backend-dispatcher-60 INFO - JesAsyncBackendJobExecutionActor [UUID(53c058fc)SomaticPairedEndSingleSampleWorkflow.GetBwaVersion:NA:1]: Status change from Running to Success; ```. Alternatively if I remove the scatter. ```; call GetBwaVersion. # Get the size of the s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:4098,error,error,4098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['error'],['error']
Availability,"d assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:2953,error,error,2953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['error'],['error']
Availability,"d benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 | grep -v cromwell_glob_control_file > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63. # create the glob control file that w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:16891,echo,echo,16891,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"d(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --constraint=""groups"" \; --qos=ded_reich \; --account=""reich"" \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. hashing-strategy: ""path"". check-sibling-md5: false; }; }; }. default-runtime-attributes {; failOnStderr: false; continueOnReturnCode: 0; }; }; }; }; }. services {; MetadataService {; }. Instrumentation {; }; HealthMonitor {; config {; }; }; LoadController {; config {; }; }; }. database {; driver = ""slick.jdbc.MySQLProfile$"". db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://database.host/callcachingdatabase?rewriteBatchedStatements=true""; user = ${USER}; password = ${MYSQL_DB_PW}; connectionTimeout = 5000; }. migration {; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:3606,alive,alive,3606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['alive'],['alive']
Availability,"d.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.Timers.aroundReceive(Timers.scala:44); 	at akka.actor.Timers.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-05-02 15:22:54,71] [[38;5;1merror[0m] bc4644da:batch_for_variantcall:-1:1: Hash error, disabling call caching for this job.; java.io.FileNotFoundException: Cannot hash file null because it can't be found; 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.usingStandardInitData$1(ConfigHashingStrategy.scala:46); 	at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:57); 	at cromwell.backend.impl.sfs.config.ConfigBackendFileHashingActor.customHashStrategy(ConfigBackendFileHashingActor.scala:26); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor$$anonfun$fileHashingReceive$1.applyOrElse(StandardFileHashingActor.scala:79); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.akka$actor$Timers$$super$aroundReceive(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:6709,error,error,6709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['error'],['error']
Availability,"d18a728dfc5 /cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/script; Hello Docker; +++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; ++ docker wait 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + rc=0; ++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; + docker rm 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + exit 0; ```. ## Workaround. I was able to suppress this error by disabling SE Linux on Fedora 36. But this is not an ideal way. Because I think SE Linux is enabled (Current mode: enforcing) as a default on RPM-based distributions such as Fedora, RHEL, CentOS stream and etc for security reasons. ```; $ sestatus ; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: enforcing; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. Disabled like this. ```; $ sudo setenforce 0. $ sestatus; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: permissive; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. I think you can try [SE Linux on Ubuntu](https://wiki.ubuntu.com/SELinux) to reproduce this error if you only have Debian-based Linux. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905:8598,error,error,8598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905,1,['error'],['error']
Availability,"d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting:/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting:z ubuntu@sha256:2d7ecc9c5e08953d586a6e50c29b91479a48f69ac1ba1f9dc0420d18a728dfc5 /cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/script; Hello Docker; +++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; ++ docker wait 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + rc=0; ++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; + docker rm 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + exit 0; ```. ## Workaround. I was able to suppress this error by disabling SE Linux on Fedora 36. But this is not an ideal way. Because I think SE Linux is enabled (Current mode: enforcing) as a default on RPM-based distributions such as Fedora, RHEL, CentOS stream and etc for security reasons. ```; $ sestatus ; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: enforcing; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. Disabled like this. ```; $ sudo setenforce 0. $ sestatus; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: permissive; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. I think you can try [SE L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905:7525,error,error,7525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905,1,['error'],['error']
Availability,"d; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,72] [info] SubWorkflowStoreActor stopped; [2022-12-15 21:28:53,72] [info] JobStoreActor stopped; [2022-12-15 21:28:53,72] [info] CallCacheWriteActor stopped; [2022-12-15 21:28:53,72] [info] IoProxy stopped; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49425,down,down,49425,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,12,['down'],['down']
Availability,"dBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Operation not permitted. 2：; ...; ...; amToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test.tmp/cromwell-workflow-logs/workflow.8fc94dc1-722b-40d5-9840-9d6e4a66db21.log: File name too long; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 32 common frames omitted. cromwell: v36.1; my working dir is: /nfs/disk3/user/gaoyuhui/github/test, has only 2.wdl and 2.json for test, but everytime this simple task is getting recursive and finally file name too long, and when change backend to local, it is the same. I found other topic and change to other dir to run this wdl task, got the same error.; so, can someone check about this? How can I goes well. it is a bug or my mistake??; Yours, sincerely!; Gao",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4703:3565,error,error,3565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703,1,['error'],['error']
Availability,"dOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser@4e879e3e Stack trace: org.semanticweb.owlapi.rdf.turtle.parser.ParseException: Encountered unexpected token: ""<"" <ERROR> at line 1, column 1. Was expecting one of: ""("" ""@base"" ""@prefix"" ""["" <EMPTY_BLANK_NODE> <FULLIRI> <NODEID> <PNAME_LN> <PNAME_NS> org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser.parse(TurtleOntologyParser.java:58) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IOR",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:2143,ERROR,ERROR,2143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['ERROR'],['ERROR']
Availability,dard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:749); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1139); 	 at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1131); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:215); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	 at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176); 	 at akka.actor.Actor.aroundReceive(Actor.scala:539); 	 at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	 at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:215); 	 at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	 at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	 at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	 at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	 at akka.dispatch.Mailbox.exec(Mailbo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:8320,robust,robustExecuteOrRecover,8320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['robust'],['robustExecuteOrRecover']
Availability,"database, run this with the JES backend (no inputs):. ```; task x {; String s; command {echo ${s}}; output {String t = read_string(stdout())}; runtime {docker: ""ubuntu:latest""}; }. workflow w {; call x {input: s=""foo""}; call x as y {input: s=x.t}; }; ```. call x runs fine. call y _should_ get a cache hit to call x, but instead:. ```; [2016-03-22 15:20:59,921] [info] WorkflowActor [54108c5b]: starting calls: w.y; [2016-03-22 15:20:59,921] [info] WorkflowActor [54108c5b]: persisting status of y to Starting.; [2016-03-22 15:21:00,344] [info] WorkflowActor [54108c5b]: Call Caching: Cache hit. Using 54108c5b:x as results for 54108c5b:y; [2016-03-22 15:21:00,351] [info] WorkflowActor [54108c5b]: inputs for call 'y':; s -> WdlString(foo); [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: created call actor for y.; [2016-03-22 15:21:00,352] [info] WorkflowActor [54108c5b]: persisting status of y to Running.; [2016-03-22 15:21:05,567] [error] CallActor [54108c5b:y]: Failing call: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; cromwell.util.AggregatedException: Item not found: cromwell-dev/w/54108c5b-29a7-430e-b16b-07bd8dba47e7/call-y/y-stdout.log; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112); at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124); at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:640); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:424); at cromwell.engine.backend.jes.JesBackend$$anonfun$useCachedCall$1.apply(JesBackend.scala:418); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/603:963,error,error,963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/603,1,['error'],['error']
Availability,dation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.f,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:3429,Error,ErrorOr,3429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:2209,down,down,2209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['down'],['down']
Availability,"db248e3bce81b54f5ef521878fe9e9de/; 2023/04/18 21:55:01 Delocalizing output /cromwell_root/glob-db248e3bce81b54f5ef521878fe9e9de.list -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/glob-db248e3bce81b54f5ef521878fe9e9de.list; 2023/04/18 21:55:03 Delocalizing output /cromwell_root/ SAMEA104027315_pull_results.txt -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/ SAMEA104027315_pull_results.txt; 2023/04/18 21:55:04 Delocalizing output /cromwell_root/SAMEA104027315.tar -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/SAMEA104027315.tar; 2023/04/18 21:55:04 Delocalization script execution complete.; 2023/04/18 21:55:05 Done delocalization.; ```. In Job Manager, an error with the outputs can be seen. <img width=""1115"" alt=""job outputs"" src=""https://user-images.githubusercontent.com/27784612/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:3587,error,error,3587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['error'],['error']
Availability,"de the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzcToI7XtZ0zlOv7NFtVwJrGZaKdks5rYkvxgaJpZM4L1qD1>; > .; >. ---. @kcibul commented on [Fri Feb 03 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277235590). Hey guys -- just want to understand more how this is blocking mutect (ie there's no other way to do this?). Can you explain?. @cjllanwarne -- if you're on bug rotation, could you try to parse this into the separate problems described here and make the appropriate tickets? Seems like there's a problem with scattering over Array[Array[Int]] as described in the last paragraph that has nothing to do with optionals. Then there might be a problem with optionals/conditionals as well but",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:1871,Ping,Pinging,1871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['Ping'],['Pinging']
Availability,"de)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defined() check still is not enough like is the case for setting not_optional_error_code, then that should be c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2320,error,errorcode,2320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,def.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing outpu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3416,down,downloading,3416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['down'],['downloading']
Availability,"dful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:1246,Error,Error,1246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,1,['Error'],['Error']
Availability,"dinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. ## Copyright Broad Institute, 2017; ##; ## This WDL workflow runs GATK4 Mutect 2 on a single tumor-normal pair or on a single tumor sample,; ## and performs additional filtering and functional annotation tasks.; ##; ## Main requirements/expectations :; ## - One analysis-ready BAM file (and its index) for each sample; ##; ## Description of inputs:; ##; ## ** Runtime **; ## gatk_docker: docker image to use for GATK 4 Mutect2; ## preemptible: how many preemptions to tolerate before switching to a non-preemptible machine (on Google); ## max_retries: how many times to retry failed tasks -- very important on the cloud when there are transient errors; ## gatk_override: (optional) local file or Google bucket path to a GATK 4 java jar file to be used instead of the GATK 4 jar; ## in the docker image. This must be supplied when running in an environment that does not support docker; ## (e.g. SGE cluster on a Broad on-prem VM); ##; ## ** Workflow options **; ## intervals: genomic intervals (will be used for scatter); ## scatter_count: number of parallel jobs to generate when scattering over intervals; ## m2_extra_args, m2_extra_filtering_args: additional arguments for Mutect2 calling and filtering (optional); ## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional); ## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.; ##; ## ** Primary inputs **; ## ref",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:2019,toler,tolerate,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,2,"['error', 'toler']","['errors', 'tolerate']"
Availability,"dinstitute/cromwell/files/901057/test_read_data.wdl.txt); [test_write_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901056/test_write_data.wdl.txt). When I tested these I noticed that I can read and write a single Boolean value, and I can write complex types with Booleans, but reading Array[Boolean] didn't work and neither did reading any Map[Boolean,?] or Map[?,Boolean]. . **What is the best way to read in complex types that involve Boolean?**. The error I got when trying to read Array[Boolean] looks like this:. > Error: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. The errors I would get for the complex types involving Boolean look like this:. > Error: Failed to coerce one or more keys or values for creating a Map[String, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. > Error: Failed to coerce one or more keys or values for creating a Map[Int, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. Here are the files generated by test_write_data.wdl that I can read back in with test_read_data.wdl:. [a_false.txt](https://github.com/broadinstitute/cromwell/files/901058/a_false.txt); [a_float.txt](https://github.com/broadinstitute/cromwell/files/901061/a_float.txt); [a_string.txt](https://github.com/broadinstitute/cromwell/files/901063/a_string.txt); [a_true.txt](https://github.com/broadinstitute/cromwell/files/901062/a_true.txt); [a_zero.txt](https://github.com/broadinstitute/cromwell/files/901059/a_zero.txt); [arr_float.txt](https://github.com/broadinstitute/cromwell/files/901060/arr_float.txt); [arr_int.txt](https://github.com/broadinstitute/cromwell/files/901064/arr_int.txt); [arr_string.txt](https://github.com/broadinstitute/cromwell/files/901065/arr_string.txt); [map_float_float.txt](https://github.co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152:1263,Error,Error,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152,1,['Error'],['Error']
Availability,"disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8048,down,down,8048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['down'],['down']
Availability,docker hash error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7222:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222,1,['error'],['error']
Availability,"duplicatereads ${keep_duplicate_reads} --input ${input_bam} --reference ${ref_fasta} \\\n --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \\\n --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \\\n --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \\\n else touch ${entity_id}.coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n }\n\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Calculate coverage on Whole Genome Sequence using Spark.\n# This task automatically creates a target output file.\ntask WholeGenomeCoverage {\n String entity_id\n File coverage_file \n File target_file\n File input_bam\n File bam_idx\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n String gatk_jar\n Boolean isWGS\n Int wgsBinSize\n Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:37577,down,downstream,37577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,['down'],['downstream']
Availability,"e a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:1520,failure,failure-mode,1520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['failure'],['failure-mode']
Availability,"e cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1049,ERROR,ERROR,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['ERROR'],['ERROR']
Availability,"e don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1685,echo,echo,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,2,['echo'],['echo']
Availability,"e it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 2> /dev/null ) || ( ln /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*report.html /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 2> /dev/null ) || ( ln /cromwell_root/*report.html /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 | grep -v cromwell_glob_control_file > /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-a678e8cb2ce368b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:6285,echo,echo,6285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"e output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095284). I think `Pair`s are declared with parenthesis and not brackets. Does . ```; output {; Pair[File, Array[File]] p = (unmapped_bam, fastqs); }; ```. work ?. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132). Also, as long as you don't declared the zip as a workflow variable you should be fine. For example, this should work:. ```; task t {; command {; echo ""hello""; echo ""world""; }; output {; Array[String] o = read_lines(stdout()); }; }. task t2 {; Array[Pair[String, String]] p; command {; #do something; }; output {; Array[Pair[String, String]] o = p; }; }. workflow w {; call t; call t as u; call t2 { input: zip(t.o, u.o) }; }; ```. ---. @meganshand commented on [Thu Nov 17 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261247751). Pair with ( ) worked perfectly with the workaround tasks. I tried to not declare a workflow variable, but since I need to scatter over the zipped array I did the following:. `scatter(unmapped_pair in zip(QuerySortSam.sorted_bam, SamToFastq.fastqs)){`. Which gave me the error:. `Workflow input processing failed.; Unable to load namespace from workflow: Unrecognized token on line 282, column 55:`. Regardless, for my current use case I'm happy with my workaround, and using Pair seems to be working! Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:2446,echo,echo,2446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,3,"['echo', 'error']","['echo', 'error']"
Availability,"e parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array[String]? strings1; Array[String]? strings2. Array[Pair[String,String]] string_pair = zip(strings1,strings2); ```; Error:; ```; [2018-10-08 13:31:20,27] [error] WorkflowManagerActor Workflow 832af5bf-2c7e-4e4a-80c5-bc0787910477 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of equal length.; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:2116,Error,Error,2116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,2,"['Error', 'error']","['Error', 'error']"
Availability,"e the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-cromwell-genomics/cromwell-execution""; auth = ""default"". numSubmitAttempts = 3; numCreateDefinitionAttempts = 3. concurrent-job-limit = 100. default-runtime-attributes {; queueArn: ""arn:aws:batch:us-east-1:66:job-queue/GenomicsDefaultQueue""; }. filesystems {; s3 {; auth = ""default""; }; }; }; }; }; }. system {; job-rate-control {; jobs = 1; per = 1 second; }; }; ```. Would appreciate help on this.; I wonder if cromwell was ever te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:2323,echo,echo,2323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['echo'],['echo']
Availability,"e to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses fro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:3679,error,error,3679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,"['error', 'failure']","['error', 'failure']"
Availability,e we trying to upload an auth file when running in application default auth mode for both genomics and filesystems?. ```; [ERROR] [01/27/2017 14:39:36.100] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 732474fd-88b0-4a5e-ad19-5ee5cd71d141 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:81); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1$$anonfun$apply$1.applyOrElse(JesInitializationActor.scala:80); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1924:1033,recover,recoverWith,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1924,1,['recover'],['recoverWith']
Availability,"e.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:3812,ERROR,ERROR,3812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['ERROR'],['ERROR']
Availability,"e/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/CommandLineTool.scala:45: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(taskDefinition.validNelCheck, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m/home/travis/build/broadinstitute/cromwell/cwl/src/main/scala/cwl/Workflow.scala:30: value buildWomExecutable is not a member of object cwl.CwlExecutableValidation[0m; [0m[[31merror[0m] [0m CwlExecutableValidation.buildWomExecutable(womDefinition, inputFile)[0m; [0m[[31merror[0m] [0m ^[0m; [0m[[31merror[0m] [0mtwo errors found[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mdoc[0m) Scaladoc generation failed[0m; [0m[[31merror[0m] [0m(cwl/compile:[31mcompileIncremental[0m) Compilation failed[0m; [0m[[31merror[0m] [0mTotal time: 273 s, completed Oct 25, 2017 1:03:49 PM[0m. restoring stty: 500:5:bf:8a3b:3:1c:7f:15:4:0:1:0:11:13:1a:0:12:f:17:16:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0:0. travis_time:end:1f45f95e:start=1508935437087656153,finish=1508936629752201761,duration=1192664545608; [0Ktravis_fold:end:after_success; [0K[33;1mSkipping a deployment with the script provider because this is not a tagged commit[0m. Done. Your bu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:1683,error,errors,1683,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['error'],['errors']
Availability,"e/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be removed in a future release. sbt version in this project: 	1.5.5; sbt script version: 1.7.1; ```. <!-- Paste/A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:1344,error,error,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['error']
Availability,"e0f-ab6e-2aeb7e96c760/call-Mutect2_First_Filter/shard-1/JL027_Tumor-JL027_Normal.mutect2.oncefiltered.vcf.gz \; -O JL027_Tumor-JL027_Normal.mutect2.twicefiltered.vcf.gz \; -P /cromwell_root/s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt \; -R /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta \; -L /cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/SureSelect.hg19.regions.v5.interval_list \; -AM G/T -AM C/T \; ```. All the input files that were mentioned in the above command are on s3 and as you can see from the command line from above that cromwell correctly localized those. However, cromwell is failing to create a job with the following error:. ```; [2018-11-02 17:24:33,44] [error] Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; java.lang.Exception: Absolute path /s4-somaticgenomicsrd-valinor/JL027/Tigris-1.1.0.dev1/tigris_workflow/5c8ee2ab-f1bd-4c6c-ad0b-4af7b52d29f1/call-SomaticSNVInDel/vc.SomaticSNVInDel/1651349b-2144-4e0f-ab6e-2aeb7e96c760/call-CollectSequencingArtifactMetrics/shard-1/JL027_Tumor.dedup.recal.artifactmetrics.pre_adapter_detail_metrics.txt doesn't appear to be under any mount points: local-disk /cromwell_root; at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.relativePathAndVolume(AwsBatchAsyncBackendJobExecutionActor.scala:236); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.generateA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356:1622,error,error,1622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356,1,['error'],['error']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-106/cacheCopy/SR00c.NA12340.txt.gz.tbi; 1608597085601,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-32/cacheCopy/SR00c.HG01572.txt.gz; 1608597087902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz.tbi; 1608597089027,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-16/cacheCopy/SR00c.HG00625.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:40318,down,download,40318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-109/cacheCopy/SR00c.NA18499.txt.gz.tbi; 1608597136464,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz; 1608597138537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-118/cacheCopy/SR00c.NA18941.txt.gz.tbi; 1608597141037,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:54661,down,download,54661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-110/cacheCopy/SR00c.NA18507.txt.gz.tbi; 1608597115454,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-66/cacheCopy/SR00c.HG02620.txt.gz; 1608597116706,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-132/cacheCopy/SR00c.NA19449.txt.gz.tbi; 1608597118429,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:48417,down,download,48417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-12/cacheCopy/SR00c.HG00410.txt.gz.tbi; 1608597418094,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-104/cacheCopy/SR00c.NA10847.txt.gz; 1608597419738,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz.tbi; 1608597421803,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:130715,down,download,130715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-123/cacheCopy/SR00c.NA19035.txt.gz.tbi; 1608597424089,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-25/cacheCopy/SR00c.HG01344.txt.gz; 1608597426322,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz.tbi; 1608597428681,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-100/cacheCopy/SR00c.HG04158.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:132588,down,download,132588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-124/cacheCopy/SR00c.NA19062.txt.gz.tbi; 1608597222742,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-86/cacheCopy/SR00c.HG03649.txt.gz; 1608597227140,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-154/cacheCopy/SR00c.NA21102.txt.gz; 1608597229301,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-62/cacheCopy/SR00c.HG02491.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:78339,down,download,78339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-14/cacheCopy/SR00c.HG00557.txt.gz.tbi; 1608597245149,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-144/cacheCopy/SR00c.NA20346.txt.gz; 1608597247776,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-88/cacheCopy/SR00c.HG03694.txt.gz; 1608597249388,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-141/cacheCopy/SR00c.NA20126.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:83940,down,download,83940,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-153/cacheCopy/SR00c.NA20895.txt.gz.tbi; 1608597527253,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-43/cacheCopy/SR00c.HG01958.txt.gz; 1608597529228,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-112/cacheCopy/SR00c.NA18539.txt.gz.tbi; 1608597531104,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-21/cacheCopy/SR00c.HG01085.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:163356,down,download,163356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-155/cacheCopy/SR00c.NA21122.txt.gz.tbi; 1608597402775,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-68/cacheCopy/SR00c.HG02648.txt.gz; 1608597404588,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-117/cacheCopy/SR00c.NA18923.txt.gz.tbi; 1608597406434,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-48/cacheCopy/SR00c.HG02020.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:126340,down,download,126340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-23/cacheCopy/SR00c.HG01275.txt.gz.tbi; 1608597272885,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-107/cacheCopy/SR00c.NA12489.txt.gz; 1608597275740,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-33/cacheCopy/SR00c.HG01607.txt.gz; 1608597277147,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-93/cacheCopy/SR00c.HG03756.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:91426,down,download,91426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz.tbi; 1608597478676,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-136/cacheCopy/SR00c.NA19684.txt.gz; 1608597480242,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi; 1608597482359,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:148814,down,download,148814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz; 1608597179945,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-0/cacheCopy/SR00c.NA12878.txt.gz.tbi; 1608597182124,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-36/cacheCopy/SR00c.HG01790.txt.gz; 1608597185060,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-1/cacheCopy/SR00c.HG00096.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:66494,down,download,66494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-72/cacheCopy/SR00c.HG03007.txt.gz.tbi; 1608597033701,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-114/cacheCopy/SR00c.NA18553.txt.gz; 1608597036753,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-85/cacheCopy/SR00c.HG03604.txt.gz; 1608597038611,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-64/cacheCopy/SR00c.HG02588.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:25973,down,download,25973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-98/cacheCopy/SR00c.HG03888.txt.gz.tbi; 1608597306259,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-139/cacheCopy/SR00c.NA19818.txt.gz; 1608597308537,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-35/cacheCopy/SR00c.HG01747.txt.gz.tbi; 1608597310848,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-76/cacheCopy/SR00c.HG03100.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4ea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:100148,down,download,100148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"e; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:3217,alive,alive,3217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['alive'],['alive']
Availability,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34633,Error,Error,34633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"eSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script; 1608597484342,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-133/cacheCopy/SR00c.NA19661.txt.gz.tbi; 1608597487788,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-150/cacheCopy/SR00c.NA20802.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:150643,down,download,150643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"eSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:1542,error,error,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"e_mafs:NA:1] Status change from - to SharedFileSystemRunStatus(false); [2017-01-20 09:33:07,55] [info] BackgroundConfigAsyncJobExecutionActor [814c47aaaggregate_mafs_workflow.aggregate_mafs:NA:1]: BackgroundConfigAsyncJobExecutionActor [814c47aa:aggregate_mafs_workflow.aggregate_mafs:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true); [2017-01-20 09:33:07,58] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-910401033] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-814c47aa-9d11-4c81-a08c-f2b77c002b46#617869376] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-01-20 09:33:07,58] [error] WorkflowManagerActor Workflow 814c47aa-9d11-4c81-a08c-f2b77c002b46 failed (during ExecutingWorkflowState): Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; java.lang.RuntimeException: Call aggregate_mafs_workflow.aggregate_mafs:NA:1: return code was 1; 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handleExecutionResult(StandardAsyncExecutionActor.scala:432); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handleExecutionResult(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:370); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:333); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$2.apply(StandardAsyncExecutionActor.scala:332); 	at scala.util.S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:13043,error,error,13043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['error'],['error']
Availability,"e` would interpolate as relativized file paths. ```; task files {; File file ; File? maybe_file. command {; echo file: ${file} maybe_file: ${maybe_file} ${""file with concatenation: "" + file} ${""maybe_file with concatenation: "" + maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call files { input: file = file, maybe_file = file }; }; ```. On JES with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c, the ""maybe_file with concatenation"" leaves a residual `gs://` path; all other interpolations are correctly relativized. On AWS with Cromwell 9341a4dac6145233f2a33b092a8fc443c18744ea, both concatenations leave residual `s3://` paths. On Local with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c and an input file under my home directory, this throws an exception with the following trace:. ```; 2017-02-02 11:55:36,701 cromwell-system-akka.dispatchers.backend-dispatcher-44 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(5fdb357a)w.files:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:55); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); 	at wdl4s.Tas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:1122,Error,Error,1122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['Error'],['Error']
Availability,"each(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at cromwell.engine.workflow.WorkflowActor.cromwell$engine$workflow$WorkflowActor$$startRunnableCalls(WorkflowActor.scala:881); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:532); at cromwell.engine.workflow.WorkflowActor$$anonfun$2.applyOrElse(WorkflowActor.scala:467); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:604); at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:236); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:734); at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:236); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:598); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:592); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.WorkflowActor.aroundReceive(WorkflowActor.scala:236); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke_aroundBody0(ActorCell.scala:487); at akka.actor.ActorCell$AjcClosure1.run(ActorCell.scala:1); at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149); at akka.kamon.instrumentation.ActorCellInstrumentation$$anonfun$aroundBehaviourInvoke$1.apply(ActorCellInstrumentation.scala:63); 2016-04-11 22:41:17,520 cromwell-system-akka.actor.default-dispatcher-2661 ERROR - Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:12482,ERROR,ERROR,12482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,4,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"eafc←[0m]: starting calls: test.hello; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Starting.; [2016-08-08 08:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [←[38;5;2m4e20eafc←[0m:hello]: ←[38;5;5mecho 'Hello String!'←[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:7835,error,error,7835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['error'],['error']
Availability,"eam which types we can serialize and de-serialize using the available library functions. I wrote a pair of WDLs, one to write data to files and another to read data from files:. [test_read_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901057/test_read_data.wdl.txt); [test_write_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901056/test_write_data.wdl.txt). When I tested these I noticed that I can read and write a single Boolean value, and I can write complex types with Booleans, but reading Array[Boolean] didn't work and neither did reading any Map[Boolean,?] or Map[?,Boolean]. . **What is the best way to read in complex types that involve Boolean?**. The error I got when trying to read Array[Boolean] looks like this:. > Error: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. The errors I would get for the complex types involving Boolean look like this:. > Error: Failed to coerce one or more keys or values for creating a Map[String, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. > Error: Failed to coerce one or more keys or values for creating a Map[Int, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. Here are the files generated by test_write_data.wdl that I can read back in with test_read_data.wdl:. [a_false.txt](https://github.com/broadinstitute/cromwell/files/901058/a_false.txt); [a_float.txt](https://github.com/broadinstitute/cromwell/files/901061/a_float.txt); [a_string.txt](https://github.com/broadinstitute/cromwell/files/901063/a_string.txt); [a_true.txt](https://github.com/broadinstitute/cromwell/files/901062/a_true.txt); [a_zero.txt](https://github.com/broadinstitute/cromwell/files/901059/a_zero.txt); [arr_float.txt](https://github.com/broadinstitute/cromwell/files/901060/ar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152:1035,Error,Error,1035,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152,1,['Error'],['Error']
Availability,"ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; Array[String] previous. command{; echo ${sep=' ' previous} "" meuman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```. Here is the cromwell-conf:; ```; // aws.conf; include required(classpath(""application"")). webservice {; port = 8001; interface = 0.0.0.0; }. aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine {; filesystems {; s3 { auth = ""default"" }; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; root = ""s3://nrglab-c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:1912,echo,echo,1912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['echo'],['echo']
Availability,"ecipient.fasta"": ""../../data/ref_genomes/pseudomonas.fasta"",; }; ```. **task definition:**; ```; version development; task n {; input {; File fasta ; Directory blastdb; String out_file = ""~{basename(fasta)}.blast""; }; command <<<; export BLASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permitted; [2020-08-08 19:20:00,49] [error] 6e4fa8a1:main.blaster:46:1: Hash error (Is a directory), disabling call caching for this job.; ```. **contents of the BLASTDB directory:**; ```; /data/blast/blastdb$ ls; nt.00.nhd nt.01.nhd nt.02.nhd nt.03.nhd nt.04.nhd nt.05.nhd nt.06.nhd nt.07.nhd nt.08.nhd nt.09.nhd nt.10.nhd nt.11.nhd nt.12.nhd nt.13.nhd nt.14.nhd nt.15.nhd nt.16.nhd nt.17.nhd nt.18.nhd nt.19.nhd nt.20.nhd nt.21.nhd nt.22.nhd nt.23.nhd nt.24.nhd nt.nal nt.00.nhi nt.01.nhi nt.02.nhi nt.03.nhi nt.04.nhi nt.05.nhi nt.06.nhi nt.07.nhi nt.08.nhi nt.09.nhi nt.10.nhi nt.11.nhi nt.12.nhi nt.13.nhi nt.14.nhi nt.15.nhi nt.16.nhi nt.17.nhi nt.18.nhi nt.19.nhi nt.20.nhi nt.21.nhi nt.22.nhi nt.23.nhi nt.24.nhi nt.ndb nt.00.nhr nt.01.nhr nt.02.nhr nt.03.nhr nt.04.nhr nt.05.nhr nt.06.nhr nt.07.nhr nt.08.nhr nt.09.nhr nt.10.nhr nt.11.nhr nt.12.nhr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:2033,error,error,2033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,1,['error'],['error']
Availability,"ecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); ... 5 more; ```. Basically, it seems that the AWS parser for the disk specification doesn't understand specs in the form `'local-disk 100 HDD'`. This needs to be fixed, since the Broad pipelines won't run without heavy modification otherwise. I've tracked down the file spec parsing code to https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/io/AwsBatchVolume.scala#L52-L74, but I'm not informed enough about Scala to write a PR for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:3701,down,down,3701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['down'],['down']
Availability,"ecution/stderr,Some(FILE)); Output(Some(commandScript),Some(wf_hello.hello.commandScript),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE)). 2018-06-07 13:09:17,684 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES inputs (found 1):; Input(Some(commandScript),Some(wf_hello.hello.commandScript),None,/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:4200,echo,echo,4200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['echo'],['echo']
Availability,ecutionActor.scala:74); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:2311,robust,robustExecuteOrRecover,2311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['robust'],['robustExecuteOrRecover']
Availability,"ecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }. gustily ls gs://cromwell-executions/MemoryRetryTest/d54a5a39-4d3b-4ac7-9bb1-97043d761b56/call-TestOutOfMemoryRetry; TestOutOfMemoryRetry.log; gcs_delocalization.sh; gcs_localization.sh; gcs_transfer.sh; rc; script; stderr; stdout; pipelines-logs. stderr:; Killed; /cromwell_root/script: line 32: 17 Killed tail /dev/zero. rc:; 137",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:2363,error,error-keys,2363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,4,"['Error', 'error']","['Error', 'error-keys']"
Availability,"ed for containers. - If I select the `path+modtime` hashing strategy, only the first task in a workflow will succeed, as the hard-link duplication strategy will cause the path ""absolute"" be different (causing a hash differential). ## Questions. - ~What defines a cache hit, or exactly which information is used to the call hash?~; > I'll answer this one myself, by looking at the metadata returned from `/api/workflows/{version}/{id}/metadata`, within the `calls.$yourstepname.callCaching`, the hashes field has the following attributes:; > - `output count`; > - `runtime attribute`; > - `output expression`; > - `input count`; > - `backend name`; > - `command template`; > - `input`. - ~When does the command section get hashed (before or after replacements)?~; > The template gets cached. - ~What other elements go into the building the cache?~; > output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:3157,down,downsides,3157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,1,['down'],['downsides']
Availability,"ed to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Number of workers to assign to PAPI requests; request-workers = 3. genomics {; # A reference to an auth defined in the `google` stanza at the top.; # This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:2704,avail,available,2704,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['avail'],['available']
Availability,"ed_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8280,echo,echo,8280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['echo'],['echo']
Availability,"edback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi,. I recently encountered an issue on running Cromwell jobs on AWS Batch. In brief, all of my testing WDL jobs failed with the following error message:. ```; 2021-09-23 00:08:18,813 INFO - Submitting taskId: cumulus.cluster-None-1, job definition : arn:aws:batch:us-west-2:752311211819:job-definition/cromwell_quay_io_cumulus_cumulus_1_4_377407181fbea1f33a22931df258b16d20d4c6ab3:1, script: s3://gred-cumulus-dev/scripts/c157137e2097795846ae1f4069ccd7a2; 2021-09-23 00:08:20,269 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: job id: 12836c6a-6d1a-4429-bb86-68b8f9883acf; 2021-09-23 00:08:20,287 cromwell-system-akka.dispatchers.backend-dispatcher-118 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from - to Initializing; 2021-09-23 00:12:17,120 cromwell-system-akka.dispatchers.backend-dispatcher-136 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(cf00212c)cumulus.cluster:NA:1]: Status change from Initializing to Running; 2021-09-23 00:14:08,015 cromwell-system-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504:1074,error,error,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504,1,['error'],['error']
Availability,"ee [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:998,error,error,998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['error'],['error']
Availability,efinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:3372,Error,ErrorOr,3372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"eg:; ```; workflow foo {; call a { input: x=5, y=10 }; }. task a {; input { Int x }; }; ```; This should be an error: `y` is not an input to `a`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3497:111,error,error,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3497,1,['error'],['error']
Availability,"eg; ```; task foo {; String? bar; command {; echo ""result: ${default=""d"" bar}""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2636:45,echo,echo,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2636,1,['echo'],['echo']
Availability,"eive$(Actor.scala:512); Mar 09 00:55:25 web start-cromwell.sh[110916]: at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.actor.ActorCell.invoke(ActorCell.scala:496); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.run(Mailbox.scala:224); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.Mailbox.exec(Mailbox.scala:234); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); Mar 09 00:55:25 web start-cromwell.sh[110916]: at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; the minimal workflow that caused this (although not all 100% of the time) is:; ```; workflow quantification_broken {. File batch; File references; File samples_folder #folder provided as File, before now it worked perfectly. Boolean keep_sra = true; Int threads. call prepare_samples {; input: samples = batch, references = references, samples_folder = samples_folder; }; }. task prepare_samples {; File samples; File references; File samples_folder. command {; echo ""whatever command you want""; }. output {; File invalid = ""invalid.tsv""; File novel = ""novel.tsv""; File cached = ""cached.tsv""; }; }; ```; I understand that there is plan to introduce Directory type in the future. But it may take long ago and it is better not to break current workflows where people have to use File for Directory inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3383:5430,echo,echo,5430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3383,1,['echo'],['echo']
Availability,"ell on OS X with Java 8. Any ideas on how to troubleshoot the below error?. Thanks!. ```; › cromwell run hello.wdl hello.json; [2015-12-18 08:43:13,222] [info] Slf4jLogger started; [2015-12-18 08:43:13,335] [info] Default backend: LOCAL; [2015-12-18 08:43:13,335] [info] RUN sub-command; [2015-12-18 08:43:13,336] [info] WDL file: hello.wdl; [2015-12-18 08:43:13,439] [info] Inputs: hello.json; [2015-12-18 08:43:13,560] [info] input: test.hello.name => ""world""; [2015-12-18 08:43:13,776] [info] SingleWorkflowRunnerActor: launching workflow; [2015-12-18 08:43:15,936] [info] Running with database db.url = jdbc:hsqldb:mem:86473284-494c-43d2-94fd-d00107a2a787;shutdown=false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1016,error,error,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['error'],['error']
Availability,"ell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:16419,echo,echo,16419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,"elog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. labels: test-library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:3529,down,down,3529,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['down'],['down']
Availability,"elpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:2225,error,errors,2225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['error'],['errors']
Availability,emented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1497,recover,recoverAsync,1497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Availability,empting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1342,recover,recoverAsync,1342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Availability,"ence to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses fro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13325,error,error,13325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,"['error', 'failure']","['error', 'failure']"
Availability,"encies.zip; ```. and it succeeds, i.e. logging ends with; ```; [2016-12-20 13:24:52,24] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""main_output"": ""Hello sub world!""; },; ""id"": ""8a414771-fdef-43f8-85f3-a0b738fd28fc""; }; ```. but when I run in server mode with the exact same jar and WDL files submitted via REST API:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@main.wdl -FwdlDependencies=dependecies.zip; ```. it fails:; ```; 2016-12-20 13:28:19,319 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-12-20 13:28:19,365 cromwell-system-akka.dispatchers.engine-dispatcher-22 ERROR - WorkflowManagerActor Workflow cbde1e8b-00df-405e-be19-ac4f4b3ea5b8 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; error in opening zip file; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; error in opening zip file; 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:138); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.arou",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1799:1467,error,error,1467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1799,1,['error'],['error']
Availability,"ent$(JdbcBackend.scala:376); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedInsertStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:640); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:508); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); 2017-07-13 22:07:39,149 cromwell-system-akka.actor.default-dispatcher-230 ERROR - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:3612,ERROR,ERROR,3612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"entos/cromwell-executions/helloHaplotypeCaller/bf90a37b-6ffa-4122-a12c-24aced32f3b6/call-haplotypeCaller/inputs/home/centos/storage/WDLdata/bams/mother.bam \; -o NA12878.raw.indels.snps.vcf; [2017-10-04 06:06:58,15] [info] BackgroundConfigAsyncJobExecutionActor [bf90a37bhelloHaplotypeCaller.haplotypeCaller:NA:1]: executing: /bin/bash /home/centos/cromwell-executions/helloHaplotypeCaller/bf90a37b-6ffa-4122-a12c-24aced32f3b6/call-haplotypeCaller/execution/script; [2017-10-04 06:06:58,21] [info] BackgroundConfigAsyncJobExecutionActor [bf90a37bhelloHaplotypeCaller.haplotypeCaller:NA:1]: job id: 13026; [2017-10-04 06:06:58,21] [info] BackgroundConfigAsyncJobExecutionActor [bf90a37bhelloHaplotypeCaller.haplotypeCaller:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-10-04 06:07:31,28] [info] BackgroundConfigAsyncJobExecutionActor [bf90a37bhelloHaplotypeCaller.haplotypeCaller:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-10-04 06:07:31,37] [error] WorkflowManagerActor Workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6 failed (during ExecutingWorkflowState): Job helloHaplotypeCaller.haplotypeCaller:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /home/centos/cromwell-executions/helloHaplotypeCaller/bf90a37b-6ffa-4122-a12c-24aced32f3b6/call-haplotypeCaller/execution/stderr; [2017-10-04 06:07:31,37] [info] WorkflowManagerActor WorkflowActor-bf90a37b-6ffa-4122-a12c-24aced32f3b6 is in a terminal state: WorkflowFailedState; [2017-10-04 06:07:35,37] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-10-04 06:07:35,41] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RetrieveNewWorkflows$] without sender to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor#-1816723107] was not delivered. [1] dead letters encountered. Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2673:5569,error,error,5569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2673,1,['error'],['error']
Availability,"ep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$kb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitializationActor-7218c3a1-5155-4921-9adb-d96c52c32200] State is now terminal. Shutting down.; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.544] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$mb] $$mb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:2640,down,down,2640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"eparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:182); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1809,error,error,1809,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,"er (aka real-life) workflows. . For example: if node a has two outputs, which are inputs of b, then b.hashCode calls a.hasCode twice. If b produces two outputs which are inputsof c, then c.hashCode calls b.hashCode twice, and a.hashCode is called four times. If c produces two outputs which are inputs of d, d.hashCode calls c.hashCode twice, b.hashCode four times and a.hashCode eight times, etc. . ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334557027). `eq` is object equality. `equals` and `==` are the same, and there is no general rule that they need to be component-based (""bitwise""). On the contrary, `equals`/`==` should not be component-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:2546,down,down,2546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,"erAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:4:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> liver;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109705), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra37/SRR/003036/SRR3109705), WomString(name) -> WomString(GSM2042593), WomString(gsm) -> WomString(GSM2042593), WomString(title) -> WomString(cattle_liver_1))), WomString(run) -> WomString(SRR3109705), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:3932,failure,failure,3932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,2,"['down', 'failure']","['download', 'failure']"
Availability,"erAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:5:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> kidney;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109708), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra38/SRR/003036/SRR3109708), WomString(name) -> WomString(GSM2042596), WomString(gsm) -> WomString(GSM2042596), WomString(title) -> WomString(cattle_kidney_1))), WomString(run) -> WomString(SRR3109708), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:5656,failure,failure,5656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,2,"['down', 'failure']","['download', 'failure']"
Availability,"ere are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:1200,error,error,1200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,1,['error'],['error']
Availability,"erence `pbs_email` inside an expression in the `submit` definition like so:; ```; submit = """"""; qsub ${""-me ea -M"" + pbs_email}; """"""; ```; so that if `pbs_email` isn't supplied, neither are the preceding option flags (as documented in `Optional Parameters and Type Constraints -> Prepending a String to an Optional Parameter` at https://software.broadinstitute.org/wdl/devzone.php). This doesn't work anymore in 23:; ```; [ERROR] [12/09/2016 11:09:29.763] [cromwell-system-akka.dispatchers.backend-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-71ccb54a-f88c-49b2-aeee-1af92749e337/WorkflowExecutionActor-71ccb54a-f88c-49b2-aeee-1af92749e337/71ccb54a-f88c-49b2-aeee-1af92749e337-EngineJobExecutionActor-testMe.worker:NA:1/71ccb54a-f88c-49b2-aeee-1af92749e337-BackendJobExecutionActor-71ccb54a:testMe.worker:-1:1/SharedFileSystemAsyncJobExecutionActor] SharedFileSystemAsyncJobExecutionActor [UUID(71ccb54a)testMe.worker:NA:1]: Error attempting to Execute the script; java.lang.UnsupportedOperationException: Could not evaluate expression: ""-m ea -M "" + pbs_email; 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:49); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at wdl4s.Task$$anonfun$instantiateComm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:1120,Error,Error,1120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['Error'],['Error']
Availability,error calling RegisterJobDefinition when running workflow on AWS Batch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:0,error,error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['error'],['error']
Availability,"error running cromwell on SLURM: ""Fatal exception polling for status""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4651:0,error,error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651,1,['error'],['error']
Availability,error while retrieving metadata for workflow because row count exceeds configured limit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6236:0,error,error,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236,1,['error'],['error']
Availability,"error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.cor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1150,error,error,1150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,2,['error'],['error']
Availability,"errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1348,error,error,1348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"ers.api-dispatcher-23 INFO - Workflow 56b1f228-8054-4947-8dc3-372363c5e94b submitted.; 2017-02-22 13:49:57,791 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - 1 new workflows fetched; 2017-02-22 13:49:57,792 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Starting workflow UUID(56b1f228-8054-4947-8dc3-372363c5e94b); 2017-02-22 13:49:57,798 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - WorkflowManagerActor Successfully started WorkflowActor-56b1f228-8054-4947-8dc3-372363c5e94b; 2017-02-22 13:49:57,799 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-02-22 13:49:58,220 cromwell-system-akka.dispatchers.engine-dispatcher-14 INFO - MaterializeWorkflowDescriptorActor [UUID(56b1f228)]: Call-to-Backend assignments: ; 2017-02-22 13:49:58,373 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - Unexpected engine failure; java.lang.RuntimeException: Unexpected engine failure; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableScopes(WorkflowExecutionActor.scala:384); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:77); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$2.applyOrElse(WorkflowExecutionActor.scala:75); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:32); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2020:1257,failure,failure,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2020,1,['failure'],['failure']
Availability,"ersion) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:19:57,96] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:19:58,11] [info] MaterializeWorkflowDescriptorActor [caab4283]: Parsing workflow as CWL v1.0; [2018-09-14 13:20:00,08] [error] WorkflowManagerActor Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#readgroup.yml/readgroup_fastq_pe_file was referred to but not found in schema def Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#amplicon_kit.yml/amplicon_kit_set_file,Some([Lcwl.InputRecordField;@5f7affaf),record,None)), Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#amplicon_kit.yml/amplicon_kit_set_uuid,Some([Lcwl.InputRecordField;@4e2399d2),record,None)), Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#capture_kit.yml/capture_kit_set_file,Some([Lcwl.InputRecordField;@74284741),record,None)), Inl(InputRecordSchema(file:///home/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:3199,error,error,3199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['error'],['error']
Availability,"es), Cromwell will mark that as SUCCESS, and delocalize what ever is produced by the command. -----------; Expected behavior:. The job should fail. -----------; Actually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:1187,echo,echo,1187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['echo'],['echo']
Availability,"es[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1896,error,error,1896,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,2,['error'],['error']
Availability,"es_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:1626,error,errors,1626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['error'],['errors']
Availability,"ess the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actually shows the issue twice -- once in the section starting with `if(defined(variant_call_after_earlyQC_filtering.errorcode)) {` and once in the section starting with `if(defined(profile_bam.strain)) {`. Interestingly, the results of echo_b implies that select_first() is a more accurate way of checking if a variable is defined than the built-in defined().",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1681,error,errorcode,1681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,7,"['avail', 'echo', 'error']","['available', 'echo', 'error', 'errorcode']"
Availability,"est1-a"", ""us-west1-b"", ""us-west1-c""]; }; }; }; }; }. ```. However, when I tried to run a WDL workflow test which used ""gcr.io/broad-cumulus/cellranger:6.1.1"" docker image, the execution failed with the following log:. ```; 2021-09-27 13:47:50,363 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:55,753 WARN - Skipping auto-registration; 2021-09-27 13:47:55,833 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-registration; 2021-09-27 13:47:57,524 INFO - Reference disks feature for PAPIv2 backend is not configured.; 2021-09-27 13:47:58,075 INFO - Slf4jLogger started; 2021-09-27 13:47:58,470 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-69bdc1a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2021-09-27 13:47:58,845 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - Metadata summary refreshing every 1 second.; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata archiver defined in config; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata deleter defined in config; 2021-09-27 13:47:58,926 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2021-09-27 13:47:58,929 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:58,935 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:5749,heartbeat,heartbeat,5749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,"estarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # providers. } # backend. # https://gatkforums.broadinstitute.org/wdl/discussion/9536/how-do-i-set-up-a-mysql-database-for-cromwell; # http://slick.lightbend.com/doc/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1918,alive,alive,1918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['alive'],['alive']
Availability,"eterodon/1.0.0-beta1/heterodon-1.0.0-beta1-single.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:11:1: checking field `steps`; ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:19:3: checking object `../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl#compile`; ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:20:5: Field `run` contains undefined reference to `file:///var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/arguments.cwl`; ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:12:3: checking object `../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl#untar`; ../../../../../var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/c8eb79b9-eee9-4796-b374-4841ff18a27d.cwl:13:5: Field `run` contains undefined reference to `file:///var/folders/g_/4k204pzx6sg6gcnqc2bqfykxx8k880/T/c8eb79b9-eee9-4796-b374-4841ff18a27d.temp.3969925185320342713/tar-param.cwl`; ```; I'm assuming `getMessage` does not work as expected here because the underlying `Throwable` comes from Python. I did try `t.value.getMessage` to potentially trim down the stack trace, but that's also `null`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4190:3433,down,down,3433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4190,1,['down'],['down']
Availability,"etingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; 2016-05-27 11:08:57,271 cromwell-system-akka.actor.default-dispatcher-4 ERROR - BackendCallExecutionActor [UUID(8c7774be):BillyBob]: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; cromwell.util.AggregatedException: Failures during localizationCould not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:13262,ERROR,ERROR,13262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-18 13:22:35,882 cromwell-system-akka.dispatchers.engine-dispatcher-42 ERROR - WorkflowManagerActor Workflow 4057b0c6-0019-4a00-b8af-e392fbf89697 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.core.CromwellFatalException$.apply(core.scala:22); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:39); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run$$$capture(Promise.scala:60); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:4390,recover,recoverWith,4390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverWith']
Availability,"existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:16479,down,downstream,16479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['downstream']
Availability,"expectation would be that all occurrences of `file` and `maybe_file` would interpolate as relativized file paths. ```; task files {; File file ; File? maybe_file. command {; echo file: ${file} maybe_file: ${maybe_file} ${""file with concatenation: "" + file} ${""maybe_file with concatenation: "" + maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call files { input: file = file, maybe_file = file }; }; ```. On JES with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c, the ""maybe_file with concatenation"" leaves a residual `gs://` path; all other interpolations are correctly relativized. On AWS with Cromwell 9341a4dac6145233f2a33b092a8fc443c18744ea, both concatenations leave residual `s3://` paths. On Local with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c and an input file under my home directory, this throws an exception with the following trace:. ```; 2017-02-02 11:55:36,701 cromwell-system-akka.dispatchers.backend-dispatcher-44 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(5fdb357a)w.files:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:55); 	at wdl4s.Task$$anonfun$instant",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:1045,ERROR,ERROR,1045,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['ERROR'],['ERROR']
Availability,"f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:1589,ERROR,ERROR,1589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['ERROR'],['ERROR']
Availability,"f0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:2006,error,error,2006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['error'],['error']
Availability,"f_dict = ref_dict,; tumor_bam = p.left.left,; tumor_bam_index = p.left.right,; tumor_sample_name = sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; normal_bam = p.right.left,; normal_bam_index = p.right.right,; normal_sample_name = sub(sub(p.right.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; scatter_count = scatter_count,; dbsnp = dbSNPVCF,; dbsnp_index = dbsnp_index,; cosmic = cosmicVCF,; cosmic_index = cosmic_index,; is_run_orientation_bias_filter = true,; is_run_oncotator = false,; oncotator_docker = ""broadinstitute/oncotator:1.9.2.0-eval-gatk-protected"",; m2_docker = ""broadinstitute/gatk-protected@sha256:08bf9835dabb5b694164dae8312bac8d8012b9d907341f30d3c8e262a0f121d6"",; preemptible_attempts = preemptible,; onco_ds_local_db_dir = ""/root/onco_dbdir/"",; artifact_modes = [""G/T"", ""C/T""],; picard_jar = picard_jar; }; # New WDL added here that calls a task, not a workflow; # NOTE: Even when I put the VcfToIntervals task inline in this WDL file, I get the same exact error.; call dl_ob_training_m2.VcfToIntervals as vcf2i {; input:; vcf = m2_tn.filtered_vcf,; entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"); }; ### End new WDL; }. output {; Array[File] unfiltered_vcf = m2_tn.unfiltered_vcf; Array[File] unfiltered_vcf_index = m2_tn.unfiltered_vcf_index; Array[File] filtered_vcf = m2_tn.filtered_vcf; Array[File] filtered_vcf_index = m2_tn.filtered_vcf_index; }; }; ```. Here is the error message I get using wdltool 0.8 and 0.12 (and cromwell):. ```; ERROR: Expression will not evaluate (line 82, col 42):. entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"). ```. I tried a few things:; - ``entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """")`` --> ``entity_id=p.left.left`` gives same error; - ``entity_id=sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """")`` --> ``entity_id=p.left`` makes the error go away (though now the WDL is semantically incorrect); - Even when I put the VcfToIntervals task inline in this WDL file, I get the same exact error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334:2815,error,error,2815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334,5,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --ta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:14901,down,downstream,14901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,2,['down'],['downstream']
Availability,"fig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/cromwellslurmsingularitynew.conf ./cromwell-84.jar server. ```; config ; ```; # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Cromwell HTTP server settings; webservice {; #port = 8000; #interface = 0.0.0.0; #binding-timeout = 5s; #instance.name = ""reference""; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. # Cromwell ""system"" settings; system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; #abort-jobs-on-terminate = false. # this tells Cromwell to retry the task with Nx memory when it sees either OutOfMemoryError or Killed in the stderr file.; memory-retry-error-keys = [""OutOfMemory"", ""Out Of Memory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:1570,error,error-keys,1570,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,"['down', 'error']","['down', 'error-keys']"
Availability,"fig.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below are the first few lines of the config shown. ```; #see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/; # include the application.conf at the top; include required(classpath(""application"")); workflow-options {; workflow-failure-mode = ContinueWhilePossible; delete_intermediate_output_files = true; final_workflow_outputs_dir = ""cromwell-results"",; use_relative_output_paths = true,; final_workflow_log_dir = ""cromwell-logs"",; final_call_logs_dir = ""cromwell-call_logs""; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=50000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }; backend {; default = ""Local""; ```. OS and java version. Centos7 and Java/11.0.16 (openjdk)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:6053,failure,failure-mode,6053,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['failure'],['failure-mode']
Availability,figAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:3628,robust,robustExecuteOrRecover,3628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['robust'],['robustExecuteOrRecover']
Availability,"figAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:637); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. Th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:4765,robust,robustExecuteOrRecover,4765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,1,['robust'],['robustExecuteOrRecover']
Availability,"figuration issue, and issues are only found right when using the ""Directory"" Type. To help in understanding my setup, I've included:. **inputs:**; ```; {; ""good_donor_good_recipient.blastdb"": ""../../data/blast/blastdb"",; ""good_donor_good_recipient.fasta"": ""../../data/ref_genomes/pseudomonas.fasta"",; }; ```. **task definition:**; ```; version development; task n {; input {; File fasta ; Directory blastdb; String out_file = ""~{basename(fasta)}.blast""; }; command <<<; export BLASTDB=~{blastdb} ; blastn \; -query ~{fasta} -db nt -num_threads 24 -evalue 1 -outfmt '6' -out ~{out_file}; >>>; output { File out = out_file }; runtime { docker: ""ncbi/blast:2.10.1"" }; }; ```. **confiuration snippet - localization only:**; ```; filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; # Call caching strategies; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; hashing-strategy: ""md5""; check-sibling-md5: false; }; }; }; ```. **logs:**; ```; [2020-08-08 19:20:00,49] [error] Failed to hash ""../../data/blast/blastdb"": Is a directory; [2020-08-08 19:20:00,49] [warn] Localization via hard link has failed: /workflows/cromwell-executions/good_donor_good_recipient/f7947643-2729-483f-b987-44ef932f88bd/call-blaster/main/6e4fa8a1-0d72-486e-a9ae-254319c4915d/call-blaster/shard-20/inputs/2058596876/blastdb -> /data/blast/blastdb: Operation not permitted; [2020-08-08 19:20:00,49] [error] 6e4fa8a1:main.blaster:46:1: Hash error (Is a directory), disabling call caching for this job.; ```. **contents of the BLASTDB directory:**; ```; /data/blast/blastdb$ ls; nt.00.nhd nt.01.nhd nt.02.nhd nt.03.nhd nt.04.nhd nt.05.nhd nt.06.nhd nt.07.nhd nt.08.nhd nt.09.nhd nt.10.nhd nt.11.nhd nt.12.nhd nt.13.nhd nt.14.nhd nt.15.nhd nt.16.nhd nt.17.nhd nt.18.nhd nt.19.nhd nt.20.nhd nt.21.nhd nt.22.nhd nt.23.nhd nt.24.nhd nt.nal nt.00.nhi nt.01.nhi nt.02.nhi nt.03.nhi nt.04.nhi nt.05.nhi nt.06.nhi nt.07.nhi nt.08.nhi nt.09.nhi nt.10.nhi nt.11.nhi nt.12.nhi nt.13.nhi nt.14.nh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737:1624,error,error,1624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737,1,['error'],['error']
Availability,"filtering (optional); ## split_intervals_extra_args: additional arguments for splitting intervals before scattering (optional); ## run_orientation_bias_mixture_model_filter: (optional) if true, filter orientation bias sites with the read orientation artifact mixture model.; ##; ## ** Primary inputs **; ## ref_fasta, ref_fai, ref_dict: reference genome, index, and dictionary; ## tumor_bam, tumor_bam_index: BAM and index for the tumor sample; ## normal_bam, normal_bam_index: BAM and index for the normal sample; ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_idx: optional panel of normals (and its index) in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_idx: optional database of known germline variants (and its index) (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_idx: VCF of common variants (and its index)with allele frequencies for calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## realignment_index_bundle: resource for FilterAlignmentArtifacts, which runs if and only if it is specified. Generated by BwaMemIndexImageCreator.; ##; ## Funcotator parameters (see Funcotator help for more details).; ## funco_reference_version: ""hg19"" for hg19 or b37. ""hg38"" for hg38. Default: ""hg19""; ## funco_output_format: ""MAF"" to produce a MAF file, ""VCF"" to procude a VCF file. Default: ""MAF""; ## funco_compress: (Only valid if funco_output_format == ""VCF"" ) If true, will compress the output of Funcotator. If false, produces an uncompressed output file. Default: false; ## funco_use_gnomad_AF: If true, will include gnomAD allele frequency annotations in output by connecting to the internet to query gnomAD (this impacts performance). If false, will not annotate with gnomAD. Default: false; ## funco_transcript_selection_mode: How to select transcripts in Funcotator. ALL, CANONICAL, or BEST_EFFECT; ## funco_transcript_selectio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:3583,down,downloads,3583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['down'],['downloads']
Availability,fix markdown formatting errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5323:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5323,1,['error'],['errors']
Availability,fixed bracket error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7095:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7095,1,['error'],['error']
Availability,fixing errors in documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/327:7,error,errors,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/327,1,['error'],['errors']
Availability,"flow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:1411,error,error,1411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,1,['error'],['error']
Availability,"foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$work",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:2754,error,error,2754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['error'],['error']
Availability,"forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. Docker Script (generated by cromwell):; ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.1de26137"" && echo ""/cromwell_root/tmp.1de26137""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); out5d4c4459=""${tmpDir}/out.$$"" err5d4c4459=""${tmpDir}/err.$$""; mkfifo ""$out5d4c4459"" ""$err5d4c4459""; trap 'rm ""$out5d4c4459"" ""$err5d4c4459""' EXIT; tee '/cromwell_root/stdout' < ""$out5d4c4459"" &; tee '/cromwell_root/stderr' < ""$err5d4c4459"" >&2 &; (; cd /cromwell_root. /app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq; ) > ""$out5d4c4459"" 2> ""$err5d4c4459""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*fastqc.zip /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84 2> /dev/nu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:4718,echo,echo,4718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"forums.broadinstitute.org/wdl/discussion/7690/running-in-server-mode-jobs-that-have-localization-error-become-immortal#latest). ---. **User Report**. Running cromwell in server mode, with default configuration in each case, I can reproduce the following behaviour in 0.18, 0.19 and 0.19_hotfix (HEAD):. Submit a workflow that has non-existent file as input to a task, e.g.:. ```; task BillyBob {; File bbInput; command { echo ""done"" }; }; workflow badLocalization {; call BillyBob { input: bbInput=""/foo/bar/baz"" }; }; ```. The server log shows ""Failures during localization"" error (below) - as expected initially, I guess - but then _repeats_ the error every 30 seconds or so, forever, and hitting the API `<workflowId>/status` endpoint shows the job in a ""Running"" state, forever. I would expect this error to cause the task, and then the workflow, to die. example of a single block of the server log error: . ```; 2016-05-27 11:08:57,269 cromwell-system-akka.actor.default-dispatcher-5 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /foo/bar/baz -> /home/conradL/cromwell-executions/badLocalization/8c7774be-7917-4c6a-88c4-55e495bbb9ec/call-BillyBob/foo/bar/baz; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:1030,ERROR,ERROR,1030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,2,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640:499,echo,echo,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640,1,['echo'],['echo']
Availability,"g API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84700,failure,failure,84700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['failure'],['failure']
Availability,"g `docker.hash-lookup.method=""local""` is used), the image is not found. The offending lines of code are:; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:1316,error,error,1316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['error'],['error']
Availability,"g calls: atac.filter:1:1; [2017-11-18 19:30:05,51] [warn] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Unrecognized runtime attribute keys: time; [2017-11-18 19:30:05,52] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: python $(which encode_filter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-bowtie2/shard-1/glob-3bcbe4e7489c90f75e0523ac6f3a9385/ENCFF463QCX.trim.merged.R1.bam \; \; --multimapping 4 \; \; \; \; --nth 2; [2017-11-18 19:30:15,43] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:3606,error,error,3606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['error'],['error']
Availability,"g on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3958,Down,Downloading,3958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,"g.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1999,error,errorcode,1999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,g.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$S,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:2828,Robust,RobustClientHelperSpec,2828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,gAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeScript(SharedFileSystemAsyncJobExecutionActor.scala:220); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:189); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeOrRecover(SharedFileSystemAsyncJobExecutionActor.scala:188); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:69); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:3978,robust,robustExecuteOrRecover,3978,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['robust'],['robustExecuteOrRecover']
Availability,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:4899,echo,echo,4899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['echo'],['echo']
Availability,"ge](https://user-images.githubusercontent.com/9449764/36913061-ae9bb686-1e16-11e8-9d8e-0f30eebc8dd6.png). ### What is it; This may be more information than you need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly fille",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:1326,failure,failures,1326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,2,"['down', 'failure']","['down', 'failures']"
Availability,"ger(1), WdlInteger(2))"". (*Side question: why is it trying to make a list out of values in two different scattered rows?*) Using `Int?` in the conditional instead of `Int` does not make a difference. Another bizarre twist: if instead of reading in from a file I hardcode the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkzcToI7XtZ0zlOv7NFtVwJrGZaKdks5rYkvxgaJpZM4L1qD1>; > .; >. ---. @kcibul commented on [Fri Feb 03 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277235590). Hey guys -- just want to understand more how this is blocking mutect (ie there's no other way to do this?). Can you explain?. @cjllanwarne -- if you're on bug rotation, could you try to parse this into the separate p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:1594,Ping,Pinging,1594,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['Ping'],['Pinging']
Availability,"get_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then an empty file gets passed downstream\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_cnv_coverage_file_gcbias = \""${entity_id}.gc_corrected_coverage.tsv\""\n }\n}\n\n# Perform tangent normalization (noise reduction) on the proportional coverage file.\ntask NormalizeSomaticReadCounts {\n String entity_id\n File coverage_file\n File padded_target_file\n File pon\n String gatk_jar\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \\\n --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \\\n --betaHatsOutput ${",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:39217,down,downstream,39217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['downstream']
Availability,"ggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 0EDB33C059ED489B1F78F3502B7DB8AC.; [2022-12-15 21:28:23,78] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:5:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,79] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:1:1-20000000039 [788d8048main.all_qced_sample_lists:1:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,79] [info] BT-322 788d8048:main.all_qced_sample_lists:1:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = DC6FF6846E1CC843B0D79723739936B2.; [2022-12-15 21:28:23,79] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:1:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,80] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:3:1-20000000035 [788d8048main.all_qced_sample_lists:3:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,80] [info] BT-322 788d8048:main.all_qced_sample_lists:3:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 262BA6F0AB83375414FCD228C0CC6E47.; [2022-12-15 21:28:23,80] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:3:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:2:1-20000000034 [788d8048main.all_qced_sample_lists:2:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,81] [info] BT-322 788d8048:main.all_qce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:43374,failure,failures,43374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"ggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 110EC9F4C25BC9902A0E5B3B8EAB2725.; [2022-12-15 21:28:23,81] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:2:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:4:1-20000000036 [788d8048main.all_qced_sample_lists:4:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,82] [info] BT-322 788d8048:main.all_qced_sample_lists:4:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 51C3D11209F9A7985345B2FD76E1C699.; [2022-12-15 21:28:23,82] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:4:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,86] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:0:1-20000000038 [788d8048main.all_qced_sample_lists:0:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,86] [info] BT-322 788d8048:main.all_qced_sample_lists:0:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 801EC388A847FBAB78685AE96643853A.; [2022-12-15 21:28:23,86] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:0:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.all_qced_sample_lists' (scatter index: Some(5), attempt 1); [2022-12-15 21:28:26,54] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:45426,failure,failures,45426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"ggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 262BA6F0AB83375414FCD228C0CC6E47.; [2022-12-15 21:28:23,80] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:3:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:2:1-20000000034 [788d8048main.all_qced_sample_lists:2:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,81] [info] BT-322 788d8048:main.all_qced_sample_lists:2:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 110EC9F4C25BC9902A0E5B3B8EAB2725.; [2022-12-15 21:28:23,81] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:2:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:4:1-20000000036 [788d8048main.all_qced_sample_lists:4:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,82] [info] BT-322 788d8048:main.all_qced_sample_lists:4:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 51C3D11209F9A7985345B2FD76E1C699.; [2022-12-15 21:28:23,82] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:4:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,86] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:0:1-20000000038 [788d8048main.all_qced_sample_lists:0:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,86] [info] BT-322 788d8048:main.all_qce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:44742,failure,failures,44742,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"ggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = DC6FF6846E1CC843B0D79723739936B2.; [2022-12-15 21:28:23,79] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:1:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,80] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:3:1-20000000035 [788d8048main.all_qced_sample_lists:3:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,80] [info] BT-322 788d8048:main.all_qced_sample_lists:3:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 262BA6F0AB83375414FCD228C0CC6E47.; [2022-12-15 21:28:23,80] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:3:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:2:1-20000000034 [788d8048main.all_qced_sample_lists:2:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,81] [info] BT-322 788d8048:main.all_qced_sample_lists:2:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 110EC9F4C25BC9902A0E5B3B8EAB2725.; [2022-12-15 21:28:23,81] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:2:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,81] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:4:1-20000000036 [788d8048main.all_qced_sample_lists:4:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,82] [info] BT-322 788d8048:main.all_qce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:44058,failure,failures,44058,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"gle.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:3455,error,errors,3455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"gle\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:17:00.937007Z""; },; {; ""startTime"": ""2018-08-14T16:16:33.718991Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:35.937599Z""; },; {; ""startTime"": ""2018-08-14T16:16:35.937599Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:21214,echo,echo,21214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"gnments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:5182,echo,echo,5182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['echo'],['echo']
Availability,google error conditions to retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1872:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1872,1,['error'],['error']
Availability,"google.com/storage/browser/broad-dsde-methods/cromwell-execution-34/PindelSmallVariants/?project=broad-dsde-methods&organizationId=548622027621) using the DSDE-methods Cromwell server V.34. The errors I'm experiencing is described below. Resubmitting the job fixes the problem. ## failure for job ID ; * `2ebeed9a-8f42-418b-8569-8d80f5654d50` (shard-40), ; * `1cc79dda-9c6e-41b4-ac99-e1a422258039` (shard-20). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; /bin/bash: /cromwell_root/script: No such file or directory; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `4c5c8530-b79d-465b-8050-f7ba7368c057` (shard-26), ; * `5cbb2124-c526-4ca0-978e-4154ff4501cd` (shard-0). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Loading reference genome done.; Initializing parameters done.; Please use samtools to index your reference file.; .fai is missing. sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `7f219a29-69c1-4116-9c54-5d4656ee0124`. ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Error: fasta line starts with  instead of '>'. Aborting.; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148:1607,failure,failure,1607,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"gumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:1876,echo,echo,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['echo'],['echo']
Availability,hard-link and copy link errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7097:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7097,1,['error'],['errors']
Availability,"hat making them optional like `String? memory_mb` and then using syntax like `${""--mem "" + round(memory_mb) + ""m""} \` in the submit script means that argument will only be added if `memory` is defined, and will be omitted if `memory` is not defined. I've followed the documentation as closely as I can. However, when I try to submit a test job without `cpu` and `memory` set as a runtime attribute, I get a failure with these exceptions:; ```; cromwell.core.CromwellAggregatedException: Initialization Failure:; Runtime validation failed:; 	Task myTask has an invalid runtime attribute cpu = !! NOT FOUND !!; 	Task myTask has an invalid runtime attribute memory = !! NOT FOUND !!; 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:356); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:339); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); ```. Here is the test WDL I'm using:. ```; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; call myTask. }. task myTask {; command <<<; echo ""hello world""; >>>; output {; String out = read_string(stdout()); }; }; ```. And my complete configuration for this backend:; ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int? runtime_minutes; Int? cpu; Float? memory_mb; String? docker; String? partition; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; ${""-t "" + runtime_minutes} \; ${""-c "" + cpu} \; ${""--mem "" + round(memory_mb) + ""m""} \; ${""-p "" + partition} \; --wrap ""/bin/bash ${script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7455:2163,echo,echo,2163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7455,2,"['alive', 'echo']","['alive', 'echo']"
Availability,"he command. -----------; Expected behavior:. The job should fail. -----------; Actually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call Do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:1315,failure,failure,1315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['failure'],['failure']
Availability,"he purpose of this task is to prove the memory-retry mechanism is configured correctly in our system. Result of TestBadCommandRetry:; The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf2b-49a2-be87-483; 9e96a08eb/call-TestOutOfMemoryRetry/stderr; Killed ; ``` . We have also noticed that in the out of memory case, no retrurnCode is written to the metadata. **Test wdl for reproduction:**; `version 1.0. workflow MemoryRetryTest {; input {; String message = ""Killed""; }; call TestOutOfMemoryRetry {}; call TestBadCommandRetry {}; }. task TestOutOfMe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:1380,failure,failure,1380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,1,['failure'],['failure']
Availability,"he slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdbc.Driver""; user = ""<user>""; password = ""<pass>"" ; connectionTimeout = 5000; }; }; }. call-caching; {; enabled = true; invalidate-bad-cache-result = true; }. docker {; hash-lookup {; enabled = true; }; }. backend {; default = sge; providers {. ; sge {; 	actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; #concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:2485,alive,alive,2485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,2,['alive'],['alive']
Availability,hello world runs without error on cromwell 36. on cromwell 37 if tinishes but throws several exceptions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['error'],['error']
Availability,"her-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1740,Error,Error,1740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['Error'],['Error']
Availability,"hesitant to try to follow the same rules as Bash for parsing a command because I worry that it'll be error prone and difficult to pin down the details. It feels like we'd essentially reimplement a Bash parser in WDL. It'd be difficult and hard to get the nuances of something like this:. ```; command {; python <<CODE; print(""#octothorps!!#""); CODE; }; ```. Maybe something as simple as syntax highlighting could help too... if `#`s are not highlighted differently in the command section then that could also be a hint. Granted, I know syntax highlighters won't always be used. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200525363). If it's clear to WDL users how # is interpreted (or not interpreted) in the; command-block maybe that would be the best thing? Would a sentence/blurb; in the docs address this? Possibly as mentioned earlier good error; messages?. -eddie. On Wed, Mar 23, 2016 at 3:43 PM, Scott Frazer notifications@github.com; wrote:. > I'm hesitant to try to follow the same rules as Bash for parsing a command; > because I worry that it'll be error prone and difficult to pin down the; > details. It feels like we'd essentially reimplement a Bash parser in WDL.; > It'd be difficult and hard to get the nuances of something like this:; > ; > command {; > python <<CODE; > print(""#octothorps!!#""); > CODE; > }; > ; > Maybe something as simple as syntax highlighting could help too... if #s; > are not highlighted differently in the command section then that could also; > be a hint. Granted, I know syntax highlighters won't always be used.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200514792. ## . Edward A. Salinas; Senior Software Engineer - Getz Lab; Broad Institute of MIT and Harvard; 75 Ames Street, Rm 4013; Cambridge, MA 02142; Ph: 617-714-7905; Cell: 210-274-3172",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:7895,error,error,7895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,2,"['down', 'error']","['down', 'error']"
Availability,"hi all,; I'm trying to run cromwell 84 will the following config, with, as far as I understand, a local HTSQLDB cache :. ```; include required(classpath(""application"")); # the file below was fixed in https://github.com/broadinstitute/cromwell/issues/7007; include ""SGE.conf"". call-caching {; 	enabled = true; invalidate-bad-cache-results = false; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }; ```. but when I execute the workflow . `java -Dconfig.file=${PWD}/app.conf -jar ${CROMWELL_JAR} run test.wdl --inputs input.json`. the configuration takes a long time with messages about `Checkpoint...` that takes about 10 minutes. . ```; (...); [2023-02-08 16:24:28,90] [info] dataFileCache commit start; [2023-02-08 16:24:28,91] [info] dataFileCache commit end; [2023-02-08 16:24:28,94] [info] checkpointClose end; [2023-02-08 16:24:28,96] [info] Checkpoint end - txts: 3051; [2023-02-08 16:24:29,05] [info] Checkpoint start; [2023-02-08 16:24:29,05] [info] checkpointClose start; [2023-02-08 16:24:29,07] [info] checkpointClose synched; [2023-02-08 16:24:29,08] [info] checkpointClose script done; [2023-02-08 16:24:29,08] [info] dataFileCache commit start; [2023-02-08 16:24:29,20] [info] dataFileCache commit end; [2023-02-08 16:24:29,53] [info] checkpointClose end; [2023-02-08 16:24:29,53] [info] Checkpoint end - txts: 3058; [2023-02-08 16:24:29,53] [info] Checkpoint start; [2023-02-08 16:24:29,53] [info] checkpointClose start; [2023-02-08 16:24:30,52] [info] checkpointClose synched; [2023-02-08 16:24:30,52] [info] checkpointClose script done; [2023-02-08 16:24:30,52] [info] dataFileCache commit start; [2023-02-08 16",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:959,Checkpoint,Checkpoint,959,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['Checkpoint'],['Checkpoint']
Availability,"hich a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:1424,error,error,1424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['error'],['error']
Availability,"hingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:5698,ERROR,ERROR,5698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,2,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,how to use UUID to reboot?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6920:19,reboot,reboot,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6920,1,['reboot'],['reboot']
Availability,"https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case en",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:1191,echo,echo,1191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['echo'],['echo']
Availability,"https://broadinstitute.atlassian.net/browse/GAWB-3950. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/790/; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/833/; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/804/. tc: KeyValueServiceActor should insert a key/value; tc: KeyValueServiceActor should return error if key doesn't exist; tc: KeyValueServiceActor should be able to overwrite values. {quote}; org.scalatest.exceptions.TestFailedException: The future returned an exception of type: akka.pattern.AskTimeoutException, with message: Ask timed out on [Actor[akka://KeyValueServiceActorSpec/user/$a#-1019375090]] after [200000 ms]. Sender[null] sent message of type ""cromwell.services.keyvalue.KeyValueServiceActor$KvPut"".. at org.scalatest.concurrent.Futures$FutureConcept.tryTryAgain$1(Futures.scala:531) at org.scalatest.concurrent.Futures$FutureConcept.futureValueImpl(Futures.scala:550) at org.scalatest.concurrent.Futures$FutureConcept.futureValueImpl$(Futures.scala:479) at org.scalatest.concurrent.ScalaFutures$$anon$1.futureValueImpl(ScalaFutures.scala:275) at org.scalatest.concurrent.Futures$FutureConcept.futureValue(Futures.scala:476) at org.scalatest.concurrent.Futures$FutureConcept.futureValue$(Futures.scala:475) at org.scalatest.concurrent.ScalaFutures$$anon$1.futureValue(ScalaFutures.scala:275) at cromwell.services.keyvalue.impl.KeyValueServiceActorSpec.$anonfun$new$2(KeyValueServiceActorSpec.scala:46) at ; {quote}",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4320:422,error,error,422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4320,1,['error'],['error']
Availability,"https://broadworkbench.atlassian.net/browse/WX-746. Lifesciences seems to fail when there are lots of Actions downloading DRS files (even though lots of Actions downloading GCS files seems to work). However, downloading lots of DRS files from a single Action seems to work, so that's what these changes do. Cromwell creates and uploads a manifest for cromwell-drs-localizer. (Note: This is specifically not a `getm` manifest. cromwell-drs-localizer could be enhanced to take this new manifest, query Martha, and generate a `getm` manifest to enable parallel downloads. See the [DRS Performance](https://broadworkbench.atlassian.net/browse/WX-456) epic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6914:110,down,downloading,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6914,4,['down'],"['downloading', 'downloads']"
Availability,"https://broadworkbench.atlassian.net/browse/WX-765. This compiles and passes tests, though I have a sense that the work done in `YamlUtils.scala` may now be duplicating remediations that are now built in to SnakeYAML. I feel like this PR disables built-in safety checks that we then reimplement. Another possible solution may be to stop disabling the safety checks and adjust the test expectations to match how SnakeYAML reports errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6927:429,error,errors,429,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927,1,['error'],['errors']
Availability,"https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml#!/Workflows/post_workflows_version_id_abort. When going through swagger, the abort takes a long time and then gives an error: ; Response Code 500; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request."". The workflow is removed from WORKFLOW_STORE_ENTRY but the associated jobs are still present in JOB_STORE_ENTRY. . There are no errors in the logs: ; `; 2016-12-12 18:22:26,139 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(8a965a5e)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:29:42,727 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(73be7f27)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:31:29,146 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(13965e09)]: Abort received. Aborting 10 EJEAs; 2016-12-12 18:31:46,093 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(804a56b6)]: Abort received. Aborting 11 EJEAs; 2016-12-12 18:32:05,063 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(2c6302c8)]: Abort received. Aborting 6 EJEAs; 2016-12-12 18:32:25,094 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(793dd16f)]: Abort received. Aborting 5 EJEAs; 2016-12-12 18:32:38,555 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(4fddebd4)]: Abort received. Aborting 13 EJEAs; 2016-12-12 18:32:50,674 cromwell-system-akka.dispatchers.engine-dispatcher-120 INFO - WorkflowExecutionActor [UUID(aadd5082)]: Abort received. Aborting 7 EJEAs; 2016-12-12 18:33:01,265 cromwell-system-akka.dispatchers.engine-dispatcher-1282 INFO - WorkflowExecutionActor [UUID(edaa9993)]: Abort received. Aborting 4 EJEAs; 2016-12-12 18:33:12,453 cromwell-system-akka.disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1775:210,error,error,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1775,3,['error'],"['error', 'errors']"
Availability,"https://cromwell.readthedocs.io/en/stable/GettingHelp/ suggests that users get support at a forum that doesn't exist anymore. Normally when I find a dead link in docs, I just replace it in a PR, but I'm really not sure if there's anything to replace it with. There is a new GATK forum, but from what I've seen it doesn't really take questions about non-GATK WDLs even in the Community/Other section. There is a Cromwell Slack, but Slack is not available in all countries, isn't indexed, and the workspace is on a free plan (some old messages are already unavailable), so it's not a good option for actual support. The same goes for the OpenWDL Slack - not available in all places, not indexed by search engines, continuously overwriting itself due to being on a free plan. Terra Support is only focused on Terra-specific usage of Cromwell, even though it's common to test WDLs locally before running them in Terra.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801:444,avail,available,444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801,2,['avail'],['available']
Availability,"https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/754. 02:11:16 cromwell-test_1 | [info] *** 1 TEST FAILED ***; 02:11:16 cromwell-test_1 | [info] EnhancedRhinoSandboxSpec:; 02:11:16 cromwell-test_1 | [info] EnhancedRhinoSandbox ; 02:11:16 cromwell-test_1 | [info] - should synchronize global ContextFactory initialization *** FAILED *** (1 second, 15 milliseconds); 02:11:16 cromwell-test_1 | [info] all threads did not complete successfully: Vector(true, false, false, false, false, false, false, false, true, false) did not contain only (true) (EnhancedRhinoSandboxSpec.scala:37); 02:11:16 cromwell-test_1 | [error] Failed: Total 125, Failed 1, Errors 0, Passed 124, Ignored 3; 02:11:16 cromwell-test_1 | [error] Failed tests:; 02:11:16 cromwell-test_1 | [error] 	cwl.internal.EnhancedRhinoSandboxSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4306:636,error,error,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4306,4,"['Error', 'error']","['Errors', 'error']"
Availability,https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/884/; https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1179/. java.lang.IllegalStateException: cannot create children while terminating or terminated at akka.actor.dungeon.Children.makeChild(Children.scala:270) at akka.actor.dungeon.Children.attachChild(Children.scala:48) at akka.actor.dungeon.Children.attachChild$(Children.scala:47) at akka.actor.ActorCell.attachChild(ActorCell.scala:431) at akka.actor.ActorSystemImpl.systemActorOf(ActorSystem.scala:745) at akka.testkit.TestKitBase.$init$(TestKit.scala:170) at akka.testkit.TestKit.<init>(TestKit.scala:896) at akka.testkit.TestProbe.<init>(TestKit.scala:954) at akka.testkit.TestProbe.<init>(TestKit.scala:956) at akka.testkit.TestProbe$.apply(TestKit.scala:990) at cromwell.core.actor.RobustClientHelperSpec.$anonfun$new$7(RobustClientHelperSpec.scala:109) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.Fla,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:844,Robust,RobustClientHelperSpec,844,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,2,['Robust'],['RobustClientHelperSpec']
Availability,"https://github.com/broadinstitute/cromwell#continueonreturncode; doesn't seem to work as documented for me. Here's the test I ran, and the behavior was identical to the behavior without continueOnReturnCode as far as I can tell. . This isn't a high priority for me - I believe we can get the information we need without this option. ; # cat error_continue.wdl. task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && exit 1; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }; # curl -v ""localhost:8000/api/workflows/v1"" -F wdlSource=@error_continue.wdl -F workflowInputs=@hello.inputs. -> {; ""id"": ""dbd26ad6-5a29-4c80-8f49-8b5f53830782"",; ""status"": ""Submitted""; }; curl -v ""localhost:8000/api/workflows/v1/dbd26ad6-5a29-4c80-8f49-8b5f53830782/status""; -> {; ""id"": ""dbd26ad6-5a29-4c80-8f49-8b5f53830782"",; ""status"": ""Failed""; }; curl -v ""localhost:8000/api/workflows/v1/dbd26ad6-5a29-4c80-8f49-8b5f53830782/outputs""; {; ""id"": ""dbd26ad6-5a29-4c80-8f49-8b5f53830782"",; ""outputs"": {. }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404:404,echo,echo,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404,1,['echo'],['echo']
Availability,"https://github.com/broadinstitute/cromwell/blob/4c386f80ab83e78c953e412382526a47d6b36ba4/cromwell.example.backends/TES.conf#L32. There is a missing `}` in this file. It also doesn't seem to work if used with Funnel (https://github.com/ohsu-comp-bio/funnel) as TES back end, although the error happens after the TES execution is complete:. ```; [INFO] [04/28/2020 13:37:00.957] [cromwell-system-akka.dispatchers.backend-dispatcher-63] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/WorkflowExecutionActor-18b18b73-17d7-4569-a58f-e44af1ec43a5/18b18b73-17d7-4569-a58f-e44af1ec43a5-EngineJobExecutionActor-md5_.cwl:NA:1/18b18b73-17d7-4569-a58f-e44af1ec43a5-BackendJobExecutionActor-md5_.cwl:NA:1/TesAsyncBackendJobExecutionActor] TesAsyncBackendJobExecutionActor [UUID(18b18b73)md5_.cwl:NA:1]: Status change from - to Complete; [INFO] [04/28/2020 13:37:01.083] [cromwell-system-akka.dispatchers.engine-dispatcher-41] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 18b18b73-17d7-4569-a58f-e44af1ec43a5 failed (during ExecutingWorkflowState): Job md5_.cwl:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /Users/asenf/devel/alexandersenf/cromwelltes/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/execution/stderr.; [First 300 bytes]:mkdir: cannot create directory '/cromwell-executions/md5_.cwl/18b18b73-17d7-4569-a58f-e44af1ec43a5/call-md5_.cwl/tmp.7db856a3': Read-only file system; chmod: cannot access '': No such file or directory; mkfifo: cannot create fifo '/out.1': Read-only file system; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5499:287,error,error,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5499,1,['error'],['error']
Availability,"https://github.com/broadinstitute/cromwell/blob/fac784dd4078b8cc12fb4ca6c9abdbb05072990b/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L192) is proving very costly for our SFS cromwell server... `sync` flushes _everything_ to disk. Depending on how much memory you've got, what else is happening on the box, and type of filesystem, that can be a really expensive call. For example on one of our servers it typically takes about 10 seconds to return and uses 100% of 1 core in system cpu. When you run cromwell on that box and throw a thousand workflows at it, you end up with load like this:; ```; top - 16:16:42 up 196 days, 6:34, 17 users, load average: 806.48, 618.79, 413.58; Tasks: 4596 total, 829 running, 3766 sleeping, 1 stopped, 0 zombie; %Cpu(s): 1.4 us, 0.5 sy, 0.0 ni, 98.1 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 26386596+total, 15019235+free, 25326680 used, 88346944 buff/cache; KiB Swap: 67108860 total, 63149068 free, 3959792 used. 21917659+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 34965 cromwel+ 20 0 42.622g 896064 20048 S 9.5 0.3 9:33.71 java ; 88343 cromwell 20 0 107892 360 284 R 7.3 0.0 0:28.21 sync ; 88838 cromwell 20 0 107892 360 284 R 7.3 0.0 0:27.78 sync ; 89911 cromwell 20 0 107892 360 284 R 7.3 0.0 0:23.89 sync ; 82582 cromwell 20 0 107892 356 284 R 6.7 0.0 0:31.86 sync ; 84039 cromwell 20 0 107892 356 284 R 6.7 0.0 0:32.20 sync ; 84917 cromwell 20 0 107892 360 284 R 6.7 0.0 0:28.46 sync ; 84962 cromwell 20 0 107892 356 284 R 6.7 0.0 0:28.67 sync ; 85475 cromwell 20 0 107892 356 284 R 6.7 0.0 0:28.88 sync ; 88464 cromwell 20 0 107892 356 284 R 6.7 0.0 0:28.14 sync ; 88465 cromwell 20 0 107892 360 284 R 6.7 0.0 0:27.49 sync ; 89112 cromwell 20 0 107892 356 284 R 6.7 0.0 0:27.73 sync ; 89398 cromwell 20 0 107892 360 284 R 6.7 0.0 0:22.13 sync ; 91615 cromwell 20 0 107892 356 284 R 6.7 0.0 0:12.59 sync ; 85206 cromwell 20 0 107892 356 284 R 6.1 0.0 0:27.26 sync ; 85628 cromwell 20 0 107892 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057:1016,avail,avail,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057,1,['avail'],['avail']
Availability,"i have to working for without a network platform, used docker load to import docker and turned off Docker Hash Lookup, but still reported the following error; ![image](https://github.com/broadinstitute/cromwell/assets/38648009/b1b66326-f1db-4d60-a2f1-5ea535160ddd)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7222:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222,1,['error'],['error']
Availability,"iPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBacken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:1911,error,error,1911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,2,"['Error', 'error']","['Error', 'error']"
Availability,"ializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(Batching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2308,failure,failure,2308,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['failure'],['failure']
Availability,"id workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:1128,down,download,1128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['down'],['download']
Availability,"if defined(mem) then mem * 1000 else 3500; Int command_mem = machine_mem - 500. parameter_meta{; intervals: {localization_optional: true}; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; tumor_bam: {localization_optional: true}; tumor_bai: {localization_optional: true}; normal_bam: {localization_optional: true}; normal_bai: {localization_optional: true}; pon: {localization_optional: true}; pon_idx: {localization_optional: true}; gnomad: {localization_optional: true}; gnomad_idx: {localization_optional: true}; gga_vcf: {localization_optional: true}; gga_vcf_idx: {localization_optional: true}; variants_for_contamination: {localization_optional: true}; variants_for_contamination_idx: {localization_optional: true}; }. command <<<; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" gatk_override}. # We need to create these files regardless, even if they stay empty; touch bamout.bam; touch f1r2.tar.gz; echo """" > normal_name.txt. gatk --java-options ""-Xmx~{command_mem}m"" GetSampleName -R ~{ref_fasta} -I ~{tumor_bam} -O tumor_name.txt -encode; tumor_command_line=""-I ~{tumor_bam} -tumor `cat tumor_name.txt`"". if [[ ! -z ""~{normal_bam}"" ]]; then; gatk --java-options ""-Xmx~{command_mem}m"" GetSampleName -R ~{ref_fasta} -I ~{normal_bam} -O normal_name.txt -encode; normal_command_line=""-I ~{normal_bam} -normal `cat normal_name.txt`""; fi. gatk --java-options ""-Xmx~{command_mem}m"" Mutect2 \; -R ~{ref_fasta} \; $tumor_command_line \; $normal_command_line \; ~{""--germline-resource "" + gnomad} \; ~{""-pon "" + pon} \; ~{""-L "" + intervals} \; ~{""--alleles "" + gga_vcf} \; -O ""~{output_vcf}"" \; ~{true='--bam-output bamout.bam' false='' make_bamout} \; ~{true='--f1r2-tar-gz f1r2.tar.gz' false='' run_ob_filter} \; ~{m2_extra_args}. ### GetPileupSummaries; # These must be created, even if they remain empty, as cromwell doesn't support optional output; touch tumor-pileups.table; touch normal-pileups.table. if [[ !",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:22738,echo,echo,22738,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['echo'],['echo']
Availability,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:3929,error,errors,3929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,1,['error'],['errors']
Availability,"ils them. After:; ```; INFO - WorkflowManagerActor: Workflow 1432d67e-3e95-40c8-acbd-d42f75040f1b failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'example2' (reason 1 of 1): Evaluating { ""second"": test, ""lowerLayer"": example1 } failed: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; INFO - WorkflowManagerActor: Workflow actor for 1432d67e-3e95-40c8-acbd-d42f75040f1b completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""1432d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(WomMap.scala:79); 	at wom.values.WomMap$.apply(WomMap.scala:54); 	at wom.values.WomMap$.coerceMap(WomMap.scala:34); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:90); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:87); 	at wdl.transforms.base.linking.expression.val",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:1371,ERROR,ERROR,1371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['ERROR'],['ERROR']
Availability,"imeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.ut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:2649,avail,available,2649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['avail'],['available']
Availability,impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeScript(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$executeOrRecover$2.apply(SharedFileSystemAsyncJobExecutionActor.scala:189); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeOrRecover(SharedFileSystemAsyncJobExecutionActor.scala:188); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:69); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:124); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:4170,robust,robustExecuteOrRecover,4170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['robust'],['robustExecuteOrRecover']
Availability,imports zip file with a subdir causes errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969:38,error,errors,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969,1,['error'],['errors']
Availability,"in the right place. -->. <!-- Which backend are you running? -->; Backend: Local. Several basic array functions are not working on optional arrays. Test code passes wdltool validate with no errors.; Tested types: Array[String]? and Array[Int]?; ### Length(); WDL code:; ```; Array[String]? strings; Int num = length(strings); ```; Error:; ```; [2018-10-08 13:12:09,55] [error] WorkflowManagerActor Workflow 3dfb9c92-4e2e-4754-a35e-cfcbf9d6c006 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.num:; length() expects one parameter of type Array but got one parameter of type Array[String]?; ```; ### Indexing; WDL code:; ```; Array[String]? strings. scatter (idx in range(4)) { # strings is provided in the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array[String]? strings1; Array[String]? strings2. Array[Pair[String,String]] string_pair = zip(strings1,strings2); ```; Error:; ```; [2018-10-08 13:31:20,27] [error] WorkflowManagerActor Workflow 832af5bf-2c7e-4e4a-80c5-bc0787910477 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(W",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:1499,Error,Error,1499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,2,"['Error', 'error']","['Error', 'error']"
Availability,"inPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6519,Fault,FaultHandling,6519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,"inWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12644,error,errors,12644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:1881,echo,echo,1881,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-16-0-unknown-operation#-371454906]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-unknown-operation#-248376973]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-5-0-unknown-operation#-865853959]] terminated abruptly; [2017-03-20 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:4059,error,error,4059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"ing {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d64752",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:1748,echo,echo,1748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['echo'],['echo']
Availability,"ing?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller tas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2078,error,errorcode,2078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,ingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.F,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:4125,Error,Error,4125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['Error'],['Error']
Availability,"ingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Then I see:. ```; [ERROR] [05/01/2017 17:36:04.203] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 5; 3e95ead-9026-4c13-89f9-f6c675214523 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the follow; ing files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD; /DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A2; 5E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n; File \""/usr/local/share/google/google-cloud-sdk/bin/bootstr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:2291,ERROR,ERROR,2291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['ERROR'],['ERROR']
Availability,"ing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:1257,echo,echo,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['echo'],['echo']
Availability,"inished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,72] [info] SubWorkflowStoreActor stopped; [2022-12-15 21:28:53,72] [info] JobStoreActor stopped; [2022-12-15 21:28:53,72] [info] CallCacheWriteActor stopped; [2022-12-15 21:28:53,72] [info] IoProxy stopped; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,75] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,75] [info] KvWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,76] [info] ServiceRegistryActor stopped; [2022-12-15 21:28:53,77] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,77] [info] DockerHashActor stopped; [2022-12-15 21:28:53,80] [info] Database closed; [2022-12-15 21:28:53,80] [info] Stream materializer shut down; [2022-12-15 21:28:53,80] [info] WDL HTTP import resolver closed; Workflow 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff transitioned to state Failed; $; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:51043,down,down,51043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,5,['down'],['down']
Availability,"input evaluation is currently done synchronously when starting a job, this is dangerous because it can involve IO when executing engine functions. It's also executing code from the backend jar as they provide their own engine functions, we should be robust to arbitrary ""bad code"" executing from the backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/786:250,robust,robust,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/786,1,['robust'],['robust']
Availability,"inputs"": {...; },; ""returnCode"": -1,; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }],; ""jobId"": ""2957"",; ""backend"": ""JES"",; ""end"": ""2016-12-02T15:05:42.655Z"",; ""stderr"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stderr"",; ""callRoot"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data"",; ""attempt"": 1,; ""executionEvents"": [...]; },; ""outputs"": {. },; ""workflowRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc"",; ""id"": ""3608d6ca-fbb4-4232-b197-268058470bfc"",; ""inputs"": {...; },; ""submission"": ""2016-12-01T21:21:40.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/workflow.logs/workflow.3608d6ca-fbb4-4232-b197-268058470bfc.log"",; ""end"": ""2016-12-02T15:05:42.868Z"",; ""start"": ""2016-12-02T15:05:40.873Z""; }; ```. Here there's no ""message"" and there are ""timestamp"" and ""failure"". ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""account_name\"": \""abaumann.firecloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:4834,failure,failures,4834,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failures']
Availability,"instead of registering the workflow id as the collection, as currently happens, register the collection name. if a collection was not specified (it's optional) create a collection name, `USERNAME_caas_collection`. . If the collection name specified already exists in sam and the user does not have write access to it it will return an error (or at least that's the belief of the author of this ticket). Make sure to detect that and return an appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2837:335,error,error,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2837,2,['error'],['error']
Availability,"integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:1924,reliab,reliably,1924,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['reliab'],['reliably']
Availability,"io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:2201,echo,echo,2201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,1,['echo'],['echo']
Availability,"ion http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting thi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:1118,error,error,1118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,3,['error'],['error']
Availability,"ion settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow polling stopped; [2019-02-11 10:13:36,32] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-11 10:13:36,33] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15261,down,down,15261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,2,['down'],['down']
Availability,"ionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); Caused by: java.lang.ArrayIndexOutOfBoundsException: null; 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Error summarizing metadata; java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:2336,ERROR,ERROR,2336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"ion\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] /build/cos-tools: \nls: cannot access '/build/cos-tools': No such file or directory\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz\ncurl: (16) Error in the HTTP2 framing layer\n\nreal\t0m3.705s\nuser\t0m0.573s\nsys\t0m1.834s\n[ERROR 2021-02-22 23:09:25 UTC] Could not download toolchain archive from https://storage.googleapis.com/chromiumos-sdk/2020/06/x86_64-cros-linux-gnu-2020.06.25.065836.tar.xz, giving up.\n"". What caused those errors? Any recommendations?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:5121,Down,Downloading,5121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,6,"['Down', 'ERROR', 'Error', 'down', 'error']","['Downloading', 'ERROR', 'Error', 'download', 'errors']"
Availability,"ironment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredentialFactory.scala:61); at cromwell.engine.backend.io.filesyste",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1261:1126,avail,available,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261,1,['avail'],['available']
Availability,"is HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:1200,error,error,1200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['error'],['error']
Availability,"is bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actually shows the issue twice -- once in the section starting with `if(defined(variant_call_after_earlyQC_filtering.errorcode)) {` and once in the section starting with `if(defined(profile_bam.strain)) {`. Interestingly, the res",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1463,error,errorcode,1463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,1,['error'],['errorcode']
Availability,"is happened after we turned off call caching to work around DSDEEPB-2938 for now and this error popped up in what seems to be post success processing of the task. Logs are in the comment. This was run on gotc-prod against JES-staging. ``` scala; 34108:2016-03-10 22:53:11,258 cromwell-system-akka.actor.default-dispatcher-8 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from - to Initializing; 50265:2016-03-10 22:55:07,880 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Initializing to Running; 64201:2016-03-10 22:57:00,293 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Running to Success. 2016-03-10 22:57:05,450 cromwell-system-akka.actor.default-dispatcher-27 ERROR - CallActor [UUID(7721686a):StripBamExtension:8]: Failing call: 500 Internal Server Error; Internal Error; cromwell.util.AggregatedException: 500 Internal Server Error; Internal Error; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112) ~[cromwell.jar:0.19]; at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:623) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:662) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:657) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureIns",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:1005,Error,Error,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,2,['Error'],['Error']
Availability,ispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8069,recover,recover,8069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,ispatcher-41 INFO - Triggering log of execution token queue status. Effective log interval = 300 seconds; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsync,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2011,ERROR,ERROR,2011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['ERROR'],['ERROR']
Availability,"isting workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/9813529/cromwell_singularity.conf.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:1560,error,errors,1560,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,1,['error'],['errors']
Availability,"ite batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1652,down,down,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['down'],['down']
Availability,"ith call caching enabled.; - server mode. This error is above my ability to debug. Cromwell failed after that. It did not exit, but was no longer responsive. ```[ERROR] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 6c383c35-d791-4971-aecd-0723726c8a7b failed (during ExecutingWorkflowState): JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; java.lang.RuntimeException: JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:147); at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validateCredential(GoogleAuthM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:1067,Fault,FaultHandling,1067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['Fault'],['FaultHandling']
Availability,"ith read = true and write = true; [2022-12-15 21:28:23,72] [info] BT-322 788d8048:main.all_qced_sample_lists:4:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:23,72] [info] BT-322 788d8048:main.all_qced_sample_lists:0:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:23,78] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:5:1-20000000037 [788d8048main.all_qced_sample_lists:5:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,78] [info] BT-322 788d8048:main.all_qced_sample_lists:5:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = 0EDB33C059ED489B1F78F3502B7DB8AC.; [2022-12-15 21:28:23,78] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:5:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,79] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:1:1-20000000039 [788d8048main.all_qced_sample_lists:1:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,79] [info] BT-322 788d8048:main.all_qced_sample_lists:1:1 cache hit copying success with aggregated hashes: initial = 8BB8C81C27BFD2533FC9743A70F55DB1, file = DC6FF6846E1CC843B0D79723739936B2.; [2022-12-15 21:28:23,79] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.all_qced_sample_lists:1:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:23,80] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.all_qced_sample_lists:3:1-20000000035 [788d8048main.all_qced_sample_lists:3:1]: Unrecognized runtime attribute keys: shortT; ask, dx_timeout; [2022-12-15 21:28:23,80] [info] BT-322 788d8048:main.all_qce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:42690,failure,failures,42690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,itionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:4036,Error,ErrorOr,4036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"itted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:85988,failure,failure-mode,85988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['failure'],['failure-mode']
Availability,java.lang.ClassNotFoundException: languages.cwl.CwlV1_0LanguageFactory error while running Cromwell 85 and 86 jar in OnPrem system,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247,1,['error'],['error']
Availability,"java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.net.SocketException: Connection reset; 	at java.net.SocketInputStream.read(SocketInputStream.java:210); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101); 	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144); 	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174); 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3011); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.i.MetadataSummaryRefreshActor - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:3615,failure,failure,3615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['failure'],['failure']
Availability,"java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21783,error,errors,21783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using the AWS CLI as follows, it works fine. ```; aws batch register-job-definition --cli-input-json file://event_history.json; {; ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:064561331775:job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1753,ERROR,ERROR,1753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['ERROR'],['ERROR']
Availability,jobs submitted to AWS Batch from cromwell give container error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604,1,['error'],['error']
Availability,"k.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online documentation (if it exists). # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:8537,echo,echo,8537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['echo'],['echo']
Availability,"k.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.impl.MetadataServiceActor - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:5996,failure,failure,5996,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['failure'],['failure']
Availability,"kQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in the WorkflowExecutionActor. kshakir [4:59 PM]; uploaded and commented on this file ; caas_cromwell_logs.txt; 6 MB Plain Text; Scratch the-restart-in-the-middle idea. I think they might have been submitted after the last restart. Gotta take off for now but here's the log dump for chatting at standup tomorrow. mcovarr [5:02 PM]; that looks odd, it didn't actually restart anything; how was this shut down?. kshakir [5:05 PM]; I assume caas shutdowns are _not_ clean. mcovarr [5:06 PM]; Just curious, I *thought* Cromwell always put the workflow store into a sane state when it started up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:3071,heartbeat,heartbeat,3071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,2,"['down', 'heartbeat']","['down', 'heartbeat']"
Availability,"ka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:2500,echo,echo,2500,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,kendJobExecutionActor.startMetadataKeyValues(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:516); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.executeOrRecover(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.core.retry.Retry$.withRetry(Retry.scala:36); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:50); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:77); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at akka.actor.Actor$class.aroundReceive(Actor.scala:496); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:4669,robust,robustExecuteOrRecover,4669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['robust'],['robustExecuteOrRecover']
Availability,"ker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COPY ./app-config /app-config. ENTRYPOINT [""/bin/sh"", ""-c""]; ```. when i submit a wdl did not use docker it was ok. but when i submit a wdl need to use docker, a error apear.; ```; /cromwell-working-dir/cromwell-executions/RNAseq/26e3c339-39d3-442f-b93e-8269dc7f9fa6/call-fastp_pe/shard-7/execution/script.submit: line 2: docker: command not found; ```. Is that means I shoud install a docker deamon in cromwell dockerfile or i cloud change some config setting to fix this. please help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:2982,error,error,2982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['error'],['error']
Availability,"kflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.sex_aneuploidy' (scatter index: None, attempt 1); [2022-12-15 21:22:59,78] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.month_of_birth' (scatter index: None, attempt 1); [2022-12-15 21:22:59,84] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.year_of_birth:-1:1-20000000028 [9e4f5894main.year_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,84] [info] BT-322 9e4f5894:main.year_of_birth:-1:1 cache hit copying success with aggregated hashes: initial = 09247459DDA5EA8DF661D5F490C81E8B, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,84] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.year_of_birth:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,36] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.phenotype:-1:1-20000000025 [9e4f5894main.phenotype:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,36] [info] BT-322 9e4f5894:main.phenotype:-1:1 cache hit copying success with aggregated hashes: initial = 018D1BC619E22671C2125EEDE82AB210, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:23:00,36] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.phenotype:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:23:00,37] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.date_of_death:-1:1-20000000026 [9e4f5894main.date_of_death:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:23:00,37] [info] BT-322 9e4f5894:main.date_of_death:-1:1 cache hit copying success with aggregated hashes: initial = 179EA0EE9B87629",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:25253,failure,failures,25253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"kjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,912 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:9736,error,error,9736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['error'],['error']
Availability,"kpointClose script done; [2023-02-08 16:24:30,52] [info] dataFileCache commit start; [2023-02-08 16:24:30,53] [info] dataFileCache commit end; [2023-02-08 16:24:30,58] [info] checkpointClose end; [2023-02-08 16:24:30,58] [info] Checkpoint end - txts: 3060; [2023-02-08 16:24:30,59] [info] Checkpoint start; [2023-02-08 16:24:30,59] [info] checkpointClose start; [2023-02-08 16:24:31,52] [info] checkpointClose synched; [2023-02-08 16:24:31,65] [info] checkpointClose script done; [2023-02-08 16:24:31,65] [info] dataFileCache commit start; (...); ```. And at the end:; ```; 2023-02-08 16:32:11,54] [info] checkpointClose synched; [2023-02-08 16:32:11,57] [info] checkpointClose script done; [2023-02-08 16:32:11,57] [info] dataFileCache commit start; [2023-02-08 16:32:11,57] [info] dataFileCache commit end; [2023-02-08 16:32:11,69] [info] checkpointClose end; [2023-02-08 16:32:11,69] [info] Checkpoint end - txts: 5342; [2023-02-08 16:32:21,70] [info] Checkpoint start; [2023-02-08 16:32:21,70] [info] checkpointClose start; [2023-02-08 16:32:21,70] [info] checkpointClose synched; [2023-02-08 16:32:21,74] [info] checkpointClose script done; [2023-02-08 16:32:21,74] [info] dataFileCache commit start; [2023-02-08 16:32:21,76] [info] dataFileCache commit end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:2507,checkpoint,checkpointClose,2507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,11,"['Checkpoint', 'checkpoint', 'error']","['Checkpoint', 'checkpointClose', 'error']"
Availability,"l workflows. I realize I might be using Cromwell for things it was not supposed to do, but since it is so very slow I thought I might ask anyway. Just so you understand my application, I'm not running bioinformatics pipelines: I'm rather interested in using WDL instead of Makefiles to do various scientific calculations. It's often a bunch of relatively simple Python or R scripts that I want to connect in a chain, and then sweep some parameters, etc. While developing and debugging the workflows (the majority of the time) I have a need for running and re-running single tasks or small sub-workflows many times. I was hoping I could use Cromwell to do this, but at present it just is too slow. Let's say I run a tiny workflow like the [Hello world example](http://cromwell.readthedocs.io/en/develop/tutorials/FiveMinuteIntro/#step-2-writing-your-first-workflow-description) from Cromwell's docs:; ```wdl ; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. I'm on Ubuntu 16.04, I start a Cromwell server and wait for it to spin up. Then. ```bash; $ time /usr/lib/jvm/java-8-openjdk-amd64/bin/java -jar cromwell-31.jar submit wf.wdl ; [2018-04-04 15:37:15,92] [info] Slf4jLogger started; [2018-04-04 15:37:17,06] [info] Workflow ab6ec0d5-20d2-4113-a58a-0b0e15097476 submitted to http://localhost:8000. real	0m2.536s; user	0m5.316s; sys	0m0.292s; ```. Just submitting the job takes 2.5 seconds wall clock time. Then watching the server do the job takes another 21 seconds:. ```bash; 2018-04-04 15:37:17,001 cromwell-system-akka.dispatchers.api-dispatcher-117 INFO - WDL (Unspecified version) workflow ab6ec0d5, so essentially the job is equivalent to-20d2-4113-a58a-0b0e15097476 submitted. [various log outputs]. 2018-04-04 15:37:38,440 cromwell-system-akka.dispatchers.engine-dispatcher-6 INFO - WorkflowExecutionActor-ab6ec0d5-20d2-4113-a58a-0b0e15097476 [UUID(ab6ec0d5)]: Workflow myWorkflow complete. ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3477:1035,echo,echo,1035,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3477,1,['echo'],['echo']
Availability,"l$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7733,error,error,7733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:5454,robust,robustExecuteOrRecover,5454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,2,"['Error', 'robust']","['Error', 'robustExecuteOrRecover']"
Availability,"l_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1872,error,error,1872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['error']
Availability,"l_root\/cwl.output.json 2>\/dev\/null | jq -r '.. | .path? \/\/ .location? \/\/ empty | gsub(\\\""file:\/\/\\\""; \\\""\\\"")' > \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt\"""",; ""endTime"": ""2018-08-14T16:16:57.673071Z""; },; {; ""startTime"": ""2018-08-14T16:17:05.013311Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:17:06.327716Z""; },; {; ""startTime"": ""2018-08-14T16:14:28.018319Z"",; ""description"": ""Started pulling \""ubuntu@sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7\"""",; ""endTime"": ""2018-08-14T16:14:32.007439Z""; },; {; ""startTime"": ""2018-08-14T16:13:55.499871Z"",; ""description"": ""Started pulling \""google\/cloud-sdk:alpine\"""",; ""endTime"": ""2018-08-14T16:14:03.091743Z""; },; {; ""startTime"": ""2018-08-14T16:14:35.424927Z"",; ""description"": ""Started running \""\/bin\/sh -c mkdir -p $(dirname \/cromwell_root\/cgp-commons-multi-region-public\/topmed_open_access\/711e55a4-c3e9-50af-8a85-41829fb84cae\/NWD455342.recab.cram) && \/scripts\/dosUrlLocalizer.sc dos:\/\/dg.4503\/4d427aa3-5640-4f00-81ae-c33443f84acf \/cromwell_root\/cgp-commons-multi-region-p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:5037,echo,echo,5037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,la:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:6931,robust,robustExecuteOrRecover,6931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['robust'],['robustExecuteOrRecover']
Availability,la:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:5197,Error,ErrorOr,5197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,la:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2846,Error,ErrorOr,2846,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"ld probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. Here's what I added to my config for 37 that causes the missing class errors:; ```; services { ; MetadataService { ; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; } ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:4371,alive,alive,4371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,2,"['alive', 'error']","['alive', 'errors']"
Availability,"le Backend, which prevents running wdl workflows on many machine types available on GCP, including those provisioned with modern GPUs. I believe the simplest and most general solution would be to pass the machine type directly from the wdl configuration to the Google Batch API. The idea is that this approach would be more resilient to machine types being added or deprecated on GCP, as users would only need to update their wdl workflows in such cases. An alternative approach of mapping machine specs (e.g.: cpu platform and gpu requirements) to standard machine types would potentially introduce an additional layer of maintenance with little benefit. This PR adds support for a new standardMachineType key in the runtime section, which is only parsed for the Google backend. ### Testing. I deployed this internally and verified I can successfully run the following wdl workflow:. ```; version 1.0. task nvidia_smi {; input {; String docker_version; }. command <<<; nvidia-smi. touch .done; echo ""Finished at $(date)""; >>>. runtime {; docker: <internal image>; disks: ""local-disk 50 SSD""; memory: ""32G""; preemptible: 0; gpuCount: 1; gpuType: ""nvidia-tesla-a100""; standardMachineType: ""a2-highgpu-1g""; }. output {; File done = "".done""; }; }. workflow nvidia_smi_wf {; input {; String docker_version; }; ; call nvidia_smi as nvidia_smi_call {; input:; docker_version = docker_version; }. output {; File done = "".done""; }; }; ```. ### Next steps. - [ ] Confirm this approach is in the right direction with the cromwell team.; - [ ] Work on proper unit tests and get this PR ready to be merged. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545:1155,echo,echo,1155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545,1,['echo'],['echo']
Availability,"le Life Sciences v2beta in the region `europe-west4` due to data localisation needs. I mention this because of another recent Hellow World related issue #6462 where the demo did seem to work. 1) The documentation for the [Hello World](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) example is out of date. In `google.conf` it still lists the configuration for ""JES"" backend. 2) In the same tutorial ([Setting up PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2)), the instructions for which roles to assign to the GCP service account are outdated. 3) Once the user puzzles together which parts to replace, the execution is still failing (for me at least).; I run the following command ` java -Dconfig.file=cromwell.BROADexamples.v4.conf -jar cromwell-66.jar run hello.wdl -i hello.inputs`, which results in the following `Request contains an invalid argument.` error (abbreviated to the relevant section):; ```; [2021-08-13 10:44:39,31] [info] Running with database db.url = jdbc:hsqldb:mem: ...; ...; [2021-08-13 10:44:54,04] [info] Reference disks feature for PAPIv2 backend is not configured.; [2021-08-13 10:44:54,46] [info] Slf4jLogger started; [2021-08-13 10:44:54,73] [info] Workflow heartbeat configuration:; ...; [2021-08-13 10:44:55,42] [info] Running with 3 PAPI request workers; ...; [2021-08-13 10:44:55,79] [info] Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not trigge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:1167,error,error,1167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['error'],['error']
Availability,"le genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set the resources used by Cromwell in local, what file I need to create/modify or how should I cange my code? Thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3251,echo,echo,3251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,2,['echo'],['echo']
Availability,"le util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So perhaps the reason this issue is intermittent is because the length of the logs from this command are occasionally too long for the `cfn-init` script? Or this might be a red herring. To aid with debugging, here are some useful logs; * [ec2_log.txt](https://github.com/broadinstitute/cromwell/files/2887960/ec2_log.txt): This is the console output from the EC2 instance that failed. I've censored out some of the key data, just in case any of it involves my own public key (probably unnecessary, but I doubt it's related); * [stack_description.json.txt](https://github.com/broadinstitute/cromwell/files/2887962/stack_description.json.txt): The output from `aws cloudformation describe-stacks` on the stack I spun up. This should give you the exact parameters I used when it last failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674:1964,error,error,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674,1,['error'],['error']
Availability,"lect.Method.invoke(Method.java:497); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.WdlFunctions$$anonfun$getFunction$1.apply(WdlFunctions.scala:12); 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:181); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.Task$$anonfun$11.apply(Task.scala:180); 	... 38 more; Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; Backend Error; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeMedia(AbstractGoogleClientRequest.java:380); 	at com.google.api.services.storage.Storage$Objects$Get.executeMedia(Storage.java:4784); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:478); 	... 74 more; ```. Offending task:. ```; task no_address {; Boolean no_address_requested. command {; echo foo; }; output {; String out = read_string(stdout()); }; runtime {; docker: ""python:latest""; noAddress: ""${no_address_requested}""; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1923:8457,echo,echo,8457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1923,1,['echo'],['echo']
Availability,"led to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83e163cce3cbd8e1c1446db372/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/callcaching/CallCacheDiffActor.scala#L164-L168)] is missing a `case (key, subObject: JsArray)`. I confirmed this by adding the case (I don't know scala, nor inner workings of Cromwell except enough to know this probably isn't a good way to do it, but just wanted to see if my suspicion was correct):. ```scala; case (key, subObject: JsArray) => Map(keyPrefix + key -> subObject.elements.mkString(""|"")).validNel; ```. Which fixed the error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:2526,error,error,2526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,1,['error'],['error']
Availability,licate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:106); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$apply$1.applyOrElse(StandardAsyncExecutionActor.scala:105); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at scala.util.Failure.recoverWith(Try.scala:203); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1.apply(StandardAsyncExecutionActor.scala:105); at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:198); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:107); at cromwell.backend.standard.StandardAsyncExecutionActor$class.commandScriptContents(StandardAsyncExecutionActor.scala:170); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:2325,recover,recoverWith,2325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,1,['recover'],['recoverWith']
Availability,"ll appears to be improperly tokenizing string interpolations when using a variable that is prepended by the letters: `if` in any format. It seems to think that this is actually the start of a conditional `if _ then _ else` block instead of a variable name. The parser does not appear to be discriminating against the lack of whitespace in variables with the form `if[a-zA-Z0-9_]+` and fails to parse with an error. ### How to reproduce. ```; cat <<EOF > test.wdl; version 1.0; task test_task {; String ifl_token=""a""; command <<<; echo ""~{ifl_token}""; >>>; }. workflow test {; call test_task; }; EOF. java -jar cromwell-84.jar run. test.wdl; ```. ### Expected; 1. The workflow to parse correctly and to echo `a` when running. ### Actual Error. The workflow fails to parse with the following error:. ```; [2022-11-25 11:10:39,68] [info] WorkflowManagerActor: Workflow 45701495-7113-40d6-ac32-dab5247f37e7 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Unexpected symbol (line 5, col 20) when parsing 'e'. Expected then, got ""}"". echo ""~{ifl_token}""; ^. $e = :identifier <=> :lparen $_gen23 :rparen -> FunctionCall( name=$0, params=$2 ); ; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:257); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:227); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:222); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); ```. ### Environment:; Tested on:; - Cromwell 84; - Cromwell 66",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6956:1101,ERROR,ERROR,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6956,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"ll-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data"",; ""attempt"": 1,; ""executionEvents"": [...]; },; ""outputs"": {. },; ""workflowRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc"",; ""id"": ""3608d6ca-fbb4-4232-b197-268058470bfc"",; ""inputs"": {...; },; ""submission"": ""2016-12-01T21:21:40.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/workflow.logs/workflow.3608d6ca-fbb4-4232-b197-268058470bfc.log"",; ""end"": ""2016-12-02T15:05:42.868Z"",; ""start"": ""2016-12-02T15:05:40.873Z""; }; ```. Here there's no ""message"" and there are ""timestamp"" and ""failure"". ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""account_name\"": \""abaumann.firecloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5\""\n}"",; ""inputs"": ""{\""aggregate_data_workflow.aggregate_data.input_array\"":[\""bar, baz\""]}"",; ""workflow"": ""task aggregate_data {\n\tArray[File] input_array\n\n\tcommand {\n echo \""foo\""\n\n\t}\n\n\toutput {\n\t\tArray[Array[File]] output_array = [input_array]\n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:5403,failure,failure,5403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failure']
Availability,"ll.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/ and I've run into the following errors during 'sbt assembly': . [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.bas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1057,error,error,1057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,llection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1685,Error,ErrorOr,1685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"llo.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2021-09-27 13:48:31,314 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2021-09-27 13:48:31,320 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - 075e0cf3-194b-4f53-a43d-d31f0b370f79-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(075e0cf3)]: Could not copy a suitable cache hit for 075e0cf3:wf_hello.hello:-1:1. No copy attempts were made.; 2021-09-27 13:48:31,449 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: `echo ""Hello Cromwell! Welcome to Cromwell . . . on AWS!""`; 2021-09-27 13:48:33,376 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Adjusting boot disk size to 16 GB: 10 GB (runtime attributes) + 5 GB (user command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:11547,echo,echo,11547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['echo'],['echo']
Availability,llo:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3060,recover,recover,3060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"llowing toy workflow, which I invoke with `-jar cromwell-25.jar run example.wdl empty_inputs.json`. It reads a tsv that has either one column or two and scatters a task over each row of the tsv. The task prints the second column if it is present. When the input `fake.tsv` has one column, everything is fine. However, when it has two columns eg; ```; 1</TAB>1; 2</TAB>2; ```; it fails with ""Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlIntegerType)) with this value: List(WdlInteger(1), WdlInteger(2))"". (*Side question: why is it trying to make a list out of values in two different scattered rows?*) Using `Int?` in the conditional instead of `Int` does not make a difference. Another bizarre twist: if instead of reading in from a file I hardcode the array, the error persists when each row of the array has the same number of columns but goes away when some rows have two columns and some do not. That is: `Array[Array[Int]] table = [[1,1,1], [2,2]]` works, but `Array[Array[Int]] table = [[1,1], [2,2]]` gives the same error as above. ```; task printInt {; Int? int. command { echo ""${int}"" > out.txt }; output { File out = ""out.txt"" }; }. workflow optional {. Array[Array[Int]] table = read_tsv(""fake.tsv""); scatter (row in table) {. if (length(row) == 2) {; Int int = row[1]; }. call printInt {input: int=int }; }; }; ```. ---. @davidbenjamin commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277091241). Pinging @LeeTL1220 because this is blocking Mutect. ---. @LeeTL1220 commented on [Thu Feb 02 2017](https://github.com/broadinstitute/wdl/issues/87#issuecomment-277095282). @kcibul This is important. On Feb 2, 2017 4:34 PM, ""David Benjamin"" <notifications@github.com> wrote:. > Pinging @LeeTL1220 <https://github.com/LeeTL1220> because this is; > blocking Mutect.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1952:1164,error,error,1164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1952,1,['error'],['error']
Availability,"llows：. > [mysqld]; max_connections=5024; thread_cache_size=1000; datadir=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/Data/; socket=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/socket/cromwell.sock; default-time-zone='+8:00'; port=3310; skip-character-set-client-handshake; init_connect='SET collation_connection = utf8mb4_unicode_ci'; init_connect='SET NAMES utf8mb4'; character-set-server=utf8mb4; collation-server=utf8mb4_unicode_ci; #open slow query logging; slow_query_log = ON; slow_query_log_file = /zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/slow_cromwell.log; long_query_time = 1; [mysqld_safe]; log-error=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/error.log; pid-file=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/pid/mysqld_cromwell.pid; [client]; default-character-set=utf8mb4; > . Then I configured the flow. The following error was reported when scatter 478 tasks were needed in one step：. > 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Failed to summarize metadata. java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:1038,ERROR,ERROR,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,1,['ERROR'],['ERROR']
Availability,"lobs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*report.html /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 2> /dev/null ) || ( ln /cromwell_root/*report.html /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952 | grep -v cromwell_glob_control_file > /cromwell_root/glob-c36f18b89b7c5f50e2dc9ce3b4cc3952.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7/cromwell_glob_control_file. # hardlink or symlink all the files into the glob directory; ( ln -L /cromwell_root/*data.txt /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7 2> /dev/null ) || ( ln /cromwell_root/*data.txt /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7 | grep -v cromwell_glob_control_file > /cromwell_root/glob-a678e8cb2ce368b6b92853d6f0816af7.list. ); mv /cromwell_root/rc.tmp /cromwell_root/rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:7397,echo,echo,7397,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"local/DockerCliClient.scala#L26; https://github.com/broadinstitute/cromwell/blob/1898d8103a06d160dc721d464862313e78ee7a2c/dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala#L78-L92. Can we instead use the image ID instead of the digest when using local images?. <details>. <summary>log output</summary>. ```; [INFO] [09/16/2019 11:07:14.821] [cromwell-system-akka.dispatchers.engine-dispatcher-40] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [INFO] [09/16/2019 11:07:14.830] [cromwell-system-akka.dispatchers.engine-dispatcher-76] [akka://cromwell-system/user/SingleWorkflowRunnerActor/JobExecutionTokenDispenser] Assigned new job execution tokens to the following groups: 2b766fe6: 1; [2019-09-16 11:07:16,20] [error] Docker pull failed; java.lang.RuntimeException: Error running: docker pull <image>; Exit code: 1; Error response from daemon: pull access denied for <image> repository does not exist or may require 'docker login': denied: requested access to the resource is denied. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:1371,Error,Error,1371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,2,['Error'],['Error']
Availability,"log](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. labels: test-library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:3385,down,down,3385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['down'],['down']
Availability,lom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementTo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:4831,Error,ErrorOr,4831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,lom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:2480,Error,ErrorOr,2480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519:3711,down,download,3711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519,1,['down'],['download']
Availability,"low task is tryting to do the following assignation...:. ```; Array[File]? y ; # ...; Array[File] x = select_first([y, []]); ```; where ```y``` is a task argument that is assigned the value of a ```Array[File]+``` argument by the invoking workflow. Eventually I fixed this by unraveling the unnecessary conversions since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception an",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755:1225,ERROR,ERROR,1225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755,1,['ERROR'],['ERROR']
Availability,"lowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1245,Error,Error,1245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['Error'],['Error']
Availability,"luate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data.log""; },; ""start"": ""2016-08-01T19:56:48.000000Z""; }]; },; ""outputs"": {. },; ""id"": ""7be16669-0f81-4e19-96a0-dbe4b72cee8e"",; ""submission"": ""2016-08-01T19:56:48.000000Z"",; ""status"": ""Failed"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""start"": ""2016-08-01T19:56:48.000000Z""; }; ```. Here the failures section has a nested structure.; ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""...""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""12677a12-bca2-41a6-b583-596262c7e0c7"",; ""inputs"": {...; },; ""submission"": ""2017-01-31T17:54:48.812Z"",; ""status"": ""Failed"",; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/4503a3b1-5b50-4474-8a31-809e73510622/workflow.logs/workflow.12677a12-bca2-41a6-b583-596262c7e0c7.log"",; ""end"": ""2017-01-31T17:55:10.439Z"",; ""start"": ""2017-01-31T17:54:50.257Z""; }; ```. This inconsistency in the format of the failure messages makes it difficult to show properly formated failure messages in our UI.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:8475,failure,failures,8475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,5,"['Error', 'failure']","['Error', 'failure', 'failures']"
Availability,"lyOrElse(CopyWorkflowOutputsActor.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowOutputsActor.aroundReceive(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerException",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:6906,ERROR,ERROR,6906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['ERROR'],['ERROR']
Availability,"m - to WaitingForReturnCode; [2022-12-15 21:28:13,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 1; [2022-12-15 21:28:13,69] [info] BT-322 788d8048:main.ethnic_sample_lists_task:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:13,85] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.ethnic_sample_lists_task:-1:1-20000000033 [788d8048main.ethnic_sample_lists_task:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:13,85] [info] BT-322 788d8048:main.ethnic_sample_lists_task:-1:1 cache hit copying success with aggregated hashes: initial = B09218865D7CA13056B00F9F90E94675, file = 66CE4C8C9D1761D150F95616CE84D5F3.; [2022-12-15 21:28:13,85] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.ethnic_sample_lists_task:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:14,50] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Job results retrieved (CallCached): 'main.ethnic_sample_lists_task' (scatter index: None, attempt 1); [2022-12-15 21:28:20,55] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-SubWorkflowActor-SubWorkflow-main:-1:1 [788d8048]: Starting main.all_qced_sample_lists (6 shards); [2022-12-15 21:28:23,68] [info] Assigned new job execution tokens to the following groups: 9e4f5894: 6; [2022-12-15 21:28:23,69] [info] BT-322 788d8048:main.all_qced_sample_lists:2:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:23,70] [info] BT-322 788d8048:main.all_qced_sample_lists:1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:23,70] [info] BT-322 788d8048:main.all_qced_sample_lists:5:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:28:23,71] [info] BT-322 788d8048:main.all_qced_sample_lists:3:1 is eligible for ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:40654,failure,failures,40654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"m Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1003,Error,Error,1003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['Error'],['Error']
Availability,"m does not exist on cromwell 30 (with jes backend). /cromwell_root is almost certainly actually mounted at /dev/sdb (that device exists, does not appear to be used anywhere, has the appropriate size as checked in /sys/block/sdb/size, and is typically what's listed as the filesystem in cromwell 30). . I know it's weird to even care about that, so to explain, my cromwell monitoring script looks at the block device corresponding to /cromwell_root in order to measure disk IO, which can potentially be a source of problems for some of the SV algorithms we're trying to debug/string together. the .wdl file; ```; workflow GetSystemInfo {; call get_system_info_docker; }. task get_system_info_docker {; command <<<; echo ""**** df -h""; df -h; ; echo; echo ""**** /""; ls -l /; ; echo; echo ""**** /mnt""; ls -l /mnt; ; echo; echo ""**** /dev""; ls -l /dev; ; if [ -d /dev/disk ]; then; echo; echo ""**** /dev/disk""; ls /dev/disk; fi; ; echo; echo ""**** /proc/mounts""; cat /proc/mounts; ; echo; echo ""**** /etc/mtab""; cat /etc/mtab; ; echo; echo ""**** /sys/block""; find -L /sys/block -maxdepth 2; ; echo; echo ""**** /sys/block/sdb/size (converted to integer GB)""; echo ""$(($(cat /sys/block/sdb/size) * 512 / 2**30))""; ; echo; echo ""**** /sys/devices""; find -L /sys/devices -maxdepth 3; >>>; ; runtime {; docker: ""talkowski/delly""; memory: ""1.7 GB""; cpu: ""1""; disks: ""local-disk 250 HDD""; preemptible: 3; }; }; ```; Snips of relevant output from cromwell 36 (edited for brevity):; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/disk/by-id/google-local-disk; 245.1G 60.0M 245.0G 0% /cromwell_root; **** /dev; total 0; lrwxrwxrwx 1 root root 11 Nov 14 21:16 core -> /proc/kcore; lrwxrwxrwx 1 root root 13 Nov 14 21:16 fd -> /proc/self/fd; crw-rw-rw- 1 root root 1, 7 Nov 14 21:16 full; drwxrwxrwt 2 root root 40 Nov 14 21:16 mqueue; crw-rw-rw- 1 root root 1, 3 Nov 14 21:16 null; lrwxrwxrwx 1 root root 8 Nov 14 21:16 ptmx -> pts/ptmx; drwxr-xr-x 2 root root 0 Nov 14 21:16 pts; crw-rw-rw- 1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388:1111,echo,echo,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388,20,['echo'],['echo']
Availability,"m.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1321,error,error,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,mAsyncJobExecutionActor.scala:130); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:4830,robust,robustExecuteOrRecover,4830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,2,['robust'],['robustExecuteOrRecover']
Availability,mAsyncJobExecutionActor.scala:136); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:306); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:43); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:71); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.process,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:4553,robust,robustExecuteOrRecover,4553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['robust'],['robustExecuteOrRecover']
Availability,"mediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115))),List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115)))); echo $? > rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:2138,error,error,2138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,2,"['echo', 'error']","['echo', 'error']"
Availability,"ment.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:46); 	at slick.jdbc.StatementInvoker.foreach(StatementInvoker.scala:15); 	at slick.jdbc.StreamingInvokerAction.run(StreamingInvokerAction.scala:22); 	at slick.jdbc.StreamingInvokerAction.run$(StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 2018-11-12 21:58:32,022 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Cromwell 36 service started on 0:0:0:0:0:0:0:0:8000...; ```; ---; ### database conf info; ```; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; driver = ""slick.driver.MySQLDriver$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&allowPublicKeyRetrieval=true""; user = ""user""; password = ""123456""; connectionTimeout = 5000; }; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:metadata-db-file-path;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }; }; ```; if the metadata{...} is not set, than the error is same to #3346",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382:3688,error,error,3688,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382,1,['error'],['error']
Availability,"ments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-01 20:01:04,82] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: echo test1 > test1.txt; echo test2 > test2.txt; [2017-12-01 20:01:04,86] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: executing: /bin/bash /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/script; [2017-12-01 20:01:04,91] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:3233,error,error,3233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['error'],['error']
Availability,"mit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; apptainer exec --cleanenv --bind ""${cwd}:${docker_cwd},<path>"" ""$IMAGE"" ""${job_shell}"" ""${docker_script}""; """""". default-runtime-attributes; {; failOnStderr: false; continueOnReturnCode: 0; }; }; }. sge_docker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:3732,down,downloads,3732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,5,"['down', 'echo']","['downloads', 'echo']"
Availability,"mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap \; ""module load apptainer; apptainer exec \; --containall \; --bind /mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references:/mainfs/wrgl/reference_files/reference_genome/gcp-public-data--broad-references \; --bind ${cwd}:${docker_cwd} \; --bind /tmp:/tmp \; /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; ${job_shell} \; ${docker_script}""'; """""". kill = ""'scancel ${job_id}'"". check-alive = ""'squeue -j ${job_id}'"". job-id-regex = ""Submitted batch job (\\d+).*"". }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:2327,echo,echo,2327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,2,"['alive', 'echo']","['alive', 'echo']"
Availability,"mory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/MemoryRetryTest/3035199e-bf2b-49a2-be87-483; 9e96a08eb/call-TestOutOfMemoryRetry/stderr; Killed ; ``` . We have also noticed that in the out of memory case, no retrurnCode is written to the metadata. **Test wdl for reproduction:**; `version 1.0. workflow MemoryRetryTest {; input {; String message = ""Killed""; }; call TestOutOfMemoryRetry {}; call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; command <<<; echo ""Killed"" >&2; tail /dev/zero; >>>; runtime {; docker: ""ubuntu:latest""; cpu: ""1""; memory: ""1 GB""; disks: ""local-disk "" + 16 + "" HDD""; maxRetries: 1; preemptible: 0; }; }. task TestBadCommandRetry {; command <<<; echo ""Killed"" >&2; bedtools intersect nothing with nothing; >>>; runtime {; docker: ""ubuntu:latest""; cpu: ""1""; memory: ""1 GB""; disks: ""local-disk "" + 16 + "" HDD""; maxRetries: 1; preemptible: 0; }; }`. input_json:; `{; ""MemoryRetryTest.message"": ""Killed""; }`. Would appreciate your kind assistence!; Doron Shem-Tov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:2416,echo,echo,2416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,2,['echo'],['echo']
Availability,"mponent-based if an object is mutable. Your typical GraphNode is mutable, because it indirectly contains `GraphNodeSetter`. ---. @danbills commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875). I suggest we leave this as-is with the understanding that it could be a performance issue down the road. . >rework the whole thing later. This is a specific anti-goal. As I suggested, I would like to discuss w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > —; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diabetesgenetics.org/>, Broad Institute; <http://www.broadinstitute.org/>. ---. @geoffjentry commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577969). +1 on waiting for chris. @curoli- at the moment our priority is ge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:3251,down,down,3251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['down'],['down']
Availability,mwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2094,Error,Error,2094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,"['Error', 'Recover']","['Error', 'Recover']"
Availability,mwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1101,recover,recover,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,mwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:3968,robust,robustExecuteOrRecover,3968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['robust'],['robustExecuteOrRecover']
Availability,"mwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357) ~[cromwell.jar:0.19]; at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01T20:19:40.362-0400: 104296.863: [GC (Allocation Failure) [PSYoungGen: 1130870K->235812K(1864192K)] 3755109K->2861554K(7456768K), 0.0464226 secs] [Times: user=0.25 sys=0.00, real=0.04 secs] ; 2016-06-01T20:19:42.554-0400: 104299.055: [GC (Allocation Failure) [PSYoungGen: 1052454K->177588K(1864192K)] 3678197K->2815074K(7456768K), 0.0805924 secs] [Times: user=0.59 sys=0.00, real=0.08 secs] ; 2016-06-01T20:20:06.449-0400: 104322.950: [GC (Allocation Failure) [PSYoungGen: 1109940K->381765K(1864192K)] 3747426K->3196632K(7456768K), 0.1380993 secs] [Times: user=1.00 sys=0.01, real=0.14 secs] ; 2016-06-01T20:20:06.832-0400: 104323.333: [GC (Allocation Failure) [PSYoungGen: 1157703K->173892K(1864192K)] 3972570K->3267545K(7456768K), 0.0688009 secs] [Times: user=0.50 sys=0.00, real=0.07 secs] ; 2016-06-02 00:20:07,325 cromwell-system-akka.actor.default-dispatcher-332 INFO - JesBackend [UUID(fa18fa5f):RotateGVCF]: `python <<CODE; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:5596,Failure,Failure,5596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,4,['Failure'],['Failure']
Availability,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:2041,error,error,2041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,2,"['Error', 'error']","['Errors', 'error']"
Availability,"mwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:43.002063Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.575023Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:17:00.937007Z""; },; {; ""startTime"": ""2018-08-14T16:16:33.718991Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-40",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:20261,echo,echo,20261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"my code end ENV:. config file: . backend {; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. filesystems {; local {; localization: [; ""soft-link"",; ""hard-link"",; ""copy""; ]; }; }. runtime-attributes = """"""; String time = ""2-0""; Int cpus = 2; Int memory = 8000; String queue = ""compute""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${time} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${memory} \; --wrap ""/bin/bash ${script}""; """""". job-id-regex = ""Submitted batch job (\\d+).*""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; }; }; }. wdl file :; task SamToFastqAndBwaMem {; ......; ......; command <<<; set -o pipefail; set -e. # set the bash variable needed for the command-line; bash_ref_fasta=${ref_fasta}. java -Dsamjdk.compression_level=${compression_level} ${java_opt} -jar ${gotc_path}picard.jar \; SamToFastq \; INPUT=${input_bam} \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true \; | \; ${bwa_path}${bwa_commandline} /dev/stdin - 2> >(tee ${output_bam_basename}.bwa.stderr.log >&2) \; | \; samtools view -1 - > ${output_bam_basename}.bam. >>>; #runtime {; # backend: ""SLURM""; # memory: mem_size; # cpus: num_cpu; #}; output {; File output_bam = ""${output_bam_basename}.bam""; File bwa_stderr_log = ""${output_bam_basename}.bwa.stderr.log""; }; }. all parameters goes ok, but below are some problems:. 1:; Caused by: common.exception.AggregatedMessageException: Error(s):; :; Could not localize -> /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test:; doesn't exist; Cannot localize directory with symbolic links; /nfs/disk3/user/gaoyuhui/github/test/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/8fc94dc1-722b-40d5-9840-9d6e4a66db21/call-SamToFastqAndBwaMem/inputs/-1845554049/test -> /nfs/disk3/user/gaoyuhui/github/test: Ope",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4703:614,alive,alive,614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4703,1,['alive'],['alive']
Availability,"n also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11297,error,error,11297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,3,"['down', 'error']","['down', 'error']"
Availability,"n http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi Cromwell team,; I am running a Cromwell server on Google Cloud have recently upgraded from Cromwell 50 to 53. I am running into an issue that I believe relates to how the new `monitoring_image_script` interacts with the docker entrypoint. I have docker images where each container needs to ping a license server for authentication, and this happens via and `ENTRYPOINT`-executed script called `auth.sh`. With Cromwell 53, this is no longer being run when the container is being executed. This is how the actual docker command . ```; # Cromwell 53; 2020/09/30 13:07:42 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint=/bin/bash gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /cromwell_root/script ; ```. ```; #Cromwell 50; 2020/09/23 06:03:37 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= gcr.io/bioskryb/sentieon-201911-run@sha256:b4af9423297bb6566763b2c47b1da1620a68a4d34c210f0786a34a0ae85f62db /bin/bash /cromwell_root/script ; ```. I have tried setting this as is, I have tried setting `monitoring_image_script` to an empty string, and I have tried setting `monitoring_image_script=""auth.sh""`. None work, and the setting the `monitoring_image_script` to the auth script results in the same c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5901:1228,ping,ping,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5901,1,['ping'],['ping']
Availability,"n$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaze.util.TickWheelExecutor$Bucket.checkNext$1(TickWheelExecutor.scala:207); at org.http4s.blaze.util.TickWheelExecutor$Bucket.prune(TickWheelExecutor.scala:213); at org.http4s.blaze.util.TickWheelExecutor.go$3(TickWheelExecutor.scala:168); at org.http4s.blaze.util.TickWheelExecutor.org$http4s$blaze$util$TickWheelExecutor$$cycle(TickWheelExecutor.scala:171); at org.http4s.blaze.util.TickWheelExecutor$$anon$1.run(TickWheelExecutor.scala:68). To confirm https (port 443) access to the quay.io/50.17.122.58 in this environment, ; I executed wget; $ wget https://50.17.122.58/; but it caused . Resolving ... 202.241.78.237; Connecting to 202.241.78.237|:8080... connected. ERROR: certificate common name ‘*.quay.io’ doesn't match requested host name ‘50.17.122.58’.; To connect to 50.17.122.58 insecurely, use `--no-check-certificate'. I guess an option similar to `--no-check-certificate' in wget need to be implemented in cromwell ; to fix this problem. Could you please do it?. Many thanks in advance. Koji Yahara; Group Leader; Antimicrobial Resistance Research Center; National Institute of Infectious Diseases; Japan",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:2671,ERROR,ERROR,2671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,1,['ERROR'],['ERROR']
Availability,n(ProcessRunner.scala:20); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$isAlive$1(SharedFileSystemAsyncJobExecutionActor.scala:196); 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.isAlive$(SharedFileSystemAsyncJobExecutionActor.scala:191); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.isAlive(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.reconnectToExistingJob(SharedFileSystemAsyncJobExecutionActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.recover$(SharedFileSystemAsyncJobExecutionActor.scala:159); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecove,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:1618,recover,recover,1618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['recover'],['recover']
Availability,"n.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1379,error,error,1379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,n.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:4342,error,errors,4342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,2,['error'],"['error', 'errors']"
Availability,n:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveM,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3325,error,error,3325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['error'],['error']
Availability,"n:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:1659,error,error,1659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['error'],['error']
Availability,nActor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275:2408,robust,robustExecuteOrRecover,2408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275,1,['robust'],['robustExecuteOrRecover']
Availability,nCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:6758,Error,ErrorOr,6758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,nComponent.scala:522) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:32) ~[cromwell-0.19.jar:0.19]; > at slick.driver.JdbcActionComponent$SimpleJdbcDriverAction.run(JdbcActionComponent.scala:29) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.liftedTree1$1(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at slick.backend.DatabaseComponent$DatabaseDef$$anon$2.run(DatabaseComponent.scala:237) ~[cromwell-0.19.jar:0.19]; > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74]; > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74]; > at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74]; > Caused by: org.hsqldb.HsqlException: integrity constraint violation: unique constraint or index violation; UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO table: SYMBOL; > at org.hsqldb.error.Error.error(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Constraint.getException(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.index.IndexAVLMemory.insert(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.persist.RowStoreAVL.indexRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.TransactionManagerMVCC.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.addInsertAction(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Table.insertSingleRow(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDML.insertRowSet(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementInsert.getResult(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.StatementDMQL.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.executeCompiledStatement(Unknown Source) ~[cromwell-0.19.jar:0.19]; > at org.hsqldb.Session.execute(Unknown Source) ~[cromwell-0.19.jar:0.19]; > ... 23 c,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/869:4279,Error,Error,4279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/869,1,['Error'],['Error']
Availability,"nReturnCode` is set as `true` or list of some return codes.; I think this is intended as described in the documentation but why?. This is very weird. In most cases, return code of OOM is just 137.; Why don't we have something like `memoryRetryReturnCode`. I think it's too dangerous too set `continueOnReturnCode` as `true`.; Cromwell will pass any failure in all tasks.; So I set it as `[0, 137]` to catch `SIGKILL` due to OOM.; I also tried with `true` though. Here is my simple OOM tester WDL. I tested it with PAPIv2 beta based on Life Sciences API. ```wdl; version 1.0. workflow mem_retry {; call fail_oom; }. task fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92"",; ""description"": ""PickedUp""; },; {; ""description"": ""Finished"",; ""cromwellId"": ""cromid-0a29b92"",; ""timestamp"": ""2020-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:1109,error,error-keys,1109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['error'],['error-keys']
Availability,"nStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data.log""; },; ""start"": ""2016-08-01T19:56:48.000000Z""; }]; },; ""outputs"": {. },; ""id"": ""7be16669-0f81-4e19-96a0-dbe4b72cee8e"",; ""submission"": ""2016-08-01T19:56:48.000000Z"",; ""status"": ""Failed"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""start"": ""2016-08-01T19:56:48.0000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:7242,error,errors,7242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['error'],['errors']
Availability,nTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttribut,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:5025,Fault,FaultHandling,5025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,"nTimeout = 120000; numThreads = 1; }; }. call-caching {; enabled = true; }. backend {; default = ""Local""; providers { ; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10; run-in-background = true; submit = ""/usr/bin/env bash ${script}""; root = ""cromwell-executions""; filesystems {; local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hasing-strategy: [""path+modtime""]; }; }; }; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int threads = 1; String memory = ""2g""; String dx_timeout; """"""; submit = """"""; sbatch; --account <account>; --partition ind-shared; --nodes 1; --job-name=${job_name}-%j; # --output=logs/{job_name}/$j.out; 	 -o ${out} -e ${err} ; --mail-type FAIL --mail-user <email-address>; --ntasks-per-node=${threads}; --mem=${memory}; --time=${dx_timeout}; --parsable; --chdir ${cwd}; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }}; ```. Here's the log printed to the terminal. Notice the jump from [2022-12-15 21:15:03,84] to [2022-12-15 21:22:59,01]; ```; $ java -Dconfig.file=workflow/cromwell.conf -jar utilities/cromwell-84.jar run workflow/expanse_workflow.wdl; [2022-12-15 21:14:44,99] [info] Running with database db.url =; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3. [2022-12-15 21:14:45,71] [info] dataFileCache open start; [2022-12-15 21:14:45,74] [info] dataFileCache open end; [2022-12-15 21:14:46,59] [info] checkpointClose start; [2022-12-15 21:14:46,59] [info] checkpointClose synched; [2022-12-15 21:14:46,71] [info] checkpointClose script ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:2768,alive,alive,2768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['alive'],['alive']
Availability,"n\t}\n\n\truntime {\n\t\tdocker : \""broadgdac/aggregate_data:31\""\n\t}\n\n\tmeta {\n\t\tauthor : \""Tim DeFreitas\""\n\t\temail : \""timdef@broadinstitute.org\""\n\t}\n\n}\n\nworkflow aggregate_data_workflow {\n\tcall aggregate_data\n}""; },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""preemptible"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:7123,failure,failure,7123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failure']
Availability,"n_list) then "" --transcript-list "" else """"; String annotation_def_arg = if defined(annotation_defaults) then "" --annotation-default "" else """"; String annotation_over_arg = if defined(annotation_overrides) then "" --annotation-override "" else """"; String filter_funcotations_args = if defined(filter_funcotations) && (filter_funcotations) then "" --remove-filtered-variants "" else """"; String excluded_fields_args = if defined(funcotator_excluded_fields) then "" --exclude-field "" else """"; String interval_list_arg = if defined(interval_list) then "" -L "" else """"; String extra_args_arg = select_first([extra_args, """"]). String dollar = ""$"". parameter_meta{; ref_fasta: {localization_optional: true}; ref_fai: {localization_optional: true}; ref_dict: {localization_optional: true}; input_vcf: {localization_optional: true}; input_vcf_idx: {localization_optional: true}; }. command <<<; set -e; export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. # Extract our data sources:; echo ""Extracting data sources zip file...""; mkdir datasources_dir; tar zxvf ~{data_sources_tar_gz} -C datasources_dir --strip-components 1; DATA_SOURCES_FOLDER=""$PWD/datasources_dir"". # Handle gnomAD:; if ~{use_gnomad} ; then; echo ""Enabling gnomAD...""; for potential_gnomad_gz in gnomAD_exome.tar.gz gnomAD_genome.tar.gz ; do; if [[ -f ~{dollar}{DATA_SOURCES_FOLDER}/~{dollar}{potential_gnomad_gz} ]] ; then; cd ~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:36449,echo,echo,36449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['echo'],['echo']
Availability,"n_pangenome_vcf=PANGENOME_VCF,; in_reference_genome=REF_GENOME,; in_executable=EXE_PATH,; in_fastq_file=reads_extraction_and_merging.fastq_file, # how to feed a task output to another one!!!; prefix_vcf=VCF_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:2966,echo,echo,2966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['echo'],['echo']
Availability,ncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:86); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.aroundReceive(GcpBatchAsyncBackendJob,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:4435,robust,robustExecuteOrRecover,4435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['robust'],['robustExecuteOrRecover']
Availability,"nd ; - localhost MySQL with call caching enabled.; - server mode. This error is above my ability to debug. Cromwell failed after that. It did not exit, but was no longer responsive. ```[ERROR] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow 6c383c35-d791-4971-aecd-0723726c8a7b failed (during ExecutingWorkflowState): JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; java.lang.RuntimeException: JesAsyncBackendJobExecutionActor failed and didn't catch its exception.; at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:147); at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:144); at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); at akka.actor.dungeon.FaultHandling$class.handleFailure(FaultHandling.scala:263); at akka.actor.ActorCell.handleFailure(ActorCell.scala:374); at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:459); at akka.actor.ActorCell.systemInvoke(ActorCell.scala:483); at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); at akka.dispatch.Mailbox.run(Mailbox.scala:223); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: Google credentials are invalid: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:66); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.valida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:1033,Fault,FaultHandling,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['Fault'],['FaultHandling']
Availability,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:1656,heartbeat,heartbeat,1656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"ne 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16319,recover,recoverWith,16319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Availability,"ne functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. WDL:. ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. input. ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. Gcloud log (edited):. ```; done: true; metadata:; '@type': type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata; createTime: '2021-08-03T15:21:55.984657Z'; endTime: '2021-08-03T15:24:03.533702405Z'; events:; - description: Worker released; timestamp: '2021-08-03T15:24:03.533702405Z'; workerReleased:; instance: google-pipelines-worker-xxxxxx; zone: us-central1-b; - containerStopped:; actionId: 19; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:24:02.823519462Z'; - containerStarted:; actionId: 19; description: Started running ""-c python -c 'i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:6143,echo,echo,6143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['echo'],['echo']
Availability,"neJobExecutionActor-main.genetic_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:54,15] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.genetic_sex' (scatter index: None, attempt 1); [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.categorical_covariates:0:1-20000000027 [9e4f5894main.categorical_covariates:0:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.categorical_covariates:0:1 cache hit copying success with aggregated hashes: initial = C760DC2B9015D0B787EF7BEE7D21AA58, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.categorical_covariates:0:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:55,79] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.pcs:-1:1-20000000010 [9e4f5894main.pcs:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:55,79] [info] BT-322 9e4f5894:main.pcs:-1:1 cache hit copying success with aggregated hashes: initial = 58D108557F21E539CF9BE064A9528392, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:55,79] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.pcs:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:56,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.ethnicity_self_report:-1:1-20000000008 [9e4f5894main.ethnicity_self_report:NA:1]: Unrecognized runtime attribute keys: dx_t; imeout; [2022-12-15 21:27:56,12] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 cache hit copying success with aggregated hashes: initial = A32F403CF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:30803,failure,failures,30803,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"ne_transcripts_dir_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/ensemble_data_cache_hg38""; },; ""viral_hosts_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/viral_host_ref.csv""; },; ""replication_origins_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/heli_rep_origins.bed""; },; ""line_element_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/line_elements.hg38.csv""; },; ""fragile_site_file_linx"": {; ""class"": ""File"",; ""location"": ""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/fragile_sites_hmf.hg38.csv""; }; }; ```. </details>. ## /opt/cromwell/configs/options.json. ```; {; ""final_workflow_outputs_dir"": ""outputs"",; ""use_relative_output_paths"": true,; ""final_workflow_log_dir"": ""wf_logs"",; ""final_call_logs_dir"": ""call_logs""; }; ```. # Metadata Error. <details>. <summary>Click to Expand</summary>. ```; {; ""workflowProcessingEvents"": [; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""Finished"",; ""timestamp"": ""2020-09-02T09:23:06.270Z"",; ""cromwellVersion"": ""52""; },; {; ""cromwellId"": ""cromid-0fe86cb"",; ""description"": ""PickedUp"",; ""timestamp"": ""2020-09-02T09:23:04.924Z"",; ""cromwellVersion"": ""52""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""v1.0"",; ""submittedFiles"": {; ""workflow"": ""{\n \""$graph\"": [\n {\n \""class\"": \""CommandLineTool\"",\n \""doc\"": \""AMBER is designed to generate a tumor BAF file for use in PURPLE from a provided VCF of likely heterozygous SNP sites.\\n\\nWhen using paired reference/tumor bams,\\nAMBER confirms these sites as heterozygous in the reference sample bam then calculates the\\nallelic frequency of corresponding sites in the tumor bam.\\nIn tumor only mode, all provided sites are examined in the tumor with additional filtering then applied.\\n\\nThe Bioconductor copy n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:60685,Error,Error,60685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['Error'],['Error']
Availability,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:2026,ping,pings,2026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,1,['ping'],['pings']
Availability,"nfig is set to only run one task at a time, ie, only one shard of a scattered task runs at a time. The workflow stopped processing on shard 885, which was the 233rd shard to start (shards appear to start in a random order, that's not an issue). It looks like the Docker container in question is getting created, but not used. The container is not running according to Docker Desktop and the Docker CLI tools (see output below). ### Workflow; I've seen this happen with a few workflows, but this time around it's this one (failure is occurring on second task): https://github.com/aofarrel/SRANWRP/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528; [; {; ""Id"": ""cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528"",; ""Created"": ""2022-11-09T05:37:16.872195116Z"",; ""Path"": ""/bin/bash"",; ""Args"": [; ""/bark-bark/I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:1251,error,error,1251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,3,"['error', 'ping']","['error', 'ping']"
Availability,"nfo] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-1/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-3/execution/script; [2018-10-25 21:21:19,05] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:4300,echo,echo,4300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7311,down,down,7311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,5,['down'],['down']
Availability,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:5841,down,down,5841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,6,['down'],['down']
Availability,"ng cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:35.937599Z""; },; {; ""startTime"": ""2018-08-14T16:16:35.937599Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/script \/cromwell_root\/script; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:16:36.306767Z""; },; {; ""startTime"": ""2018-08-14T16:18:14.462Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2018-08-14T16:18:15.459Z""; },; {; ""startTime"": ""2018-08-14T16:14:34.810415Z"",; ""description"": ""Stopped running \""\/bin\/bash -c mkdir -p \/cromwell_root && chmod -R a+rwx \/cromwell_root\"""",; ""endTime"": ""2018-08-14T16:14:35.424927Z""; },; {; ""startTime"": ""2018-08-14T16:16:37.108175Z"",; ""description"": ""Stopped running \""\/bin\/bash \/cromwell_root\/script\"""",; ""endTime"": ""2018-08-14T16:16:37.479167Z""; },; {; ""startTime"": ""2018-08-14T16:16:56.518719Z"",; ""description"": ""Stopped pulling \""stedolan\/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09\"""",; ""endTime"": ""2018-08-14T16:16:56.948463Z""; }; ],; ```. Although there's a lot of events -- there are a ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:22092,echo,echo,22092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"ng(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2044,Error,ErrorOr,2044,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"ng(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:2172,failure,failure,2172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['failure'],['failure']
Availability,"ngExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshakir [4:52 PM]; How does :heartbeat:-ing work? There was a restart event in the middle of these workflows.; i think. mcovarr [4:55 PM]; timer in t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:2171,heartbeat,heartbeats,2171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,1,['heartbeat'],['heartbeats']
Availability,"nge https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide what works best in their own situation. ; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:1407,echo,echo,1407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,2,"['down', 'echo']","['downsides', 'echo']"
Availability,ngine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:1652,recover,recoverWith,1652,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['recover'],['recoverWith']
Availability,"ning? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz\n+ COS_KERNEL_SRC_HEADER=kernel-headers.tgz\n+ TOOLCHAIN_URL_FILENAME=toolchain_url\n+ TOOLCHAIN_ARCHIVE=toolchain.tar.xz\n+ TOOLCHAIN_ENV_FILENAME=toolchain_env\n+ TOOLCHAIN_PKG_DIR=/build/cos-tools\n+ CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk\n+ ROOT_OS_RELEASE=/root/etc/os-release\n+ KERNEL_SRC_DIR=/build/usr/src/linux\n+ KERNEL_SRC_HEADER=/build/usr/src/linux-headers\n+ NVIDIA_DRIVER_VERSION=450.51.06\n+ NVIDIA_DRIVER_MD5SUM=\n+ NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia\n+ NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:1983,error,error,1983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['error'],['error']
Availability,"nitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a99-b386-5931ae245324 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Can't index (ArrayOrMapLookup:; lhs=(MemberAccess:; lhs=<string:19:29 identifier ""dDI="">,; rhs=<string:19:32 identifier ""b3V0"">; ),; rhs=<string:19:36 identifier ""aQ=="">; ); at cromwell.engine.workflow.lifecycle.materialization.Materializ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007:2585,error,error,2585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007,1,['error'],['error']
Availability,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:5925,echo,echo,5925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['echo'],['echo']
Availability,"nknown Source); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at wdl4s.wdl.expression.WdlFunctions.$anonfun$getFunction$1(WdlFunctions.scala:11); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:190); 	at wdl4s.wdl.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:56); 	at wdl4s.wdl.WdlExpression$.evaluate(WdlExpression.scala:91); 	at wdl4s.wdl.WdlExpression.evaluate(WdlExpression.scala:172); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); 	... 34 more; Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1065); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:333); 	... 80 more; ```. If this was due to an extended GCS service outage, a workflow failure is expected. But if cromwell didn't retry at all after a single cloud hiccup, that should be fixed as cloud hiccups are expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:8899,outage,outage,8899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,2,"['failure', 'outage']","['failure', 'outage']"
Availability,nonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(Wor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:3335,Error,ErrorOr,3335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"ns since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:108); at cats.data.Validated.map(Validated.scala:194); ... After this exception there is a log [ERROR] entry appears reporting the exception and exception stack trace. The timing diagram show the tasks hanging in the ""Starting"" state forever and the metadata does not report anything apart than these tasks are ""Starting"". So the error is silenced and the only recurse left is to abort. A re-submit of the workflow would just get stuck in the same place.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755:2189,ERROR,ERROR,2189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"nt cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. My intent is for the user to only provide arguments for `cpu`, `memory`, `runtime_minutes`, and `partition` if they intend to override the SLURM cluster's defaults. I do not want to have cromwell supply defaults, because if these arguments are omitted from the call to `sbatch` then the cluster's defaults will be used. My understanding is that making them optional like `String? memory_mb` and then using syntax like `${""--mem "" + round(memory_mb) + ""m""} \` in the submit script means that argument will only be added if `memory` is defined, and will be omitted if `memory` is not defined. I've followed the documentation as closely as I can. However, when I try to submit a test job without `cpu` and `memory` set as a runtime attribute, I get a failure with these exceptions:; ```; cromwell.core.CromwellAggregatedException: Initialization Failure:; Runtime validation failed:; 	Task myTask has an invalid runtime attribute cpu = !! NOT FOUND !!; 	Task myTask has an invalid runtime attribute memory = !! NOT FOUND !!; 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:356); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:339); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); ```. Here is the test WDL I'm using:. ```; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; call myTask. }. task myTask {; command <<<; echo ""hello world""; >>>; output {; String out = read_string(stdout()); }; }; ```. And my complete configuration for this backend:; ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int? runtime_minutes; Int? cpu;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7455:1430,Failure,Failure,1430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7455,1,['Failure'],['Failure']
Availability,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4710,error,errors,4710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['error'],['errors']
Availability,"nt or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; dupli",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:14356,error,errors,14356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['error'],['errors']
Availability,"nt/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `$TMPDIR` paths are always shorter than some reasonable length. Because different backends with-or-without docker may generate different paths for `$TMPDIR` the test should only use `<=` not `=` to verify the length of the environment variable. Other links:; - https://gatkforums.broadinstitute.org/firecloud/discussion/11980/problem-running-gdc-client-tool; - https://stackoverflow.com/questions/34829600/why-is-the-maximal-path-length-allowed-for-unix-sockets-on-linux-108; - https://unix.stackexchange.com/questions/367008/why-is-socket-path-length-limited-to-a-hundred-chars; - https://gith",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3647:1833,echo,echo,1833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647,2,['echo'],['echo']
Availability,ntToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:3285,Error,ErrorOr,3285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"nthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2851,Down,Downloaded,2851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloaded']
Availability,nthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; Comman,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3322,down,download,3322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,2,"['down', 'error']","['download', 'error']"
Availability,"o {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1739,failure,failure,1739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['failure'],['failure']
Availability,"o/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:1958,echo,echo,1958,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"ob-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:7513,error,error,7513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['error'],['error']
Availability,obExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:305); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:305); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.recoverAsync(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:574); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:569); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:190); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2963:2751,robust,robustExecuteOrRecover,2751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2963,1,['robust'],['robustExecuteOrRecover']
Availability,obExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:12689,robust,robustExecuteOrRecover,12689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['robust'],['robustExecuteOrRecover']
Availability,obscure validation error: Expected rbrace after `input:` in call,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256,1,['error'],['error']
Availability,"ode-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. runtime-attributes = """"""; String time = ""11:00:00""; Int cpu = 4; Float? memory_gb; String sge_queue = ""hammer.q""; String? sge_project; String? docker; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; /usr/bin/env bash ${script}; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". submit-docker = """""" ; #location for .sif files and other apptainer tmp, plus lockfile; 	 export APPTAINER_CACHEDIR=<path>; export APPTAINER_PULLFOLDER=<path>; export APPTAINER_TMPDIR=<path>; export LOCK_FILE=""$APPTAINER_CACHEDIR/lockfile""; export IMAGE=$(echo ${docker} | tr '/:' '_').sif; if [ -z $APPTAINER_CACHEDIR ]; then; exit 1; fi; CACHE_DIR=$APPTAINER_CACHEDIR; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; # downloads sifs only one at a time; apptainer sif db doesn't handle concurrency well; out=$(flock --exclusive --timeout 1800 $LOCK_FILE apptainer pull $IMAGE docker://${docker} 2>&1); ret=$?; if [[ $ret == 0 ]]; then; echo ""Successfully pulled ${docker}!""; else; if [[ $(echo $out | grep ""exists"" ) ]]; then; echo ""Image file already exists, ${docker}!""; else; echo ""Failed to pull ${docker}"" >> /dev/stderr; exit $ret; fi; fi; #full path to sif for qsub command; IMAGE=""$APPTAINER_PULLFOLDER/$IMAGE""; qsub \; -terse \; -V \; -b y \; -N ""${job_name}"" \; -wd ""${cwd}"" \; -o ""${out}.qsub"" \; -e ""${err}.qsub"" \; -pe smp ""${cpu}"" \; ${""-l mem_free="" + memory_gb + ""g""} \; ${""-q "" + sge_queue} \; ${""-P "" +",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:3527,echo,echo,3527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['echo'],['echo']
Availability,"ode:NA:1]: Status change from - to Done; [2019-07-10 14:32:59,19] [info] Not triggering log of token queue status. Effective log interval = None; [2019-07-10 14:33:00,77] [info] WorkflowExecutionActor-41d3eecf-c5a9-42e4-8a29-8be9c252b7f5 [41d3eecf]: Starting scMeth.trimAdapters; [2019-07-10 14:33:01,19] [info] Assigned new job execution tokens to the following groups: 41d3eecf: 1; [2019-07-10 14:33:01,22] [warn] Localization via hard link has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/cromwell-executions/scMeth/41d3eecf-c5a9-42e4-8a29-8be9c252b7f5/call-trimAdapters/inputs/13016223/fastq -> /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,23] [warn] Localization via copy has failed: /data/PROJECTS/2019_01_28_scMeth/scripts/Automated_Pipeline/cromwell/example_wdl/fastq; [2019-07-10 14:33:01,24] [error] BackgroundConfigAsyncJobExecutionActor [41d3eecfscMeth.trimAdapters:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:576); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:511); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:319); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:318); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:9536,Error,Error,9536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['Error'],['Error']
Availability,"ogging level?. [2021-05-06 08:27:27,34] [info] Checkpoint start; [2021-05-06 08:27:27,34] [info] checkpointClose start; [2021-05-06 08:27:27,34] [info] checkpointClose synched; [2021-05-06 08:27:27,34] [info] checkpointClose script done; [2021-05-06 08:27:27,34] [info] dataFileCache commit start; [2021-05-06 08:27:27,34] [info] dataFileCache commit end; [2021-05-06 08:27:27,36] [info] checkpointClose end; [2021-05-06 08:27:27,36] [info] Checkpoint end - txts: 6340; [2021-05-06 08:27:27,36] [info] Checkpoint start; [2021-05-06 08:27:27,36] [info] checkpointClose start; [2021-05-06 08:27:27,36] [info] checkpointClose synched; [2021-05-06 08:27:27,37] [info] checkpointClose script done; [2021-05-06 08:27:27,37] [info] dataFileCache commit start; [2021-05-06 08:27:27,37] [info] dataFileCache commit end; [2021-05-06 08:27:27,39] [info] checkpointClose end; [2021-05-06 08:27:27,39] [info] Checkpoint end - txts: 6347; [2021-05-06 08:27:27,39] [info] Checkpoint start; [2021-05-06 08:27:27,39] [info] checkpointClose start; [2021-05-06 08:27:27,39] [info] checkpointClose synched; [2021-05-06 08:27:27,39] [info] checkpointClose script done; [2021-05-06 08:27:27,39] [info] dataFileCache commit start; [2021-05-06 08:27:27,39] [info] dataFileCache commit end; [2021-05-06 08:27:27,41] [info] checkpointClose end; [2021-05-06 08:27:27,41] [info] Checkpoint end - txts: 6349; [2021-05-06 08:27:27,41] [info] Checkpoint start; [2021-05-06 08:27:27,41] [info] checkpointClose start; [2021-05-06 08:27:27,41] [info] checkpointClose synched; [2021-05-06 08:27:27,41] [info] checkpointClose script done; [2021-05-06 08:27:27,41] [info] dataFileCache commit start; [2021-05-06 08:27:27,41] [info] dataFileCache commit end; [2021-05-06 08:27:27,42] [info] checkpointClose end; [2021-05-06 08:27:27,42] [info] Checkpoint end - txts: 6351; [2021-05-06 08:27:27,42] [info] Checkpoint start; [2021-05-06 08:27:27,42] [info] checkpointClose start; [2021-05-06 08:27:27,42] [info] checkpointClose synched; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6337:219,Checkpoint,Checkpoint,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337,27,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpointClose']"
Availability,"old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} | sed -s ""s@#\!/bin/bash@#\!/bin/bash\nsource '/etc/profile' @g"" > $phoenix_script. sbatch --uid=${userid} --gid=${userid} \; -J ${job_name} \; -p ${partitions} \; -N ${nodes} \; -n ${cores} \; --mem=${memory_per_node} \; --time=${time} \; -D $phoenix_home_cwd \; -o $phoenix_home_out \; -e $phoenix_home_err \; $phoenix_script; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*"". root = ""/fast/gdr/uat/cromwell-executions""; }; }. } # prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1754,alive,alive,1754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['alive'],['alive']
Availability,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.error(Logger.java:558); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:10076,error,error,10076,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['error'],['error']
Availability,"om.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2647,error,error,2647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"ommonwl.org/v1.0/CommandLineTool.html#Parameter_references; >If the value of a field has non-whitespace leading or trailing characters around a parameter reference, it is subject to string interpolation. The effective value of the field is a string containing the leading characters, followed by the string value of the parameter reference, followed by the trailing characters. The string value of the parameter reference is its textual **JSON** representation with the following rules:; > ; > * Leading and trailing quotes are stripped from strings; > * Objects entries are sorted by key; >; > Multiple parameter references may appear in a single field. This case must be treated as a string interpolation. After interpolating the first parameter reference, interpolation must be recursively applied to the trailing characters to yield the final string value. Test code: `runtime-dump.cwl`; ``` cwl; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; baseCommand: echo; inputs: []; arguments: [ '{""runtime"":$(runtime)}' ]; stdout: runtime.json; outputs:; runtime: stdout; ```; Example contents of the generated `runtime.json` (linewrapped for readability):; ``` json; {""runtime"":object {ram: 4, cores: 1, tmpdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/tmp.095e5005"",; outdir: ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/execution"",; outdirSize: 2147483647, tmpdirSize: 2147483647}}; ```; Corrected version of the generated `runtime.json`:; 1. No `output ` prefix; 2. JSON, not YAML: key names are in quotes; 3. Objects entries are sorted by key; ``` json; {""runtime"":{""cores"": 1, ""outdir"": ""/home/mcrusoe/src/cromwell/cromwell-executions/c4e0e9bd-432a-4182-93dc-e842171dde6e.cwl/c4e0e9bd-432a-4182-93dc-e842171dde6e/call-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3710:1001,echo,echo,1001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3710,1,['echo'],['echo']
Availability,"omwell (v.84). The wdl file link: https://github.com/gatk-workflows/intel-gatk3-4-germline-snps-indels/blob/master/PairedSingleSampleWf_noqc_nocram_optimized.wdl. But I am facing this issue and I don't know how to proceed with it. `[2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: job id: 95887; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:14:1]: job id: 95962; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:1:1]: job id: 96075; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:16:1]: job id: 96073; [2022-11-10 13:45:54,44] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:8:1]: job id: 95871; [2022-11-10 13:45:54,45] [error] KvWriteActor Failed to properly process data; cromwell.core.CromwellFatalException: java.sql.SQLDataException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:55); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:46); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.sql.SQLDataException: data exception: string data, right ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:1043,error,error,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['error'],['error']
Availability,"omwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1274,down,download,1274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['down'],['download']
Availability,"on(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:2011,avail,available,2011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,1,['avail'],['available']
Availability,"on(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:2771,echo,echo,2771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2394,recover,recoverWith,2394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['recover'],['recoverWith']
Availability,on.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14668,recover,recoverWith,14668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['recover'],['recoverWith']
Availability,"on/synthetic.challenge.set1.tumor-vs-synthetic.challenge.set1.normal.ob_filtered.vcf"", ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-Mutect2/shard-1/Mutect2/56dd28f2-d4af-449d-961a-eface7c9a288/call-FilterByOrientationBias/execution/background.synth.challenge2.snvs.svs.tumorbackground-vs-synthetic.challenge.set2.normal.ob_filtered.vcf""],; ""Mutect2_Multi.unfiltered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-unfilteredOutputList/execution/unfiltered.list"",; ""Mutect2_Multi.ob_filtered_vcfs"": ""/home/lichtens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3115,error,error,3115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,onActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1730,recover,recoverAsync,1730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Availability,"onTimeout = 120000; numThreads = 1; }; }; ```. but when I execute the workflow . `java -Dconfig.file=${PWD}/app.conf -jar ${CROMWELL_JAR} run test.wdl --inputs input.json`. the configuration takes a long time with messages about `Checkpoint...` that takes about 10 minutes. . ```; (...); [2023-02-08 16:24:28,90] [info] dataFileCache commit start; [2023-02-08 16:24:28,91] [info] dataFileCache commit end; [2023-02-08 16:24:28,94] [info] checkpointClose end; [2023-02-08 16:24:28,96] [info] Checkpoint end - txts: 3051; [2023-02-08 16:24:29,05] [info] Checkpoint start; [2023-02-08 16:24:29,05] [info] checkpointClose start; [2023-02-08 16:24:29,07] [info] checkpointClose synched; [2023-02-08 16:24:29,08] [info] checkpointClose script done; [2023-02-08 16:24:29,08] [info] dataFileCache commit start; [2023-02-08 16:24:29,20] [info] dataFileCache commit end; [2023-02-08 16:24:29,53] [info] checkpointClose end; [2023-02-08 16:24:29,53] [info] Checkpoint end - txts: 3058; [2023-02-08 16:24:29,53] [info] Checkpoint start; [2023-02-08 16:24:29,53] [info] checkpointClose start; [2023-02-08 16:24:30,52] [info] checkpointClose synched; [2023-02-08 16:24:30,52] [info] checkpointClose script done; [2023-02-08 16:24:30,52] [info] dataFileCache commit start; [2023-02-08 16:24:30,53] [info] dataFileCache commit end; [2023-02-08 16:24:30,58] [info] checkpointClose end; [2023-02-08 16:24:30,58] [info] Checkpoint end - txts: 3060; [2023-02-08 16:24:30,59] [info] Checkpoint start; [2023-02-08 16:24:30,59] [info] checkpointClose start; [2023-02-08 16:24:31,52] [info] checkpointClose synched; [2023-02-08 16:24:31,65] [info] checkpointClose script done; [2023-02-08 16:24:31,65] [info] dataFileCache commit start; (...); ```. And at the end:; ```; 2023-02-08 16:32:11,54] [info] checkpointClose synched; [2023-02-08 16:32:11,57] [info] checkpointClose script done; [2023-02-08 16:32:11,57] [info] dataFileCache commit start; [2023-02-08 16:32:11,57] [info] dataFileCache commit end; [2023-02-08 16:32:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:1167,checkpoint,checkpointClose,1167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,18,"['Checkpoint', 'checkpoint']","['Checkpoint', 'checkpointClose']"
Availability,"onal configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The defaul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:3254,avail,available,3254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['avail'],['available']
Availability,"onfun$applyOrElse$1(Slf4jLogger.scala:69); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:65); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-03-09 15:52:58,29] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:11144,error,error,11144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['error'],['error']
Availability,"oning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:1120,echo,echo,1120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['echo'],['echo']
Availability,"oo, ideally) use the second. By way of example:. ```; api.response.count.200; ```. versus. ```; api_response_count{code=""200""}; ```. This boils down to how the different systems query metrics. In a StatsD world, you can have a query like `api.response.count.*`, but that's not possible for Prometheus/Stackdriver--the actual metric name needs to be fully static, and you conceptually do `api_response_count{code=""*""}`. Cromwell metric names right now are this:. ```scala; type InstrumentationPath = NonEmptyList[String]. CromwellBucket(prefix: List[String], path: InstrumentationPath); ```. Metric names are guaranteed to not be empty by the path. The path is what is actually assembled in code and passed around, and prefix (or an empty list) is added at the outer edge--most of Cromwell treats the `NonEmptyList[String]` as the metric name. When the metrics are actually sent to StatsD, the strings are joined with periods and sent off. I spent several days trying to figure out a way to reliably parse labels out from these names, and I couldn't figure it out. There's two key pieces of info known at the time the `NonEmptyList[String]` is assembled that are not recorded:; - When a particular string being added to the list is expected to vary frequently (and thus needs to be a label); - What that string means (labels are key-value pairs and we're usually missing the key). > This parsing-based approach actually did work with Rawls, by doing the parsing outside Rawls's codebase, not inside it. That worked for Rawls because Rawls has relatively few actually unique metrics, and the metrics that it does have are extremely stable--no one is going to change them--so we literally hardcode the exact glob matching and regex patterns. With the number of metric Cromwell has, that raw hardcoding approach would get unmaintainable very, very quickly. My idea was to instead have metric names effectively be the following:. ```scala; type InstrumentationPath = NonEmptyList[String | (String, String)]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6681:1634,reliab,reliably,1634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681,1,['reliab'],['reliably']
Availability,oogle.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBacke,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3340,recover,recoverAsync,3340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Availability,"ooks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be removed in a future release. sbt version in this project: 	1.5.5; sbt script version: 1.7.1; ```. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:1395,error,error,1395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,3,['error'],['error']
Availability,"ootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:2311,error,error,2311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['error'],['error']
Availability,"operties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has prov",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2065,avail,available,2065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['avail'],['available']
Availability,"or.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2018-06-13 14:29:47,368 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowExecutionActor-a67833cb-b894-4790-872f-9f3104cab60c [UUID(a67833cb)]: Starting demux_only.illumina_demux; 2018-06-13 14:29:48,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - Failed to hash /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:5014,ERROR,ERROR,5014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['ERROR'],['ERROR']
Availability,"or.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2023-11-07 14:51:17,39] [info] Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692#-686070856] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; In fact, the program becomes unresponsive to even a Ctrl+C kill command and I have to close the terminal entirely to stop it. . The WDL passes `womtool validate` (version 84) and was run using Cromwell version 84. . When run in Terra, the workflow just immediate goes into an aborting state without any helpful error message. It would be great to incorporate this type of support for `None` inside struct fields.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:7156,error,error,7156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['error'],['error']
Availability,"or.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:536); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:46); 	at com.google.cloud.BaseWriteChannel.close(BaseWriteChannel.java:149); 	at com.google.cloud.storage.contrib.nio.CloudStorageWriteChannel.close(CloudStorageWriteChannel.java:57); 	at java.nio.channels.Channels$1.close(Channels.java:178); 	at java.nio.file.Files.write(Files.java:3300); 	at cromwell.backend.impl.jes.io.package$PathEnhanced$.writ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:3297,error,error,3297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,8,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"or; [2018-10-25 21:17:13,86] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-10-25 21:17:13,87] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-1/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:12773,echo,echo,12773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063:2781,down,down,2781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063,2,"['down', 'mainten']","['down', 'maintenance']"
Availability,org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:4679,Error,ErrorHandling,4679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,2,['Error'],['ErrorHandling']
Availability,orkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6340,recover,recover,6340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,"orkJoinWorkerThread.java:107). 2020-10-08 16:08:58,573 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: `which python; python --version; echo ""Hello ZZZ!"" > file.txt`; 2020-10-08 16:08:58,593 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: executing: module load proxy; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in stdout.submit.; flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec --containall docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 echo ""Success pulling docker!""; echo ""module load singularity/v3.5.2 && singularity exec --containall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:7586,echo,echo,7586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,2,['echo'],['echo']
Availability,"orkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6413,Fault,FaultHandling,6413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,"orkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.cloud.storage.StorageException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:536); 	at com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:49); 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:46); 	at com.google.cloud.BaseWriteChannel.close(BaseWriteChannel.java:149); 	at com.google.cloud.storage.contrib.nio.CloudStorageWriteChannel.close(CloudStorageWriteChannel.java:57); 	at java.nio.channels.Channels$1.close(Channels.java:178); 	at java.nio.file.Files.write(Files.java:3300); 	at cromwell.backend.impl.jes.io.package$PathEnhanced$.writeAsJson$extension(package.scala:13); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:62); 	... 24 more; Caused by: com.google.api.client.http.HttpResponseException: 503 Service Unavailable; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1070); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:518); 	... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:4671,error,error,4671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,4,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,orkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6464,recover,recoverAsync,6464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recoverAsync']
Availability,"ormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:50) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) LINENO: 1 - Could not find tag separator ':' in line. LINE: <?xml version=""1.0""?> org.obolibrary.oboformat.parser.OBOFormatParser.error(OBOFormatParser.java:1337) org.obolibrary.oboformat.parser.OBOFormatParser.getParseTag(OBOFormatParser.java:760) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClause(OBOFormatParser.java:409) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderClauseNl(OBOFormatParser.java:402) org.obolibrary.oboformat.parser.OBOFormatParser.parseHeaderFrame(OBOFormatParser.java:385) org.obolibrary.oboformat.parser.OBOFormatParser.parseOBODoc(OBOFormatParser.java:266) org.obolibrary.oboformat.parser.OBOFormatParser.parse(OBOFormatParser.java:235) org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser.parse(OBOFormatOWLAPIParser.java:44) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser@5326aaf7 Stack trace:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:5588,error,error,5588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['error'],['error']
Availability,"ormatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hi!. We've been looking into migrating from PAPIv2 backend to GCPBATCH backend. Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.scala:246); cromwell_1 | 	at flatMap @ fs2.internal.CompileScope.$anonfun$close$6(CompileScope.scala:245); cromwell_1 | 	at map @ fs2.internal.CompileScope.fs2$internal$CompileScope$$traverseError(CompileScope.scala:222); c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:1126,Error,Errors,1126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['Error'],['Errors']
Availability,"ormation: s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/prep_samples_to_rec-stderr.log.; Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 48, in process; fnargs, parallel, out_keys, input_files = _world_from_cwl(args.name, fnargs[1:], work_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 185, in _world_from_cwl; assert os.path.exists(os.path.join(work_dir, ""cwl.inputs.json"")); AssertionError. [2019-01-25 13:58:40,07] [info] WorkflowManagerActor WorkflowActor-2c2e5a10-8c57-4f9f-8d80-c2fccacbb452 is in a terminal state: WorkflowFailedState; [2019-01-25 13:58:59,66] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-01-25 13:59:03,42] [info] SingleWorkflowRunnerActor writing metadata to /home/chapmanb/drive/work/cwl/test_bcbio_cwl/aws/cromwell_work/somatic-metadata.json; ```; I can see this file in the s3 bucket, although it's not in the `execution` directory which is normally where things get staged (at least on local runs):; ```; $ aws s3 ls s3://bcbio-batch-cromwell-test/cromwell-execution/main-somatic.cwl/2c2e5a10-8c57-4f9f-8d80-c2fccacbb452/call-prep_samples_to_rec/; PRE glob-b34dfc006a981a93d6da067cf50036fe/; 2019-01-25 13:51:55 0; 2019-01-25 13:51:59 6059 cwl.inputs.json; 2019-01-25 13:58:01 0 glob-b34dfc006a981a93d6da067cf50036fe.list; 2019-01-25 13:58:01 2 prep_samples_to_rec-rc.txt; 2019-01-25 13:58:40 571 prep_samples_to_rec-stderr.log; 2019-01-25 13:58:02 0 prep_samples_to_rec-stdout.log; ```; Does this error look familiar to anyone?. I'm excited to be making progress with this and will also work on writing up and documenting the setup and run process so far. Thanks for any suggestions or pointers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586:4512,error,error,4512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586,1,['error'],['error']
Availability,"orted Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1181,Error,Error,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['Error'],['Error']
Availability,"ory"",""Out of memory""]; # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; #graceful-server-shutdown = true. # Cromwell will cap the number of running workflows at N; #max-concurrent-workflows = 5000. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; #max-workflow-launch-count = 50. # Number of seconds between workflow launches; #new-workflow-poll-rate = 20. # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; #number-of-workflow-log-copy-workers = 10. # Default number of cache read workers; #number-of-cache-read-workers = 25. io {; # throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; # #number-of-requests = 100000; # #per = 100 seconds; # }. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; #lines = 128000; #bool = 7; #int = 19; #float = 50; #string = 128000; #json = 128000; #tsv = 128000; #map = 128000; #object = 128000; }. abort {; # These are the default values in Cromwell, in most circumstances there should not be a need to change them. # How frequently Cromwell should scan for aborts.; scan-frequency: 30 seconds. # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.; # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask; # the associated WorkflowActor to abort again.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:2628,avail,availble,2628,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['avail'],['availble']
Availability,"os-tools\n+ CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk\n+ ROOT_OS_RELEASE=/root/etc/os-release\n+ KERNEL_SRC_DIR=/build/usr/src/linux\n+ KERNEL_SRC_HEADER=/build/usr/src/linux-headers\n+ NVIDIA_DRIVER_VERSION=450.51.06\n+ NVIDIA_DRIVER_MD5SUM=\n+ NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia\n+ NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3372,Down,Downloading,3372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['Down'],['Downloading']
Availability,ot copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-dispatcher-84 ERROR - GcpBatchAsyncBackendJobExecutionActor [UUID(119e11a5)wf_hello.hello:NA:1]: Error attempting to Recover(StandardAsyncJob(projects/broad-dsde-cromwell-dev/locations/us-central1/jobs/job-7ce25791-3731-4a69-97f1-b7b65ac8ff71)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(Stand,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:2826,recover,recover,2826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"ot for the missing input (gender_mask_bed); call CallSampleGender as CallSampleGender {; input:; analysis_directory = analysis_directory,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; genome_mask = genome_mask,; genome_mask_index = genome_mask_index,; ploidy_map = ploidy_map,; header_bam = ExtractBamSubset.header_bam,; header_bam_index = IndexHeaders.header_index,; gender_mask_bed = gender_mask_bed,; read_count_index = Index.read_count_index; }; }; ```. And here's the task, which does have the missing variable **File gender_mask_bed**. ```; task CallSampleGender {; String analysis_directory; File ref_fasta; File ref_fasta_index; File ref_dict; File genome_mask; File genome_mask_index; File ploidy_map; File header_bam; File header_bam_index; File gender_mask_bed; File read_count_index. command {; mkdir ${analysis_directory}; java -Xmx4000m \; -classpath /usr/gitc/svtoolkit2.00/lib/SVToolkit.jar:/usr/gitc/svtoolkit2.00/lib/gatk/GenomeAnalysisTK.jar \; org.broadinstitute.sv.apps.CallSampleGender \; -R ${ref_fasta} \; -genomeMaskFile ${genome_mask} \; -ploidyMapFile ${ploidy_map} \; -md ${analysis_directory} \; -I ${header_bam} \; -bedFile ${gender_mask_bed} \; -O sample_gender.report.txt; }; runtime {; docker: ""kbergin/kbergin_test""; memory: ""4 GB""; }; output{; File sample_gender_report = ""sample_gender.report.txt""; }; }; ```. ---. @knoblett commented on [Mon Apr 25 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-214441149). In running a highlight command today, I noticed that the wdltool caught an undeclared input error. Has this bug been fixed, or is wdltool still not picking up undeclared inputs within tasks only? (my example is missing a declared input in the workflow's call to the task, due to a typo.). ![screen shot 2016-04-25 at 12 55 02 pm](https://cloud.githubusercontent.com/assets/13629186/14791714/0ca069f8-0ae5-11e6-8b80-37e2a7fecae9.png). I am using the Cromwell 0.18 release and wdltool 0.1 release.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:5385,error,error,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,1,['error'],['error']
Availability,"ould not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55) cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92) ; `. Basically cromwell fails to read a stdout file written by a job.; When we check the file it exists and contains data so i suspect some kind of IO problem. All Data is on NFS shares which are usually quite stable and we see no errors in any of the filesystem/nfs backends. It seems to mostly happen in this scatter step, so i suspect some race condition or timeout somewhere in there, however, this job was creating a scatter to 12 bed files, so its really not that big . Just re-running the job usually works, but its extremely annoying. Call caching is disabled:; `call-caching {; enabled = false; }`. Any idea what to do ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:2563,error,errors,2563,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['error'],['errors']
Availability,"our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1320,error,error,1320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['error'],['error']
Availability,"our question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```. Any idea, what is going wrong here?; Thanks a lot!. Cheers,; Flo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082:1181,Error,Error,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082,1,['Error'],['Error']
Availability,"ovars:NA:1 [788d8048]: Could not copy a suitable cache hit for 788d8048:main.load_shared_covars:-1:1. No copy attempts were; made.; [2022-12-15 21:28:03,88] [warn] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: Unrecognized runtime attribute keys: dx_timeout, memory; [2022-12-15 21:28:04,00] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_aneuploidy_sample_list:-1:1-20000000012 [788d8048main.sex_aneuploidy_sample_list:NA:1]: Unrecognized runtime attribute; keys: shortTask, dx_timeout; [2022-12-15 21:28:04,00] [info] BT-322 788d8048:main.sex_aneuploidy_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = DDF9190E939D36D999E513158D534532.; [2022-12-15 21:28:04,00] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.sex_aneuploidy_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.white_brits_sample_list:-1:1-20000000013 [788d8048main.white_brits_sample_list:NA:1]: Unrecognized runtime attribute keys:; shortTask, dx_timeout; [2022-12-15 21:28:04,01] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.low_genotyping_quality_sample_list:-1:1-20000000014 [788d8048main.low_genotyping_quality_sample_list:NA:1]: Unrecognized ru; ntime attribute keys: shortTask, dx_timeout; [2022-12-15 21:28:04,01] [info] BT-322 788d8048:main.white_brits_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = B2C071CED641A1EB183DE4A4655F45ED, file = 9675960412B5394D5D0816ED198FB6EB.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.white_brits_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:35073,failure,failures,35073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"ove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:1995,error,error,1995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['error'],['error']
Availability,overly verbose Checkpointing info logging msgs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6337:15,Checkpoint,Checkpointing,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337,1,['Checkpoint'],['Checkpointing']
Availability,"ow, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools fastq -c9 -@4 -n -o ${SampleName}.fq.gz ${InputBam} . }. #Run time attributes:; runtime {; docker: ""drpintothe2nd/ac3_xysupp""; memory: mem_size; cpu: ""4""; disks: ""local-disk "" + disk_size + "" HDD""; 	}; ; output {; 	File outputReads = ""${SampleName}.fq.gz""; 	}; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Running locally on windows 10 WSL, ubuntu 20.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:1970,failure,failure,1970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['failure'],['failure']
Availability,"ow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown is a multi-phase process, each phase having its own configurable timeout. See the Dev Wiki for more details.; 	graceful-server-shutdown = true; max-concurrent-workflows = 5000. io {; throttle {; # # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # # the quota availble on the GCS API; number-of-requests = 100000; per = 100 seconds; }; }; }. akka {; # Optionally set / override any akka settings; http {; server {; # Increasing these timeouts allow rest api responses for very large jobs; # to be returned to the user. When the timeout is reached the server would respond; # `The server was not able to produce a timely response to your request.`; # https://gatkforums.broadinstitute.org/wdl/discussion/10209/retrieving-metadata-for-large-workflows; request-timeout = 600s; idle-timeout = 600s; }; }; }. services {; MetadataService {; #class = ""cromwell.services.metadata.impl.MetadataServiceActor""; config {; metadata-read-row-number-safety-threshold = 2000000; # # For normal usage the default value of 200 should be fine but for larger/production environments we recommend a; # # value of at least 500. There'll be no one size fits all number here so we recommend benchmarking performance and; # # tuning the value to match your environmen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:8363,avail,availble,8363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['avail'],['availble']
Availability,"ows(final_matrix); CODE; >>>. runtime {; docker: ""python:2.7""; memory: ""1 GB""; }. output {; File output_tsv = ""output.tsv""; Array[Array[String]] output_matrix = read_tsv(""output.tsv""); }; }; ```. It seems that if you give it an `input_matrix` that is greater than 85 MB worth of data, you get the stack trace below. I reached this size of data with an inputs Array of Arrays that had dimensions 500x901 where each element was a gcs path like ""gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/cab6dea2-8a01-4130-8973-96023637de29/call-SplitGvcf/shard-100/split_gvcfs/11C118416.7ca5a79a-1369-4dc1-8f41-4180d7b3c1ab.0000.g.vcf.gz.tbi"". Whoever picks up this ticket can talk to me about specifics and why we think its an 85MB issue. ```; 2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:1668,Failure,Failure,1668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['Failure'],['Failure']
Availability,p.apply(JFunction0$mcV$sp.java:12) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at c,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:1948,Robust,RobustClientHelperSpec,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,"params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.04 and int=3 together I get `No coercion defined from wom value(s) '3.0' of type 'Float' to 'Int?'.` which is strange (seems like it's casting the 3 to 3.0 etc.). In any case this only seems to be a problem with defining structs inline. The conversion seems to work okay if you use input.json or if you read_json from some params.json at runtime (however I want to fix parameters inline for my current use case). The current workaround is to use object definition inline:. ```; Params p = object {; boolean: true,; float",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:1517,echo,echo,1517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,1,['echo'],['echo']
Availability,"patch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:9972,error,error,9972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"patchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:6967,recover,recoverWith,6967,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['recover'],['recoverWith']
Availability,"pe of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 retries will use a preemptible machine -- regardless of the reason for the error. Additional retries beyond that will not use a preemptible machine. This approach would let failed_task_retries act as a floor for the entire workflow that guarantees all tasks, preemptible or not, will retry at least failed_task_retries times upon failure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:2216,error,error,2216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,2,"['error', 'failure']","['error', 'failure']"
Availability,per.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:51,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2731,robust,robustExecuteOrRecover,2731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['robust'],['robustExecuteOrRecover']
Availability,perEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:3347,Robust,RobustClientHelperSpec,3347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,pl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275:2748,robust,robustExecuteOrRecover,2748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275,2,['robust'],['robustExecuteOrRecover']
Availability,"port failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:tr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:2282,Down,DownstreamConsumer,2282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['Down'],['DownstreamConsumer']
Availability,postgres migration error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['error'],['error']
Availability,"pp$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:2831,down,down,2831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['down'],['down']
Availability,"pression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".inval",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:3148,error,error,3148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,2,['error'],['error']
Availability,"ps://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:1175,failure,failure,1175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,2,['failure'],['failure']
Availability,"put {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean keep_duplicate_reads; Boolean disable_all_read_filters; String transform; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File gatk_jar; Boolean disable_sequence_dictionary_validation; Boolean isWGS; Int mem. # Note that when isWGS is true, this task is still called by the workflow.; # In that case, an empty coverage file is created and passed to the WholeGenomeCoverage ; # task to satisfy input and output requirements.; command <<<; if [ ${isWGS} = false ]; then; java -Xmx${mem}g -jar ${gatk_jar} CalculateTargetCoverage --output ${entity_id}.coverage.tsv \; --groupBy ${grouping} --transform ${transform} --targets ${padded_target_file} --targetInformationColumns FULL \; --input ${input_bam} --reference ${ref_fasta} --disableAllReadFilters ${disable_all_read_filters} \; $(if [ ${keep_duplicate_reads} = true ]; then echo "" --disableReadFilter NOT_DUPLICATE ""; else echo """"; fi) \; --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; else; touch ${entity_id}.coverage.tsv; fi; >>>. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }; ....snip....; ```. [sge_application.conf.txt](https://github.com/broadinstitute/cromwell/files/569269/sge_application.conf.txt). Shell command:; ```bash; #!/bin/bash -l; . /broad/tools/scripts/useuse; reuse -q GridEngine8; use .hdfview-2.9; use Java-1.8; use .r-3.1.3-gatk-only. ###############. java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell-23-79f6e12-SNAPSHOT.jar \; run pon_gatk_workflow.wdl \; pon_gatk_workflow.json \; sg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646:2863,echo,echo,2863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646,2,['echo'],['echo']
Availability,qstat is the command to check aliveness,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3192:30,alive,aliveness,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3192,1,['alive'],['aliveness']
Availability,quick hack to mark conformance failures as PAPI-only,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3137:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3137,2,['failure'],['failures']
Availability,"r globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 | grep -v cromwell_glob_control_file > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:17947,echo,echo,17947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['echo'],['echo']
Availability,r$run(SwaggerServiceSpec.scala:17) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromiam.webservice.SwaggerServiceSpec.run(SwaggerServiceSpec.scala:17) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4357:4732,Error,ErrorHandling,4732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4357,2,['Error'],['ErrorHandling']
Availability,r.scala:189); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.executeOrRecover(SharedFileSystemAsyncJobExecutionActor.scala:188); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:124); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:45); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:69); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:124); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:4531,robust,robustExecuteOrRecover,4531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['robust'],['robustExecuteOrRecover']
Availability,r.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	... 5 more; ```. The same workflow runs with local backend and local files both by `cwltool` and `cromwell` and on AWSBatch without specifying Resource requirements. That makes me assume that the,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:8236,robust,robustExecuteOrRecover,8236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['robust'],['robustExecuteOrRecover']
Availability,r.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:3143,robust,robustExecuteOrRecover,3143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['robust'],['robustExecuteOrRecover']
Availability,"r.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl, exceeded import_max_depth; circular imports?; None; ```. ## Backends; * Terra (kind of); * Mac OS on womtool 76; * Ubuntu on womtool 56. I say ""kind of"" for Terra since I can't see the error message -- if you try to upload this workflow to the Broad Methods Repo, a 500 error will result, and I've a hunch that's the result of the stack overflow. ## Example workflow; ```; version 1.0. import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # note: that workflow also has the line import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # I meant to actually import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segfault.wdl"". workflow Segment_Scatter {; 	input {; 		# if you input 10 files and n_segments = 5, each segment gets 2 files; 		Array[File] input_files; 		Int n_segments; 	}. 	call segfault.segfault {; 		input:; 			inputs = input_files,; 			n_segments = n_segments; 	}. 	scatter(segment in segfault.segments) {; 		call echo_files {; 			input:; 				files_to_echo = segment; 		}; 	}; }. task echo_files {; 	input {; 		Array[File] files_to_echo; 	}. 	command <<<; 	python3 << CODE; 	files = [""~{sep='"",""' files_to_echo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:2810,error,error,2810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,2,['error'],['error']
Availability,"r:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools fastq -c9 -@4 -n -o ${SampleName}.fq.gz ${InputBam} . }. #Run time attributes:; runtime {; docker: ""drpi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:1089,ERROR,ERROR,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['ERROR'],['ERROR']
Availability,"rElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: write_lines(array_of_files): [Attempted 1 time(s)] - FileSystemException: /tmp/temp-s3-6826696772619511254cromwell-execution_best_practise_3bf9c436-31f7-4d86-ba87-cf54248e05; bc_call-ConvertPairedFastQsToUnmappedBamWf_unmap.ConvertPairedFastQsToUnmappedBamWf_e10704e8-7ff4-4e4c-89ad-48270734ba25_call-CreateFoFN_write_lines_238fd975f9e3d166dbec07b20ad88c51.tmp: File name too lon; g; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 31 common frames omitted; ```. It seems that each ""segment"" of this filename, e.g. `bc_call-ConvertPairedFastQsToUnmappedBamWf_unmap.ConvertPairedFastQsToUnmappedBamWf_e10704e8-7ff4-4e4c-89ad-48270734ba25`, should probably be in a different folder, not just as a different segment of the same filename.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279:4008,Error,Error,4008,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279,1,['Error'],['Error']
Availability,"r] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiatio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2633,error,error,2633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"r_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. app",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:3183,echo,echo,3183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,2,['echo'],['echo']
Availability,radL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsy,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:2386,recover,recoverWith,2386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['recover'],['recoverWith']
Availability,"rage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5036,ERROR,ERROR,5036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['ERROR'],['ERROR']
Availability,"raining.metadata; ```. full_dl_ob_training.wdl:; ```; import ""dl_ob_training.wdl"" as dl_ob_training. workflow full_dl_ob_training {. ....snip.... scatter (p in variant_files_pair) {; call dl_ob_training.dl_ob_training {; input:; ....snip....; }; }; }. ```. dl_ob_training.wdl:; ```; workflow dl_ob_training {. ....snip.... call CollectSequencingArtifactMetrics {; input:; .....snip.....; }. call CreateObIntervalList {; input:; .....snip.....; }. call ExtractReadInfo {; input:; ....snip.....; }. output {; ExtractReadInfo.read_infos; }; }. task CollectSequencingArtifactMetrics {; ....snip....; output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File bait_bias_summary_metrics = ""${output_location_prepend}.bait_bias_summary_metrics""; }; }. # Build an interval list from oncotated M1; task CreateObIntervalList {. ....snip.... output {; File interval_list = ""${entity_id}_variant_intervals.list""; }; }. task ExtractReadInfo {. ....snip.... output {; Array[File] read_infos = glob(""out/*""); }; }. ```. sge_application.conf:; ```; ...snip...; workflow-options {; # These workflow options will be encrypted when stored in the database; encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; workflow-failure-mode: ""ContinueWhilePossible""; }. ....snip.... ; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1895:2833,failure,failure-mode,2833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1895,2,['failure'],['failure-mode']
Availability,"rator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5632,error,error,5632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:1923,echo,echo,1923,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,2,"['alive', 'echo']","['alive', 'echo']"
Availability,"rder to measure disk IO, which can potentially be a source of problems for some of the SV algorithms we're trying to debug/string together. the .wdl file; ```; workflow GetSystemInfo {; call get_system_info_docker; }. task get_system_info_docker {; command <<<; echo ""**** df -h""; df -h; ; echo; echo ""**** /""; ls -l /; ; echo; echo ""**** /mnt""; ls -l /mnt; ; echo; echo ""**** /dev""; ls -l /dev; ; if [ -d /dev/disk ]; then; echo; echo ""**** /dev/disk""; ls /dev/disk; fi; ; echo; echo ""**** /proc/mounts""; cat /proc/mounts; ; echo; echo ""**** /etc/mtab""; cat /etc/mtab; ; echo; echo ""**** /sys/block""; find -L /sys/block -maxdepth 2; ; echo; echo ""**** /sys/block/sdb/size (converted to integer GB)""; echo ""$(($(cat /sys/block/sdb/size) * 512 / 2**30))""; ; echo; echo ""**** /sys/devices""; find -L /sys/devices -maxdepth 3; >>>; ; runtime {; docker: ""talkowski/delly""; memory: ""1.7 GB""; cpu: ""1""; disks: ""local-disk 250 HDD""; preemptible: 3; }; }; ```; Snips of relevant output from cromwell 36 (edited for brevity):; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/disk/by-id/google-local-disk; 245.1G 60.0M 245.0G 0% /cromwell_root; **** /dev; total 0; lrwxrwxrwx 1 root root 11 Nov 14 21:16 core -> /proc/kcore; lrwxrwxrwx 1 root root 13 Nov 14 21:16 fd -> /proc/self/fd; crw-rw-rw- 1 root root 1, 7 Nov 14 21:16 full; drwxrwxrwt 2 root root 40 Nov 14 21:16 mqueue; crw-rw-rw- 1 root root 1, 3 Nov 14 21:16 null; lrwxrwxrwx 1 root root 8 Nov 14 21:16 ptmx -> pts/ptmx; drwxr-xr-x 2 root root 0 Nov 14 21:16 pts; crw-rw-rw- 1 root root 1, 8 Nov 14 21:16 random; drwxrwxrwt 2 root root 40 Nov 14 21:16 shm; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stderr -> /proc/self/fd/2; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdin -> /proc/self/fd/0; lrwxrwxrwx 1 root root 15 Nov 14 21:16 stdout -> /proc/self/fd/1; crw-rw-rw- 1 root root 5, 0 Nov 14 21:16 tty; crw-rw-rw- 1 root root 1, 9 Nov 14 21:16 urandom; crw-rw-rw- 1 root root 1, 5 Nov 14 21:16 zero. **** /proc/mounts; /dev/disk/by",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388:1904,Avail,Available,1904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388,1,['Avail'],['Available']
Availability,"read_tsv(""output.tsv""); }; }; ```. It seems that if you give it an `input_matrix` that is greater than 85 MB worth of data, you get the stack trace below. I reached this size of data with an inputs Array of Arrays that had dimensions 500x901 where each element was a gcs path like ""gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/cab6dea2-8a01-4130-8973-96023637de29/call-SplitGvcf/shard-100/split_gvcfs/11C118416.7ca5a79a-1369-4dc1-8f41-4180d7b3c1ab.0000.g.vcf.gz.tbi"". Whoever picks up this ticket can talk to me about specifics and why we think its an 85MB issue. ```; 2016-06-02 00:18:27,540 cromwell-system-akka.actor.default-dispatcher-341 INFO - WorkflowActor [UUID(fa18fa5f)]: persisting status of RotateGVCFIndex to Running.; 2016-06-01T20:19:00.527-0400: 104257.028: [GC (Allocation Failure) [PSYoungGen: 1821155K->270322K(1864192K)] 4006446K->2894561K(7456768K), 0.1718483 secs] [Times: user=1.22 sys=0.01, real=0.17 secs] ; 2016-06-02 00:19:39,491 ForkJoinPool-3-worker-7 ERROR - Exception not convertible into handled response; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72]; at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:1859,ERROR,ERROR,1859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['ERROR'],['ERROR']
Availability,"reak; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 60; done\"""",; ""endTime"": ""2018-08-14T16:14:34.534911Z""; },; {; ""startTime"": ""2018-08-14T16:14:03.127407Z"",; ""description"": ""Started pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:27.981711Z""; },; {; ""startTime"": ""2018-08-14T16:14:32.463087Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 30; done\"""",; ""endTime"": ""2018-08-14T16:14:33.113135Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.937007Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:13133,echo,echo,13133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"rectory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:2238,alive,alive,2238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,3,['alive'],['alive']
Availability,"red to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2019-07-21 23:34:38,667 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(Hikari",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3588,error,errors,3588,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['error'],['errors']
Availability,"rent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:19501,error,errors,19501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs:/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:4000,down,downloading,4000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['down'],['downloading']
Availability,rg/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/ failed; CommandException: No URLs matched: /cromwell_root/rc; 2019/07/10 18:38:32 Waiting 5 seconds and retrying; 2019/07/10 18:38:38 rm -f $HOME/.config/gcloud/gce && gsutil cp /cromwell_root/rc gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:4101,down,download,4101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,2,"['down', 'error']","['download', 'error']"
Availability,"ridge, MA 02142; Ph: 617-714-7905; Cell: 210-274-3172. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200514792). I'm hesitant to try to follow the same rules as Bash for parsing a command because I worry that it'll be error prone and difficult to pin down the details. It feels like we'd essentially reimplement a Bash parser in WDL. It'd be difficult and hard to get the nuances of something like this:. ```; command {; python <<CODE; print(""#octothorps!!#""); CODE; }; ```. Maybe something as simple as syntax highlighting could help too... if `#`s are not highlighted differently in the command section then that could also be a hint. Granted, I know syntax highlighters won't always be used. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200525363). If it's clear to WDL users how # is interpreted (or not interpreted) in the; command-block maybe that would be the best thing? Would a sentence/blurb; in the docs address this? Possibly as mentioned earlier good error; messages?. -eddie. On Wed, Mar 23, 2016 at 3:43 PM, Scott Frazer notifications@github.com; wrote:. > I'm hesitant to try to follow the same rules as Bash for parsing a command; > because I worry that it'll be error prone and difficult to pin down the; > details. It feels like we'd essentially reimplement a Bash parser in WDL.; > It'd be difficult and hard to get the nuances of something like this:; > ; > command {; > python <<CODE; > print(""#octothorps!!#""); > CODE; > }; > ; > Maybe something as simple as syntax highlighting could help too... if #s; > are not highlighted differently in the command section then that could also; > be a hint. Granted, I know syntax highlighters won't always be used.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200514",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:7679,error,error,7679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['error'],['error']
Availability,"ring out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | java.lang.RuntimeException: Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigWdlNamespace.<init>(ConfigWdlNamespace.scala:50); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace$lzycompute(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.configWdlNamespace(ConfigInitializationActor.scala:37); cromwell_1 | 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigIniti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:1638,Error,Error,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['Error'],['Error']
Availability,"ring] phenolist = paaaa.phenotype_out; }; }; task paaaa {; input{; String phenotype; File annotation; }; command <<<; set -eux; >>>; output {; File plots = phenotype + ""_plot.pdf""; String phenotype_out = phenotype; }; }; ```; The difference between the two being different task names, ""aaaa"" and ""paaaa"", respectively. Note that both of the workflows have an input named ""phenolist"", and an output named ""phenolist"". ; When running `womtool-85.jar inputs working.wdl`, the results are; ```json; {; ""gwas_validation.phenolist"": ""File"",; ""gwas_validation.aaaa.annotation"": ""File""; }; ```; As they should. For the failing workflow, the results are; ```; {; ""gwas_validation.phenolist"": ""File""; }; ```; As you can see, the input ""gwas_validation.paaaa.annotation"": ""File"" has been dropped.; womtool and cromwell-84 (not tested on cromwell-85) also drop all outputs from the outputs, returning only ""{}"".; The task also fails on cromwell, since cromwell does not recognize that there should be any other inputs than ""gwas_validation.phenolist"", and raises an error on that. ; Please note that this is a minified example. renaming variables, erasing variables, adding variables etc can change the failing example to working and vice versa. Based on the behaviour of the examples, I suspect this is related to building the workflow graph somehow. I have included the workflows as well as their womgraphs as files in this issue. [fail.graph.txt](https://github.com/broadinstitute/cromwell/files/11033034/fail.graph.txt); [fail.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033035/fail.wdl.txt); [work.graph.txt](https://github.com/broadinstitute/cromwell/files/11033036/work.graph.txt); [work.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033037/work.wdl.txt); <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ## Configuration:; Latest womtool-85.jar, downloaded from releases OR; cromwell-84.jar OR; womtool-84.jar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7096:2547,error,error,2547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7096,2,"['down', 'error']","['downloaded', 'error']"
Availability,"rkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:1639,error,error,1639,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['error'],['error']
Availability,rkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Ret,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:8194,recover,recoverAsync,8194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recoverAsync']
Availability,"rkflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2181,failure,failure,2181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['failure'],['failure']
Availability,"roadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1360,down,download,1360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['down'],['download']
Availability,"roblems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forComm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1389,Error,ErrorOr,1389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiR,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3273,error,error,3273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['error'],['error']
Availability,"robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:1607,error,error,1607,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['error'],['error']
Availability,"rpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:48); at io.grpc.netty.shaded.io.netty.internal.tcnative.SSLTask.run(SSLTask.java:42); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.runAndResetNeedTask(ReferenceCountedOpenSslEngine.java:1496); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.access$700(ReferenceCountedOpenSslEngine.java:94); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine$TaskDecorator.run(ReferenceCountedOpenSslEngine.java:1471); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler$SslTasksRunner.run(SslHandler.java:1787); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); ... 1 common frames omitted; Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target; at java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:148); at java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:129); at java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297); at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:383); ... 16 common frames omitted. Deadline RPC error example:. Failed to submit job (job-2fd08385-36fe-XXXX-XXXX-cf9e03ae29fe) to GCP, workflowId = 6fd00a65-XXXX-XXXX-84ac-2c0030ba224c; com.google.api.gax.rpc.DeadlineExceededException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: Deadline expired before operation could complete.; ; WorkflowManagerActor: Workflow 415b5011-XXXX-XXXX-953b-68923c164eb0 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: com.google.api.gax.rpc.DeadlineExceededException: io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: Deadline expired before operation could complete.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:9359,error,error,9359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['error'],['error']
Availability,"rror] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2153,Error,ErrorOr,2153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"rror] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1612:1318,error,error,1318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612,1,['error'],['error']
Availability,rse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7438,Error,ErrorOr,7438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,rtGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:3986,Error,ErrorOr,3986,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"rts to release 24 server intermittently gives errors in the logs. A simple workflow: ; `cat goodImport.wdl`:. ```; import ""bar.wdl"" as doIt. workflow testMe {; 	call doIt.doIt; }; ```; And a bunch of wdl tasks in a folder, only one of which is the actual dependency (`bar.wdl`); ```; conradL@qimr13054 ~]$ unzip -l foo.zip ; Archive: foo.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 14:46 foo/; 99 02-07-2017 14:45 foo/bar7.wdl; 98 02-07-2017 14:00 foo/bar.wdl; 99 02-07-2017 14:46 foo/bar8.wdl; 99 02-07-2017 14:45 foo/bar2.wdl; 100 02-07-2017 14:45 foo/bar10.wdl; 99 02-07-2017 14:46 foo/bar9.wdl; 99 02-07-2017 14:45 foo/bar1.wdl; 99 02-07-2017 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:1026,echo,echo,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. ap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:3330,failure,failure,3330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,2,"['echo', 'failure']","['echo', 'failure']"
Availability,"run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:21626,error,error,21626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); Caused by: java.lang.ArrayIndexOutOfBoundsException: null; 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Error summarizing metadata; java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); Caused by: java.lang.ArrayIndexOutOfBoundsException: null; > . These two days I also saw some people here have a similar error, it seems that this is not the process of the problem, error of this step process can run normally, but he cannot update the data in the database, cause I will continue to run again don't think this step is to run, and start to run this step, then the metadata can't collect the mistakes, then the process stopped again, I don't know how to solve this anymore, can anyone give me some help?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:3616,error,error,3616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,2,['error'],['error']
Availability,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072:1814,error,error,1814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072,1,['error'],['error']
Availability,"ry to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" +",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1280,error,error,1280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['error'],['error']
Availability,"rying to introduce SQLite support into Cromwell. I have highlighted reasons to support Sqlite on Cromwell elsewhere. This is as far as I could get without help:. + Migration scheme correctly implemented. All the necessary tables with all the correct constraints (foreign key, unique, primary key) are created on startup.; + Updated upstream liquibase in order to allow unique constraints to be defined properly.; + Made sure all the types were converted in SQLite types (TEXT, INTEGER, BLOB etc.); + Updated the testing to understand SQLite types properly. So far so good. Unfortunately the testing does not recognize the foreign key, primary key or unique constraints, even though they are defined (clearly visible in the sqlitebrowser). . Since the testing is just testing, I also decided to run cromwell with a workflow, but that does not work: ; ```; [ERROR] [07/20/2020 14:01:02.134] [cromwell-system-akka.dispatchers.engine-dispatcher-50] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreEngineActor] Error trying to fetch new workflows; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:1010); at org.sqlite.core.DB.newSQLException(DB.java:1022); at org.sqlite.core.DB.throwex(DB.java:987); at org.sqlite.core.NativeDB.prepare_utf8(Native Method); at org.sqlite.core.NativeDB.prepare(NativeDB.java:134); at org.sqlite.core.DB.prepare(DB.java:264); at org.sqlite.core.CorePreparedStatement.<init>(CorePreparedStatement.java:45); at org.sqlite.jdbc3.JDBC3PreparedStatement.<init>(JDBC3PreparedStatement.java:30); at org.sqlite.jdbc4.JDBC4PreparedStatement.<init>(JDBC4PreparedStatement.java:19); at org.sqlite.jdbc4.JDBC4Connection.prepareStatement(JDBC4Connection.java:35); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:241); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:205); at com.zaxxer.hikari.pool.ProxyCon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:1088,Error,Error,1088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,1,['Error'],['Error']
Availability,"s a WDL syntax error, but in a comment!; #echo {an_input} ; }. output {. }; }. workflow test {; call comment_bug; }; ```. EDIT: error message, for completeness:. ```; $ java -jar wdltool.jar validate comment_bug.wdl; ERROR: Finished parsing without consuming all tokens. output {; ^; ```. I can get rid of the error by changing the comment line to `#echo ${an_input}`. ; I think errors in comment lines should probably be ignored by the validator. As an aside, is there a more helpful error message for this? The message sounds like an unused input variable or something, not that the bracket syntax was off, so it was hard to diagnose (The above is a toy example, the real task had a much more complicated command). ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200416975). This is happening because the the `command {...}` section is parsed differently than the rest of WDL. The parser thinks that the closing brace in `#echo {an_input}` is actually trying to close the `command` section. If you use the alternative delimiters (`command <<< ... >>>`) this is another way to get around it. We parse the command as ""opaque strings intermixed with `${...}` blocks"". That means that the `#`-style comments inside a command section are not interpreted as WDL comments, but instead as part of the command. More thought would have to go into figuring out what the right thing to do here is. ---. @tmdefreitas commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200442613). Admittedly, I have never written a parser before, so I don't know how feasible this is, but can you write the grammar/parser such that everything on a line after a `#` character is ignored?. The only edge cases I imagine are when the `#` character is in a quoted string. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200505343). @tmdefreitas Yes, that is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:1403,echo,echo,1403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['echo'],['echo']
Availability,"s a complex WDL, so a more basic example is listed below. ## background; WDL doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1330,error,errorcode,1330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,"s eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.ethnicity_self_report:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:23:03,70] [info] BT-322 9e4f5894:main.pcs:-1:1 is eligible for call caching with read = true and write = true; [2022-12-15 21:27:50,35] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.assessment_ages:-1:1-20000000002 [9e4f5894main.assessment_ages:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,35] [info] BT-322 9e4f5894:main.assessment_ages:-1:1 cache hit copying success with aggregated hashes: initial = EEC3507DAE39FE605FDE6F9F6FC0A5A8, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,35] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.assessment_ages:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:50,48] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.assessment_ages' (scatter index: None, attempt 1); [2022-12-15 21:27:50,82] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.genetic_sex:-1:1-20000000011 [9e4f5894main.genetic_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:27:50,82] [info] BT-322 9e4f5894:main.genetic_sex:-1:1 cache hit copying success with aggregated hashes: initial = FD7DC79B974CF6706FC3376F067965B9, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:27:50,82] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.genetic_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:27:54,15] [info] WorkflowExecutionActor-9e4f5894-f7e6-4e2f-be4b-f547d6de7fff [9e4f5894]: Job results retrieved (CallCached): 'main.genetic_sex' (scatter index: None, atte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:29101,failure,failures,29101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"s for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. I executed `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/hom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:994,error,error,994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,2,['error'],['error']
Availability,"s found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:2436,Error,Error,2436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['Error'],['Error']
Availability,s in the following example where Ammonite dependency failed:. ```; 2019/07/10 18:29:15 Starting container setup.; 2019/07/10 18:29:24 Done container setup.; 2019/07/10 18:29:31 Starting localization.; 2019/07/10 18:29:37 Localizing input dos://dg.4503/cbdb14f5-cc89-4481-bad7-2ef8f36a1290 -> /cromwell_root/topmed-irc-share/genomes/NWD127112.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; Compiling /scripts/dosUrlLocalizer.sc; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:2957,Down,Downloaded,2957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['Down'],['Downloaded']
Availability,"s whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:981,error,error,981,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['error'],['error']
Availability,"s"" documentation shows `memory_gb` as an optional `runtime-attribute`: https://cromwell.readthedocs.io/en/develop/tutorials/HPCIntro/. ```; backend.providers.SGE.config {; runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? sge_queue; String? sge_project; """"""; }; ```. My intent is for the user to only provide arguments for `cpu`, `memory`, `runtime_minutes`, and `partition` if they intend to override the SLURM cluster's defaults. I do not want to have cromwell supply defaults, because if these arguments are omitted from the call to `sbatch` then the cluster's defaults will be used. My understanding is that making them optional like `String? memory_mb` and then using syntax like `${""--mem "" + round(memory_mb) + ""m""} \` in the submit script means that argument will only be added if `memory` is defined, and will be omitted if `memory` is not defined. I've followed the documentation as closely as I can. However, when I try to submit a test job without `cpu` and `memory` set as a runtime attribute, I get a failure with these exceptions:; ```; cromwell.core.CromwellAggregatedException: Initialization Failure:; Runtime validation failed:; 	Task myTask has an invalid runtime attribute cpu = !! NOT FOUND !!; 	Task myTask has an invalid runtime attribute memory = !! NOT FOUND !!; 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:356); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:339); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); ```. Here is the test WDL I'm using:. ```; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; call myTask. }. task myTask {; command <<<; echo ""hello world""; >>>; output {; String out = read_string(stdout()); }; }; ```. And my complete configuration for this backend:; ```; backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7455:1335,failure,failure,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7455,1,['failure'],['failure']
Availability,s-dsl_2.12-0.18.17.pom.sha1; Downloading https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-46,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3542,down,download,3542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,2,"['down', 'error']","['download', 'error']"
Availability,"s.; [2018-10-25 21:21:09,93] [info] WorkflowManagerActor Successfully started WorkflowActor-0bb77c74-4c5c-4314-8463-072e7055ee7c; [2018-10-25 21:21:09,93] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-10-25 21:21:09,93] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-10-25 21:21:09,96] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Parsing workflow as WDL draft-2; [2018-10-25 21:21:10,57] [info] MaterializeWorkflowDescriptorActor [0bb77c74]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:21:12,86] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Condition met: 'go'. Running conditional section; [2018-10-25 21:21:16,98] [info] WorkflowExecutionActor-0bb77c74-4c5c-4314-8463-072e7055ee7c [0bb77c74]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:21:19,02] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-2/execution/script; [2018-10-25 21:21:19,04] [info] BackgroundConfigAsyncJobExecutionActor [0bb77c74test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/sh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:3812,echo,echo,3812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"s.zip](https://github.com/broadinstitute/cromwell/files/662706/dependencies.zip) I can run this:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run main.wdl - - dependencies.zip; ```. and it succeeds, i.e. logging ends with; ```; [2016-12-20 13:24:52,24] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""main_output"": ""Hello sub world!""; },; ""id"": ""8a414771-fdef-43f8-85f3-a0b738fd28fc""; }; ```. but when I run in server mode with the exact same jar and WDL files submitted via REST API:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@main.wdl -FwdlDependencies=dependecies.zip; ```. it fails:; ```; 2016-12-20 13:28:19,319 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-12-20 13:28:19,365 cromwell-system-akka.dispatchers.engine-dispatcher-22 ERROR - WorkflowManagerActor Workflow cbde1e8b-00df-405e-be19-ac4f4b3ea5b8 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; error in opening zip file; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; error in opening zip file; 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:138); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:130); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:117); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(Materialize",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1799:1151,ERROR,ERROR,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1799,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"s: initial = 3C891C9939496580DDF747805F991E06, file = AAFFF98AC7D58B07E7CE25978A906B00.; [2022-12-15 21:28:04,01] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.low_genotyping_quality_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,02] [warn] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-BackendCacheHitCopyingActor-788d8048:main.sex_mismatch_sample_list:-1:1-20000000015 [788d8048main.sex_mismatch_sample_list:NA:1]: Unrecognized runtime attribute keys; : shortTask, dx_timeout; [2022-12-15 21:28:04,02] [info] BT-322 788d8048:main.sex_mismatch_sample_list:-1:1 cache hit copying success with aggregated hashes: initial = 03340ED60152B24B7D0988669F47CF2B, file = EB6A9909BDF3705B7BB543E4096DA08A.; [2022-12-15 21:28:04,02] [info] 788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2-EngineJobExecutionActor-main.sex_mismatch_sample_list:NA:1 [788d8048]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:28:04,35] [info] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main; /788d8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-915037270/load_shared_covars.py . /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d80; 48-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-949496038/ukb46122_cal_chr1_v2_s488176.fam /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d; 8048-ef2b-4d7c-b3cb-6e04b3cbbdc2/call-load_shared_covars/inputs/-1401422240/22009.txt /home/cromwell-executions/main/9e4f5894-f7e6-4e2f-be4b-f547d6de7fff/call-main/main/788d8048-ef2b-4d7c-b3cb-; 6e04b3cbbdc2/call-load_shared_covars/inputs/-1717412047/21003.txt; [2022-12-15 21:28:04,43] [info] BackgroundConfigAsyncJobExecutionActor [788d8048main.load_shared_covars:NA:1]: executing: /usr/bin/env ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:37208,failure,failures,37208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"s:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\""); fi '\"""",; ""endTime"": ""2018-08-14T16:16:57.822335Z""; },; {; ""startTime"": ""2018-08-14T16:14:33.759759Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 60; done\"""",; ""endTime"": ""2018-08-14T16:14:34.534911Z""; },; {; ""startTime"": ""2018-08-14T16:14:03.127407Z"",; ""description"": ""Started pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:27.981711Z""; },; {; ""startTime"": ""2018-08-14T16:14:32.463087Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/google\/logs\/output gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:11932,echo,echo,11932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"sMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:8794,ERROR,ERROR,8794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['ERROR'],['ERROR']
Availability,"sbt assembly: build errors ""cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:20,error,errors,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,1,['error'],['errors']
Availability,"scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:201); 	 at wom.values.WomSingleFile.mapFile(WomFile.scala:182); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.mapCommandLineWomFile(SharedFileSystemAsyncJobExecutionActor.scala:145); 	 at cromwell.ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1866,ERROR,ERROR,1866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['ERROR'],['ERROR']
Availability,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:8278,down,down,8278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,2,['down'],['down']
Availability,"scala:176); 	at akka.actor.Actor.aroundReceive(Actor.scala:539); 	at akka.actor.Actor.aroundReceive$(Actor.scala:537); 	at cromwell.engine.workflow.WorkflowDockerLookupActor.aroundReceive(WorkflowDockerLookupActor.scala:40); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:612); 	at akka.actor.ActorCell.invoke(ActorCell.scala:581); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2020-10-08 16:08:58,573 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: `which python; python --version; echo ""Hello ZZZ!"" > file.txt`; 2020-10-08 16:08:58,593 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: executing: module load proxy; # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; export SINGULARITY_CACHEDIR=/scratch/$USER/.singularity/cache; export SINGULARITY_LOCALCACHEDIR=/scratch/$USER/.singularity/localcache; export SINGULARITY_TMPDIR=/scratch/$USER/.singularity/tmp; mkdir -p $SINGULARITY_CACHEDIR; mkdir -p $SINGULARITY_LOCALCACHEDIR; mkdir -p $SINGULARITY_TMPDIR; export SINGULARITY_BINDPATH=input_data/hello,$EXECUTION_ROOT:/cromwell-executions,/usr/prog/nx/cromwell/test; # echo ""SINGULARITY_CACHEDIR: $SINGULARITY_CACHEDIR""; # echo ""SINGULARITY_BINDPATH: $SINGULARITY_BINDPATH""; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $SINGULARITY_CACHEDIR; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:6851,echo,echo,6851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['echo'],['echo']
Availability,"scala:33); at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:22); at slick.jdbc.Invoker.foreach(Invoker.scala:47); at slick.jdbc.Invoker.foreach$(Invoker.scala:46); at slick.jdbc.StatementInvoker.foreach(StatementInvoker.scala:16); at slick.jdbc.StreamingInvokerAction.run(StreamingInvokerAction.scala:22); at slick.jdbc.StreamingInvokerAction.run$(StreamingInvokerAction.scala:20); at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$2.run(JdbcActionComponent.scala:214); at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$2.run(JdbcActionComponent.scala:214); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834). [INFO] [07/20/2020 14:01:02.780] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka.actor.ActorSystemImpl(cromwell-system)] Cromwell 53-40069a5-SNAP service started on 0:0:0:0:0:0:0:0:8000...; [INFO] [07/20/2020 14:01:06.523] [cromwell-system-akka.dispatchers.engine-dispatcher-52] [akka://cromwell-system/user/cromwell-service/JobExecutionTokenDispenser] Not triggering log of token queue status. Effective log interval = None; [ERROR] [07/20/2020 14:01:22.141] [cromwell-system-akka.dispatchers.api-dispatcher-84] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreSubmitActor] Workflow java.sql.SQLException: not implemented by SQLite JDBC driver submit failed.; ```. I have poked a bit around in the code, but I could not find a lead. I am not at all experienced with slick and database debugging so at the least I need a few pointers to get going again. Is there somebody around who is familiar enough with these systems to help me out?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:3952,ERROR,ERROR,3952,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,1,['ERROR'],['ERROR']
Availability,"scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:21,51] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Starting test1.hello; [2019-02-11 10:13:22,36] [info] Assigned new job execution tokens to the following groups: 52999e15: 1; [2019-02-11 10:13:22,66] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: echo ""Hello World"" > World.txt; [2019-02-11 10:13:22,75] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: executing: /usr/bin/env bash /spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/script; [2019-02-11 10:13:26,29] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: job id: 12910; [2019-02-11 10:13:26,30] [info] BackgroundConfigAsyncJobExecutionActor [52999e15test1.hello:NA:1]: Status change from - to Done; [2019-02-11 10:13:27,61] [info] Request method=GET uri=https://auth.docker.io/token?service=registry.docker.io&scope=repository%3Alibrary/ubuntu%3Apull headers= threw an exception on attempt #4. Giving up.; java.nio.channels.UnresolvedAddressException: null; at sun.nio.ch.Net.checkAddress(Net.java:101); at sun.nio.ch.UnixAsynchronousSocketChannelImpl.implConnect(UnixAsynchronousSocketChannelImpl.java:301); at sun.nio.ch.AsynchronousSocketChannelImpl.connect(Async",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:10911,echo,echo,10911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['echo'],['echo']
Availability,"seUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:57); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:125); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1229); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1211); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:600); 	... 16 common frames omitted; Caused by: org.postgresql.util.PSQLException: ERROR: syntax error at or near ""as""; Position: 73; 	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2440); 	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2183); 	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:308); 	at org.postgresql.jdbc.PgStatement.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:36726,ERROR,ERROR,36726,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"ser command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.P",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:12839,error,error,12839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['error'],['error']
Availability,series of database creation errors fixed by changing double quotes to single quotes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:28,error,errors,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['error'],['errors']
Availability,"sh ${script}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; ""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')""; """"""; filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; hasing-strategy: ""fingerprint""; check-sibling-md5: true; fingerprint-size: 1048576 # 1 MB ; }; }; }; }; }; # For running jobs by submitting them from an interactive node to the cluster; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:5385,alive,alive,5385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['alive'],['alive']
Availability,"sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4004:2228,echo,echo,2228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004,1,['echo'],['echo']
Availability,"shard-0/glob-db248e3bce81b54f5ef521878fe9e9de.list; 2023/04/18 21:55:03 Delocalizing output /cromwell_root/ SAMEA104027315_pull_results.txt -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/ SAMEA104027315_pull_results.txt; 2023/04/18 21:55:04 Delocalizing output /cromwell_root/SAMEA104027315.tar -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/SAMEA104027315.tar; 2023/04/18 21:55:04 Delocalization script execution complete.; 2023/04/18 21:55:05 Done delocalization.; ```. In Job Manager, an error with the outputs can be seen. <img width=""1115"" alt=""job outputs"" src=""https://user-images.githubusercontent.com/27784612/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:3977,error,errors,3977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['error'],['errors']
Availability,"ship_count:-1:1 cache hit copying success with aggregated hashes: initial = 40DB3965745EAB4613A3E2804F447EFE, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,03] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.kinship_count:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,12] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.reported_sex:-1:1-20000000001 [9e4f5894main.reported_sex:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,12] [info] BT-322 9e4f5894:main.reported_sex:-1:1 cache hit copying success with aggregated hashes: initial = 91C81CABBB083C238800E3CF59AF537D, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,12] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.reported_sex:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,16] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.sex_aneuploidy:-1:1-20000000003 [9e4f5894main.sex_aneuploidy:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,17] [info] BT-322 9e4f5894:main.sex_aneuploidy:-1:1 cache hit copying success with aggregated hashes: initial = 86896541F0DCB2C2B959EEF37F266B30, file = EF056BD27B3A512F77663A400D778CCF.; [2022-12-15 21:22:59,17] [info] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-EngineJobExecutionActor-main.sex_aneuploidy:NA:1 [9e4f5894]: Call cache hit process had 0 total hit failures before completing successfully; [2022-12-15 21:22:59,32] [warn] 9e4f5894-f7e6-4e2f-be4b-f547d6de7fff-BackendCacheHitCopyingActor-9e4f5894:main.month_of_birth:-1:1-20000000024 [9e4f5894main.month_of_birth:NA:1]: Unrecognized runtime attribute keys: dx_timeout; [2022-12-15 21:22:59,32] [info] BT-322 9e4f5894:main.month_of_birth:-1:1 cache hit copying success with aggregated hashes: ini",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:22535,failure,failures,22535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['failure'],['failures']
Availability,"sk.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2023-02-04 08:55:08,24] [info] WorkflowManagerActor WorkflowActor-48f62f22-25fe-4f0f-b5fe-21191f035abd is in a terminal state: WorkflowFailedState; [2023-02-04 08:55:08,24] [info] $a [[38;5;2m48f62f22[0m]: Copying workflow logs from /mnt/g/ELM-WES-pipeline/cromwell-workflow-logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log to /mnt/g/ELM-WES-pipeline/cromwell_wf_logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log; [2023-02-04 08:55:15,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2023-02-04 08:55:17,27] [info] Workflow polling stopped; [2023-02-04 08:55:17,29] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2023-02-04 08:55:17,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:16016,down,down,16016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,3,['down'],['down']
Availability,"sks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent norma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:15850,down,downstream,15850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['down'],['downstream']
Availability,sl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… ; https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_… . Downloaded https://repo1.maven.org/maven2/org/http4s/http4s-dsl_2.12/0.18.17/http4s-dsl_2.12-0.18.17.pom; Downloaded; ...; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… ; https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcompon… . Failed to resolve ivy dependencies:; org.apache.httpcomponents:httpcomponents-core:4.0.1 ; not found: /root/.ivy2/local/org.apache.httpcomponents/httpcomponents-core/4.0.1/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/httpcomponents/httpcomponents-core/4.0.1/httpcomponents-core-4.0.1.pom; org.apache.commons:commons-parent:5 ; not found: /root/.ivy2/local/org.apache.commons/commons-parent/5/ivys/ivy.xml; download error: Caught java.net.UnknownHostException: repo1.maven.org (repo1.maven.org) while downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/5/commons-parent-5.pom; download error: Caught java.net.UnknownHostException: oss.sonatype.org (oss.sonatype.org) while downloading https://oss.sonatype.org/content/repositories/snapshots/org/apache/commons/commons-parent/5/commons-parent-5.pom; ...; CommandException: No URLs matched: /cromwell_root/stderr; 2019/07/10 18:38:31 Delocalizing output /cromwell_root/rc -> gs://fc-94bba050-4ef1-42fb-8436-cd89da17ec53/306ddffc-0ee6-46ff-ac3e-5069668a0eb0/ga4ghMd5/a14f0b9d-839c-4684-863c-93d0e8e2d527/call-md5/rc; 2019/07/10 18:38:32 rm -f $HOME/.config/gcloud/gce && gsu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:3638,down,downloading,3638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,1,['down'],['downloading']
Availability,slow down significantly when I set call-caching,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6958:5,down,down,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6958,1,['down'],['down']
Availability,spatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:6117,recover,recover,6117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,"spatcher-29 INFO - WorkflowManagerActor: Workflow actor for cf00212c-9c18-4f14-a953-c7a4450ada7c completed with status 'Failed'. The workflow will be removed from the workflow store.; ```. In particular, Cromwell tried to fetch file `s3://s3.amazonaws.com/gred-cumulus-dev/cromwell-execution/cumulus/cf00212c-9c18-4f14-a953-c7a4450ada7c/call-cluster/cluster-rc.txt`, while I only have `s3://gred-cumulus-dev` bucket. . I'm wondering where `s3.amazonaws.com` prefix came from. I searched through Cromwell's source code, and found [this](https://github.com/broadinstitute/cromwell/blob/develop/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L47.) file is probably the place to check. Due to its specification, The S3 Filesystem Provider will attach `s3.amazonaws.com` if `endpoint` is missing, to be `s3://[endpoint]/{bucket}/{key}` for the URI. However, I could not find any information regarding endpoint on Cromwell's readthedocs website. I'm wondering if this error was because I missed something in my conf file. My conf file is the following:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]. region = ""us-west-2""; }. engine {; filesystems {; s3 {; auth = ""default""; }; }; }. backend {; default = ""AWSBatch"". providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; default-runtime-attributes {; scriptBucketName = ""gred-cumulus-dev""; queueArn: ""arn:aws:batch:us-west-2:752311211819:job-queue/priority-gwfcore-test""; }. filesystems {; s3 {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""default""; }; }. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in the cloud.; slow-job-warning-time: ""24 hours"". concurrent-job-limit = 1000; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; # A reference to ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504:7549,error,error,7549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504,1,['error'],['error']
Availability,"spatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:7890,error,errors,7890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"src/ci/resources/slurm_application.conf. > # https://slurm.schedmd.com/squeue.html; > --; > 25 | check-alive = ""squeue -j ${job_id}"". The job state is being checked by the exit code: 0 means job not complete, non-zero is assumed to be job complete. This assumption is false. This is depending on site configured behavior about how quickly finished jobs are moved from the active controller the sacct database, as only after that happens the squeue command ""fails"" because the job isn't in the active DB anymore. Furthermore, if the job fails or is cancelled, cromwell will also falsely presume the job is complete since it's also no longer in the active DB. When the squeue command itself fails or times out, a non-zero exit code is also returned, which is again incorrectly interpreted as a completed job. . ""But if your squeue command fails you're whole machine is already broken!""; No. On a very busy slurm machine it is expected behavior that sometimes commands will time out when the controller is busy servicing a sudden burst of job submissions, state queries, job starts, or job completions. It would be an improvement to use sacct and check the job state like this:. check-alive = ""sacct -j ${job_id} -X -n -o state | grep -v COMPLETED"". That also decouples you from slurm controller noise, as sacct is going to a different database, but you'll still get the wrong results if _that_ database is down for some reason and the sacct command itself fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5400:103,alive,alive,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400,3,"['alive', 'down']","['alive', 'down']"
Availability,"st stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-16-0-unknown-operation#-371454906]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-unknown-operation#-248376973]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-5-0-unknown-operation#-865853959]] terminated abruptly; [2017-03-20 15:30:35,49] [info] WorkflowManagerActor: Received shutdown signal.; [2017-03-20 15:30:35,49] [info] WorkflowManagerActor All workflows finished. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:4295,error,error,4295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,6,['error'],['error']
Availability,st.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at cromwell.core.TestKitSuite.org$scalatest$BeforeAndAfterAll$$super$run(TestKitSuite.scala:16) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at cromwell.core.actor.RobustClientHelperSpec.org$scalatest$FlatSpecLike$$super$run(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$run$1(FlatSpecLike.scala:1795) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FlatSpecLike.run(FlatSpecLike.scala:1795) at org.scalatest.FlatSpecLike.run$(FlatSpecLike.scala:1793) at cromwell.core.actor.RobustClientHelperSpec.run(RobustClientHelperSpec.scala:14) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:3647,Robust,RobustClientHelperSpec,3647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,st.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: Submitted did not equal Succeeded; at org.scalatest.MatchersHelper$.indicateFailure(MatchersHelper.scala:346); at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6668); at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:4550,Error,ErrorHandling,4550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['Error'],['ErrorHandling']
Availability,stExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Could not evaluate expression: write_lines(array_of_files): null (Service: S3Client; Status Code: 403; Request ID: null); at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:563); ... 31 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275:3957,Error,Error,3957,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275,1,['Error'],['Error']
Availability,"stKitSpec.scala:250) at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314) at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507) at sbt.TestRunner.runTest$1(TestFramework.scala:113) at sbt.TestRunner.run(TestFramework.scala:124) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282) at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282) at sbt.TestFunction.apply(TestFramework.scala:294) at sbt.Tests$.processRunnable$1(Tests.scala:347) at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353) at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46) at sbt.std.Transform$$anon$4.work(System.scala:67) at sbt.Execute.$anonfun$submit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.Promis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:5543,Error,ErrorHandling,5543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,1,['Error'],['ErrorHandling']
Availability,"stand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs-prod2, 2018-01-02T03:00:06Z, 2018-01-02T03:53:16Z, ggp-17542369260334007071, us-east1-c/n1-standard-2; > broad-wgs-prod2, 2018-01-02T04:05:53Z, 2018-01-03T04:21:41Z, ggp-18057230858599167003, us-central1-f/n1-standard-2; > broad-wgs-prod2, 2018-01-03T04:26:14Z, 2018-01-03T04:46:10Z, ggp-7937324860344917086, us-central1-b/n1-standard-2; > broad-wgs-prod5, 2018-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:5694,error,error,5694,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['error'],['error']
Availability,standard output and error file names can be expressions.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3279:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3279,1,['error'],['error']
Availability,staring at test failures on branch leads to realization that test was broken on develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/452:16,failure,failures,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/452,1,['failure'],['failures']
Availability,stderr/out aren't available during runs and are sometimes in application/octet-stream,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3967:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967,1,['avail'],['available']
Availability,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:11469,down,download,11469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,2,"['down', 'error']","['download', 'error']"
Availability,"sts. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be tr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:1680,error,error,1680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,submitting empty json file returns Internal server error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4086:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086,1,['error'],['error']
Availability,"sults=""${outputprefix}.pathway_gsea_mrnaseq_subtypes.zip"" ; }. runtime {; docker : ""broadgdac/tool_gsea_mrnaseq_subtypes:22""; }. meta {; author : ""Juok Cho""; email : ""jcho@broadinstitute.org""; }. }. workflow tool_gsea_mrnaseq_subtypes_workflow {; call tool_gsea_mrnaseq_subtypes; }; ```. Thanks,; Tim. ---. @geoffjentry commented on [Tue Mar 29 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-203042213). When talking to @knoblett she said that this appears to be something which previously was resolved - perhaps there's been a regression, or it's slightly different somehow. ---. @kbergin commented on [Wed Mar 30 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-203469521). I also observed something similar. One of my tasks was failing because I had an input designated as an input in the call, and in the task, but it actually wasn't an input for the workflow nor in the json. I feel like validate or some error handling should have caught that? Instead it just didn't make it through JES and said it 'failed to localize inputs'. But Brad pointed out that I think I used swagger to validate not wdltools, so this could be entirely my fault, as he said those may not be in sync / up to date wise? I wasn't aware. Anyways here was my situation in case it's at all helpful, the missing var is **File gender_mask_bed**:. ```; workflow GenomeStripBamWorkflow {; String sample_name; String bam_name; String analysis_directory; File bam; File ref_fasta; File ref_fasta_index; File ref_dict; File ref_genome_sizes; File ploidy_map; File copy_number_mask; File copy_number_mask_index; File read_depth_mask; File ref_profile; File genome_mask; File genome_mask_index; File configs. ##This is the call that had the issue, there are some before it that it depends on, ; ##but not for the missing input (gender_mask_bed); call CallSampleGender as CallSampleGender {; input:; analysis_directory = analysis_directory,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:2929,error,error,2929,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,1,['error'],['error']
Availability,"sy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[String?] or Array[String?]?. [An example WDL, which passes womtool and miniwdl check, is available here.](https://gist.github.com/aofarrel/547c35468c248331b678b3f766f83591) It actually shows the issue twice -- once in the section starting with `if(defined(variant_call_after_earlyQC_filtering.errorcode)) {` and once in the section starting with `if(defined(profile_bam.strain)) {`. Interestingly, the results of echo_b implies that select_first() is a more accurate way of checking if a variable is defined than t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:1519,error,errorcode,1519,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,2,"['echo', 'error']","['echo', 'errorcode']"
Availability,syncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:5191,robust,robustExecuteOrRecover,5191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['robust'],['robustExecuteOrRecover']
Availability,syncExecutionActor.scala:300); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:300); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:43); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:47); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:71); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:4914,robust,robustExecuteOrRecover,4914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['robust'],['robustExecuteOrRecover']
Availability,"syncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:4636,error,error,4636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['error'],['error']
Availability,"system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]: Starting wf_hello.hello; 2018-06-07 13:09:17,680 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: `echo ""Hello World! Welcome to Cromwell . . . on AWS!""`; 2018-06-07 13:09:17,684 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES outputs (found 4):; Output(Some(rc),Some(wf_hello.hello.rc),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc,Some(FILE)); Output(Some(stdout),Some(wf_hello.hello.stdout),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout,Some(FILE)); Output(Some(stderr),Some(wf_hello.hello.stderr),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr),/cromwell-executions/wf_h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:2185,echo,echo,2185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,1,['echo'],['echo']
Availability,t akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute va,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:4899,Fault,FaultHandling,4899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,"t be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4041,down,downloading,4041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['down'],['downloading']
Availability,"t cromwell provides `server` mode where we can submit runs via REST API end points. However, we are working on HPC cluster where we don't have admin privileges to start server and submit requests to api. Backend: `slurm`; Workflow: [Link](https://github.com/biowdl/RNA-seq/blob/develop/RNA-seq.wdl). <details>; <summary>Config</summary>. ```; backend {. default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:1591,alive,alive,1591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['alive'],['alive']
Availability,"t evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean keep_duplicate_reads; Boolean disable_all_read_filters; String transform; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File gatk_jar; Boolean disable_sequence_dictionary_validation; Boolean isWGS; Int mem. # Note that when isWGS is true, this task is still called by the workflow.; # In that case, an empty coverage file is created and passed to the WholeGenomeCoverage ; # task to satisfy input and output requirements.; command <<<; if [ ${isWGS} = false ]; then; java -Xmx${mem}g -jar ${gatk_jar} CalculateTargetCoverage --output ${entity_id}.coverage.tsv \; --groupBy ${grouping} --transform ${transform} --targets ${padded_target_file} --targetInformationColumns FULL \; --input ${input_bam} --reference ${ref_fasta} --dis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646:1758,echo,echo,1758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646,1,['echo'],['echo']
Availability,"t gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:39.695039Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.673071Z"",; ""description"": ""Started running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\""); fi '\"""",; ""endTime"": ""2018-08-14T16:16:57.822335Z""; },; {; ""startTime"": ""2018-08-14T16:14:33.759759Z"",; ""description"": ""Started running \""\/bin\/sh -c while true; do retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stdout gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:11069,echo,echo,11069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,2,['echo'],['echo']
Availability,"t or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1855,recover,recoverWith,1855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['recover'],['recoverWith']
Availability,"t runs successfully. It looks a ""stochastic"" error. Below you can find the full logs for that task and, as you can see, the file was successfully localized. ```; timestamp,message; 1608596940672,*** LOCALIZING INPUTS ***; 1608596942260,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:1041,down,download,1041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"t scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at akka.actor.Actor$class.aroundReceive(Actor.scala:496); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:67); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); ... 5 more; Caused by: com.google.api.client.http.HttpResponseException: 500 Internal Server Error; {; ""error"" : ""internal_failure""; }; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1070); at com.google.auth.oauth2.UserCredentials.refreshAccessToken(UserCredentials.java:207); at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:149); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply$mcV$sp(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply(GoogleAuthMode.scala:62); at cromwell.filesystems.gcs.auth.GoogleAuthMode$$anonfun$validateCredential$1.apply(GoogleAuthMode.scala:62); at scala.util.Try$.apply(Try.scala:192); at cromwell.filesystems.gcs.auth.GoogleAuthMode$class.validate(GoogleAuthMode.scala:65); ... 46 more. [INFO] [05/12/2017 21:47:03.777] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor WorkflowActor-6c383c35-d791-4971-aecd-0723726c8a7b is in a te",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:6003,Error,Error,6003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,2,"['Error', 'error']","['Error', 'error']"
Availability,t scala.collection.AbstractIterable.head(Iterable.scala:54); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.execute(AwsAsyncJobExecutionActor.scala:53); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeAsync(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeAsync(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:502); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeOrRecover(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.aroundReceive(AwsAsyncJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:3032,robust,robustExecuteOrRecover,3032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,1,['robust'],['robustExecuteOrRecover']
Availability,"t scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:35,304 cromwell-system-akka.dispatchers.engine-dispatcher-30 WARN - Couldn't find a suitable DSN, defaulting to a Noop one.; 2017-12-06 04:38:35,336 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - Using noop to send events.; 2017-12-06 04:38:35,447 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - EJEA_20f2c75f:ps:-1:1: Error reading from JobStore; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:3344,ERROR,ERROR,3344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"t will fail. In these cases the developer has to:. 1. Manually restart the test at least once.; 2. Wait for the entire test suite to re-run.; 3. Remember to collect and report the error for further triage. **Background**. ScalaTest has a way to wait for slow tests to `Eventually` pass. However for failing tests that need to be restarted ScalaTest has the `Retries` trait that can be used for ""flickering"" tests. ScalaTest even has a way to mark tests as `Retryable` tag meaning that the retry code could be widely applied while only running on certain tagged tests. Here is an example output of a tagged test retried by ScalaTest:. ```; [info] All tests passed.; [info] FlakySpec:; [info] Flaky ; [info] - should maybe fail !!! CANCELED !!! (9 milliseconds); [info] Test canceled because flickered: initially failed, but succeeded on retry (Retries.scala:349); [info] Passed: Total 104, Failed 0, Errors 0, Passed 104, Canceled 1; ```. Because these `TestCanceled` events are likely to be ignored by developers the error events should be reported and aggregated. ScalaTest allows one to create a custom `Reporter` to catch `TestFailed` or `TestCanceled` events. A custom reporter could be built that captures the failed and flickering test events and forwards them to an external system for aggregation and reporting. Unfortunately as shown above the behavior of `org.scalatest.Retries.withRetry` is to try twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:1159,error,error,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,"t {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(Batching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:1813,failure,failure,1813,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['failure'],['failure']
Availability,"tInTransaction(JdbcBackend.scala:470); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:39); 	at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:36); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.io.EOFException: Can not read response from server. Expected to read 4 bytes, read 0 bytes before connection was unexpectedly lost.; 	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3014); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472); 	... 16 common frames omitted.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42973] ERROR c.s.m.impl.MetadataServiceActor - Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(My",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:5851,ERROR,ERROR,5851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,1,['ERROR'],['ERROR']
Availability,t_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:1635,Error,ErrorOr,1635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"tainall --bind cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello:/cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; \; docker://python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4 /bin/bash /cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/script"" | qsub \; -terse \; -b n \; -N cromwell_45d03417_say_hello \; -wd cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello \; -o cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stdout \; -e cromwell-executions/hello/45d03417-24b0-4f91-a3a2-1f3fa945e36c/call-say_hello/execution/stderr \; \; -l m_mem_free=$(expr 4096 / 1)m \; -l h_rt=3600 \; -l s_rt=3600 \; \; \; \; -V; 2020-10-08 16:09:02,038 cromwell-system-akka.dispatchers.backend-dispatcher-534 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: job id: 3309379; 2020-10-08 16:09:02,040 cromwell-system-akka.dispatchers.backend-dispatcher-533 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(45d03417)hello.say_hello:NA:1]: Cromwell will watch for an rc file *and* double-check every 600 seconds to make sure this job is still alive; ```. Workflow:; ```; version 1.0. workflow hello {; input {; String name = ""John"". }; call say_hello {; input:; name = name; }; call upper {; input:; in = say_hello.out; }. output {; String out = say_hello.out; }; }. task say_hello {; input {; String name; }; output {; File out = ""file.txt""; }; command <<<. which python; python --version; echo ""Hello ~{name}!"" > file.txt. >>>; runtime {; docker: ""python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4""; }; }. task upper {; input {; File in; }; output {; File out = ""output.txt""; }; command <<<; cat ~{in} | awk '{print toupper($0)}' > output.txt; >>>; runtime {; docker: ""python@sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:9500,alive,alive,9500,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,2,"['alive', 'echo']","['alive', 'echo']"
Availability,tandardCachingActorHelper.scala:64); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jobPaths$lzycompute(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.jobPaths(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardCachingActorHelper$class.startMetadataKeyValues(StandardCachingActorHelper.scala:76); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.startMetadataKeyValues(JesAsyncBackendJobExecutionActor.scala:339); at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:516); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.executeOrRecover(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:54); at cromwell.core.retry.Retry$.withRetry(Retry.scala:36); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:50); at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:54); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:77); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); at scala.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:4120,robust,robustExecuteOrRecover,4120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['robust'],['robustExecuteOrRecover']
Availability,"tch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2940,error,error,2940,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['error'],['error']
Availability,"tcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$ada",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:1475,ERROR,ERROR,1475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['ERROR'],['ERROR']
Availability,"tchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); cromwell_1 | at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); cromwell_1 | at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. Docker Script (generated by cromwell):; ```bash; #!/bin/bash. cd /cromwell_root; tmpDir=$(mkdir -p ""/cromwell_root/tmp.1de26137"" && echo ""/cromwell_root/tmp.1de26137""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); out5d4c4459=""${tmpDir}/out.$$"" err5d4c4459=""${tmpDir}/err.$$""; mkfifo ""$out5d4c4459"" ""$err5d4c4459""; trap 'rm ""$out5d4c4459"" ""$err5d4c4459""' EXIT; tee '/cromwell_root/stdout' < ""$out5d4c4459"" &; tee '/cromwell_root/stderr' < ""$err5d4c4459"" >&2 &; (; cd /cromwell_root. /app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq; ) > ""$out5d4c4459"" 2> ""$err5d4c4459""; echo $? > /cromwell_root/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print0 | xargs -0 -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9a5013be5b75be907a1e45a835412b84. # create the glob co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:4088,echo,echo,4088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['echo'],['echo']
Availability,"te 2 minutes.; [2018-10-25 21:17:13,95] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Parsing workflow as WDL draft-2; [2018-10-25 21:17:14,52] [info] MaterializeWorkflowDescriptorActor [e22c6324]: Call-to-Backend assignments: test_opt_array.t1 -> Local; [2018-10-25 21:17:20,89] [info] WorkflowExecutionActor-e22c6324-5aec-4694-8750-f62160e2ca81 [e22c6324]: Starting test_opt_array.t1 (5 shards); [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: echo 0 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: echo 4 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: echo 1 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:3:1]: echo 3 > out.txt; [2018-10-25 21:17:22,98] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: echo 2 > out.txt; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:2:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-2/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:1:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-1/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:0:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4694-8750-f62160e2ca81/call-t1/shard-0/execution/script; [2018-10-25 21:17:23,01] [info] BackgroundConfigAsyncJobExecutionActor [e22c6324test_opt_array.t1:4:1]: executing: /bin/bash /users/leepc12/code/test_wdl/cromwell-executions/test_opt_array/e22c6324-5aec-4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:13017,echo,echo,13017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,te)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1864,recover,recoverWith,1864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['recover'],['recoverWith']
Availability,"teValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the foll",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1622,error,error,1622,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"tead, Cromwell seems to think it is defined, and it has a length. This can cause all sorts of issues, such as breaking downstream tasks that are only supposed to run if the optional upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,Some( ))]""`; * This error occurs even if echo_array takes in non-optional Array[Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:996,error,errorcode,996,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,2,"['error', 'redundant']","['errorcode', 'redundant']"
Availability,"tem and copy it to another S3 bucket. This task itself does not generate any files locally but at the end of its execution I expect a new S3 object to appear in the destination bucket. The inputs and outputs are marshalled using a struct with a ""File"" variable for the S3 path of the objects. After the task executes Cromwell throws an error scala.MatchError - it recognises that the output is a `cromwell.filesystems.s3.S3Path` but dosn't appear to know what to do with it. Im wondering if it is because the ""file"" is created outside of the workflow workspace?. My work around is to use the `String` type in place of `File` type for ""path"" and cast back to `File` later in the workflow but this feels inelegant. Input:. {; ""main.bucket"" : ""my_bucket_2"" ,; ""main.file_list"":[; { ""path"": ""s3://my_bucket_1/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""}; ]; }. Expected Output:. [; {; ""id"": ""123""; ""path"": ""s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html""; }; ]. Code:. struct file_thing {; String? id; File path; }. task copy_file_list{; input{; String bucket; Array[file_thing] file_list; }. command <<<; copy_files \; --bucket ~{bucket} \; --json_in ~{write_json(file_list)} \; --json_out outputs.json; >>>. output {; Array[file_thing] outputs = read_json(""outputs.json""); }. runtime {; docker: ""my_copy_tool:latest""; }; }. Error:. WorkflowManagerActor: Workflow e7a60e4b-8dc4-471b-aec6-b8cc1481f889 failed (during ExecutingWorkflowState): ; scala.MatchError: s3://my_bucket_2/a2c193f0-8f08-11ec-8c2a-0a58a9feac02/bob.html (of class ; cromwell.filesystems.s3.S3Path); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath(SharedFileSystem.scala:239); 	at cromwell.backend.sfs.SharedFileSystem.hostAbsoluteFilePath$(SharedFileSystem.scala:237); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.hostAbsoluteFilePath(SharedFileSystemJobCachingA ctorHelper.scala:13); at cromwell.backend.sfs.SharedFileSystem.mapJobWomFile(SharedFileSystem.scala:251); 	...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6716:1543,Error,Error,1543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6716,1,['Error'],['Error']
Availability,"tem-akka.dispatchers.engine-dispatcher-440 ERROR - WorkflowManagerActor Workflow 28605745-a8d2-43c4-ab02-70e5c5c032fe failed (during ExecutingWorkflowStat; e): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:0:1 failed. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:1581,Error,Error,1581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Error'],['Error']
Availability,"tement(JdbcBackend.scala:491); at slick.jdbc.JdbcActionComponent$ReturningInsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:660); at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$SingleInsertAction.run(JdbcActionComponent.scala:517); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:25); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); at java.base/java.lang.Thread.run(Thread.java:1589); Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; table: JOB_KEY_VALUE_ENTRY column: STORE_VALUE; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.Table.enforceTypeLimits(Unknown Source); at org.hsqldb.Table.generateAndCheckData(Unknown Source); at org.hsqldb.Table.insertSingleRow(Unknown Source); at org.hsqldb.StatementDML.insertRowSet(Unknown Source); at org.hsqldb.StatementInsert.getResult(Unknown Source); at org.hsqldb.StatementDMQL.execute(Unknown Source); at org.hsqldb.Session.executeCompiledStatement(Unknown Source); at org.hsqldb.Session.execute(Unknown Source); ... 17 common frames omitted; Caused by: org.hsqldb.HsqlException: data exception: string data, right truncation; at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.error.Error.error(Unknown Source); at org.hsqldb.types.CharacterType.convertToTypeLimits(Unknown Source); ... 25 common frames omitted; [2022-11-10 13:45:54,45] [info] BackgroundConfigAsyncJobExecutionActor [5c89d3e8PairedEndSingleSampleWorkflow.BaseRecalibrator:15:1]: Status change from - to WaitingForReturnCode; [2022-11-10 13:45:54,45] [info]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6947:3922,error,error,3922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6947,1,['error'],['error']
Availability,"tens/debug_m2_wdl/cromwell-executions/Mutect2_Multi/0239d302-1154-4c39-9870-55574d000765/call-orientationBiasFilteredOutputList/execution/ob_filtered.list""; },; ""id"": ""0239d302-1154-4c39-9870-55574d000765""; }; [2017-03-20 15:30:35,34] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/debug_m2_wdl/test_m2_wdl.metadata; [2017-03-20 15:30:35,46] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-21-0-unknown-operation#1356917576]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-17-0-unknown-operation#-291022515]] terminated abruptly; [2017-03-20 15:30:35,47] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-15-0-unknown-operation#-925665144]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-3-0-unknown-operation#-2130885356]] terminated abruptly; [2017-03-20 15:30:35,48] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-4-0-unknown-operation#-1268876796]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-16-0-unknown-operation#-371454906]] terminated abruptly; [2017-03-20 15:30:35,49] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-unknown-operation#-248376973]] terminated abruptly; [2017-03-20 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:3823,error,error,3823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,"termittently fails and succeeds, I'm not sure why it sometimes does and sometimes doesn't work. When it fails, it's because the EC2 instance fails to initialize. The most promising section I can find from the EC2 logs shows the following:; ```; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 49: ordinal not in range(128); Logged from file util.py, line 476; Traceback (most recent call last):; File ""/usr/lib64/python2.7/logging/__init__.py"", line 891, in emit; stream.write(fs % msg.encode(""UTF-8"")); UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 71: ordinal not in range(128); Logged from file util.py, line 476; [ 72.975338] EXT4-fs (dm-3): mounted filesystem with ordered data mode. Opts: (null); Error occurred during build: Command 04InstallECSAdditions failed; ```; The line `EXT4-fs (dm-3): mounted filesystem with ordered data mode` is repeated about 100 times in the real logs (below), I've just abridged it here for clarity. Anyway, the main thing this tells us that it's failing during step 04 of the EC2 startup script, which does the following:; ```yaml; 04InstallECSAdditions:; command:; Fn::If:; - UseCromwell; - !Join ["" "", [""sh"", ""/opt/ecs-additions/ecs-additions-cromwell.sh""]]; - echo ""OK""; env:; PATH: ""/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin""; ```. My best guess as to what is happening, is [this blog post](http://www.codeandcompost.com/post/cfn,-utf8-and-two-days-i%E2%80%99ll-never-get-back), which suggests:; > Apparently cfn-init has a limit on the amount of output it can process from a command, and I was pushing that limit. > I suspect the reason for the UTF8 error is that the output was truncated between two bytes or something, and when the parser underneath cfn-init tried to parse it, it encountered what appeared to be an invalid UTF8 character. . So per",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674:1054,Error,Error,1054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674,1,['Error'],['Error']
Availability,"the Cromwell metadata as described by [the paragraph about metadata in the Cromwell docs](https://cromwell.readthedocs.io/en/stable/SubWorkflows/). When executing a workflow written in WDL and executed with Cromwell (the scientific workflow engine) one can extract metadata out of the Cromwell database. Within this metadata, the following ""executionEvents"" are available for each ""workflow.task"" in the ""calls"" objects. Pending; Requesting ExecutionToken; WaitingFor ValueStore; PreparingJob; CallCache Reading; RunningJob; Updating CallCache; Updating JobStore. From the documentation:; [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. The main purpose of the [Job Store table](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/workflowSubworkflowAndJobStores/#job-store-job_store_entry) is to support resuming execution of a workflow when Cromwell is restarted by recovering the outputs of completed jobs. I couldn't find a description of the Execution Token nor of the [Value Store](https://cromwell.readthedocs.io/en/stable/developers/bitesize/workflowExecution/jobKeyValueStore/) in [the docs](https://cromwell.readthedocs.io/en/develop/developers/Arch). My questions are the following:. What is the engine waiting on when a task/job is ""Pending""?; Is Requesting an Execution Token something that happens for every task because of security reasons, or does it have to do with the allowed capacity for Cromwell? What types of token are we talking about?; What happens during Value Store, where are which values stored and why are we waiting on it rather than doing it?; is this, for example, collecting default environment variables that should be set before running the workflow; or; is it collecting the values of variables that are used in the workflow, provided with the `inputs.json`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5579:1638,recover,recovering,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5579,1,['recover'],['recovering']
Availability,"the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ```; Our regexes did not allow for the `\n` in the Google errors. I believe this bug came about when copy-pasting to create the test cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:1037,down,download,1037,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,4,"['down', 'error']","['download', 'errors']"
Availability,"the time being in favor of higher-priority work. Building on #140, confirm other JES parameters are making it to the VM as expected. . I have some WIP [here](https://github.com/broadinstitute/centaur/commit/d7f6a3aa26ea6abc37ce26c8399a39b30c9e9322). In developing this it became apparent that both GCE VM and JES metadata can be introspected, and in most cases this is boilerplate that would be common to all the `check_a_thing` tasks. This should be refactored out into a common script so it's not copy/pasted into each task. Probably the best way to do this is to have the inputs to each task include a common `String script` which would dump GCE VM and JES metadata to files. The `check_a_thing` tasks would then do something like:. ```; task check_a_thing {; String script; String specified_value_of_attr. command <<<; $(script) # writes jes_metadata.yaml and gce_metadata.yaml; grep -Po attr_pattern jes_metadata.yaml; >>>. output {; File jes_metadata = ""jes_metadata.yaml""; File gce_metadata = ""gce_metadata.yaml""; String attr_value = read_string(stdout()); }; ; runtime {; docker: ""google/cloud-sdk""; attr: ""${specified_value_of_attr}""; }; ```. This should confirm that Cromwell's intended attribute values are communicated into JES metadata correctly. It can also be useful to make sure that intent makes it to the GCE metadata correctly (this was particularly key for preemptible). And for attributes like memory or cpu it may also be useful to check that the machine is actually provisioned as expected (e.g. checking /proc/cpu or /proc/memory). ---. @ruchim commented on [Mon Jan 30 2017](https://github.com/broadinstitute/centaur/issues/144#issuecomment-276111326). Pinging @kcibul for prioritization -- Miguel brought this up at standup today that it's a very long list of JES attribute values that can be asserted against their expected values. If you believe any specific attribute (other than preemptible) would be an important addition to the tests, those tickets can be prioritized.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2892:1796,Ping,Pinging,1796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2892,1,['Ping'],['Pinging']
Availability,"thin China. There is no known Alibaba Cloud provided CDN or cache for docker images on BCS. To significantly speed up docker pulls users are able to upload docker images to their own private docker registry hosted within one of their OSS buckets. This uses a plugin contributed to the docker codebase that stores and retrieves docker images via an OSS client. Currently the BCS backend allows users to specify the private OSS registry within the `docker` runtime attribute. For portability, the `docker` runtime attribute should only specify the image, and a separate `dockerRegistry` runtime attribute should optionally specify a private OSS registry. Ideally there should be a way for a user to cache docker images on their own private OSS registry while still using contributed by others WDLs. One particular issue for call caching may be that the docker image hashes are probably registry specific. Cromwell's call caching code requires WDL to specify a hash that may only be available on docker hub, and may not be available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518:1071,avail,available,1071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518,2,['avail'],['available']
Availability,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3022:36,error,errors,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022,3,['error'],"['error', 'errors']"
Availability,"ticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1380,down,down,1380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['down'],['down']
Availability,time.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at cromwell.core.actor.RobustClientHelperSpec.withFixture(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest$(FlatSpecLike.scala:1674) at cromwell.core.actor.RobustClientHelperSpec.runTest(RobustClientHelperSpec.scala:14) at org.scalatest.FlatSpecLike.$anonfun$runTests$1(FlatSpecLike.scala:1750) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:373) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:410) at scala.collection.immutable.List.foreach(List.scala:389) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FlatSpecLike.runTests(FlatSpecLike.scala:1750) at org.scalatest.FlatSpecLike.runTests$(FlatSpecLike.scala:1749) at cromwell.core.actor.RobustClientHelperSpec.runTests(RobustClientHelperSpec.scala:14) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4351:1917,Robust,RobustClientHelperSpec,1917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4351,1,['Robust'],['RobustClientHelperSpec']
Availability,"timeout=timeout_property).read()\n File \""/usr/lib/python2.7/urllib2.py\"", line 401, in open\n response = self._open(req, data)\n File \""/usr/lib/python2.7/urllib2.py\"", line 419, in _open\n '_open', req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 379, in _call_chain\n result = func(*args)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1211, in http_open\n return self.do_open(httplib.HTTPConnection, req)\n File \""/usr/lib/python2.7/urllib2.py\"", line 1184, in do_open\n r = h.getresponse(buffering=True)\n File \""/usr/lib/python2.7/httplib.py\"", line 1072, in getresponse\n response.begin()\n File \""/usr/lib/python2.7/httplib.py\"", line 408, in begin\n version, status, reason = self._read_status()\n File \""/usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; java.lang.Exception: Task m2.Mutect2.M2:1:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam -> /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam (cp failed: gsutil -q -m cp gs://5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam /mnt/local-disk/5aa919de-0aa0-43ec-9ec3-288481102b6d/tcga/STAD/DNA/WXS/BI/ILLUMINA/C440.TCGA-HU-A4H3-10A-01D-A25E-08.2.bam, command failed: Traceback (most recent call last):\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 75, in <module>\n main()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py\"", line 22, in main\n project, account = bootstrapping.GetActiveProjectAndAccount()\n File \""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\"", l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:9151,error,error,9151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['error'],['error']
Availability,"ting a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1556:1100,error,error,1100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556,1,['error'],['error']
Availability,"tionActor.aroundReceive(StandardInitializationActor.scala:42); cromwell_1 | 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); cromwell_1 | 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); cromwell_1 | 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); cromwell_1 | 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); cromwell_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | Caused by: wdl4s.parser.WdlParser$SyntaxError: ERROR: Variable docker_user does not reference any declaration in the task (line 31, col 25):; cromwell_1 | ; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | ^; cromwell_1 | ; cromwell_1 | Task defined here (line 16, col 6):; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ^; cromwell_1 | ; cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19$$anonfun$apply$22.apply(WdlNamespace.scala:405); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19$$anonfun$apply$22.apply(WdlNamespace.scala:403); cromwell_1 | 	at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:683); cromwell_1 | 	at scala.collection.immutable.List.foreach(List.scala:381); cromwell_1 | 	at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:682); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19.apply(WdlNamespace.scala:403); cromwell_1 | 	at wdl4s.WdlNamespace$$anonfun$42$$anonfun$apply$19.apply(WdlNamespace.scala:402); cromwell_1 | 	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241); cromwell_1 | 	at scala.collection.Trave",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:5847,ERROR,ERROR,5847,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,1,['ERROR'],['ERROR']
Availability,tionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutio,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3584,recover,recoverAsync,3584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Availability,"tionActor.startMetadataKeyValues$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.startMetadataKeyValues(AwsBatchAsyncBackendJobExecutionActor.scala:362); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); ... 5 more; ```. Basically, it seems that the AWS parser for the disk specification doesn't understand specs in the form `'local-disk 100 HDD'`. This needs to be fixed, since the Broad pipelines won't run w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:2651,robust,robustExecuteOrRecover,2651,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['robust'],['robustExecuteOrRecover']
Availability,"tions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1746,error,error,1746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['error'],['error']
Availability,"titute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; Trying to set up a genomics workflow with AWS backend; References : https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/; https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/ . <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; AWS ; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs; ; ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.back",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:1431,Error,Error,1431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['Error'],['Error']
Availability,"titute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4456,failure,failures,4456,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failures']
Availability,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1545:284,down,down,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545,4,"['down', 'error']","['down', 'error']"
Availability,"tober 29, 2018) with java10:; ```; $ java -version; openjdk version ""10.0.1"" 2018-04-17; OpenJDK Runtime Environment (build 10.0.1+10-Ubuntu-3ubuntu1); OpenJDK 64-Bit Server VM (build 10.0.1+10-Ubuntu-3ubuntu1, mixed mode); ```. For both the working and failing CWL files, I use the following input:; ```; {; ""paired_parameters"": {; ""itemA"": ""one"",; ""itemB"": ""two""; }; }; ```; Below is the **successful** CWL file. In particular, note the `inputs.type.name` which is set to a garbage value.; ```; {; ""cwlVersion"": ""v1.0"",; ""class"": ""CommandLineTool"",; ""inputs"": [; {; ""id"": ""paired_parameters"",; ""type"": {; ""type"": ""record"",; ""name"": ""SOME JUNK VALUE"",; ""fields"": [; {; ""name"": ""itemA"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-A="",; ""separate"": false; }; },; {; ""name"": ""itemB"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-B="",; ""separate"": false; }; }; ]; }; }; ],; ""outputs"": {; ""example_out"": {; ""type"": ""stdout""; }; },; ""stdout"": ""output.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error] WorkflowManagerActor Workflow de935a6c-85a6-476f-845f-cf5360bbef03 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error when parsing file /tmp/cwl_temp_dir_9897655526044348367/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:1298,echo,echo,1298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['echo'],['echo']
Availability,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:4525,down,down,4525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['down'],['down']
Availability,tor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.executeOrRecover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:46); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:62); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:3451,recover,recoverAsync,3451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,2,['recover'],['recoverAsync']
Availability,tor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:264); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeOrRecover$2.apply(StandardAsyncExecutionActor.scala:258); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:258); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	... 4 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:12084,robust,robustExecuteOrRecover,12084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['robust'],['robustExecuteOrRecover']
Availability,"tp://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; Trying to set up a genomics workflow with AWS backend; References : https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/; https://cromwell.readthedocs.io/en/stable/tutorials/AwsBatch101/ . <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; AWS ; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs; ; ![AWS-Batch](https://user-images.githubusercontent.com/25282254/153039990-0d0b2c96-a33b-454f-9617-aee83137337a.PNG); [Cromwell-Error.docx](https://github.com/broadinstitute/cromwell/files/8026009/Cromwell-Error.docx); ; <!-- Paste/Attach your workflow if possible: -->; java -Dconfig.file=aws-cromwell-batch.conf -jar cromwell-75.jar run hello.wdl -i hello.inputs. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; include required(classpath(""application"")). aws {. application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3.auth = ""default""; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; docker {; hash-lookup {; enabled = false; # How should docker hashes be looked up. Possible values are ""local"" and ""remote""; # ""local"": Lookup hashes on the local docker daemon using the cli; # ""remote"": Lookup hashes on docker hub and gcr; method = ""remote""; }; }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671:1509,Error,Error,1509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671,1,['Error'],['Error']
Availability,"tps://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-substring-string-string) should only work on requireds, and indeed, other implementations of WDL do not have this restriction. As a result, it seems I have to make Cromwell-specific workarounds involving select_first() which shouldn't really be necessary. ## Example using basename(); This code should work just fine on Cromwell, but doesn't. ```; task reference_prepare {; 	input {; 		# You need to define either this...; 		File? reference_fa_file. 		# Or both of these.; 		File? reference_zipped_directory; 		String? reference_fa_filename_in_zipped_directory; 	}. 	# get the basename of the reference; 	String? basename_reference = basename(reference_zipped_directory). 	command <<<; 		set -eux -o pipefail. 		if [[ ! ""~{reference_zipped_directory}"" = """" ]]; 		then; 			cp ~{reference_zipped_directory} .; 			unzip ~{basename_reference}; 		fi. 		# do other things here; 	>>>; }. workflow foo {; 	call reference_prepare; }; ```; ### `womtool validate` output; Results in a fatal error.; > Failed to process task definition 'reference_prepare' (reason 1 of 1): Failed to process expression 'basename(reference_zipped_directory)' (reason 1 of 1): Invalid parameter 'IdentifierLookup(reference_zipped_directory)'. Expected 'File' but got 'File?'. ### `miniwdl check` output (for comparison); Results in some warnings.; > refprep.wdl; > workflow foo; > call reference_prepare; > task reference_prepare; > (Ln 24, Col 3) UnusedDeclaration, nothing references File? reference_fa_file; > (Ln 28, Col 3) UnusedDeclaration, nothing references String? reference_fa_filename_in_zipped_directory; > (Ln 32, Col 2) UnnecessaryQuantifier, unnecessary optional quantifier (?) for non-input String? basename_reference. (The actual workflow also runs successfully on miniwdl.). ### Workaround; Because all of our variables are optional, there isn't a required variable we can fall back on. But because we only use basename_reference if the variable ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840:1319,error,error,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840,1,['error'],['error']
Availability,"tput.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error] WorkflowManagerActor Workflow de935a6c-85a6-476f-845f-cf5360bbef03 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error when parsing file /tmp/cwl_temp_dir_9897655526044348367/cwl_temp_file_de935a6c-85a6-476f-845f-cf5360bbef03.cwl; DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: String; ``` ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND O",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:2253,error,error,2253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['error'],['error']
Availability,transient lack of Google Cloud resources causes permanent workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:67,failure,failure,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['failure'],['failure']
Availability,tre/home/conradL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:200); 	at cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1.applyOrElse(SharedFileSystem.scala:199); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at scala.util.Failure.recoverWith(Try.scala:203); 	at cromwell.backend.sfs.SharedFileSystem$class.localizeInputs(SharedFileSystem.scala:199); 	at cromwell.backend.sfs.SharedFileSystemJobCachingActorHelper$$anon$1.localizeInputs(SharedFileSystemJobCachingActorHelper.scala:40); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLinePreProcessor$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.wdl.Command$.instantiate(Command.scala:27); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.instantiatedCommand(StandardAsyncExecutionActor.scala:83); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedComma,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:2378,Failure,Failure,2378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,1,['Failure'],['Failure']
Availability,tributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1601,robust,robustBuildAttributes,1601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['robust'],['robustBuildAttributes']
Availability,"tring(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1472,error,error,1472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['error'],['error']
Availability,"try/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr"",; ""callRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom"",; ""attempt"": 1,; ""executionEvents"": [; {; ""description"": ""CallCacheReading"",; ""startTime"": ""2020-08-29T00:00:44.174Z"",; ""endTime"": ""2020-08-29T00:00:44.237Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.044Z"",; ""description"": ""Pending"",; ""endTime"": ""2020-08-29T00:00:42.064Z""; },; {; ""description"": ""RunningJob"",; ""startTime"": ""2020-08-29T00:00:44.237Z"",; ""endTime"": ""2020-08-29T00:04:05.347Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.531Z"",; ""endTime"": ""2020-08-29T00:00:44.174Z"",; ""description"": ""PreparingJob""; },; {; ""startTime"": ""2020-08-29T00:00:42.064Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2020-08-29T00:00:42.516Z""; },; {; ""endTime"": ""2020-08-29T00:00:42.531Z"",; ""description"": ""WaitingForValueStore"",; ""startTime"": ""2020-08-29T00:00:42.516Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/fail_oom.log""; },; ""start"": ""2020-08-29T00:00:42.022Z""; }; ]; },; ""outputs"": {},; ""workflowRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/"",; ""actualWorkflowLanguage"": ""WDL"",; ""id"": ""87492280-9828-4afa-b53e-bec675103c42"",; ""inputs"": {},; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42"",; ""caper-backend"": ""gcp"",; ""caper-user"": ""leepc12""; },; ""submission"": ""2020-08-29T00:00:38.568Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""message"": ""Workflow failed""; }; ],; ""end"": ""2020-08-29T00:04:06.071Z"",; ""start"": ""2020-08-29T00:00:38.789Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:7166,failure,failures,7166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['failure'],['failures']
Availability,"ts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:85761,failure,failure-mode,85761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['failure'],['failure-mode']
Availability,"tsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:1699,error,errors,1699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['errors']
Availability,"tting down.; 2016-08-03 15:20:03,592 cromwell-system-akka.dispatchers.engine-dispatcher-86 INFO - WorkflowExecutionActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: WorkflowExecutionActor [UUID(eaeaa32d)] done. Shutting down.; 2016-08-03 15:20:03,593 cromwell-system-akka.dispatchers.engine-dispatcher-85 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from ExecutingWorkflowState to FinalizingWorkflowState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationPendingState to FinalizationInProgressState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-84 INFO - WorkflowFinalizationActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: State is transitioning from FinalizationInProgressState to FinalizationSucceededState.; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-138 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transitioning from FinalizingWorkflowState to WorkflowFailedState; 2016-08-03 15:20:03,594 cromwell-system-akka.dispatchers.engine-dispatcher-138 INFO - WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 [UUID(eaeaa32d)]: transition from FinalizingWorkflowState to WorkflowFailedState. Shutting down.; 2016-08-03 15:20:03,595 cromwell-system-akka.dispatchers.engine-dispatcher-68 ERROR - WorkflowManagerActor Workflow eaeaa32d-057d-4f2e-b986-6e8b738dd512 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Error reading gs://miguel-cromwell-dev/DeliciousFileSpam/eaeaa32d-057d-4f2e-b986-6e8b738dd512/call-StringSpam/shard-237/file.txt at position 0; 2016-08-03 15:20:03,595 cromwell-system-akka.dispatchers.engine-dispatcher-133 INFO - WorkflowManagerActor WorkflowActor-eaeaa32d-057d-4f2e-b986-6e8b738dd512 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2887:2378,down,down,2378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2887,3,"['ERROR', 'Error', 'down']","['ERROR', 'Error', 'down']"
Availability,"ttps://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at su",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:1309,ERROR,ERROR,1309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['ERROR'],['ERROR']
Availability,"tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1439,error,error,1439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,['error'],['error']
Availability,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:6007,down,down,6007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['down'],['down']
Availability,type = {slides} was missing a comma at the end of the line which causes an error in the latex compilation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6503:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6503,1,['error'],['error']
Availability,"t}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $SINGULARITY_CACHEDIR/pull/$docker_image.sif ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }. }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8796,echo,echo,8796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,4,"['alive', 'echo', 'error']","['alive', 'echo', 'error']"
Availability,"u:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""50"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File cat_txt = ""merged.tx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:2424,Down,DownstreamConsumer,2424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['Down'],['DownstreamConsumer']
Availability,"ual bug or in the category of ""you shouldn't expect that to work""... given:; ```; /mnt/lustre/home/conradL/foo; ├── bar; ├── baz -> bar; └── quux -> /mnt/lustre/home/conradL/foo/bar; ```; The following WDL works fine when the input specifies `/mnt/lustre/home/conradL/foo/quux`, i.e. a symbolic link to absolute path, but not when it specifies `/mnt/lustre/home/conradL/foo/baz`, i.e. a link to a relative path:; ```; workflow symLinkTest {; call referenceTheLink { input:; #f1=""/mnt/lustre/home/conradL/foo/quux""; f1=""/mnt/lustre/home/conradL/foo/baz""; }; }. task referenceTheLink {; File f1; command {}; }; ```. In the second case it dies with:; ```; [2017-02-03 15:49:01,37] [error] WorkflowManagerActor Workflow 6cf6c785-dc48-4409-bbd1-6f1411211f42 failed (during ExecutingWorkflowState): Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; cromwell.backend.sfs.SharedFileSystem$$anonfun$localizeInputs$1$$anon$1: Failures during localization:; Could not localize /mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz:; 	/mnt/lustre/home/conradL/bar doesn't exists; 	File not found /mnt/lustre/home/conradL/cromwell-executions/symLinkTest/6cf6c785-dc48-4409-bbd1-6f1411211f42/call-referenceTheLink/inputs/mnt/lustre/home/conradL/foo/baz -> /mnt/lustre/home/conradL/bar; 	File not found /mnt/lustre/home/conradL/bar; 	File not ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1950:705,error,error,705,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1950,2,"['Failure', 'error']","['Failures', 'error']"
Availability,"ually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = Up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:1352,echo,echo,1352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['echo'],['echo']
Availability,"uatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:149: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; [error] ^; [error] two errors found; ```. NB: . - ~~No tests for this yet.~~ There are tests.; - Resolved: I'll need to clarify `sepFunctionEvalutor` in `BiscayneTypeEvaluators.scala` to only accept an Array of Strings. I presume I just need to change the validateParamType block to be:. ```scala; validateParamType(a.arg2, linkedValues, WomArrayType(WomAnyType)) flatMap {; case WomArrayType(WomStringType) => WomStringType.validNel; case other => s""Cannot invoke 'sep' on type '${other.stableName}'. Expected an array"".invalidNel; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:3418,error,error,3418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,4,['error'],"['error', 'errors']"
Availability,"ue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:161:72: value evaluateValue is not a member of wdl.model.draft3.elements.ExpressionElement; [error] processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOpt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:2036,Error,ErrorOr,2036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"ue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:12487,error,error,12487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,1,['error'],['error']
Availability,"ule00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-91/cacheCopy/SR00c.HG03727.txt.gz.tbi; 1608597646131,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/write_lines_1aa3abac483dac7d55fbf1572054f418.tmp; 1608597648902,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-58/cacheCopy/SR00c.HG02367.txt.gz; 1608597650698,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-137/cacheCopy/SR00c.NA19746.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:195820,down,download,195820,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"umask overriden when creating directories, leads to access denied errors with linux access control lists",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333:66,error,errors,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333,1,['error'],['errors']
Availability,"un$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwell/files/1635810/SomaticPairedSingleSampleWf.txt). Sub workflow - [SplitLargeRG.txt](https://github.com/broadinstitute/cromwell/files/1635814/SplitLargeRG.txt). Dependencies zip - [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1635815/SomaticPairedSingleSampleWfDependencies.zip). Let me know if theres any more information that might be useful.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:4349,error,error,4349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['error'],['error']
Availability,"unnerActor: Submitting workflow; [2017-12-01 20:01:02,94] [info] Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 submitted.; [2017-12-01 20:01:02,94] [info] SingleWorkflowRunnerActor: Workflow submitted 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Successfully started WorkflowActor-132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-01 20:01:03,27] [info] MaterializeWorkflowDescriptorActor [132d7527]: Call-to-Backend assignments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-01 20:01:04,82] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: echo test1 > test1.txt; echo test2 > test2.txt; [2017-12-01 20:01:04,86] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: executing: /bin/bash /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/script; [2017-12-01 20:01:04,91] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,fal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:2534,echo,echo,2534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['echo'],['echo']
Availability,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:1814,avail,avail,1814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['avail'],['avail']
Availability,"untime attribute keys: time; [2017-11-18 19:30:05,52] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: python $(which encode_filter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-bowtie2/shard-1/glob-3bcbe4e7489c90f75e0523ac6f3a9385/ENCFF463QCX.trim.merged.R1.bam \; \; --multimapping 4 \; \; \; \; --nth 2; [2017-11-18 19:30:15,43] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:3754,error,error,3754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['error'],['error']
Availability,"updated Single sample wdl; added haplotypcaller, jointdiscovery, and data preprocessing wdls. All wdls using gatk4; increased travis heartbeat to 180, ; increased max time for travis to 3 hours",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3212:133,heartbeat,heartbeat,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3212,1,['heartbeat'],['heartbeat']
Availability,"ure$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); at scala.util.Try$.apply(Try.scala:192); at scala.util.Success.map(Try.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2552,Failure,Failure,2552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Failure'],['Failure']
Availability,use MySQL Server: 5.5.56-MariaDB error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3346:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3346,1,['error'],['error']
Availability,use MySQL5.5 error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382,1,['error'],['error']
Availability,"usr/lib/python2.7/httplib.py\"", line 366, in _read_status\n line = self.fp.readline()\n File \""/usr/lib/python2.7/socket.py\"", line 447, in readline\n data = self._sock.recv(self._rbufsize)\nsocket.timeout: timed out\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$.StandardException(JesAsyncBackendJobExecutionActor.scala:63); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:411); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:67); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:666); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$1.applyOrElse(StandardAsyncExecutionActor.scala:663); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:16394,recover,recoverWith,16394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,1,['recover'],['recoverWith']
Availability,"ut_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-secon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:4200,Error,Error,4200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['Error'],['Error']
Availability,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:5301,error,error,5301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,5,"['echo', 'error']","['echo', 'error']"
Availability,"utect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O tumor-pileups.table. if [[ ! -z """" ]]; then; gatk --java-options ""-Xmx3000m"" GetPileupSummaries -R gs://nicholas-b-test/references/genome.fa -I --interval-set-rule INTERSECTION -L gs://nicholas-b-test/Mutect2_multisample/4f039d1f-f981-4a6d-98b3-be9cd92d3e62/call-Mutect2/shard-439/Mutect2/d19d9b83-bda0-40b6-89e6-7f74d3f76988/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0013-scattered.interval_list \; -V -L -O normal-pileups.table; fi; fi; OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fbe6d000000, 4345298944, 0) failed; error='Not enough space' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 4345298944 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /home/nicholas/projects/genomics/ukbb/ch/analyses/1_9_2020/hs_err_pid24834.log; ```; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; GCS; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. #; # Description of inputs; # intervals: genomic intervals; # ref_fasta, ref_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:2123,error,error,2123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['error'],['error']
Availability,"utionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:4503,error,error,4503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['error'],['error']
Availability,"v		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:1876,MASK,MASK,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,5,"['MASK', 'mask']","['MASK', 'mask']"
Availability,v86 GCPBATCH errors when have multiple zones in the config,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7232:13,error,errors,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232,1,['error'],['errors']
Availability,"va.lang.Exception: Job 2de677d8-0842-4e17-ab26-288ffc3d8aaa failed for reason: unknown error: . {JobName: cromwell-job,JobId: 2de677d8-0842-4e17-ab26-288ffc3d8aaa,JobQueue: arn:aws:batch:us-east-1:369228243869:job-queue/mcovarr-queue-nouveau,Status: FAILED,StatusReason: **Container.image contains invalid characters.**,CreatedAt: 1488362254138,DependsOn: [],JobDefinition: arn:aws:batch:us-east-1:369228243869:job-definition/cromwell-job-definition:125,Parameters: {},Container: {**Image: library/python@sha256:d23845e4757f13266b42877c25b845e455127b85ec12e5d551bec5d8162e7cd4**,Vcpus: 1,Memory: 1907,Command: [/bin/sh, -c, /bin/bash /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/script > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stdout 2> /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stderr < /dev/null || echo -1 > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/rc],Volumes: [{Host: {SourcePath: /usr/share/iodir},Name: cromwell-volume}],Environment: [],MountPoints: [{ContainerPath: /usr/share/iodir,ReadOnly: false,SourceVolume: cromwell-volume}],Ulimits: [],}}. For this reason found in the ECS javadocs:. ```; Amazon ECS task definitions currently only support tags as image identifiers within a specified repository; (and not <code>sha256</code> digests); ```. Of course it would be preferable if these digests actually were supported by ECS, in which case we wouldn't need to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2044:1356,echo,echo,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2044,1,['echo'],['echo']
Availability,"vard ; > kcibul@broadinstitute.org ; > ; > ; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #3 Jan 10, 2018 08:58AM ; > Not sure if you need any additional opsids - let me know if you do. While I have not gathered specific statistics on the frequency of this happening - our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4348,failure,failures,4348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['failure'],['failures']
Availability,"variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); 		}; 	}; ```. One could argue that this is technically correct, since the equality check only runs if the variant_caller.errorcode is defined. And indeed, `womtool validate` does not see any issue with this. However, at runtime, I get this error:. `Failed to evaluate 'if_condition' (reason 1 of 1): Evaluating !((variant_call_after_earlyQC_filtering.errorcode[0] == pass)) failed: Sorry! Operation == is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value.`. I get this error whether or not the variant caller task actually ran, even though whether or not it ran should cause an issue, since it's under a defined() check. If the defined() check still is not enough like is the case for setting not_optional_error_code, then that should be caught before runtime. ## backends effected; The womtool validation bug affects at least Terra-womtool and local-womtool. Runtime error happened on Terra-Cromwell but would probably happen on every backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:2625,error,errorcode,2625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,5,['error'],"['error', 'errorcode']"
Availability,vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:7086,Error,ErrorOr,7086,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,1,['Error'],['ErrorOr']
Availability,"ved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-30 17:53:22,20] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-30 17:53:22,21] [info] Using noop to send events.; [2018-08-30 17:53:22,25] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-30 17:53:22,30] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Parsing workflow as WDL draft-2; [2018-08-30 17:53:23,48] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Call-to-Backend assignments: HelloWorld.WriteGreeting -> Local; [2018-08-30 17:53:24,95] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Starting HelloWorld.WriteGreeting; [2018-08-30 17:53:26,42] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: echo ""Hello World""; [2018-08-30 17:53:26,49] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: executing: /bin/bash /gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/script; [2018-08-30 17:53:31,09] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:3105,echo,echo,3105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['echo'],['echo']
Availability,"ver logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:2229,echo,echo,2229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['echo'],['echo']
Availability,"version: 44; We have cromwell running as a server and aws as backend.; Today, all jobs failed. Looks like ecs-agent-proxy container is not started when a job is submitted. So no input files or the script file can be downloaded from s3 to running ec2 instance. any idea? Thanks!; Example log:; /bin/bash: /cromwell_root/somepath/cromwell-execution/somejob/2d60df76-99a5-430b-9a9c-bcccb0aac336/call-dz/script: No such file or directory",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5538:216,down,downloaded,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5538,1,['down'],['downloaded']
Availability,"version: `""cromwell"": ""30-f58c191-SNAP""`. I'm trying to get metadata for some workflows and it seems that for workflows whose subworkflows were running when cromwell was restarted are unable to retrieve their metadata. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true` . returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; even if i try compressed payload or any other kind of trick I could think of. . If I try grabbing the metadata without expanding the subworkflows. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=false`. returns the metadata just fine almost instantly. If I try to get the metadata of the subworkflow(s) directly it works as well. These workflows also have interesting responses to `includeKeys` parameter. When trying to get only the key `calls` from the workflow that was timing out (in hopes to make it not time out by requesting less data) . `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=calls`. returns; ```; {; ""status"": ""error"",; ""message"": ""Received unexpected response while waiting for sub workflow metadata.""; }; ```; note `calls` is a key that normally doesn't return anything so normally you would expect to get. `/api/workflows/v1/905e2b4c-908d-4e93-a99c-ad20f6e4c41a/metadata?expandSubWorkflows=true&includeKey=calls`; ```; {}; ```; if you try to filter down the metadata to a key that does exist in the metadata like. `/api/workflows/v1/8e9802db-f846-4ea5-a72c-55f257e53abe/metadata?expandSubWorkflows=true&includeKey=call`. returns; ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```; for the workflow that was restarted mid run (like it did when we were asking for the full metadata); and returns the `calls` metadata successfully for the workflow that always returned the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3209:1154,error,error,1154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3209,2,"['down', 'error']","['down', 'error']"
Availability,"version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:4471,Down,Downloading,4471,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Down'],['Downloading']
Availability,"w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 30; done\"""",; ""endTime"": ""2018-08-14T16:14:33.113135Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.937007Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:17:05.013311Z""; },; {; ""startTime"": ""2018-08-14T16:16:45.309551Z"",; ""description"": ""Started pulling \""stedolan\/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09\"""",; ""endTime"": ""2018-08-14T16:16:56.518719Z""; },; {; ""startTime"": ""2018-08-14T16:13:25.620872Z"",; ""description"": ""Worker \""google-pipelines-worker-4247201c1820698c5c935fb230c4a278\"" assigned in \""us-central1-f\"""",; ""endTime"": ""2018-08-14T16:13:55.499871Z""; },; {; ""startTime"": ""2018-08-14T16:14:27.981711Z"",; ""description"": ""Stopped pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:28.018319Z""; },; {; ""startTime"": ""2018-08-14T16:14:34.534911Z"",; ""description"": ""Started running \""\/bin\/bash -c mkdir -p \/cromwell_root && chmod -R a+rwx \/cromwell_roo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:14160,echo,echo,14160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,wdl4s not erroring when a scatter variable is missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2699:10,error,erroring,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2699,1,['error'],['erroring']
Availability,wdl4s not erroring when an output variable is missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2700:10,error,erroring,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2700,1,['error'],['erroring']
Availability,wdltool doesn't throw error if input isn't defined,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,1,['error'],['error']
Availability,website: https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team appears to be down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6355:93,down,down,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6355,1,['down'],['down']
Availability,"well will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:494); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:2568,down,down,2568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['down'],['down']
Availability,"well-system-akka.dispatchers.engine-dispatcher-28 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2019-07-21 23:34:39,131 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - Running with 3 PAPI request workers; 2019-07-21 23:34:39,132 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,157 cromwell-system-akka.dispatchers.backend-dispatcher-37 INFO - PAPI request worker batch interval is 33333 milliseconds; 2019-07-21 23:34:39,233 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PAPI request worker batch interval is 33333 milliseconds; ```. but then it immediately starts printing these errors:; ```; 2019-07-21 23:34:40,010 cromwell-system-akka.actor.default-dispatcher-32 ERROR - Error searching for abort requests; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where (""WORKFLOW_STATE"" = cast('Aborting' as varchar(1677' at line 1; 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:120); 	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97); 	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:970); 	at com.mysql.cj.jdbc.ClientPreparedStatement.execute(ClientPreparedStatement.java:387); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:3765,error,error,3765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['error'],['error']
Availability,"well.jar:0.19]; 2016-04-11 22:41:00,528 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,543 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,556 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,617 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,841 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,912 cromwell-system-akka.actor.default-dispatcher-2666 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2676] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]. **java.lang.OutOfMemoryError: Java heap space**; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.lang.StringBuilder.toString(StringBuilder.java:407); at scala.StringContext.standardInterpolator(StringContext.scala:128); at scala.StringContext.s(StringContext.scala:95); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:885); at cromwell.engine.workflow.WorkflowActor$$anonfun$34.apply(WorkflowActor.scala:881); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:155); at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428); at scala.collection.immutable.HashMap$HashTrieMap.foreac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:9855,error,error,9855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['error'],['error']
Availability,womtool does not show errors when the file does not exist,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4665:22,error,errors,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4665,1,['error'],['errors']
Availability,"womtool passes WDL workflow, cromwell gives ""No coercion"" error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,1,['error'],['error']
Availability,"womtool validate handles optional arrays inconsistently, leading to technically-impossible runtime errors",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:99,error,errors,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errors']
Availability,"work. Setting the `TMPDIR` environment variable to a long path will cause an error in Python `mulitprocessing` library. ```; Process SyncManager-1: ; Traceback (most recent call last):; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 258, in _bootstrap; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.process"", line 114, in run; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 550, in _run_server; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.managers"", line 162, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 132, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/multiprocessing.connection"", line 256, in __init__; File ""/home/cdompierre/gdc-client/bin/build/gdc-client/out00-PYZ.pyz/socket"", line 224, in meth; error: AF_UNIX path too long; ```. The Python `mulitprocessing` library appears to create sockets in `$TMPDIR`. If the `$TMPDIR` path is too long then the path to the socket extends past the length limits for socket paths. This can be reproduced by running the following command with `tmp_dbg` set to 80 characters long. 79 characters works ok. ```shell; docker run -it --rm docker.io/broadinstitute/gdc_downloader:1.0 bash -c '; # 1 2 3 4 5 6 7 8; tmp_dbg=/234567890123456789012345678901234567890123456789012345678901234567890123456789; tmp_dbg=/2345678901234567890123456789012345678901234567890123456789012345678901234567890; tmpDir=$(; set -e; tmpDir=""$(mkdir -p ""${tmp_dbg}"" && echo ""${tmp_dbg}"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir"". python /opt/src/gdc_downloader.py 6ca4c640-758d-455d-ba5b-b965069a39b4/nationwidechildrens.org_biospecimen.TCGA-4C-A93U.xml; '; ```. A/C:; - A centaur test that checks that generated `",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3647:1151,error,error,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3647,1,['error'],['error']
Availability,"workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitializationActor-7218c3a1-5155-4921-9adb-d96c52c32200] State is now terminal. Shutting down.; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.544] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.544] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$mb] $$mb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$mb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb] $$mb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.546] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user/$$mb/WorkflowInitial",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:3271,down,down,3271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,write date of failure to BigQuery,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4170:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4170,1,['failure'],['failure']
Availability,wrong cyclic dependency error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176,1,['error'],['error']
Availability,ws `java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor` exception when it tries to recover a running job. Stacktrace:; ```; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(4057b0c6)generate_10gb_file.generate_file:NA:1]: Error attempting to Recover(StandardAsyncJob(4704e5c9-3a79-4280-a464-d737f36056ec)); java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:995,recover,recover,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,"xpressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions), a.arg2.evaluateValue(inputs, ioFunctionSet, forCommandInstantiationOptions)) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. Gives the following error:. ```; [error] /Users/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1657,Error,ErrorOr,1657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:3126,error,error,3126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['error']
Availability,"y d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```; However, the directory `gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir` does exist from the previous run, so I am puzzled by the `FileNotFoundException` exception. Is it the case that directories cannot get cached by Crowmell and therefore callCaching does not work for tasks that have `Directory` as inputs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:1643,error,error,1643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,1,['error'],['error']
Availability,"y having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [error] ^; [info] common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]] <: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]?; ```. I also tried switching this to use:. ```scala; processTwoValidatedValues[WomArray, WomString, WomString](a.arg1.evaluateValue(inputs, ioFunctionSe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:1381,Error,ErrorOr,1381,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,1,['Error'],['ErrorOr']
Availability,"y"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8096,echo,echo,8096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['echo'],['echo']
Availability,"y/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stdout"",; ""shardIndex"": -1,; ""runtimeAttributes"": {; ""docker"": ""broadgdac/aggregate_data:31"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""returnCode"": -1,; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }],; ""jobId"": ""2957"",; ""backend"": ""JES"",; ""end"": ""2016-12-02T15:05:42.655Z"",; ""stderr"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stderr"",; ""callRoot"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data"",; ""attempt"": 1,; ""executionEvents"": [...]; },; ""outputs"": {. },; ""workflowRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc"",; ""id"": ""3608d6ca-fbb4-4232-b197-268058470bfc"",; ""inputs"": {...; },; ""submission"": ""2016-12-01T21:21:40.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:3886,failure,failures,3886,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failures']
Availability,yncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:3074,robust,robustExecuteOrRecover,3074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['robust'],['robustExecuteOrRecover']
Availability,"you recently released version ""28"", then found a bug, and changed the JAR download, but still called it ""28"". This has caused problems for brew, conda etc who package releases and use SHA256 to verify. It would have been better if you made it a ""29"" release, and delete the ""28"" release. Thank you for considering this opinion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2480:74,down,download,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480,1,['down'],['download']
Availability,"ype: text\/plain; charset=UTF-8\\\"" cp \/cromwell_root\/stderr gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:42.510303Z""; },; {; ""startTime"": ""2018-08-14T16:16:43.002063Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing cp \/cromwell_root\/rc gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:16:45.288207Z""; },; {; ""startTime"": ""2018-08-14T16:16:57.822335Z"",; ""description"": ""Stopped running \""\/bin\/sh -c cat \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt 2>\/dev\/null | xargs -I % sh -c 'gsutil -m cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(echo % | sed -e \\\""s\/^\\\\\/\/\/\\\"") 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -m -u dos-testing cp -r % gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/$(ec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:2025,echo,echo,2025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['echo'],['echo']
Availability,"yping/606107ba-fb48-489f-8d14-e19192331b82/call-SplitGvcf/shard-1/glob-a859a146d6b8e5fd268a04d997240e7e/NWD123256.a69a5041-cdcc-4c7d-82e4-47f3d422c441.0001.g.vcf.gz.tbi"", ""gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/606107ba-fb48-489f-8d14-e19192331b82/call-SplitGvcf/shard-2/glob-a859a146d6b8e5fd268a04d997240e7e/NWD145410.61ccb202-d666-4fad-ba17-351fabd79cd1.0001.g.vcf.gz.tbi""]`. **Start of the logs**. 2016-04-11 22:41:00,479 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: persisting status of TileDBCombineGVCF:1048 to Starting.; 2016-04-11 22:41:00,479 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,490 cromwell-system-akka.actor.default-dispatcher-2683 INFO - WorkflowActor [UUID(606107ba)]: Call Caching: cache miss; 2016-04-11 22:41:00,521 cromwell-system-akka.actor.default-dispatcher-2683 ERROR - WorkflowActor [UUID(606107ba)]: Failed to fetch locally qualified inputs for call TileDBCombineGVCF:1048; wdl4s.WdlExpressionException: Failed to find index Success(WdlInteger(1048)) on array:. **and the following stacktrace associated with these:**. 1048; at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:112) ~[cromwell.jar:0.19]; at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:79) ~[cromwell.jar:0.19]; at wdl4s.WdlExpression.evaluate(WdlExpression.scala:192) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1$$anonfun$apply$16.apply(WorkflowActor.scala:987) ~[cromwell.jar:0.19]; at cromwell.engine.workflow.WorkflowActor$$anonfun$fetchLocallyQualifiedInputs$1$$anonfun$apply$16.apply(WorkflowActor.scala:982) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245) ~[cromwell.jar:0.19]; at scala.collecti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/695:3054,ERROR,ERROR,3054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/695,1,['ERROR'],['ERROR']
Availability,"z.tbi; 1608597511949,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-142/cacheCopy/SR00c.NA20320.txt.gz.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMergi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:159612,down,download,159612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"z.tbi; 1608597513691,download: s3://focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed to focal-sv-resources/broad-references/v0/sv-resources/resources/v1/hg38_primary_contigs.bed; 1608597515955,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-127/cacheCopy/SR00c.NA19184.txt.gz.tbi; 1608597517316,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz.tbi; 1608597520303,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-41/cacheCopy/SR00c.HG01880.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:160241,down,download,160241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,1,['down'],['download']
Availability,"{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:1982,Error,Error,1982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,2,['Error'],['Error']
Availability,"{job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; /usr/bin/env bash ${script}; """""". submit-docker = """"""; module load tools/singularity/3.8.3; SINGULARITY_MOUNTS='<redacted>'; export SINGULARITY_CACHEDIR=$HOME/.singularity/cache; LOCK_FILE=$SINGULARITY_CACHEDIR/singularity_pull_flock. export SINGULARITY_DOCKER_USERNAME=<redacted>; export SINGULARITY_DOCKER_PASSWORD=<redacted>. flock --exclusive --timeout 900 $LOCK_FILE \; singularity exec docker://${docker} \; echo ""Sucessfully pulled ${docker}"". bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpu} \; -R 'rusage[mem=${memory_mb}] span[hosts=1]' \; -M ${memory_mb} \; singularity exec --containall $SINGULARITY_MOUNTS --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}; """""". job-id-regex = ""Job <(\\d+)>.*""; kill = ""bkill ${job_id}""; kill-docker = ""bkill ${job_id}""; check-alive = ""bjobs -w ${job_id} |& egrep -qvw 'not found|EXIT|JOBID'"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path+modtime""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3;; hsqldb.log_size=0; """"""; connectionTimeout = 86400000; numThreads = 2; }; insert-batch-size = 2000; read-batch-size = 5000000; write-batch-size = 5000000; metadata {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7203:2182,alive,alive,2182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7203,1,['alive'],['alive']
Availability,"} > $MUTECT1_CS; cat ${sep =' ' mutect1_cs} | grep -Pv '#'|grep -Pv '^contig' >> $MUTECT1_CS. #mutect2 call_stats merging; MUTECT2_CS=""MuTect2.call_stats.txt""; cat ${mutect2_cs[0]} |grep -P '^#' > $MUTECT2_CS ;; cat ${sep=' ' mutect2_cs} |grep -Pv '^#' >> $MUTECT2_CS ;; -eddie. On Wed, Mar 23, 2016 at 3:25 PM, Scott Frazer notifications@github.com; wrote:. > @tmdefreitas https://github.com/tmdefreitas Yes, that is definitely; > possible.; > ; > However, we try to not make assumptions about the type of characters that; > your script can have in it. I'm perhaps being a little overly cautious, but; > I'd hate for there to be a case where somebody wants to use a # in their; > command but it gets interpreted as a comment. That could lead to the same; > kind of confusion that we're seeing now.; > ; > I vacillate on this because I also see the pragmatism in implementing your; > suggestion for the common case. In most cases I can think of, a # is a; > comment; > ; > Maybe some approach like Eddie's where I can have the parser give a better; > error message is the best solution.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200505343. ## . Edward A. Salinas; Senior Software Engineer - Getz Lab; Broad Institute of MIT and Harvard; 75 Ames Street, Rm 4013; Cambridge, MA 02142; Ph: 617-714-7905; Cell: 210-274-3172. ---. @tmdefreitas commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200510863). @eddiebroad But those are all quoted strings, and don't look the same as a WDL comment. From an implementation perspective, doesn't cromwell pipe the command block to /bin/bash anyway? And following bash rules unquoted `#` characters start a comment, so maybe WDL just has to follow the same comment parsing rules as bash?. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadins",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:4593,error,error,4593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['error'],['error']
Availability,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:1819,heartbeat,heartbeat,1819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,2,['heartbeat'],"['heartbeat', 'heartbeatInterval']"
Availability,"~Opening draft PR for early access viewing. Also to get assistance from Travis to run the full test suite.~; Un-drafting this PR. I'd still like to build in some monitoring of checksum failures, but right now I'm more interested in getting the current changes reviewed. Still TODO:; * ~enumerate hash types~; * ~implement remaining hash calculations~; * ~add additional tests for successful checksum and retry~; * figure out how to monitor checksum failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683:185,failure,failures,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683,2,['failure'],['failures']
Availability,"~~Might not make much difference until *all* our PRs are rebased to include this throttle~~. Actually this PR is more about allowing the AWS backend to hook into the existing poll retry logic to allow more resilience in the face of ""429 / Too Many Request"" exceptions during status polling.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4817:206,resilien,resilience,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4817,1,['resilien'],['resilience']
Availability,"~~Pulling this back to a pregull, I want to think about and possibly robustify simpleton key handling.~~ Have at it!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3470:69,robust,robustify,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3470,1,['robust'],['robustify']
Availability,"~~This is simply a checkpoint of WIP and does not need to be reviewed unless it's going to be picked up by someone else in my absence. The grouping of execution events by correlation to labeled actions seems to basically be working. But from the current timing diagram it looks like there are a lot more actions that need labeling and/or correlation before this could possibly be called ""Done"".~~ All good now review it for realz!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4305:19,checkpoint,checkpoint,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4305,1,['checkpoint'],['checkpoint']
Availability,"… all moving in the right direction. Test Infrastructure updated to handle the test re-enabled here, including ThreeStep. There are a few intermittent test failures, bugged as #850 #876 #877",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/878:156,failure,failures,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/878,1,['failure'],['failures']
Availability,"…, with ability to have multiple errors",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163,1,['error'],['errors']
Availability,….error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/340:2,error,error,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/340,1,['error'],['error']
Availability,…lues which caused the jobs to fail once cromwell recovered after a migration. For Develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2242:50,recover,recovered,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2242,1,['recover'],['recovered']
Availability,"…old performance increase under load prior to DB gumming up. . Things to note:; - Effectively removes Metadata acks & failure notices (see #1811) via no longer emitting the messages but does not fully remove them. They still technically exist, I'll remove them as part of a separate PR; - Completely reworks `CromwellApiServiceSpec` to actually be testing `CromwellApiService` and not a general integration test of our REST endpoints. Two specific tests didn't make the cut (#1828 and #1829) I'll address in separate PRs. There were other tests which did not make the cut but were already effectively being tested in their appropriate units.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836:118,failure,failure,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836,1,['failure'],['failure']
Availability,"💯 to Saloni for the original architecture of this code back in 2018/19, it was very easy to work with. Unfortunately I've found in my own work with DRS that the original error reporting does not hold up too well anymore",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5845:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5845,1,['error'],['error']
Deployability,"	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command line of no more than N bytes; 	-x	Exit if size is exceeded; ```. Notice that Nextflow had a [similar issue](https://github.com/nextflow-io/nextflow/issues/321) that I reported a few months ago, and is now fixed, allowing seamless integration of Biocontainers with Nextflow pipelines. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:3346,integrat,integration,3346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,2,"['integrat', 'pipeline']","['integration', 'pipelines']"
Deployability, 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2223,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," ""application-default""; }; }; }. call-caching {; enabled = true; }. backend {; default = GCPBATCH; providers {; GCPBATCH {; // life sciences; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; ## Google project; project = ""$PROJECT"". ## Base bucket for workflow executions; root = ""$BUCKET""; name-for-call-caching-purposes: PAPI; #60000/min in google; ##genomics-api-queries-per-100-seconds = 90000; virtual-private-cloud {; network-name = ""$NET""; subnetwork-name = ""$SUBNET""; }; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; 	 request-workers = 4; batch-timeout = 7 days; 	 # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; 	 slow-job-warning-time: 24 hours; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; compute-service-account = ""default""; # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false; ## Location; location = ""europe-west1"". ; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""$PROJECT""; caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""reference""; }; }; }. default-runtime-attribut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:10592,Pipeline,Pipelines,10592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['Pipeline'],['Pipelines']
Deployability," ""doc"": ""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\n"",; ""inputBinding"": {; ""prefix"": ""--assembly""; },; ""default"": "".assembly.bam"",; ""id"": ""#gridss-2.9.4.cwl/assembly""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - BED file containing regions to ignore\n"",; ""inputBinding"": {; ""prefix"": ""--blacklist""; },; ""id"": ""#gridss-2.9.4.cwl/blacklist""; },; {; ""type"": ""string"",; ""doc"": ""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\n"",; ""inputBinding"": {; ""prefix"": ""--concordantreadpairdistribution""; },; ""default"": ""0.995"",; ""id"": ""#gridss-2.9.4.cwl/concordantreadpairdistribution""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - configuration file use to override default GRIDSS settings.\n"",; ""inputBinding"": {; ""prefix"": ""--configuration""; },; ""id"": ""#gridss-2.9.4.cwl/configuration""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\n"",; ""inputBinding"": {; ""prefix"": ""--externalaligner""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/externalaligner""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of GRIDSS jar\n"",; ""inputBinding"": {; ""prefix"": ""--jar""; },; ""default"": ""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar"",; ""id"": ""#gridss-2.9.4.cwl/jar""; },; {; ""type"": ""boolean"",; ""doc"": ""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\n"",; ""inputBinding"": {; ""prefix"": ""--jobindex""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobindex""; },; {; ""type"": ""boolean"",; ""doc"": ""total number of assembly jobs (only required when performing parallel assembly across multiple computers). Note than an assembly jobs is required after all indexed jobs have been completed to gather the output files together.\n"",; ""inputBinding"": {; ""prefix"": ""--jobnodes""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobno",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:11464,configurat,configuration,11464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability," # Funcotator inputs; Boolean? run_funcotator; String? sequencing_center; String? sequence_source; String? funco_reference_version; String? funco_output_format; Boolean? funco_compress; Boolean? funco_use_gnomad_AF; File? funco_data_sources_tar_gz; String? funco_transcript_selection_mode; File? funco_transcript_selection_list; Array[String]? funco_annotation_defaults; Array[String]? funco_annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? funco_filter_funcotations; String? funcotator_extra_args. String funco_default_output_format = ""MAF""; ; # Use as a last resort to increase the disk given to every task in case of ill behaving data; Int? emergency_extra_disk; }. Int contig_size = select_first([min_contig_size, 1000000]); Int preemptible_or_default = select_first([preemptible, 2]); Int max_retries_or_default = select_first([max_retries, 2]). Runtime standard_runtime = {""gatk_docker"": gatk_docker, ""gatk_override"": gatk_override,; ""max_retries"": max_retries_or_default, ""preemptible"": preemptible_or_default, ""cpu"": small_task_cpu,; ""machine_mem"": small_task_mem * 1000, ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + ""--max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:6479,configurat,configuration,6479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['configurat'],['configuration']
Deployability," ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['configurat'],['configuration']
Deployability," && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1949,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }. gustily ls gs://cromwell-executions/MemoryRetryTest/d54a5a39-4d3b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1943,pipeline,pipeline-timeout,1943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['pipeline'],['pipeline-timeout']
Deployability, 1.23 to 1.26.; [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatical,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5687:1089,release,releases,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5687,1,['release'],['releases']
Deployability, 1.26 to 1.27.; [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatical,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5848:1089,release,releases,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5848,1,['release'],['releases']
Deployability, 1.27 to 1.28.; [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatical,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6281:1089,release,releases,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6281,1,['release'],['releases']
Deployability, 1.28 to 1.29.; [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatical,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6429:1089,release,releases,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6429,1,['release'],['releases']
Deployability," 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from - to Initializing; [2017-11-18 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from - to Initializing; [2017-11-18 17:44:21,09] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from Initializing to Running; [2017-11-18 19:30:04,06] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from Running to Success; [2017-11-18 19:30:05,51] [info] WorkflowExecutionActor-d57a5f97-8542-4fcc-89c4-b7c487957dea [d57a5f97]: Starting calls: atac.filter:1:1; [2017-11-18 19:30:05,51] [warn] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Unrecognized runtime attribute keys: time; [2017-11-18 19:30:05,52] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: python $(which encode_filter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-bowtie2/shard-1/glob-3bcbe4e7489c90f75e0523ac6f3a9385/ENCFF463QCX.trim.merged.R1.bam \; \; --multimapping 4 \; \; \; \; --nth 2; [2017-11-18 19:30:15,43] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:2939,pipeline,pipeline-workflows,2939,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-workflows']
Deployability," = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; # Google project which will be billed for the requests; project = ""xxxxx-xxxxx-xxxxx"". caching {; # When a cache hit is found, the following",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:4276,configurat,configuration,4276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,['configurat'],['configuration']
Deployability," ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1379,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1379,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," GS URLs referencing the same data and does work. The workflow fails with:; ```; java.io.FileNotFoundException: Cannot hash file https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genom; es/hg19/seq/hg19.fa; ```; when running tasks. The files get downloaded to the input directories but get numerical values instead of the original file names so never seem to sync over and get translated correctly to the workflow; ```; ls -lh cromwell_work/cromwell-executions/main-somatic.cwl/eaa632df-52a8-4aae-826f-647a42fa7145/call-prep_samples_to_rec/inputs/1515144/; total 136K; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 225050424226294657; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 2612405277530248055; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 503001634356675169; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 5802330287039666628; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 5809676514510180826; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6090832304768530540; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6105514522473810611; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 6807576659333162957; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 6853384576121493061; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7483350933664987331; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7538690575330349970; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 7691692211431528147; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7783203266940950463; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 8389565043859020157; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 8932347409858620277; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 993751307168383758; ```; My configuration is:; ```; engine {; filesystems {; gcs {; auth = ""application-default""; }; http {}; }; }. backend {; providers {; Local {; config {; filesystems {; http { }; }; }; }; }; }; ```; Am I doing anything wrong with my configuration or setup that I could tweak? Thanks so much for any pointers/suggestions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184:2105,configurat,configuration,2105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184,2,['configurat'],['configuration']
Deployability," Stopped pulling ""ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9""; pullStopped:; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; timestamp: '2021-08-03T15:23:16.274159840Z'; - description: Started pulling ""ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9""; pullStarted:; imageUri: ubuntu@sha256:1e48201ccc2ab83afc435394b3bf70af0fa0055215c1e26a5da9b50a1ae367c9; timestamp: '2021-08-03T15:23:12.246258428Z'; - description: Stopped pulling ""gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStopped:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:23:12.246251722Z'; - description: Started pulling ""gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim""; pullStarted:; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; timestamp: '2021-08-03T15:22:42.922496298Z'; - description: Worker ""google-pipelines-worker-xxxxxx""; assigned in ""us-central1-b"" on a ""custom-1-2048"" machine; timestamp: '2021-08-03T15:22:07.789742627Z'; workerAssigned:; instance: google-pipelines-worker-xxxxxx; machineType: custom-1-2048; zone: us-central1-b; labels:; cromwell-workflow-id: cromwell-xxxxxx; wdl-task-name: hello; pipeline:; actions:; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Starting\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 300s; - commands:; - -c; - mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: ContainerSetup; mounts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ container\ setup.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: ContainerSetup; timeout: 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:15348,pipeline,pipelines-worker-xxxxxx,15348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['pipeline'],['pipelines-worker-xxxxxx']
Deployability," Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.J",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1296,pipeline,pipelines,1296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['pipeline'],['pipelines']
Deployability," \""id\"": \""#gridss-2.9.4.cwl/jobnodes\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""size of JVM heap for assembly and variant calling.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jvmheap\""\n },\n \""default\"": \""$(get_max_memory_from_runtime_memory(runtime.ram))m\"",\n \""id\"": \""#gridss-2.9.4.cwl/jvmheap\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""keep intermediate files. Not recommended except for debugging due to the high disk usage.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--keepTempFiles\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/keepTempFiles\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""comma separated labels to use in the output VCF for the input files.\\nSupporting read counts for input files with the same label are aggregated\\n(useful for multiple sequencing runs of the same sample).\\nLabels default to input filenames, unless a single read group with a non-empty sample name\\nexists in which case the read group sample name is used\\n(which can be disabled by \\\""useReadGroupSampleNameCategoryLabel=false\\\"" in the configuration file).\\nIf labels are specified, they must be specified for all input files.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--labels\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/labels\""\n },\n {\n \""type\"": [\n \""null\"",\n \""int\""\n ],\n \""doc\"": \""Optional - maximum coverage. Regions with coverage in excess of this are ignored.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--maxcoverage\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/maxcoverage\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""do not use JNI native code acceleration libraries (snappy, GKL, ssw, bwa).\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--nojni\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/nojni\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""output gzipped VCF file\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--output\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/output\""\n },\n {\n \""type\"": [\n \""null\"",\n ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:75937,configurat,configuration,75937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability," \; --multimapping 4 \; \; \; \; --nth 2; [2017-11-18 19:30:15,43] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:4091,pipeline,pipeline-genome-data,4091,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-genome-data']
Deployability," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:2564,configurat,configuration,2564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,1,['configurat'],['configuration']
Deployability," a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chrom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1648,configurat,configuration,1648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,3,"['configurat', 'update']","['configuration', 'configurations', 'updated']"
Deployability," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1428,install,installed,1428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability," activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:1540,configurat,configuration,1540,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,1,['configurat'],['configuration']
Deployability," after Cromwell calls docker, which executes a shell script, the following shell command fails:. java -Xmx3000m -jar /usr/gitc/picard.jar \; SamToFastq \; INPUT=/cromwell-executions/PairedEndSingleSampleWorkflow/610155cd-e94f-4db2-bd9e-9b2322fc412f/call-SamToFastqAndBwaMem/shard-0/inputs/home/orodeh/genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.bam \; FASTQ=/dev/stdout \; INTERLEAVE=true \; NON_PF=true | \; /usr/gitc/bwa mem -K 100000000 -p -v 3 -t 16 $bash_ref_fasta /dev/stdin - 2> >(tee genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bwa.stderr.log >&2) | \; samtools view -1 - > genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bam && \; grep -m1 ""read .* ALT contigs"" genomics-public-data/test-data/dna/wgs/hiseq2500/NA12878/H06HDADXX130110.1.ATCACGAT.20k_reads.unmerged.bwa.stderr.log | \; grep -v ""read 0 ALT contigs"". The input file exists, but something goes wrong, and the outputs are not generated. This caused the rest of the pipeline to fail. Perhaps that provides more debugging information. . This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/gatk/discussion/comment/37003#Comment_37003. ---. @sooheelee commented on [Mon Mar 13 2017](https://github.com/broadinstitute/dsde-docs/issues/1815#issuecomment-286223385). Since Kate said she hasn't answered any questions, perhaps she would like to take ownership of this one. I've also gone ahead and asked two devs. ---. @sooheelee commented on [Mon Mar 13 2017](https://github.com/broadinstitute/dsde-docs/issues/1815#issuecomment-286238890). Devs are busy and say to send this to redteam@broad. So @katevoss if you find there is no obvious answer, please be sure to seek help. ---. @sooheelee commented on [Mon Mar 13 2017](https://github.com/broadinstitute/dsde-docs/issues/1815#issuecomment-286242796). @ruchim will look into this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2064:1503,pipeline,pipeline,1503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2064,1,['pipeline'],['pipeline']
Deployability, at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBackends.scala:14); at scala.collection.immutable.List.map(List.scala:246); at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); at cromwell.engine.backen,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1905,Pipeline,PipelinesApiLifecycleActorFactory,1905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability, at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3238,pipeline,pipelines,3238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['pipeline'],['pipelines']
Deployability," backend configuration, since whether this is something you want to do or not will depend on the infrastucture your workflow is running on. One potential way to configure this, might be to add multipliers for certain runtime attributes in the backend configuration:; ```; [...]; config {; runtime-attributes = """"""; Int? cpu = 1; Int? memory = 4; """"""; runtime-attribute-retry-multipliers = {; memory: 1.5; }; [...]; }; [...]; ```. This would, for example, cause the memory attribute to be multiplied by `1.5` with each retry. For the first attempt it would be `4`, for the the second `6`, the third `9` etc. Another option might be that the values here indicate a fraction of the original attribute which it should be increased it by each retry, so in the above example it would be: `4` -> `10` -> `16` etc. You would probably want set a lower value in that case, though. Another option would be to supply a list of numbers with each indicating a multiplier for a certain attempt. A value of `[1.5, 2]` (or maybe `[1, 1.5, 2]`) would cause the value to be multiplied by `1.5` on the second attempt and `2` on the third, repeating the last multiplier if neccesary. (ie. `4` -> `6` -> `8` -> `8`). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4346:2235,configurat,configuration,2235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4346,1,['configurat'],['configuration']
Deployability," change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table crea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:3154,update,updateSchema,3154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['update'],['updateSchema']
Deployability," changed the actual project name to generic ""project""); ```; include required(classpath(""application"")). google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""project-test1""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""project-test1"". // Base bucket for workflow executions; root = ""gs://project-test1/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""project-test1""; }; }; }; }; }; }; ```. I created the service account from https://cloud.google.com/docs/authentication/getting-started and give the role: Project -> Owner. I've downloaded Google Cloud SDK and run these; ```; gcloud auth login juha.wilppu@gmail.com; gcloud auth application-default login; gcloud config set project project-test1; gsutil ls gs://project-test1 // This command works, so authentication is successful.; ```; **project-test1-59b66448c3ab.json**; ```; {; ""type"": ""service",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:4937,Pipeline,Pipelines,4937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,1,['Pipeline'],['Pipelines']
Deployability," com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:953); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1092); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1040); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1350); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:716); 	... 27 common frames omitted; ```. Initially nothing was persisting to the database making debugging tricky, so I updated the database to increase the column size of METADATA_ENTRY.METADATA_KEY from varchar(255) to something silly like varchar(3000) and re-ran the offending script. The culprit showed itself as:. ```; SELECT length(METADATA_KEY) FROM cromwell.METADATA_ENTRY, x.METADATA_KEY xWHERE length(METADATA_KEY) = (SELECT max(length(METADATA_KEY)) FROM METADATA_ENTRY). 323, ""inputs:batch_files:/mnt/data/cromwell-executions/build_bob_ep/40573452-6a92-4e26-8f0d-02bd980970b7/call-build_bob/build_bob/b6e60c8e-2d0a-4db7-8b70-b71cde217b30/call-building_taxonomy/building_taxonomy/c2e537ff-e231-4b51-a423-f750604dca7c/call-classify_f33ef23grwsg32fgv/inputs/2065711490/GTDB_GB_GCA_123456789.1.mask.fasta""; ```. Basically, as the complexity of the workflows increases the potential length of the inputs increases and the limit of varchar(255) is exceeded. Going forward, this will not be our most complicated workflow so I expect to hit this more frequently. So firstly, am I doing anything wrong?. Secondly would it be possible to increase the maximum size of the column METADATA_KEY that can accommodate increasing levels workflow complexity? I can do this post deployment using ansible but that feels a little bit messy. (I have also posted this on your JIRA backlog as Im not sure which is the best place to raise this: https://broadworkbench.atlassian.net/browse/CROM-6721). Best,; Jon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:5427,deploy,deployment,5427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,1,['deploy'],['deployment']
Deployability," configuration:; ...; [2021-08-13 10:44:55,42] [info] Running with 3 PAPI request workers; ...; [2021-08-13 10:44:55,79] [info] Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googlea",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2548,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1313,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1395,pipeline,pipelines,1395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,3,"['Pipeline', 'pipeline']","['PipelinesApiBackendLifecycleActorFactory', 'pipelines']"
Deployability," fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92"",; ""description"": ""PickedUp""; },; {; ""description"": ""Finished"",; ""cromwellId"": ""cromid-0a29b92"",; ""timestamp"": ""2020-08-29T00:04:06.072Z"",; ""cromwellVersion"": ""53""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""1.0"",; ""submittedFiles"": {; ""workflow"": ""version 1.0\n\nworkflow mem_retry {\n call fail_oom \n}\n\ntask fail_oom {\n command {\n set -e\n # This one-liner triggers 137 (SIGKILL due to OOM)\n # https://askubuntu.com/a/823798\n tail /dev/zero\n }\n runtime {\n cpu: 1\n memory: \""2 GB\""\n docker: \""ubuntu:latest\""\n }\n}\n\n"",; ""root"": """",; ""options"": ""{\n \""backend\"": \""gcp\"",\n \""default_runtime_attributes\"": {\n \""maxRetries\"": 1\n },\n \""monitoring_script\"": \""gs://caper-data/scripts/resource",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:1696,pipeline,pipeline-test-runs,1696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['pipeline'],['pipeline-test-runs']
Deployability," if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/scala/org/scalatest/Retries.scala#L565-L577; - https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:3494,configurat,configuration,3494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['configurat'],['configuration']
Deployability," it was fresh in my mind:. 1. In the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, the *Docker Lookup* section has detailed documentation on the modes but never mentions the actual config key used to set it, nor the config string values for the different modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing becau",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:1248,release,release,1248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,1,['release'],['release']
Deployability," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015:2768,update,update,2768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015,1,['update'],['update']
Deployability," log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-01-31 18:29:35,282 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:35,461 INFO - Successfully released change log lock; 2019-01-31 18:29:35,469 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.lang.ArrayIndexOutOfBoundsException: 1; 	at liquibase.datatype.DataTypeFactory.fromDescription(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(Liquibas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:2160,release,released,2160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['release'],['released']
Deployability," log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:1733,update,update,1733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability," message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5 we currently use in Cromwell. This PR implements the xxhash algorithm for call-caching in Cromwell:. + The default strategy will still be using md5 for backwards compatibility.; + A new `xxh64` strategy is implemented using the 64-bit xxhash algorithm. (I didn't make the xxh32 algorithm available. Is there any Cromwell server still running on 32-bit?) This can be set in the call caching configuration.; + A new `fingerprint` strategy suggested by @illusional, which takes the modtime, size and a xxh64 hash of the first 10 mb of the file to create a virtually unique fingerprint.; + The `file` strategy get's a new alias `md5` which is more clear. Although `file` will still work in the config for backwards compatibility. . I feel we should move to xxh64 as default after it has proven itself in a few releases. The speed-up is an order of magnitude.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:2685,configurat,configuration,2685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,2,"['configurat', 'release']","['configuration', 'releases']"
Deployability," mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1608,install,install,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['install'],['install']
Deployability," name-for-call-caching-purposes: PAPI; slow-job-warning-time: ""24 hours""; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600; request-workers = 3; genomics {; auth = ""application-default""; endpoint-url = ""https://genomics.googleapis.com/""; location = ""us-west1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""application-default""; project = ""xxxx""; caching {; duplication-strategy = ""copy""; }; }; http { }; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b""]; }; include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```; When I run with the above config using:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json; ```; I am getting the following error message:; ```; [2021-08-24 22:05:33,60] [info] WorkflowManagerActor: Workflow 6cc303b4-295d-49fa-a996-b5cf7ec9beea failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Execution failed: allocating: creating instance: inserting instance: Invalid value for field 'resource.networkInterfaces[0].network': ''. The referenced network resource cannot be found.; ```; I have tried passing the vpc and subnet id using the following config:; ```; virtual-private-cloud {; network-label-key = ""xxx""; subnetwork-label-key = ""xxx""; auth = ""application-default""; }; ```. The above values are my actual vpc and subnet id/name. However, it is still giving me that error message. Is there something I am missing from a configuration perspective. Any help would be greatly appreciated. Our VPC network's are not created in auto mode and that is not something we have control over unfortunately. Thanks,; -Simran",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:2578,configurat,configuration,2578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,1,['configurat'],['configuration']
Deployability," output count, runtime attribute, output expression, input count, backend name, command template, input. - What are the downsides with `check-sibling-md5`, can it be used in conjunction with `system.file-hash-cache`. - **Is the only way to use call-caching with containers without fully hashing the file?**. ## Possible resolutions. I was thinking the following might be potential solutions for my problem, but I don't know how good / bad they are, and they'd require changes to Cromwell. - Potential for a _cheaper_ (and potentially dirtier) hash for files? ; - When cromwell links from a cached result, store a map of { newpath : original } link to use or call caching, so when the hashDifferential is calculated, it uses the hash of the original cached result. (This would mean we could use the path+modtime strategy). ## Current attempt. I realised I may have run into another error here: https://github.com/broadinstitute/cromwell/issues/5348. This is my current configuration, it will successfully pull cache for the FIRST step in a workflow, but then fail afterwards. <details><summary>Click to show configuration</summary><p>. ```hocon; include required(classpath(""application"")). system: {; ""job-shell"": ""/bin/sh"",; ""cromwell_id"": ""cromwell-fdcce1"",; ""cromwell_id_random_suffix"": false; }; database: {; ""db"": {; ""driver"": ""com.mysql.cj.jdbc.Driver"",; ""url"": ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"",; ""user"": ""root"",; ""connectionTimeout"": 5000; },; ""profile"": ""slick.jdbc.MySQLProfile$""; }; backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""/Users/franklinmichael/janis/cache_test/20200110_090106_f8ee04/janis/execution"",; ""filesystems"": {; ""local"": {; ""caching"": {; ""hashing-strategy"": ""path+modtime""; }; }; }; }; }; }; }; call-caching: {; ""enabled"": true; }; ```; </p></details>. Thanks in advance for your help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5346:4005,configurat,configuration,4005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5346,2,['configurat'],['configuration']
Deployability," please point us to it,. Command:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1169,pipeline,pipelines,1169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['pipeline'],['pipelines']
Deployability, specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4339,pipeline,pipeline,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability," tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturer’s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinus™ RNA fraction as described in the manufacturer’s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illumina’s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519:2314,pipeline,pipeline,2314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519,1,['pipeline'],['pipeline']
Deployability," that one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as possible (considering exponentially long waiting times in between retries) until a threshold amount of time has passed since the first try (10 minutes by default",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:1131,integrat,integrate,1131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,1,['integrat'],['integrate']
Deployability," this because the resulting .jar file name contains a ""35"" instead (```server/target/scala-2.12/cromwell-35-71debed-SNAP.jar```). A subworkflow task is tryting to do the following assignation...:. ```; Array[File]? y ; # ...; Array[File] x = select_first([y, []]); ```; where ```y``` is a task argument that is assigned the value of a ```Array[File]+``` argument by the invoking workflow. Eventually I fixed this by unraveling the unnecessary conversions since in this case there is no need for a ""?"" in the ```y``` nor the ```x``` or the invokation of ```select_first```; however I have to say that I don't see why this ""coversion"" would be invalid but I'm not much of a wdl or scala expert. Now the for-sure issue here is that instead of failing indicating what is going on the workflow was still running and the offending task(s) were reported as ""Starting"" in the metadata and the timing and they stayed that way forever. . In order to find out what was going on I needed to install and run a locally v36 server (I usually use dsde-method's community cromwell servers). The logs show first the causing wdl bug like so:. ```; [ERROR] [03/19/2019 09:52:14.444] [cromwell-system-akka.dispatchers.engine-dispatcher-47] [akka://cromwell-system ... Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomAnyType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([""gs:// .... 70.tsv.gz""])), []); ```; Notice that Skipped most of the message text showing (what I think are) the important bits . . This meesage is follow for java exception directly dump into the log output file. java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(W....z""])), []); at wom.values.WomArray$.apply(WomArray.scala:34); at wom.values.WomArray$.apply(WomArray.scala:38); at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755:1074,install,install,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755,1,['install'],['install']
Deployability," to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5571,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,2,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockCont",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1850,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability," will be able to utilize results from all calls that are in that database.""_. Secondly, if this can. **Question 2.**. Can call caching be initiated if a scatter, wraps a workflow, which then wraps tools.; Or will the entire workflow need to be in one script? (I have attached an example as zip); And, the options file.; [DsTrim - Broken.zip](https://github.com/broadinstitute/cromwell/files/3842334/DsTrim.-.Broken.zip). **Question 3.**. What exactly triggers callcaching to change from ""CallCachingOff"" to on, in the following result?; `; ""callCaching"": {; ""effectiveCallCachingMode"": ""CallCachingOff"",; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss""; },`. **If the in-memory is the issue, then please close and we will set-up a UAT correctly.; If not any additional assistance or comments will be most apprecitated.** . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5280:2849,configurat,configuration,2849,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5280,1,['configurat'],['configuration']
Deployability," |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R2Fastq|""9f1cf8859a902eb75202a5c048cd43aa-388""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input:File R1Fastq|""62396abd6b589747ee16034888c9a0b5-381""|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:input count|9BF31C7FF062936A96D3C8BD1F8F2FF3|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:command template|7BCEDB02C5FC300FF83F07417B49229E|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:hashes:backend name|2267EF43AEF6BB551F414FEC2390F68A|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:effectiveCallCachingMode|ReadAndWriteCache|; |29791b64-b47a-44ba-aff0-7ab48bc10677|callCaching:allowResultReuse|true|; |5de042e3-7a03-4c77-8972-f0e4cd010e4b|callCaching:allowResultReuse|true|. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7248:8022,configurat,configuration,8022,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7248,1,['configurat'],['configuration']
Deployability," }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""1.0"",; ""submittedFiles"": {; ""workflow"": ""version 1.0\n\nworkflow mem_retry {\n call fail_oom \n}\n\ntask fail_oom {\n command {\n set -e\n # This one-liner triggers 137 (SIGKILL due to OOM)\n # https://askubuntu.com/a/823798\n tail /dev/zero\n }\n runtime {\n cpu: 1\n memory: \""2 GB\""\n docker: \""ubuntu:latest\""\n }\n}\n\n"",; ""root"": """",; ""options"": ""{\n \""backend\"": \""gcp\"",\n \""default_runtime_attributes\"": {\n \""maxRetries\"": 1\n },\n \""monitoring_script\"": \""gs://caper-data/scripts/resource_monitor/resource_monitor.sh\""\n}"",; ""inputs"": ""{}"",; ""workflowUrl"": ""/mnt/data2/scratch/leepc12/test_wdl1_sub/test_mem_1.wdl"",; ""labels"": ""{\n \""caper-backend\"": \""gcp\"",\n \""caper-user\"": \""leepc12\""\n}""; },; ""calls"": {; ""mem_retry.fail_oom"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stdout"",; ""backendStatus"": ""Success"",; ""compressedDockerSize"": 28591363,; ""commandLine"": ""set -e\n# This one-liner triggers 137 (SIGKILL due to OOM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:3094,pipeline,pipeline-test-runs,3094,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['pipeline'],['pipeline-test-runs']
Deployability,"! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->; [2022-03-03 19:26:59,66] [info] WorkflowManagerActor: Workflow 496206d8-8854-48c1-abed-3717510ceb4e failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt: s3://s3.amazonaws.com/mys3-cloudformation/cromwell-execution/wf_hello/496206d8-8854-48c1-abed-3717510ceb4e/call-hello/hello-rc.txt. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->; ![image](https://user-images.githubusercontent.com/96741804/156643007-76a24c99-509c-4480-8484-df1c6f7b9c72.png). <!-- Which backend are you running? -->. AWS Batch. <!-- Paste/Attach your workflow if possible: -->. I have see this as an issue previously reported ; I am trying to set up a genomics work flow using AWS batch and Cromwell . How to solve this issue; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6695:2138,configurat,configuration,2138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6695,1,['configurat'],['configuration']
Deployability,"![screen shot 2015-06-14 at 8 36 19 am](https://cloud.githubusercontent.com/assets/58551/8148606/0f7b94f6-1273-11e5-8f6e-8fb7b23aa935.png). No rush to review this. This is ancillary to the sprint but it'd be nice if we could get it in by the end of the sprint. Changes:. 1) SLF4J logging hooked in with the actor system too. 2) Two modes of logging, set by the Java Property `CROMWELL_LOGGER=[SERVER|CONSOLE]`:; - In SERVER mode, it logs to a rolling file appender with all the bells and whistles. This will default to DEBUG level.; - In CONSOLE mode, there's code in `cromwell.logging` that handles these messages from SLF4J and prints them out to the console is as human-readable way as possible. I welcome comments about how to make it more readable. Though, if you are going to do that make sure you first run it so you can see the colors, which are an important aspect of this! CONSOLE logs on INFO, WARN, ERROR.; - The modes are toggled either by explicitly setting CROMWELL_LOGGER, or based on the CLI sub-command you chose: `server` will do SERVER logging and every other sub-command uses CONSOLE logging. 3) I've tried to establish some conventions for logging:; - INFO, WARN, ERROR is meant to be read by _users_ to debug their WDL executions. It should equally be helpful for _developers_ to debug many issues. We must keep in mind that these are also show up in the server logs so they could also help us add context to debugging an issue easier if we're used to these messages from the command line.; - Messages should contain the workflow UUID wherever appropriate. Anything that exists only in a context of a workflow execution: CallActors, WorkflowActors, SymbolStores, etc.; - Messages should be chosen to craft a story about how a workflow is progressing. Highlight the big points (something starts, something finishes, something is launched, symbol store entry is updated, etc)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44:443,rolling,rolling,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44,3,"['rolling', 'toggle', 'update']","['rolling', 'toggled', 'updated']"
Deployability,"![screen shot 2018-09-27 at 11 52 59 am](https://user-images.githubusercontent.com/14941133/46158478-eda2b800-c24b-11e8-9987-94fec95bb97d.png). It seems that on Cromwell 34 hotfix -- the name of the stdout/stderr files are:; `.../<call-name>-stdout.log`; `.../<call-name>-stderr.log`. However, the call metadata reports stdout/stderr locations to be:; `.../<call-name>/stdout`; `.../<call-name>/stderr`. AC: Ensure that the metadata values for log paths match up to actual log paths. This impacts FC and Job Manager users on Cromwell 34 hotfix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166:173,hotfix,hotfix,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166,2,['hotfix'],['hotfix']
Deployability,""", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # De",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12391,Pipeline,Pipelines,12391,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['Pipelines']
Deployability,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4054:1,Pipeline,Pipeline,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054,3,['Pipeline'],['Pipeline']
Deployability,"""Requester pays bucket access requires authentication"" when deploy cloud function",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:60,deploy,deploy,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,1,['deploy'],['deploy']
Deployability,"""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an hour. . This looks like a bug. I've since restarted the worklow with call-caching so it seems unlikely I can reproduce this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:5151,Pipeline,Pipelines,5151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['Pipeline'],['Pipelines']
Deployability,"""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-07-10 14:32:54,11] [info] Metadata summary refreshing every 1 second.; [2019-07-10 14:32:54,12] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-07-10 14:32:54,12] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-07-10 14:32:54,13] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,13] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-07-10 14:32:54,18] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-07-10 14:32:54,43] [info] SingleWorkflowRunnerActor: Version 42; [2019-07-10 14:32:54,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-07-10 14:32:54",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:4835,configurat,configuration,4835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['configurat'],['configuration']
Deployability,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3817:1584,PATCH,PATCH,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817,1,['PATCH'],['PATCH']
Deployability,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:60,Pipeline,Pipelines,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,1,['Pipeline'],['Pipelines']
Deployability,"## About this PR; 📦 Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from `0.40.10` to `0.40.17`. 📜 [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.17) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.17). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7270:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7270,11,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from `0.14.1` to `0.14.6`. 📜 [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.6) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.6). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7292:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7292,11,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates ; * [io.github.jbwheatley:pact4s-circe](https://github.com/jbwheatley/pact4s); * [io.github.jbwheatley:pact4s-scalatest](https://github.com/jbwheatley/pact4s). from `0.9.0` to `0.10.1-java8`. 📜 [GitHub Release Notes](https://github.com/jbwheatley/pact4s/releases/tag/v0.10.1-java8) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.9.0...v0.10.1-java8). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/H",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,5,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update']"
Deployability,"## About this PR; 📦 Updates ; * [org.http4s:http4s-ember-client](https://github.com/http4s/http4s); * [org.http4s:http4s-ember-server](https://github.com/http4s/http4s). from `0.21.31` to `0.21.34`. 📜 [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7311:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7311,11,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates ; * [org.junit.jupiter:junit-jupiter-api](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-engine](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-params](https://github.com/junit-team/junit5). from `5.9.3` to `5.10.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.junit.jupiter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.junit.jupiter"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7312:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7312,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from `2.7.0` to `2.10.0`. 📜 [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.10.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.10.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7320:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7320,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [cglib:cglib-nodep](https://github.com/cglib/cglib) from `3.2.7` to `3.2.12`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""cglib"", artifactId = ""cglib-nodep"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""cglib"", artifactId = ""cglib-nodep"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7259:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7259,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from `2.18.0` to `2.33.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7269:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7269,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-core-http-okhttp](https://github.com/Azure/azure-sdk-for-java) from `1.11.10` to `1.11.17`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7262:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7262,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from `1.7.1` to `1.11.9`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7263:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7263,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-core-test](https://github.com/Azure/azure-sdk-for-java) from `1.18.0` to `1.18.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.18.0).; You might want to review and update them manually.; ```; cloud-nio/cloud-nio-impl-drs/src/test/scala/cloud/nio/impl/drs/DrsPathResolverSpec.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7264:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7264,11,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-core](https://github.com/Azure/azure-sdk-for-java) from `1.40.0` to `1.45.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7261:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7261,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-identity-extensions](https://github.com/azure/azure-sdk-for-java) from `1.1.4` to `1.1.10`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7266:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7266,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from `1.9.1` to `1.9.2`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7265:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7265,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-storage-blob](https://github.com/Azure/azure-sdk-for-java) from `12.23.0-beta.1` to `12.23.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7267:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7267,10,"['Update', 'configurat', 'release', 'update']","['Updates', 'configuration', 'release', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.azure:azure-storage-common](https://github.com/Azure/azure-sdk-for-java) from `12.22.0-beta.1` to `12.22.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-common"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-common"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7268:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7268,10,"['Update', 'configurat', 'release', 'update']","['Updates', 'configuration', 'release', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` ⚠. 📜 [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.fasterxml.jackson.dataformat:jackson-dataformat-xml](https://github.com/FasterXML/jackson-dataformat-xml) from `2.13.3` to `2.13.5`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7272:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7272,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.github.cb372:sbt-explicit-dependencies](https://github.com/cb372/sbt-explicit-dependencies) from `0.2.16` to `0.3.1`. 📜 [GitHub Release Notes](https://github.com/cb372/sbt-explicit-dependencies/releases/tag/v0.3.1) - [Version Diff](https://github.com/cb372/sbt-explicit-dependencies/compare/v0.2.16...v0.3.1). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7273:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7273,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.github.sbt:junit-interface](https://github.com/sbt/junit-interface) from `0.13.2` to `0.13.3`. 📜 [GitHub Release Notes](https://github.com/sbt/junit-interface/releases/tag/v0.13.3) - [Version Diff](https://github.com/sbt/junit-interface/compare/v0.13.2...v0.13.3). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7274:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7274,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client) from `2.1.4` to `2.2.0`. 📜 [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v2.2.0) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v2.1.4...v2.2.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7276:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7276,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.api.grpc:proto-google-cloud-batch-v1](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. 📜 [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7277:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7277,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.api.grpc:proto-google-cloud-resourcemanager-v3](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7278:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7278,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.api:gax-grpc](https://github.com/googleapis/sdk-platform-java) from `2.25.0` to `2.38.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7275:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7275,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from `1.5.3` to `1.20.0`. 📜 [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.20.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.20.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7281:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7281,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.cloud:google-cloud-batch](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. 📜 [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7282:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7282,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.cloud:google-cloud-bigquery](https://github.com/googleapis/java-bigquery) from `2.25.0` to `2.34.2`. 📜 [GitHub Release Notes](https://github.com/googleapis/java-bigquery/releases/tag/v2.34.2) - [Version Diff](https://github.com/googleapis/java-bigquery/compare/v2.25.0...v2.34.2). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7283:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7283,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7284:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7284,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.google.cloud:google-cloud-storage](https://github.com/googleapis/java-storage) from `2.17.2` to `2.29.1`. 📜 [GitHub Release Notes](https://github.com/googleapis/java-storage/releases/tag/v2.29.1) - [Version Diff](https://github.com/googleapis/java-storage/compare/v2.17.2...v2.29.1). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.17.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7285:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7285,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [com.typesafe:config](https://github.com/lightbend/config) from `1.4.2` to `1.4.3`. 📜 [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.3) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.2...v1.4.3). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7286:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7286,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [commons-codec:commons-codec](https://github.com/apache/commons-codec) from `1.15` to `1.16.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.15).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_wdlom.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-codec"", artifactId = ""commons-codec"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-codec"", artifactId = ""commons-codec"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7287:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7287,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [commons-io:commons-io](https://commons.apache.org/proper/commons-io/) from `2.11.0` to `2.15.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.11.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-io"", artifactId = ""commons-io"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-io"", artifactId = ""commons-io"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7288:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7288,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from `3.8.0` to `3.10.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7289:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7289,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [eu.timepit:refined](https://github.com/fthomas/refined) from `0.10.1` to `0.10.3`. 📜 [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.10.3) - [Version Diff](https://github.com/fthomas/refined/compare/v0.10.1...v0.10.3). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7290:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7290,11,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.circe:circe-config](https://github.com/circe/circe-config) from `0.8.0` to `0.10.1`. 📜 [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.1). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7291:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7291,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.circe:circe-optics](https://github.com/circe/circe-optics) from `0.14.1` to `0.15.0`. 📜 [GitHub Release Notes](https://github.com/circe/circe-optics/releases/tag/v0.15.0) - [Version Diff](https://github.com/circe/circe-optics/compare/v0.14.1...v0.15.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-optics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-optics"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7293:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7293,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from `1.54.1` to `1.54.2`. 📜 [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.54.2) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.54.1...v1.54.2). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7295:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7295,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.projectreactor:reactor-test](https://github.com/reactor/reactor-core) from `3.4.29` to `3.4.34`. 📜 [GitHub Release Notes](https://github.com/reactor/reactor-core/releases/tag/v3.4.34) - [Version Diff](https://github.com/reactor/reactor-core/compare/v3.4.29...v3.4.34). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7296:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7296,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from `5.7.4` to `7.0.0` ⚠. 📜 [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/7.0.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...7.0.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7297:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7297,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from `1.0.56` to `1.0.68`. 📜 [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.68) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.68). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7298:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7298,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from `2.4.10` to `2.4.11`. 📜 [GitHub Release Notes](https://github.com/netplex/json-smart-v2/releases/tag/2.4.11) - [Version Diff](https://github.com/netplex/json-smart-v2/compare/2.4.10...2.4.11). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.minidev"", artifactId = ""json-smart"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""net.minidev"", artifactId = ""json-smart"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7300:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7300,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.apache.commons:commons-lang3](https://commons.apache.org/proper/commons-lang/) from `3.12.0` to `3.14.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7301:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7301,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.apache.tika:tika-core](https://tika.apache.org/) from `2.3.0` to `2.9.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7303:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7303,11,"['Update', 'configurat', 'integrat', 'update']","['Updates', 'configuration', 'integrationTestCases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from `3.1.7` to `3.1.11`. 📜 [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.7...3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.11). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7307:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7307,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. 📜 [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from `4.1.1` to `4.4.0`. 📜 [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.4.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7309:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7309,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.hsqldb:hsqldb](http://hsqldb.org) from `2.6.1` to `2.7.2`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7310:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7310,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from `2.7.4` to `2.7.11`. 📜 [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.11) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.11). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7314:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7314,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from `4.11.0` to `5.7.0` ⚠. 📜 [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.7.0) - [Version Diff](https://github.com/mockito/mockito/compare/v4.11.0...v5.7.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7315:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7315,10,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from `1.13.1` to `1.13.6`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7316:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7316,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from `2.13.9` to `2.13.12`. 📜 [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.12) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.12). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7317:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7317,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from `3.2.15` to `3.2.17`. 📜 [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.17) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.15...release-3.2.17). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7318:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7318,15,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'release-', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from `2.0.4` to `2.0.9`. 📜 [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.9) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.9). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7319:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7319,12,"['Release', 'Update', 'configurat', 'patch', 'release', 'update']","['Release', 'Updates', 'configuration', 'patch', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from `2.3.2` to `3.1.0` ⚠. 📜 [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.1.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.1.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7321:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7321,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from `1.0.11` to `1.2.2`. 📜 [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.2) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.2). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7322:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7322,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [org.yaml:snakeyaml](https://bitbucket.org/snakeyaml/snakeyaml/src) from `1.33` to `2.2`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. <sup>; labels: test-library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7324:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7324,9,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates [se.marcuslonnberg:sbt-docker](https://github.com/marcuslonnberg/sbt-docker) from `1.9.0` to `1.11.0`. 📜 [GitHub Release Notes](https://github.com/marcuslonnberg/sbt-docker/releases/tag/v1.11.0) - [Changelog](https://github.com/marcuslonnberg/sbt-docker/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/marcuslonnberg/sbt-docker/compare/v1.9.0...v1.11.0). ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7325:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7325,11,"['Release', 'Update', 'configurat', 'release', 'update']","['Release', 'Updates', 'configuration', 'releases', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates bio.terra:workspace-manager-client from `0.254.452-SNAPSHOT` to `0.254.966-SNAPSHOT`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>🔍 Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7258:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7258,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates com.google.apis:google-api-services-cloudkms from `v1-rev20230421-2.0.0` to `v1-rev20231012-2.0.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7279:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7279,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates com.google.apis:google-api-services-lifesciences from `v2beta-rev20220916-2.0.0` to `v2beta-rev20230707-2.0.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7280:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7280,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates mysql:mysql-connector-java from `8.0.28` to `8.0.33`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7299:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7299,10,"['Update', 'configurat', 'patch', 'update']","['Updates', 'configuration', 'patch', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.apache.commons:commons-text from `1.10.0` to `1.11.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-text"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-text"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7302:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7302,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-2147824`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7304:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7304,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-5781917`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9ac858c7e61f43ed3648f0fabc7104d0951cce67/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7331:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7331,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.broadinstitute.dsde.workbench:workbench-model from `0.15-f9f0d4c` to `0.19-8376167`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7305:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7305,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.broadinstitute.dsde.workbench:workbench-util from `0.6-65bba14` to `0.10-8376167`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7306:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7306,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.liquibase:liquibase-core from `4.8.0` to `4.25.0`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7313:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7313,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## About this PR; 📦 Updates org.webjars:swagger-ui from `4.5.2` to `4.19.1`. ## Usage; ✅ **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>⚙ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7323:20,Update,Updates,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7323,8,"['Update', 'configurat', 'update']","['Updates', 'configuration', 'update', 'updates']"
Deployability,"## Bug. I am trying to run a workflow using the GCP backend, however no matter what set of configurations I use, I am unable to have it succeed. The workflows Batch task appears to fail on the 3rd task, just after the `Setup Container`. This is basically causing every task to fail for some strange reason. ```; docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons.; ```. [This](https://cromwellhq.slack.com/archives/CGQ7WK5A6/p1697484861117659) thread suggested that the logic in [these two lines](https://github.com/broadinstitute/cromwell/blob/86/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/runnable/RunnableBuilder.scala#L63-L64) may be the culprit under specific condirtions. ## Information. Cromwell Version: 87-c9d4ce4; <!-- Which backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. task hello {. input {; String name; }; command <<<; echo 'hello ~{name}!'; >>>. output {; File response = stdout(); }. runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:91,configurat,configurations,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['configurat'],['configurations']
Deployability,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:152,pipeline,pipeline,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['pipeline'],['pipeline']
Deployability,"## Motivation; * Test longevity of Cromwell, to prove that it can continuously run at scale over multiple days.; * This is Green team ideally running 20k jobs/day okr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4794:66,continuous,continuously,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4794,1,['continuous'],['continuously']
Deployability,"## Today. Cromwell workers are deployed as a Terraform `instance,` most likely w/ a `count` of 1.*. ## What we want. ### # of instances. The `instance` stanza should reflect a minimum # of instances = `1`, max=`4`. The 1 represents the normal case where we want 1 Cromwell running, and only scales up when necessary. . ### Autoscaling. Should be set to `CPU Usage` where target == `50%`. Add an additional `group` stanza that creates a GCP instance group and refers to the Cromwell workers.*. *Raph Luckom made this presumption, the author does not have access to verify.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4798:31,deploy,deployed,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4798,1,['deploy'],['deployed']
Deployability,### Description. * Don't run E2E test automatically on a schedule; * Don't automatically update terra-helmfile when Cromwell PRs merge. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7524:89,update,update,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7524,6,"['Release', 'release', 'update']","['Release', 'release', 'update', 'updated']"
Deployability,"### Description. * Removed `includeTaskBreakdown` and `includeSubworkflowBreakdown` options to keep things simple for now - these aren't needed for our current work.; * Updated stub code to actually compute workflow cost from metadata events. Note that nothing is currently *creating* the metadata events, that's in progress in another branch. For my testing, I hard-coded some extra metadata entries for each task in `StandardCachingActorHelper.startMetadataKeyValues`. ; * Added subworkflow support. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7537:169,Update,Updated,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7537,6,"['Release', 'Update', 'release', 'update']","['Release', 'Updated', 'release', 'updated']"
Deployability,"### Description. - Define new workflow option for workflow outputs mode, move or copy; - Enhance Centaur to allow nested directories of `.test` files, which is required for the next step to be sane; - Relocate regression tests for existing copy behavior so they actually run; - Add test for new move mode; - Created new `centaur-ci-us-east1` bucket in `broad-dsde-cromwell-dev` and used it to replace `cloud-cromwell-dev-self-cleaning-fast` bucket. It was really hard to tell whether one was successfully testing a cross-region move. ### Release Notes Confirmation. The move mode is documented but is not ready for a release note because we are not updating metadata yet. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7472:538,Release,Release,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7472,6,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. - Ignoring this index results in empirically better performance; - If we were going to design an index from scratch for this table, it would not be this one. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7452:179,Release,Release,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7452,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. - Necessary but not sufficient to finish the story; - Breaking out into separate PR for clarity and to verify tests are still green. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7479:154,Release,Release,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7479,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. <!-- What is the purpose of this change? What should reviewers know? -->. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7456:95,Release,Release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7456,25,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. <!-- What is the purpose of this change? What should reviewers know? -->. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7445:95,Release,Release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7445,75,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. <!-- What is the purpose of this change? What should reviewers know? -->. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7467:95,Release,Release,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7467,10,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. <!-- What is the purpose of this change? What should reviewers know? -->. fixes #3201. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7469:108,Release,Release,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. <!-- What is the purpose of this change? What should reviewers know? -->; Fixes #7459; In brief, the `google_project` and `google_compute_service_account` workflow options were not working when using the GCP Batch backend. This PR restores this functionality (which was present in PAPI backend).; ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7460:318,Release,Release,318,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7460,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. <!-- What is the purpose of this change? What should reviewers know? -->; Jira: https://broadworkbench.atlassian.net/browse/WX-1675. A table to track when a group or billing project last ran into Cloud Quota exhaustion. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7487:241,Release,Release,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7487,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. <!-- What is the purpose of this change? What should reviewers know? -->; This change adds AWS ECR remote hashing support, which helps fix call caching for jobs using AWS ECR hosted containers. This change supports both private and public ECR registies. There were a couple non-standard behaviours from ECR which have been mitigated in this PR:; - Private ECR requires Basic authentication. The ability to override the authorization scheme for a particular registry was added.; - ECR does not return a `Docker-Content-Digest` header, so a fallback to calculate the image digest from the response body has been added.; - ECR supports images with no repository e.g. `123456790.dkr.ecr.eu-west-2.amazonaws.com/foo`. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7444:734,Release,Release,734,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7444,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Add a PR template to help us keep all our release notes up to date. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7437:59,release,release,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7437,6,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:1304,Release,Release,1304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. After using it in practice for a while, here's a small round of changes:; - Remove JDK. This was by far the largest by megabytes and the most fickle build process. It was really only there in case I wanted to use `jstack` as a backup if I couldn't connect YourKit; but we now have a [blessed procedure](https://docs.google.com/document/d/1bmlrM3lpNP2c1_wnm2TzQmvtbsid2g-ZEdx41LcsECw/edit) to run YourKit in any environment.; - Message-of-the-day on container login. Enhanced situational awareness to make sure you're on the container you want, and the container is running the version you think it is. Without the JDK, the image is 634 MB, only 16% larger than baseline at 547 MB. MOTD example:; ```; > kubectl exec -it -n terra-dev cromwell1-runner-76f7b5d5df-qpwdl -c cromwell1-runner-app -- bash; Version 88-6e242af-DEBUG built at 2024-05-21 18:07:36; root@cromwell1-runner-76f7b5d5df-qpwdl:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7443:925,Release,Release,925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7443,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. An exception that aggregates throwables that originate somewhere else pretty much by definition carries no value in its stack trace. It just causes clutter and looks ugly. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7484:193,Release,Release,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7484,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. As part of preparing for the fall 2024 audit, we were asked to fix the permissions on the various healthcheck buckets such as `gs://cromwell-ping-me-dev`. It would have taken some tinkering to make sure the permissions are secure enough _and_ the healthcheck still works, so I decided to drop the healthcheck. I am not aware of any times it's helped us and doesn't pass the ""would we add this today"" test. The only notable bucket we'd want to make sure Cromwell itself has permissions on is the workflow archiver - and it uses [a separate service account from the rest of Cromwell](https://github.com/broadinstitute/terra-helmfile/blob/master/charts/cromwell/templates/config/_cromwell.conf.tpl#L267-L271), so it's not a valid test. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7533:754,Release,Release,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Call caching prefixes and blacklist removed from GCP Batch backend. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7559:89,Release,Release,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7559,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Clear out types that were introduced for CWL and were less than obvious in their obsolescence. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7499:116,Release,Release,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7499,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Combined with https://github.com/broadinstitute/cromwell/pull/7462, re-applies upgrade to Java 17 as seen in https://github.com/broadinstitute/cromwell/pull/7342. . Requires these terra-helmfile changes: https://github.com/broadinstitute/terra-helmfile/pull/5721. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7470:96,upgrade,upgrade,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7470,6,"['Release', 'release', 'update', 'upgrade']","['Release', 'release', 'updated', 'upgrade']"
Deployability,### Description. Created a service that can fetch and cache the public cost catalog from Google. Provides a public `getSku` method which can be used to lookup a sku given certain runtime attributes. . #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7507:230,update,updated,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7507,4,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Creates `vmCostPerHour` metadata records for PAPIv2beta workflow tasks. Currently this cost includes only RAM and CPU, not GPU, disks, etc. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7550:161,Release,Release,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7550,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Creates an API endpoint to fetch cost data. This endpoint returns a correct workflow status but dummy cost data - real cost data will be handled in a future ticket. I deliberately excluded the Swagger update, because we don't want to advertise this endpoint yet. Example responses:. ```; $ http ""localhost:8000/api/workflows/v1/379b6915-e896-4fd4-a18e-53e58eb3694c/cost""; HTTP/1.1 200 OK; Content-Length: 165; Content-Type: application/json; Date: Wed, 21 Aug 2024 20:39:16 GMT. {; ""cost"": 3.5,; ""currency"": ""USD"",; ""id"": ""379b6915-e896-4fd4-a18e-53e58eb3694c"",; ""status"": ""Succeeded""; }. $ http ""localhost:8000/api/workflows/v1/379b6915-e896-4fd4-a18e-53e58eb3694c/cost?includeTaskBreakdown=true&includeSubworkflowBreakdown=true""; HTTP/1.1 200 OK; Content-Length: 165; Content-Type: application/json; Date: Wed, 21 Aug 2024 20:39:16 GMT. {; ""cost"": 3.5,; ""currency"": ""USD"",; ""id"": ""379b6915-e896-4fd4-a18e-53e58eb3694c"",; ""status"": ""Succeeded"",; ""subworkflowBreakdown"": {; ""foo.baz"": 3.5; },; ""taskBreakdown"": {; ""foo.bar"": 3.5; }; }; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7508:218,update,update,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7508,6,"['Release', 'release', 'update']","['Release', 'release', 'update', 'updated']"
Deployability,"### Description. Cromwell will now correctly emit nothing when concatenating a string with an empty optional, as outlined in the [WDL spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#prepending-a-string-to-an-optional-parameter). Before:; `""Something something ${""hello"" + myEmptyOption}` -> throws either `""Sorry! Operation + is not supported on empty optional values.""` or `""No implementation of FileEvaluator[StringExpression]""`. . After: ; `""Something something ${""hello"" + myEmptyOption}` correctly evaluates to the string `""Something something""`. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7471:578,Release,Release,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7471,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Currently, and as described in https://github.com/broadinstitute/cromwell/issues/7535, only general-purpose machine types are supported in Google Backend, which prevents running wdl workflows on many machine types available on GCP, including those provisioned with modern GPUs. I believe the simplest and most general solution would be to pass the machine type directly from the wdl configuration to the Google Batch API. The idea is that this approach would be more resilient to machine types being added or deprecated on GCP, as users would only need to update their wdl workflows in such cases. An alternative approach of mapping machine specs (e.g.: cpu platform and gpu requirements) to standard machine types would potentially introduce an additional layer of maintenance with little benefit. This PR adds support for a new standardMachineType key in the runtime section, which is only parsed for the Google backend. ### Testing. I deployed this internally and verified I can successfully run the following wdl workflow:. ```; version 1.0. task nvidia_smi {; input {; String docker_version; }. command <<<; nvidia-smi. touch .done; echo ""Finished at $(date)""; >>>. runtime {; docker: <internal image>; disks: ""local-disk 50 SSD""; memory: ""32G""; preemptible: 0; gpuCount: 1; gpuType: ""nvidia-tesla-a100""; standardMachineType: ""a2-highgpu-1g""; }. output {; File done = "".done""; }; }. workflow nvidia_smi_wf {; input {; String docker_version; }; ; call nvidia_smi as nvidia_smi_call {; input:; docker_version = docker_version; }. output {; File done = "".done""; }; }; ```. ### Next steps. - [ ] Confirm this approach is in the right direction with the cromwell team.; - [ ] Work on proper unit tests and get this PR ready to be merged. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545:400,configurat,configuration,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545,3,"['configurat', 'deploy', 'update']","['configuration', 'deployed', 'update']"
Deployability,### Description. Demonstrates successful pull and run of a public Docker image having a v1 manifest.; ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7522:106,Release,Release,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7522,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Docker image caching not implemented in the GCP Batch backend, delete all references to this copy/pasted in from PAPI v2 beta code and annotate Centaur tests appropriately. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7558:194,Release,Release,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7558,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Draft PR to see how CI is doing. Needs tests, docs. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7483:73,Release,Release,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7483,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Enables calling the workflow cost endpoint through CromIAM. Deliberately omitting Swagger change, that will be done in AN-145 once we are ready to advertise this feature. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7572:192,Release,Release,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7572,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Fix a couple of PAPI v2 Centaur test to actually run on the backends they suggested they were running on, add an IntelliJ run config for PAPI v2. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7511:167,Release,Release,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7511,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Fix scopes and the metadata endpoints used by Centaur tests. The scope fixes are required not just for GCSA tests here, but also for the USA tests and possibly any other test types that reach out to Google services. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7553:237,Release,Release,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7553,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Fixed now with scope changes for service accounts. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7555:72,Release,Release,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Fixes CI failures caused by errors like:; ```; $ docker pull quay.io/broadinstitute/cromwell-docker-test:centaur; centaur: Pulling from broadinstitute/cromwell-docker-test; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of quay.io/broadinstitute/cromwell-docker-test:centaur to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. The very old images need to be updated to get around this. For Python, we can use a newer version. For the Quay image we manage, using a different one because I fear push access to the current one has been lost in the mists of time. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [X] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [X] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:346,release,release,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,8,"['Release', 'release', 'update', 'upgrade']","['Release', 'release', 'updated', 'upgrade']"
Deployability,"### Description. Fixes job recovery on restart for GCP Batch, addresses #7495. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7498:85,Release,Release,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7498,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. GCP Batch requires mount points to go under `/mnt/disks`. Document and create some GCP Batch tests accordingly. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7534:133,Release,Release,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7534,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. GCP Batch versions of the VPC tests. Basically the same as their PAPI v2 counterparts with different backend names, different network and label names, and no subnetwork specifications. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7538:206,Release,Release,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7538,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. I found myself crafting the following query to make the logging more information-dense.; ```; ""Hog groups currently experiencing quota exhaustion""; AND ( NOT ""Hog groups currently experiencing quota exhaustion: 0"" ); ```. Before:. <img width=""876"" alt=""Screenshot 2024-09-26 at 15 47 10"" src=""https://github.com/user-attachments/assets/8475d8da-ae1b-439c-b8d8-be012cc627c7"">. After:. <img width=""876"" alt=""Screenshot 2024-09-26 at 15 43 40"" src=""https://github.com/user-attachments/assets/973e33d3-1f4d-4e90-afe7-86728598c30f"">. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7561:550,Release,Release,550,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7561,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. I'm not really sure what to do about the Spring issue. Even if I only end up using 17 locally to experiment, it would be handy to have this stuff checked in. Derived from https://github.com/broadinstitute/cromwell/pull/7344. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7462:246,Release,Release,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7462,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. In addition to turning off automatic update of Cromwhelm, adjusted names of GHA to make it clear what is happening and what is not. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7525:54,update,update,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7525,6,"['Release', 'release', 'update']","['Release', 'release', 'update', 'updated']"
Deployability,"### Description. Instead of pushing the logs file after the job completes, the logs are now streamed to GCS. This is how it works:; - Mount the main GCS bucket as a disk in the VM filesystem.; - Configure Batch to store the logs in the mounted disk.; - The log file belongs to the same path used by the task files. Notes:; - I haven't found a way to `tail` the GCS file but running `cat` continuously display the new logs.; - I'm not sure whether the logs are streamed live or when the runnable completes, if needed, I can evaluate this.; - There are some tricks I used to get this done, I'm open to suggestions for improving the approach.; - Follows up from #7491. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7529:388,continuous,continuously,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529,6,"['Release', 'continuous', 'release', 'update']","['Release', 'continuously', 'release', 'updated']"
Deployability,"### Description. Issue: Last week, Cromwell started logging stuff like `Status change from AwaitingCloudQuota to AwaitingCloudQuota`. This is noisy and not useful. . Cause: [This PR](https://github.com/broadinstitute/cromwell/pull/7527/files#diff-2679a7465ba7f1f56db9d588d0e64c3e4a4fe47b404299d7b7f18d66dccddfbf) changed the definitions of certain statuses, causing the `==` operator to not return true for statuses of the same type but with different lists of execution events. . Fix: Go back to the old behavior and rely only on the type of the run status, not what data it may contain. . #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7536:620,update,updated,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7536,4,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Jira: https://broadworkbench.atlassian.net/browse/WX-1835. Note: the scheduled logging will happen even if `quota-exhaustion-job-start-control` is disabled. This is because even if JobTokenDispenser doesn't account for quota exhausted groups, Cromwell is always recording which groups are in quota exhaustion. And scheduled logging will help easily see that list even if the feature `quota-exhaustion-job-start-control` is disabled. Example logs at different times:; ```; 2024-09-12 18:32:11 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - GroupMetricsActor configured to log groups experiencing quota exhaustion at interval of 5 minutes.; 2024-09-12 18:37:11 cromwell-system-akka.dispatchers.engine-dispatcher-58 INFO - Hog groups currently experiencing quota exhaustion: 3. Group IDs: [cromwell-dev, sshah-test-1, sshah-test-2].; ....; 2024-09-12 18:42:11 cromwell-system-akka.dispatchers.engine-dispatcher-71 INFO - Hog groups currently experiencing quota exhaustion: 1. Group IDs: [cromwell-dev].; ....; 2024-09-12 19:02:12 cromwell-system-akka.dispatchers.engine-dispatcher-60 INFO - Hog groups currently experiencing quota exhaustion: 0. Group IDs: [].; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7539:1199,Release,Release,1199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7539,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Jobs running in PAPI will now emit a `vmStartTime` metadata entry as soon as they start spending money. They will also emit a `vmEndTime` when they reach a terminal status. . Some incoming follow up branches:; - [ ] Add this same functionality to the GCP Batch backend. ; - [ ] Change the `vmCostUsd` key to be `vmCostPerHour` instead.; - [ ] Actually do the work to figure out the correct `vmCostPerHour`. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7527:428,Release,Release,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7527,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Links on the slurm doc page were missing .md and not resolving. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7447:85,Release,Release,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7447,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. LogsPolicy is now configurable. - When the ""logs-policy"" config entry is missing, ""CLOUD_LOGGING"" is set which is the default policy from Batch.; - When the ""logs-policy"" is set to ""PATH"", a ""task.log"" file is stored within the file system under the task files, this is later pushed to Google Cloud Storage. This is an exampel where ""task.log"" can be found:; - `gs://project-id/cromwell-execution-root/workflow-name/workflow-id/call-myTask/task.log` (workflow-id would be a UUID). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491:502,Release,Release,502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Minor fixes to publish correct monitoring log metadata as well as a GCP Batch ""alt"" Centaur test for a slightly changed metadata path. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7549:156,Release,Release,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7549,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Part 2 of https://github.com/broadinstitute/cromwell/pull/7432. Detects and retries the new fatal quota errors we've been seeing. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7439:151,Release,Release,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7439,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Rationalize the non-testing of boot disk sizes for GCP Batch. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7516:83,Release,Release,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7516,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Reference disks now working in the GCP Batch backend 🎉 . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502:78,Release,Release,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Remove unused config key. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7577:47,Release,Release,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7577,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Renamed some cost related metadata keys to be slightly less ambiguous. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7530:94,Release,Release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7530,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. Replaces https://github.com/broadinstitute/cromwell/pull/7460; Fixes https://github.com/broadinstitute/cromwell/issues/7459. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7466:146,Release,Release,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7466,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Replaces https://github.com/broadinstitute/cromwell/pull/7472. As I complained in Slack, I dislike a lot of the naming in this PR. Suggestions welcome. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7565:173,Release,Release,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7565,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes 🎉 ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:469,release,release,469,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,6,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Run config for developers to be easily able to exercise the GCP Batch backend locally. To use:. 1. Start a mysql container by running `processes/release_processes/scripts/start_publish_mysql_docker.sh`; 1. Run this config in IntelliJ: 'Repo template: Cromwell GCPBATCH server'. ### Release Notes Confirmation. Dev-only, no release notes.; #### `CHANGELOG.md`. - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7492:299,Release,Release,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7492,6,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. See Jira ticket for discussion of the problem and proposed solution. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7485:90,Release,Release,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7485,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Some code to demo the issues I'm seeing trying to use VPC in Google Batch. ""Borrows"" the commit from #7504 to fix one of the network issues I saw, but I have not yet been able to work around the subnetwork issue. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7505:234,Release,Release,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Testing with 1000 files, Montreal datacenter. `develop` – **1:24:00**; ```; 2024-05-21 16:03:23 cromwell-system-akka.dispatchers.backend-dispatcher-350 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(f03ed299)lots_of_inputs.make_array:NA:1]: Status change from Running to Success; 2024-05-21 17:27:23 cromwell-system-akka.dispatchers.engine-dispatcher-54 INFO - WorkflowExecutionActor-f03ed299-99cb-4adc-b152-687783914ab2 [UUID(f03ed299)]: Workflow lots_of_inputs complete. Final Outputs:; {; ""lots_of_inputs.size_kb"": 24.4609375; }; ```. This branch – **0:00:08**; ```; 2024-05-21 17:38:26 cromwell-system-akka.dispatchers.backend-dispatcher-93 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(63400d25)lots_of_inputs.make_array:NA:1]: Status change from Running to Success; 2024-05-21 17:38:34 cromwell-system-akka.dispatchers.engine-dispatcher-125 INFO - WorkflowExecutionActor-63400d25-4351-467a-a557-b467033ec990 [UUID(63400d25)]: Workflow lots_of_inputs complete. Final Outputs:; {; ""lots_of_inputs.size_kb"": 24.4609375; }; ```. No IO backpressuring at this scale, that only seems to start in the 10,000 file range. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7442:176,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7442,7,"['Pipeline', 'Release', 'release', 'update']","['PipelinesApiAsyncBackendJobExecutionActor', 'Release', 'release', 'updated']"
Deployability,"### Description. The Batch integration test suite runs with the Local and GCPBatch backends. This means that any cases tagged `Local` would have been picked up and (perhaps counterintuitively) run on the GCPBatch backend, because that's the default. We do have some extra cases that are not compatible with the Local backend for whatever reason, and weren't running on Batch. There's a good chance that many of them do work/should work on Batch, since it's similar to PAPI in a way that Local is not. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7440:27,integrat,integration,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7440,6,"['Release', 'integrat', 'release', 'update']","['Release', 'integration', 'release', 'updated']"
Deployability,"### Description. The PR exercises the ""retry with more memory"" Centaur tests on the GCP Batch backend. Minimal changes to production code, all of which are in the GCP Batch backend:. - The constant`RunnableUtils#MountPoint` was created with value `/mnt/disks/cromwell_root` and applied where appropriate.; - A copy/paste bug in code brought over from PAPIv2 was corrected (the `/cromwell_root` of PAPIv2 has become `/mnt/disks/cromwell_root` in Batch), using the constant described above.; - If a job fails, the *last* event message is now propagated rather than the first event message. The first event message is often a benign state transition, while the last event message is more likely to contain the actual reason for job failure.; ; Unfortunately Cromwell does not allow for dynamic backend selection (i.e. the backend name cannot be a variable), which necessitated copy/paste/renaming the Centaur test WDLs from their PAPIv2 versions, hence the magnitude of these diffs. The existing `preemptible_and_memory_retry ` Centaur test is heavily tailored to the quirks of Papi v2: a preemptible PAPI VM deletes itself and depends on the Lifesciences system mistaking that for a preemption event. tbh this is kind of a weird test and as I don't know how to induce a preemption on demand, I simply `ignore`d the GCPBATCH version. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:1335,Release,Release,1335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. The `google_legacy_machine_selection` workflow option that exists on PAPI v2 to pick JES-shaped machines was not completely wired in to the GCP Batch backend. However when I completed the wiring job and ran the `hello_google_legacy_machine_selection` Centaur test I got this:. ```; Task wf_hello.hello:NA:1 failed: Job failed when Batch tries to schedule it:; Batch Error: code - CODE_MACHINE_TYPE_NOT_FOUND, description - ; machine type predefined-1-2048 for job job-xyz, project 8675309, region us-central1, zones (if any) us-central1-b is not available.; ```. So basically `google_legacy_machine_selection` does not seem to work on GCP Batch. If anyone is using this undocumented feature to emulate the behavior of JES from many years ago, we should let them know that this support is going to be dropped when GCP Batch is rolled out. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7547:861,Release,Release,861,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7547,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. The current Centaur backend selection mode within a *.test file is 'all', which means if a given backend does not have all the backends enumerated in the backend specification then the test will be ignored. IMHO this is a dangerous default behavior because it can cause tests to be unexpectedly ignored, creating a false sense of confidence in the correctness of the code that was intended to be tested. These changes make the default backend selection mode 'any'. The failure mode here is that a test may run on backends that the test author did not intend, but if this happens that should be a noisy failure that the test author cannot ignore and must correct. Test authors can always explicitly override the `backendsMode` in a Centaur .test file if they want. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7513:785,Release,Release,785,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7513,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. The fix we put in place for this test last week was insufficiently fixy. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7454:94,Release,Release,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7454,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. This PR refactors & renames the `CostCatalogHelper` into the `PollResultMonitorActor`. Doing this allows the helper to asynchronously communicate with the `CostCatalogService`, which it needs to do in order to calculate a VM Cost Per Hour. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7544:263,Release,Release,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7544,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. This should have no effect on our existing Bard usage, just removes errors logged when NOT using Bard. * Base config was incorrect, so Bard was not registered in the Service Registry by default; * `BardEventingActor.receive` had no handling for receiving a `BardEvent` when eventing was disabled... and also nothing has ever checked for enablement before creating and sending these events. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7566:411,Release,Release,411,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7566,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. This upgrades us to a non-vulnerable version of Spring Web. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7497:22,upgrade,upgrades,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7497,6,"['Release', 'release', 'update', 'upgrade']","['Release', 'release', 'updated', 'upgrades']"
Deployability,### Description. Turn on 90ish Centaur tests for GCPBATCH. In all but one case this was just adding the GCPBATCH backend to the Centaur .test file. The one exception involved different error message text coming from the Batch system than what we get from Lifesciences. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7496:273,Release,Release,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7496,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. Turning on all the Papiv2 tests for GCPBATCH that haven't already been turned on and sorting out what happens. Per [this description](https://github.com/broadinstitute/cromwell/blob/3bc906cb8c3e617097cd045aa9b11c2dd5e8ac74/src/ci/resources/gcp_batch_shared_application.inc.conf), either the real GCPBATCH backend is specified which causes the test to run in the GCPBATCH test in Cromwell CI, or one of these nonexistent ""excuse"" GCPBATCH_BLAH backends is specified, sometimes with a comment:. ```; # pseudo GCPBATCH backends referenced in Centaur test files:; # GCPBATCH_ALT: a GCPBATCH version of a PAPI v2 test exists, usually having the same name as the PAPI v2 test file with a ""gcpbatch_"" prefix.; # GCPBATCH_NEEDS_ALT: a GCPBATCH alt of a PAPI v2 test is needed but does not yet exist.; # GCPBATCH_FAIL: a test is failing on GCPBATCH, reasons may or may not be understood yet.; # GCPBATCH_SKIP: test is not going to be run on GCPBATCH for reasons explained in test comments.; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7519:1008,Release,Release,1008,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7519,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:17,UPDATE,UPDATE,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,2,"['UPDATE', 'update']","['UPDATE', 'updated']"
Deployability,### Description. Update Bard client to pull in updated Spring Web version that patches vulnerability. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7468:17,Update,Update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7468,8,"['Release', 'Update', 'patch', 'release', 'update']","['Release', 'Update', 'patches', 'release', 'updated']"
Deployability,### Description. Update code owners so review will be required from new Github team. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7570:17,Update,Update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7570,6,"['Release', 'Update', 'release', 'update']","['Release', 'Update', 'release', 'updated']"
Deployability,"### Description. Update: DO NOT MERGE, per 2024-09-27 sync meeting Google and Burwood are going to revisit a streaming file-based solution that includes localization, task logs, and delocalization. Task log streaming for GCP Batch. Note this is not a perfect substitute for PAPI v2-style task logs as it does not include output from container setup, localization or delocalization. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7540:17,Update,Update,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7540,6,"['Release', 'Update', 'release', 'update']","['Release', 'Update', 'release', 'updated']"
Deployability,### Description. Uses `GcpBatchMachineConstraints#machineType` to choose the appropriate machine type for GCP Batch. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7518:121,Release,Release,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7518,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. WIP GCP Batch Labels tests. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7523:49,Release,Release,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7523,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. We make this query constantly, looks like the single most frequent one against metadata. All it does is check whether the workflow ID is valid by checking whether >1 metadatum exists for it. We already started down the path of checking summary instead of metadata, see https://github.com/broadinstitute/cromwell/pull/4617. It just makes way more sense to me to check a table with 77M rows than 36B. ```; select ; exists(; select ; `CALL_FQN`, ; `METADATA_KEY`, ; `WORKFLOW_EXECUTION_UUID`, ; `METADATA_TIMESTAMP`, ; `JOB_SCATTER_INDEX`, ; `METADATA_JOURNAL_ID`, ; `JOB_RETRY_ATTEMPT`, ; `METADATA_VALUE_TYPE`, ; `METADATA_VALUE` ; from ; `METADATA_ENTRY` ; where ; `WORKFLOW_EXECUTION_UUID` = '602a4913-d666-4182-b2f1-242fbda817d2'; );; ```. It is potentially implicated in the 2021 database migration that failed at the very end, you can see a bunch of them in this screenshot (2021-11-09):. <img width=""1792"" alt=""Screen Shot 2021-11-09 at 1 14 03 AM"" src=""https://github.com/user-attachments/assets/d3eee3da-9636-4406-a6f9-9862d33cd650"">. ```; Lock wait timeout exceeded; try restarting transaction; [for Statement ""RENAME TABLE `cromwell`.`METADATA_ENTRY` TO `cromwell`.`_METADATA_ENTRY_old`,; `cromwell`.`_METADATA_ENTRY_new` TO `cromwell`.`METADATA_ENTRY`""]; at /usr/bin/pt-online-schema-change line 10922.; ```; [Slack link to contemporary discussion.](https://broadinstitute.slack.com/archives/C02LCC8968N/p1636439602084200); [Contemporary analysis in JIRA.](https://broadworkbench.atlassian.net/browse/WM-906?focusedCommentId=53394). ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7575:1564,Release,Release,1564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:1214,Release,Release,1214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. When Cromwell restarts during a failure, it must fail the 'next' upcoming tasks in order to cleanly terminate the workflow. During this process, it was logging an error that isn't really an error (and was very confusing to users). . This PR: ; - Changes the failure reason to something more relevant.; - Removes the failure reason from the 'Workflow' level failures, since it was not the reason the workflow failed. The failure reason is still attached to the task that never ran. . ### Release Notes Confirmation; #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [x] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7449:504,Release,Release,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7449,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. When evaluating a list of file sizes to compute its sum, perform the IO requests in parallel instead of in sequence. This prevents instances from asplode. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7438:176,Release,Release,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7438,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. While it's true that nano units are nine-y, when doing exponential math stuff we need to remember that the 10 contributes one order of magnitude. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7576:167,Release,Release,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7576,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"### Description. While testing on Dev, I discovered that the Wom traversal in `moveOrIdentity` is woefully inadequate. We need to support structs, maps, pairs, etc. to call this feature complete. In researching how to address this, I discovered `womValueToMetadataEvents` which seems like a much better pre-existing piece of code that already traverses all Wom types. After [cleaning it up](https://github.com/broadinstitute/cromwell/pull/7499/files#diff-854b47290dea5287619fbe2c8cbcc3db06552b4a84d3456552e261efd086ad8cL246-L266) in https://github.com/broadinstitute/cromwell/pull/7499, this PR enhances it with file location mapping. The Centaur test verifies mapping of a file in a map in a pair in a struct. Recursion!. ```; struct FooStruct {; Int simple; Pair[Array[Int], Map[String, File]] complex; }; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7509:817,Release,Release,817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7509,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description. https://broadworkbench.atlassian.net/browse/ID-1276. This pr adds azure task cpu start and end time metrics for bard events. I referenced this merged pr to figure out how to get start and end time: https://github.com/broadinstitute/cromwell/pull/7415. terra helmfile pr:; https://github.com/broadinstitute/terra-helmfile/pull/5674. leo pr:; https://github.com/DataBiosphere/leonardo/pull/4664. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7453:415,Release,Release,415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7453,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description; Change in Google Cloud Batch Backend VPC configuration attributes to remove trailing slash in network subnet address. Google Cloud Backend no longer supports a trailing slash in the URL. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7504:58,configurat,configuration,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7504,6,"['Release', 'configurat', 'release', 'update']","['Release', 'configuration', 'release', 'updated']"
Deployability,"### Description; Google has shut down their Genomics (a.k.a. PAPI v2Alpha1) API, this PR cleans up associated code. This is not to be confused with the Cloud Life Sciences API (a.k.a. PAPI v2beta, deprecated, but still in use for the next few months) or the Google Batch backend (the new hotness). . #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7532:329,update,updated,329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7532,4,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,### Description; Updated a few dependencies that we need in order to use the Cloud Billing SDK. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7490:17,Update,Updated,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7490,5,"['Release', 'Update', 'release', 'update']","['Release', 'Updated', 'release', 'updated']"
Deployability,"### Possible Workaround. Try using the `default` AWS auth scheme along with the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) setup on the system / environment. ### Background. Starting in cromwell 37, the AWS S3 SDK was upgraded from `2.0.0-preview-9` to `2.3.9`. Along with this upgrade several cromwell calls to the S3 SDK were updated to match changes in the library. Post upgrade, the existing Cromwell AWS CI tests continue to pass, however there have been reports of permissions problems or other errors.; - https://gatkforums.broadinstitute.org/firecloud/discussion/comment/56245/#Comment_56245; - https://github.com/broadinstitute/cromwell/issues/4541; - https://github.com/broadinstitute/cromwell/issues/4686; - https://github.com/broadinstitute/cromwell/issues/4731. Because the CI tests are passing, and they use default credentials, it is possible that each of these issues may be also be worked around by using the [AWS default credentials](https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/credentials.html#credentials-default) scheme. It is unclear how the very similar SDK calls worked with `2.0.0-preview-9` and not `2.3.9`. However the fix might include passing in credentials via the `Map env` / `Properties props`. | Type | Cromwell copy targeting `2.3.9` | ""Original"" targeting `2.0.0-preview-9` |; |----------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|; | (Attempted) setting of key/secret from props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:299,upgrade,upgraded,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,4,"['update', 'upgrade']","['updated', 'upgrade', 'upgraded']"
Deployability,"### Wordy Description. * We will no longer update the womtool libraries in Agora and Rawls. We will accept the minor drift which may occur between ""what Rawls thinks Cromwell could run"" and ""what Cromwell can actually run"" until womtool as a service is adopted.; * We will no longer run smoke tests before and after each update. We will rely on swatomation and daily runs to detect problems. ### Current Process:; ![Current Process](https://github.com/broadinstitute/cromwell/blob/develop/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed New Process:; ![Proposed New Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop.dot.png?raw=true). ### Proposed Hotfix Process:; ![Proposed Hotfix Process](https://github.com/broadinstitute/cromwell/blob/cjl_simplify_releases/scripts/release_processes/firecloud-develop-hotfix.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4941:43,update,update,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4941,5,"['Hotfix', 'hotfix', 'update']","['Hotfix', 'hotfix', 'update']"
Deployability,"#### Background. For background information see [this design doc](https://docs.google.com/document/d/1QqXuURg1HlwAymaQkwCL6v9HA5NWsp1pbribvGafNt0/edit?ts=5c6c875c#heading=h.56f418pyjwq8). #### Concept. * Allow Cromwell to spin up PAPIv2 jobs on high security networks ; * At project-creation time, scripts may create a high-security network for that project and record the network name in the project metadata.; * If these fields exist, Cromwell should honor them. #### Proposal. * Use a key in Cromwell's configuration to locate the appropriate project metadata; * For example, perhaps: ; ```; backend {; providers {; PAPIv2 {; config {; backend.providers.PAPIv2.config.vpc {; name: ""terra-network""; subnetwork: ""terra-subnetwork""; }; }; }; }; }; ```; * When about to submit a job to PAPI, see whether the `name` field exists in the configuration.; * If so, check whether the specified label exists in the Google project, eg:; ```; $ gcloud projects describe my-fc-project. createTime: '2017-07-07T17:07:10.345Z'; labels:; terra-network: firecloud; terra-subnetwork: firecloud; lifecycleState: ACTIVE; name: my-fc-project; ```; * If so, deduce a `NETWORK_PATH` by combining the `GOOGLE_PROJECT_ID` (from workflow options) and `NETWORK_NAME` (the label value) as: `projects/GOOGLE_PROJECT_ID/global/networks/NETWORK_NAME`; * Include this in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; }; ```; * If both the `name` and `subnetwork` fields are defined in configuration, and both exist as project labels:; * The `SUBNETWORK` is the raw value of the `subnetwork` label in the google project; * We additionally provide the subnetwork in the PAPI request:; ```; pipeline.resources.virtualMachine.network: {; name: NETWORK_PATH; subnetwork: SUBNETWORK; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4806:506,configurat,configuration,506,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4806,5,"['configurat', 'pipeline']","['configuration', 'pipeline']"
Deployability,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:22,Update,Updates,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,4,"['UPDATE', 'Update', 'rolling']","['UPDATE', 'Updates', 'rolling']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. ![Screen Shot 2021-12-01 at 4 39 47 PM](https://user-images.githubusercontent.com/4966343/144191887-75590326-1edb-442d-b2eb-ffb04968a964.png); <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Local. <!-- Paste/Attach your workflow if possible: -->; WholeGenomeGermlineSingleSample_develop 3.0.0; https://github.com/broadinstitute/warp/releases/tag/WholeGenomeGermlineSingleSample_develop. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell.conf.zip](https://github.com/broadinstitute/cromwell/files/7631795/cromwell.conf.zip). $ java -jar -Dconfig.file=cromwell.conf cromwell-71.jar server; $ curl -X POST --header ""Accept: application/json"" -v ""0.0.0.0:8000/api/workflows/v1"" -F ""workflowSource=@WholeGenomeGermlineSingleSample_develop.wdl"" -F ""workflowInputs=@WholeGenomeGermlineSingleSample_develop.inputs.local.json"" -F ""workflowDependencies=@WholeGenomeGermlineSingleSample_develop.zip""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6582:1283,release,releases,1283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6582,2,"['configurat', 'release']","['configuration', 'releases']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--. Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We want to submit workflow pipelines on GCP on specified Custom machine type, such as E2, N1, N2, n1-standard-8, etc. can you please support that?; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6217:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6217,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6459:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6459,5,['configurat'],['configuration']
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi, ; Can I ask a question? I think it's related with ; https://github.com/broadinstitute/cromwell/issues/4212. I found the `job_name` is the UUID and it's assigned with subworkflow's UUID if there is a subworflow. What I would like to ask is if there is any other system variables that store the main UUID. As we have our own backend implementation, we need to pass the main UUID to the backend. . Thanks, ; Seung",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6005:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6005,1,['configurat'],['configuration']
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. hello, ; Support for wdl step-by-step?; After executing a step, wait for the command to execute the next step.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5537:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5537,1,['configurat'],['configuration']
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Cromwell version is 55; We had submitted workflow pipelines on GCP PAPIv2 (https://genomics.googleapis.com/), in WDL file, set cpu: ""4"", memory: ""48 GB"", the actual VM created as custom (8 vCPUs, 48 GB memory), for ""memory"": ""64 GB"" ""cpu"": ""8"", the actual VM created as (10 vCPU, 64 GB), Cromwell did not created VM as configured in WDL. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6216:1063,configurat,configuration,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6216,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1135,configurat,configuration,1135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,4,"['Pipeline', 'configurat', 'pipeline']","['PipelinesApiRequestHandler', 'PipelinesApiRequestWorker', 'configuration', 'pipelines']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1022,pipeline,pipelines,1022,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. Cromwell lacks support for Shared VPC setup in GCP. Shared VPC model is quite common to large enterprises. Searching for the support I came across the pull request https://github.com/broadinstitute/cromwell/pull/6225 The code changes in this pull request seems to address the shared vpc support. What are the plans to get this on the roadmap for upcoming versions?. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. GCP. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6443:1434,configurat,configuration,1434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6443,1,['configurat'],['configuration']
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. Hi there,; I'm running Cromwell on a SLURM compute node, so I have enough RAM for the workflow database. Cromwell is used to coordinate this workflow: https://github.com/gatk-workflows/gatk4-somatic-snvs-indels/blob/master/mutect2.wdl on google cloud. When I scancel the SLURM job the google api continues to create VM instances even though the Cromwell job has been killed. How can this be prevented?. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5380:1466,configurat,configuration,1466,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5380,1,['configurat'],['configuration']
Deployability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###; The early release of Cromwell had a script that included some steps of executions and the docker run commands. Currently we are using Cromwell release 52, that script or similar script is not found, for reproducible purpose, our users want to know the actual commands, for example, if three runtime attributes are supplied: gpuType, gpuCount and nvidiaDriverVersion, what is the command line of docker run after NVIDIA driver installed?. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5792:236,release,release,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5792,4,"['configurat', 'install', 'release']","['configuration', 'installed', 'release']"
Deployability,"#4172 restored expected metadata generation of file names. A/C for this ticket would add a regression test for the functionality. The unit tests should check that the path generation, including paths sent to metadata, are ""hardcoded"" to the current expected paths. This will ensure that upon an upgrade of cromwell that running jobs will still find their outputs. TBD: This is probably something that should be addressed for _all_ backends that can resume jobs on startup. If these log paths change, the backend may not be able to delocalize or find paths on startup. An change to path generation should deprecate, not replace, the old path generation functionality. Partially related to #4187",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4188:295,upgrade,upgrade,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4188,1,['upgrade'],['upgrade']
Deployability,"#4490 introduces a couple of new builds to confirm the PAPI v1 => v2 upgrade plan will work as intended. But it's not clear what the fate of these changes should be post-upgrade. The code as written now is specific to this one-time upgrade so some choice will need to be made, whether it's trying to model upgrade scenarios in different environments or just deleting it altogether.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4513:69,upgrade,upgrade,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4513,4,['upgrade'],['upgrade']
Deployability,"#4989 continues to evolve the ~~release~~ publish WDL by running the commands via Docker. We should also add a nightly test that checks if the WDL and upcoming publish will still work. The test would be nightly because it would need to write to the ""Releases"" page of a GitHub repo. At the moment there are APIs for [creating repository forks](https://developer.github.com/v3/repos/forks/), but not for deleting / resetting forks. So instead, a single GitHub organization may be created (`broadinstitute-publish-test`?) that will contain a ""standing"" fork of `cromwell` and `homebrew-core`. The test plan is:. Setup:; - Delete all ""Releases"" from `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/cromwell`; - Force sync branches and tags to `broadinstitute-publish-test/homebrew-core`; - Get the latest version number from `broadinstitute/cromwell` and copy it to a new ""Release"" on `broadinstitute-publish-test/cromwell` (A release with empty text and no artifacts is ok! Just needs to exist w/ the version number.). Run test:; - Run the WDL for a major-release with organization `broadinstitute-publish-test`; - Run the WDL for a minor-release with organization `broadinstitute-publish-test`. Verify:; - Ensure the `broadinstitute-publish-test/cromwell` major-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the `broadinstitute-publish-test/cromwell` minor-release exists; - Check both the cromwell and womtool artifacts are attached; - For now don't check release notes; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` major-release exists; - Ensure the PR for `broadinstitute-publish-test/homebrew-core` minor-release exists. Alternative:; - Two separate nightly jobs:; 1. Run major release on `develop`; 2. Run minor release on `<latest>_hotfix`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4994:32,release,release,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4994,15,"['Release', 'release']","['Release', 'Releases', 'release']"
Deployability,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:4765,pipeline,pipelines,4765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,3,"['Pipeline', 'pipeline']","['PipelinesApiRequestHandler', 'PipelinesApiRequestManager', 'pipelines']"
Deployability,"'' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1749,update,update,1749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['update'],['update']
Deployability,'/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1648,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"(CopyWorkflowOutputsActor.scala:28); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7196,pipeline,pipeline,7196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"(No links to failed builds, as clicking Travis ""Retry Build"" erased the logs.). Travis has been having network errors recently attempting to download artifacts from maven central, and in at least one case downloading the ubuntu:latest docker image. In the various scripts under src/bin/travis, the scripts should retry:. - `sbt update` before any other sbt commands; - `docker pull ubuntu:latest`; - possibly: `sudo apt-get …`. The retries should wait some time (60 seconds?) between failures, and try at least twice? Possibly this could be put into a utility bash script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791:328,update,update,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791,1,['update'],['update']
Deployability,"(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:133); at cats.effect.internals.Callback$AsyncIdempotentCallback.apply(Callback.scala:120); at cats.effect.Async$$anon$1.run(Async.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); [2019-02-11 10:13:27,63] [info] Message [cromwell.docker.DockerInfoActor$DockerInfoFailedResponse] from Actor[akka://cromwell-system/user/HealthMonitorDockerHashActor#-638598959] to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/deadLetters]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2019-02-11 10:13:27,65] [info] WorkflowExecutionActor-52999e15-953f-44d6-aaae-1774c74d2910 [52999e15]: Workflow test1 complete. Final Outputs:; {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; }; [2019-02-11 10:13:27,69] [info] WorkflowManagerActor WorkflowActor-52999e15-953f-44d6-aaae-1774c74d2910 is in a terminal state: WorkflowSucceededState; [2019-02-11 10:13:35,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""test1.hello.out"": ""/spin1/users/wresch/test_data/cromwell/test1/cromwell-executions/test1/52999e15-953f-44d6-aaae-1774c74d2910/call-hello/execution/World.txt""; },; ""id"": ""52999e15-953f-44d6-aaae-1774c74d2910""; }; [2019-02-11 10:13:36,30] [info] Workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:14272,configurat,configuration,14272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['configurat'],['configuration']
Deployability,(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:1920,update,update,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,"(cromwell version 35-5f86a05-SNAP). call caching version from previous run . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330:786,configurat,configuration,786,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330,1,['configurat'],['configuration']
Deployability,"(during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13677,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,13677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986:2330,configurat,configuration,2330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986,1,['configurat'],['configuration']
Deployability,): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:6663,pipeline,pipelines,6663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at cromwell.engine.backend.jes.Pipeline$.createPipeline$1(Pipeline.scala:43); at cromwell.engine.backend.jes.Pipeline$.apply(Pipeline.scala:59); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$attemptToCreateJesRun$1(JesBackend.scala:563); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.util.TryUtil$$anonfun$5.apply(TryUtil.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:79); at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:706); at scala.util.Success.flatMap(Try.scala:231); at cromwell.engine.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:3830,Pipeline,Pipeline,3830,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:2332,pipeline,pipeline,2332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['pipeline'],['pipeline']
Deployability,"* Updated to reflect the current status of the [TES schema](https://github.com/ga4gh/task-execution-schemas); * Adds gcsPathBuilder; * Removed cpu, ram, and disk default values so that the TES implementation can handle defaults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2293:2,Update,Updated,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2293,1,['Update'],['Updated']
Deployability,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3014:2,Update,Updates,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014,4,"['Update', 'integrat', 'release']","['Updates', 'integration', 'releases']"
Deployability,"* Upgrade from deprecated `trusty` dist to `xenial` (needed to get psutil 5.6.4 working); * Switch from Oracle JDK to OpenJDK (needed for the change above, xenial only supports Java 9 to 14); * Fix mistakes and deprecations in `mkdocs.yml` since the configuration of the `xenial` image treats `mkdocs` warnings as errors which fail the `checkPublish` build; * Don't explicitly start `munged` for the SLURM build since it seems to already be started in `xenial`. The `munged` bit would especially benefit from @kshakir 's input, I'm pretty sure that could be done better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5262:2,Upgrade,Upgrade,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5262,2,"['Upgrade', 'configurat']","['Upgrade', 'configuration']"
Deployability,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1710:41,configurat,configuration,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710,1,['configurat'],['configuration']
Deployability,"* disabled build failures due to deprecation warnings... it's a hotfix, deadend branch!; * support public http-based imports in cromwell; * fixed a few necessary classes due to newer cats being pulled in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2734:64,hotfix,hotfix,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2734,1,['hotfix'],['hotfix']
Deployability,"**As a first step -- please confirm that this issue still exists**. It seems that some inputs are being copied over twice to the same path for all tasks. I'm not exactly sure when this started happening but the cromwell git hash we are currently using is https://github.com/broadinstitute/cromwell/tree/c66eabc3582085e28b197b667cb82b241ab6d1dd . Logs in comment. In this example the input_bam and interval_list are listed twice as inputs.; jes operations ID for the following call : operations/EI3Pz-W1KhiNs_ydkKOavrwBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; 295959:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JesBackend UUID(a7884b2d):HaplotypeCaller:12: Starting call with pre-emptible VM; 295960:2016-03-09 18:46:47,375 cromwell-system-akka.actor.default-dispatcher-12 INFO - JES Pipeline UUID(a7884b2d):HaplotypeCaller:12: Inputs:; 295961: input_bam-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295964: 2eb61371-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bam; 295965: e1220deb-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295966: interval_list-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-ScatterIntervalList/glob-cb4648beeaff920acb03de7603c06f98/20scattered.interval_list; 295968: input_bam_index-0 -> disk:local-disk relpath:broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/a7884b2d-e859-4ef3-bb74-b820623755b4/call-GatherBamFiles/M1132.bai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/576:812,Pipeline,Pipeline,812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/576,1,['Pipeline'],['Pipeline']
Deployability,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:576,Install,Installed,576,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,1,['Install'],['Installed']
Deployability,"**Command**:; `sudo java -jar cromwell-84.jar run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. **Platform**:; Ubuntu 20.04 via WSL2 on Windows 10 pro. **Java**:; openjdk 11.0.17 2022-10-18; OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu220.04); OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu220.04, mixed mode, sharing). **Docker**:; Docker version 20.10.22, build 3a2c30b; Docker desktop v.4.16.3. **Inputs & Options JSON:**; Please find attached in the zip.; [forgithub.zip](https://github.com/broadinstitute/cromwell/files/10610687/forgithub.zip). **Workflow & Error**:; I am running the main workflow (WDL attached) named ""local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl"". It first successfully finishes the first task from ""BamProcessing.wdl"" (WDL attached) specifically running the task ""GenerateSubsettedContaminationResources"" via docker: ""us.gcr.io/broad-gotc-prod/bedtools:2.27.1"". In the next step it starts the next task from ""paired-fastq-to-unmapped-bam.wdl"" (WDL attached) via docker: ""broadinstitute/gatk:latest"". Inspection of the container's log during this step shown below:. ```; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/out.1': Operation not supported; 2023-02-05 12:55:43 mkfifo: cannot create fifo '/cromwell-executions/ExomeGermlineSingleSample/9053ae04-ca8c-4d23-848d-7a04313af725/call-ConvertPairedFastQsToUnmappedBamWf/ConvertPairedFastQsToUnmappedBamWf/c19284af-355b-4a83-bc7d-5fed437ea8e7/call-PairedFastQsToUnmappedBAM/tmp.8cbe1f4a/err.1': Operation not supported; 2023-02-05 12:55:4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7002:81,pipeline,pipelines,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002,1,['pipeline'],['pipelines']
Deployability,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589:505,update,updates,505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589,1,['update'],['updates']
Deployability,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4094:57,release,release,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094,1,['release'],['release']
Deployability,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:595,configurat,configuration,595,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,1,['configurat'],['configuration']
Deployability,", ""command_mem"": small_task_mem * 1000 - 500,; ""disk"": small_task_disk, ""boot_disk_size"": boot_disk_size}. scatter (normal_bam in zip(normal_bams, normal_bais)) {; call m2.Mutect2 {; input:; intervals = intervals,; ref_fasta = ref_fasta,; ref_fai = ref_fai,; ref_dict = ref_dict,; tumor_reads = normal_bam.left,; tumor_reads_index = normal_bam.right,; scatter_count = scatter_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:4880,configurat,configuration,4880,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['configurat'],['configuration']
Deployability,"- Add load status logging where previously there was none for `PipelinesApiRequestManager.scala`; - Add high load logging to `IOActor`, which previously only had [back-to-normal logging](https://github.com/broadinstitute/cromwell/compare/develop...aen_wx_1333#diff-0be95c10972997df38906d44327436c1149e0c1a3df513bb49a43b9916ecd505R212); - Add load logging to `ServiceRegistryActor` which collects the load messages from their various sources and routes them to the sinks like `JobTokenDispenserActor`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7253:63,Pipeline,PipelinesApiRequestManager,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7253,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,"- Added comment that WDL can only handle increasing version numbers; - Sort through the first page of releases instead of using the latest-release-by-date; - Exit WDL commands that contain unset variables or have pipe failures (set -uo pipefail); - Exit WDL commands on the first error (set -e); - Log WDL commands verbosely as they run (set -x); - Replaced usages of docker/python runtimes with brew'ed jq; - Remove call to sbt test from minor releases, thus operating like major releases; - Made the WDL input ""organization"" mandatory instead of optional; - Copy release notes for major releases from develop instead of master; - Copy release notes for minor releases from hotfix branches; - Pointed to correct homebrew pull request template; - Added additional homebrew test as required in the homebrew pull request template; - Fail the WDL call/workflow if any of homebrew's build/test/verify tasks fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4744:102,release,releases,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4744,9,"['hotfix', 'release']","['hotfix', 'release', 'release-by-date', 'releases']"
Deployability,"- Added functionality to Cromwell so that a sas token can be provided as an environment variable to TES tasks ; - Added some functionality to BlobPath to help with WSM stuff; - Acknowledging that adding Terra specific stuff to a generic Azure concept isn't ideal. This seems like the least invasive way of getting the required data, since it allows us to take advantage of the already instantiated filesystems + their configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7241:418,configurat,configuration,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7241,1,['configurat'],['configuration']
Deployability,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:395,configurat,configuration,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['configurat'],['configuration']
Deployability,"- Backend: AWS; - Cromwell: 36. When running certain highly parallel WDL workflows, I'm getting the error `cromwell.core.CromwellFatalException: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:912,pipeline,pipeline,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"- Better localization and delocalization of directories in PAPI2 using hidden files to cover for empty directories; - IWDR localization is not baked in the CWL code anymore but left to the backend. This allows for the PAPI backend to opt out of it since localization is done directly on the VM.; - ~~Use configurable `job-shell` instead of hardcoded `/bin/bash`~~ It fixes 117 but also makes a bunch of centaur tests fail, so leaving as is for now.; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known. This only partially covers the possible cases. It needs a deeper tech talk discussion. This is orthogonal to the above and only here to avoid a later rebase (the files changed overlap with the refactoring mentioned).; - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3697:462,Pipeline,Pipelines,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3697,1,['Pipeline'],['Pipelines']
Deployability,"- Combines the hotfix and regular release graphs into a single diagram; - Removes the redundant ""re-run swatomation"" step from the end of the release process; - Add the creation of a new ""work in progress version"" PR to firecloud-develop. Rendered Image: . ![](https://github.com/broadinstitute/cromwell/blob/cjl_release_process_fixup/scripts/release_processes/firecloud-develop.dot.png?raw=true)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4962:15,hotfix,hotfix,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4962,3,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1289:383,update,updated,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289,1,['update'],['updated']
Deployability,- Enables 121 on PAPIv2; - Refactors Pipelines conversions in v2 (w/ typeclasses !); - Partially replaces the custom `MemorySize` with [squants](https://github.com/typelevel/squants); - Turns the CPU runtime validation from an `Int` to a `Int Refined Positive`; - Automatically fits the resources requirements in the task to the [GCE constraints](https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type#specifications); - Allow for lazy evaluation of file and directory literals so that they can be written when the backend and the appropriate IoFunctions are known,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3694:37,Pipeline,Pipelines,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3694,1,['Pipeline'],['Pipelines']
Deployability,- GcpBatchAsyncBackendJobExecutionActor -> pollBackOff had maxInterval value hardcoded instead of using the config entry.; - GcpBatchTestConfig was still referencing papi instead of batch.; - Rename PipelinesApiEmptyMountedDisk to BatchApiEmptyMountedDisk.; - GcpBatchAsyncBackendJobExecutionActorSpec was still referencing pipelines instead of batch.; - Localization was referencing papi instead of batch.; - RunnableUtils had unused definitions which are now deleted.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7428:199,Pipeline,PipelinesApiEmptyMountedDisk,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7428,2,"['Pipeline', 'pipeline']","['PipelinesApiEmptyMountedDisk', 'pipelines']"
Deployability,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754:127,release,release,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754,2,['release'],['release']
Deployability,- Makes carboniting and metadata deletion a singleton activity in our centaur testing framework.; - Turns on metadata carboniting and deletion assertions in horicromtal deployments,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5481:169,deploy,deployments,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5481,1,['deploy'],['deployments']
Deployability,"- No frills github action that either passes, or fails with a list of files that need to be fixed. ; - Formatted `ContinuousIntegration.scala` since that slipped in before this github action did. ; - `scalafmt` can be executed locally in a number of ways:; - IntelliJ Integration: Works as long as the `scala` plugin is installed. `Option + Command + L` formats the current file.; - `sbt scalafmtCheckAll`; - Install the `scalafmt` CLI tool directly via [brew](https://scalameta.org/scalafmt/docs/installation.html) and [coursier](https://get-coursier.io/docs/cli-installation).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7337:114,Continuous,ContinuousIntegration,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7337,6,"['Continuous', 'Install', 'Integrat', 'install']","['ContinuousIntegration', 'Install', 'Integration', 'installation', 'installed']"
Deployability,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3955:137,upgrade,upgrade,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955,1,['upgrade'],['upgrade']
Deployability,"- Refactor all CI TRAVIS_* variables back into create_build_variables(); - Detect hotfixes using git instead of TRAVIS variables; - Using ""force ci"" now runs all sub builds even on push; - All centaur tests should contribute to codecov; - Moved ci source files under src/ci; - Write ci log files under target/ci instead of $PWD; - Write ci generated files under target/ci, instead of sending secrets to src; - Jar file searches now return most recently modified jar; - Added allowPublicKeyRetrieval=true to MySQL url generation; - Removed cloudwell test as the combo of horicromtal + deadlock tests the same features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5202:82,hotfix,hotfixes,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5202,1,['hotfix'],['hotfixes']
Deployability,"- Removes the `fetchSize` configuration because it turns out we need to pin it at `Integer.MIN_VALUE`; - Pins the `fetchSize` at `Integer.MIN_VALUE`, for the same reason; - Adds more frequent metrics outputs during large workflow uploads, AND when there is nothing being uploaded",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6314:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314,1,['configurat'],['configuration']
Deployability,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4132:20,upgrade,upgrade,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132,4,"['integrat', 'upgrade']","['integration', 'upgrade']"
